{
    "author": "2015aroras",
    "message": "Add FlexOlmo model (#40921)\n\n* transformers add-new-model-like\n\n* Add FlexOlmo implementation\n\n* Update FlexOlmo docs\n\n* Set default tokenization for flex olmo\n\n* Update FlexOlmo tests\n\n* Update attention comment\n\n* Remove unneeded use of `sliding_window`",
    "sha": "449da6bb30911ad5bdc7d78d987c59d5db680aa0",
    "files": [
        {
            "sha": "d7fa25e185eb4c20c3cc6434d49e44f71bfc14c8",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -485,6 +485,8 @@\n         title: FLAN-UL2\n       - local: model_doc/flaubert\n         title: FlauBERT\n+      - local: model_doc/flex_olmo\n+        title: FlexOlmo\n       - local: model_doc/fnet\n         title: FNet\n       - local: model_doc/fsmt"
        },
        {
            "sha": "b771fe526d0683c9d10f7d48272ae5fec1cd0923",
            "filename": "docs/source/en/model_doc/flex_olmo.md",
            "status": "added",
            "additions": 139,
            "deletions": 0,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflex_olmo.md?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -0,0 +1,139 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-07-09 and added to Hugging Face Transformers on 2025-09-15.*\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# FlexOlmo\n+\n+[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. \n+\n+You can find all the original FlexOlmo checkpoints under the [FlexOlmo](https://huggingface.co/collections/allenai/flexolmo-68471177a386b6e20a54c55f) collection.\n+\n+> [!TIP]\n+> Click on the FlexOlmo models in the right sidebar for more examples of how to apply FlexOlmo to different language tasks.\n+\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"text-generation\",\n+    model=\"allenai/FlexOlmo-7x7B-1T\",\n+    dtype=torch.bfloat16,\n+    device=0,\n+)\n+    \n+result = pipe(\"Plants create energy through a process known as\")\n+print(result)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/FlexOlmo-7x7B-1T\"\n+)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"allenai/FlexOlmo-7x7B-1T\",\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n+\n+output = model.generate(**input_ids, max_length=50, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model allenai/FlexOlmo-7x7B-1T --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to 4-bits.\n+```py\n+\n+#pip install torchao\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+torchao_config = TorchAoConfig(\n+    \"int4_weight_only\",\n+    group_size=128\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/FlexOlmo-7x7B-1T\"\n+)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"allenai/FlexOlmo-7x7B-1T\",\n+    quantization_config=torchao_config,\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n+\n+output = model.generate(**input_ids, max_length=50, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+```\n+\n+\n+## FlexOlmoConfig\n+\n+[[autodoc]] FlexOlmoConfig\n+\n+## FlexOlmoForCausalLM\n+\n+[[autodoc]] FlexOlmoForCausalLM\n+\n+## FlexOlmoModel\n+\n+[[autodoc]] FlexOlmoModel\n+    - forward\n+\n+## FlexOlmoPreTrainedModel\n+\n+[[autodoc]] FlexOlmoPreTrainedModel\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "5c391e7162f498708387d0005f5770515e8b4da8",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -123,6 +123,7 @@\n     from .fastspeech2_conformer import *\n     from .flaubert import *\n     from .flava import *\n+    from .flex_olmo import *\n     from .florence2 import *\n     from .fnet import *\n     from .focalnet import *"
        },
        {
            "sha": "38f38cd31b401f075115a7f327c07134a64b163e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -148,6 +148,7 @@\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGanConfig\"),\n         (\"flaubert\", \"FlaubertConfig\"),\n         (\"flava\", \"FlavaConfig\"),\n+        (\"flex_olmo\", \"FlexOlmoConfig\"),\n         (\"florence2\", \"Florence2Config\"),\n         (\"fnet\", \"FNetConfig\"),\n         (\"focalnet\", \"FocalNetConfig\"),\n@@ -580,6 +581,7 @@\n         (\"flan-ul2\", \"FLAN-UL2\"),\n         (\"flaubert\", \"FlauBERT\"),\n         (\"flava\", \"FLAVA\"),\n+        (\"flex_olmo\", \"FlexOlmo\"),\n         (\"florence2\", \"Florence2\"),\n         (\"fnet\", \"FNet\"),\n         (\"focalnet\", \"FocalNet\"),"
        },
        {
            "sha": "93420820fb9e43441aedde2888775e47ecafa8e2",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -150,6 +150,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGan\"),\n         (\"flaubert\", \"FlaubertModel\"),\n         (\"flava\", \"FlavaModel\"),\n+        (\"flex_olmo\", \"FlexOlmoModel\"),\n         (\"florence2\", \"Florence2Model\"),\n         (\"fnet\", \"FNetModel\"),\n         (\"focalnet\", \"FocalNetModel\"),\n@@ -653,6 +654,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"falcon\", \"FalconForCausalLM\"),\n         (\"falcon_h1\", \"FalconH1ForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n+        (\"flex_olmo\", \"FlexOlmoForCausalLM\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"gemma\", \"GemmaForCausalLM\"),\n         (\"gemma2\", \"Gemma2ForCausalLM\"),"
        },
        {
            "sha": "7858ae5879468ac6c284719fe0e4f4e93ffb0952",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -245,6 +245,7 @@\n             (\"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None, None),\n         ),\n         (\"flaubert\", (\"FlaubertTokenizer\", None)),\n+        (\"flex_olmo\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"fnet\", (\"FNetTokenizer\", \"FNetTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"fsmt\", (\"FSMTTokenizer\", None)),\n         (\"funnel\", (\"FunnelTokenizer\", \"FunnelTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "f4213ca9b0b4c950fc3fe0443a710844a4dac518",
            "filename": "src/transformers/models/flex_olmo/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2F__init__.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_flex_olmo import *\n+    from .modeling_flex_olmo import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ae4704770e368688f3f08d5f6f5b29eb9d8a50a3",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -0,0 +1,199 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/flex_olmo/modular_flex_olmo.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_flex_olmo.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class FlexOlmoConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`FlexOlmoModel`]. It is used to instantiate an FlexOlmo\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the [allenai/FlexOlmo-7x7B-1T](https://huggingface.co/allenai/FlexOlmo-7x7B-1T).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 100352):\n+            Vocabulary size of the FlexOlmo model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`FlexOlmoModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 4096):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 100277):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 100257):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 500000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n+            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n+            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n+            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n+            these scaling strategies behave:\n+            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n+            experimental feature, subject to breaking API changes in future versions.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_experts_per_tok (`int`, *optional*, defaults to 5):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 7):\n+            Number of routed experts.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.01):\n+            The aux loss factor for the total loss.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the topk probabilities.\n+\n+    ```python\n+    >>> from transformers import FlexOlmoModel, FlexOlmoConfig\n+\n+    >>> # Initializing a FlexOlmo style configuration\n+    >>> configuration = FlexOlmoConfig()\n+\n+    >>> # Initializing a model from the FlexOlmo style configuration\n+    >>> model = FlexOlmoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"flex_olmo\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=100352,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=4096,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-06,\n+        use_cache=True,\n+        pad_token_id=100277,\n+        bos_token_id=None,\n+        eos_token_id=100257,\n+        tie_word_embeddings=False,\n+        rope_theta=500000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        num_experts_per_tok=5,\n+        num_experts=7,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.01,\n+        norm_topk_prob=False,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+        self.norm_topk_prob = norm_topk_prob\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+\n+__all__ = [\"FlexOlmoConfig\"]"
        },
        {
            "sha": "26f93c9c64a2b02eed962122515d6aa3d94dce26",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "added",
            "additions": 687,
            "deletions": 0,
            "changes": 687,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -0,0 +1,687 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/flex_olmo/modular_flex_olmo.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_flex_olmo.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from .configuration_flex_olmo import FlexOlmoConfig\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class FlexOlmoRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        FlexOlmoRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return (self.weight * hidden_states).to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class FlexOlmoRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: FlexOlmoConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+            return cos, sin\n+\n+\n+class FlexOlmoMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    q_type, k_type = q.dtype, k.dtype\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(q_type), k_embed.to(k_type)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+class FlexOlmoAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: FlexOlmoConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = FlexOlmoRMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = FlexOlmoRMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states))\n+        key_states = self.k_norm(self.k_proj(hidden_states))\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class FlexOlmoSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.gate = nn.Linear(config.hidden_size, self.num_experts, bias=False)\n+        self.experts = nn.ModuleList([FlexOlmoMLP(config) for _ in range(self.num_experts)])\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits = self.gate(hidden_states)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        if self.norm_topk_prob:\n+            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        # we cast back to the input dtype\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be selected\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        for expert_idx in range(self.num_experts):\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx])\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states, router_logits\n+\n+\n+class FlexOlmoDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: FlexOlmoConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = FlexOlmoAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = FlexOlmoSparseMoeBlock(config)\n+        self.post_attention_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_router_logits (`bool`, *optional*):\n+                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n+                and should not be returned during inference.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_values (`Cache`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states, _ = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class FlexOlmoPreTrainedModel(PreTrainedModel):\n+    config: FlexOlmoConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"FlexOlmoDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(FlexOlmoSparseMoeBlock, index=1),\n+        \"hidden_states\": FlexOlmoDecoderLayer,\n+        \"attentions\": FlexOlmoAttention,\n+    }\n+\n+\n+@auto_docstring\n+class FlexOlmoModel(FlexOlmoPreTrainedModel):\n+    def __init__(self, config: FlexOlmoConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [FlexOlmoDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = FlexOlmoRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n+            .reshape(-1, routing_weights.shape[1])\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n+    overall_loss = torch.sum(\n+        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n+    )\n+    return overall_loss * num_experts\n+\n+\n+class FlexOlmoForCausalLM(FlexOlmoPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = FlexOlmoModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.num_experts\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[tuple, MoeCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, FlexOlmoForCausalLM\n+\n+        >>> model = FlexOlmoForCausalLM.from_pretrained(\"allenai/FlexOlmo-1B-7B-0924\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/FlexOlmo-1B-7B-0924\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        'Hey, are you conscious? Can you talk to me?\\nIâ€™m not sure if youâ€™re conscious of this, but Iâ€™m'\n+        ```\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            output_router_logits=output_router_logits,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            if output_router_logits:\n+                output = (aux_loss,) + output\n+            return (loss,) + output if loss is not None else output\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+__all__ = [\"FlexOlmoForCausalLM\", \"FlexOlmoModel\", \"FlexOlmoPreTrainedModel\"]"
        },
        {
            "sha": "0d0127e4d4b2c524414c5afa421cec99c3902a1d",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "added",
            "additions": 353,
            "deletions": 0,
            "changes": 353,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -0,0 +1,353 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+import torch\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import check_model_inputs\n+from ..mixtral.modeling_mixtral import MixtralModel, MixtralPreTrainedModel\n+from ..olmo2.modeling_olmo2 import Olmo2Attention, Olmo2RMSNorm, Olmo2RotaryEmbedding\n+from ..olmoe.configuration_olmoe import OlmoeConfig\n+from ..olmoe.modeling_olmoe import (\n+    OlmoeDecoderLayer,\n+    OlmoeForCausalLM,\n+    OlmoeMLP,\n+    OlmoeSparseMoeBlock,\n+)\n+\n+\n+class FlexOlmoConfig(OlmoeConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`FlexOlmoModel`]. It is used to instantiate an FlexOlmo\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the [allenai/FlexOlmo-7x7B-1T](https://huggingface.co/allenai/FlexOlmo-7x7B-1T).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 100352):\n+            Vocabulary size of the FlexOlmo model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`FlexOlmoModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 4096):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 100277):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 100257):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 500000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n+            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n+            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n+            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n+            these scaling strategies behave:\n+            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n+            experimental feature, subject to breaking API changes in future versions.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_experts_per_tok (`int`, *optional*, defaults to 5):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 7):\n+            Number of routed experts.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.01):\n+            The aux loss factor for the total loss.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the topk probabilities.\n+\n+    ```python\n+    >>> from transformers import FlexOlmoModel, FlexOlmoConfig\n+\n+    >>> # Initializing a FlexOlmo style configuration\n+    >>> configuration = FlexOlmoConfig()\n+\n+    >>> # Initializing a model from the FlexOlmo style configuration\n+    >>> model = FlexOlmoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"flex_olmo\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=100352,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=4096,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-06,\n+        use_cache=True,\n+        pad_token_id=100277,\n+        bos_token_id=None,\n+        eos_token_id=100257,\n+        tie_word_embeddings=False,\n+        rope_theta=500000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        num_experts_per_tok=5,\n+        num_experts=7,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.01,\n+        norm_topk_prob=False,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            max_position_embeddings=max_position_embeddings,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            hidden_act=hidden_act,\n+            initializer_range=initializer_range,\n+            rms_norm_eps=rms_norm_eps,\n+            use_cache=use_cache,\n+            rope_theta=rope_theta,\n+            rope_scaling=rope_scaling,\n+            attention_bias=attention_bias,\n+            attention_dropout=attention_dropout,\n+            num_experts_per_tok=num_experts_per_tok,\n+            num_experts=num_experts,\n+            output_router_logits=output_router_logits,\n+            router_aux_loss_coef=router_aux_loss_coef,\n+            norm_topk_prob=norm_topk_prob,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+        del self.clip_qkv\n+\n+\n+# FlexOlmo RMS norm reuses Olmo2 RMS norm, which handles low precision slightly differently than the original Olmoe.\n+class FlexOlmoRMSNorm(Olmo2RMSNorm):\n+    pass\n+\n+\n+# FlexOlmo RMS norm reuses Olmo2 RMS norm, so that the output cos and sin are returned\n+# as float32 rather than the input type.\n+class FlexOlmoRotaryEmbedding(Olmo2RotaryEmbedding):\n+    pass\n+\n+\n+class FlexOlmoMLP(OlmoeMLP):\n+    pass\n+\n+\n+# FlexOlmo uses Olmo2 attention instead of OlmoE Attention since its `apply_rotary_pos_emb`\n+# implementation handles lower precision more faithfully to the Olmo codebase.\n+class FlexOlmoAttention(Olmo2Attention):\n+    pass\n+\n+\n+class FlexOlmoSparseMoeBlock(OlmoeSparseMoeBlock):\n+    pass\n+\n+\n+# FlexOlmo decoder layer is identical to OlmoE decoder layer except:\n+# - Norm is applied after attention/feedforward rather than before.\n+class FlexOlmoDecoderLayer(OlmoeDecoderLayer):\n+    def __init__(self, config: FlexOlmoConfig, layer_idx: int):\n+        super().__init__(config, layer_idx=layer_idx)\n+        self.post_attention_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.self_attn = FlexOlmoAttention(config=config, layer_idx=layer_idx)\n+        del self.input_layernorm\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states, _ = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+# FlexOlmo uses Mixtral model as its base instead of OlmoE model since Mixtral is more up-to-date with the rest\n+# of the transformers library. For example, it uses the newer mechanisms of recording submodule outputs.\n+class FlexOlmoPreTrainedModel(MixtralPreTrainedModel):\n+    pass\n+\n+\n+# FlexOlmo uses Mixtral model as its base instead of OlmoE model since Mixtral is more up-to-date with the rest\n+# of the transformers library. For example, it uses the newer mechanisms of recording submodule outputs.\n+# FlexOlmo model is identical to Mixtral model except:\n+# - FlexOlmo does not use sliding window attention.\n+class FlexOlmoModel(MixtralModel):\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class FlexOlmoForCausalLM(OlmoeForCausalLM):\n+    pass\n+\n+\n+__all__ = [\n+    \"FlexOlmoConfig\",\n+    \"FlexOlmoForCausalLM\",\n+    \"FlexOlmoModel\",\n+    \"FlexOlmoPreTrainedModel\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/flex_olmo/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/tests%2Fmodels%2Fflex_olmo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/tests%2Fmodels%2Fflex_olmo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflex_olmo%2F__init__.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0"
        },
        {
            "sha": "b73807502873f6ead44a6d5946712f5dfeb008e5",
            "filename": "tests/models/flex_olmo/test_modeling_flex_olmo.py",
            "status": "added",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/449da6bb30911ad5bdc7d78d987c59d5db680aa0/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/449da6bb30911ad5bdc7d78d987c59d5db680aa0/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py?ref=449da6bb30911ad5bdc7d78d987c59d5db680aa0",
            "patch": "@@ -0,0 +1,126 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch FlexOlmo model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+\n+from transformers import FlexOlmoConfig, is_torch_available\n+from transformers.models.auto.tokenization_auto import AutoTokenizer\n+from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        FlexOlmoForCausalLM,\n+        FlexOlmoModel,\n+    )\n+    from transformers.models.flex_olmo.modeling_flex_olmo import FlexOlmoRotaryEmbedding\n+\n+\n+class FlexOlmoModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = FlexOlmoConfig\n+        base_model_class = FlexOlmoModel\n+        causal_lm_class = FlexOlmoForCausalLM\n+\n+\n+@require_torch\n+class FlexOlmoModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (FlexOlmoModel, FlexOlmoForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": FlexOlmoModel,\n+            \"text-generation\": FlexOlmoForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    test_torchscript = False\n+    test_all_params_have_gradient = False\n+    model_tester_class = FlexOlmoModelTester\n+    rotary_embedding_layer = FlexOlmoRotaryEmbedding\n+\n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = FlexOlmoForCausalLM if is_torch_available() else None\n+\n+    @unittest.skip(\"Dynamic control flow in MoE\")\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_for_training(self):\n+        pass\n+\n+\n+@require_torch\n+class FlexOlmoIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_7b_logits(self):\n+        input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n+        model = FlexOlmoForCausalLM.from_pretrained(\"shanearora/Flex-reddit-2x7B-1T\").to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n+        out = model(torch.tensor(input_ids, device=torch_device)).logits.float()\n+        # Expected mean on dim = -1\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [[-5.4202, -5.3883, -2.3924, -2.1226, -6.0122, -5.4173, -5.4571, -5.8256]],\n+            }\n+        )\n+        EXPECTED_MEAN = torch.tensor(expectations.get_expectation(), device=torch_device)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # slicing logits[0, 0, 0:30]\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [ 0.5547, -3.6250, -7.2812, -5.0312, -5.9062, -5.3438, -4.2500, -4.6875, -3.4219, -4.6250, -6.5938, -3.1250, -6.0625, -2.0781, -6.4688, -0.4941,  1.2656,  0.7578, -0.1934, -0.4160, -0.6992, -0.9531, -0.9648, -1.3125, -1.2578, -4.5625, -2.4219, -5.6250,  0.7695, -4.5938],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor(expectations.get_expectation(), device=torch_device)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+\n+    @slow\n+    def test_model_7b_greedy_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"\"\"Simply put, the theory of relativity states that 1) the laws of physics are the same in all inertial frames of reference, and 2) the speed of light is constant in all inertial frames of reference. The first statement is called the principle of relativity, and the second is called the constancy of the speed of light. The first statement is\"\"\"\n+        prompt = \"Simply put, the theory of relativity states that \"\n+        tokenizer = AutoTokenizer.from_pretrained(\"allenai/dolma2-tokenizer\", device_map=\"auto\")\n+        model = FlexOlmoForCausalLM.from_pretrained(\"shanearora/Flex-reddit-2x7B-1T\", device_map=\"auto\")\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)"
        }
    ],
    "stats": {
        "total": 1541,
        "additions": 1541,
        "deletions": 0
    }
}