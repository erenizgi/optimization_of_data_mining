{
    "author": "LysandreJik",
    "message": "Applies the rest of the init refactor except to modular files (#35238)\n\n* [test_all] Applies the rest of the init refactor except to modular files\r\n\r\n* Revert modular that doesn't work\r\n\r\n* [test_all] TFGPT2Tokenizer",
    "sha": "b2f2977533445c4f62bf58e10b1360e6856e78ce",
    "files": [
        {
            "sha": "618fceef70d3891f4313958cdadede27d605f12c",
            "filename": "src/transformers/models/audio_spectrogram_transformer/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_audio_spectrogram_transformer import *\n-    from .convert_audio_spectrogram_transformer_original_to_pytorch import *\n     from .feature_extraction_audio_spectrogram_transformer import *\n     from .modeling_audio_spectrogram_transformer import *\n else:"
        },
        {
            "sha": "c5296fc47fc423c300cf6c43bccb5daafd6d134a",
            "filename": "src/transformers/models/bark/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbark%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbark%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,8 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bark import *\n-    from .convert_suno_to_hf import *\n-    from .generation_configuration_bark import *\n     from .modeling_bark import *\n     from .processing_bark import *\n else:"
        },
        {
            "sha": "8f4c713f4698d7c901ed06738efc8983e317805c",
            "filename": "src/transformers/models/bart/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bart import *\n-    from .convert_bart_original_pytorch_checkpoint_to_pytorch import *\n     from .modeling_bart import *\n     from .modeling_flax_bart import *\n     from .modeling_tf_bart import *"
        },
        {
            "sha": "44f838a9a5d97f5e604dc47c63db4cbef7479849",
            "filename": "src/transformers/models/beit/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_beit import *\n-    from .convert_beit_unilm_to_pytorch import *\n     from .feature_extraction_beit import *\n     from .image_processing_beit import *\n     from .modeling_beit import *"
        },
        {
            "sha": "2ef22794dde26e6275ba0ae850f6042ff6a451fd",
            "filename": "src/transformers/models/bert/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,10 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bert import *\n-    from .convert_bert_original_tf2_checkpoint_to_pytorch import *\n-    from .convert_bert_original_tf_checkpoint_to_pytorch import *\n-    from .convert_bert_pytorch_checkpoint_to_original_tf import *\n-    from .convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch import *\n     from .modeling_bert import *\n     from .modeling_flax_bert import *\n     from .modeling_tf_bert import *"
        },
        {
            "sha": "87419e69e5c7f0572076832237e10f236186ec33",
            "filename": "src/transformers/models/big_bird/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_big_bird import *\n-    from .convert_bigbird_original_tf_checkpoint_to_pytorch import *\n     from .modeling_big_bird import *\n     from .modeling_flax_big_bird import *\n     from .tokenization_big_bird import *"
        },
        {
            "sha": "d203b2a578f9a4c6125cffb6041a627167518680",
            "filename": "src/transformers/models/bigbird_pegasus/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bigbird_pegasus import *\n-    from .convert_bigbird_pegasus_tf_to_pytorch import *\n     from .modeling_bigbird_pegasus import *\n else:\n     import sys"
        },
        {
            "sha": "641cdb592117b466cf49be31be701e494ee7f9fc",
            "filename": "src/transformers/models/biogpt/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbiogpt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbiogpt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_biogpt import *\n-    from .convert_biogpt_original_pytorch_checkpoint_to_pytorch import *\n     from .modeling_biogpt import *\n     from .tokenization_biogpt import *\n else:"
        },
        {
            "sha": "3b6ba91b032f8cd1b5c6178a93c1b7e39e99d42b",
            "filename": "src/transformers/models/bit/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bit import *\n-    from .convert_bit_to_pytorch import *\n     from .image_processing_bit import *\n     from .modeling_bit import *\n else:"
        },
        {
            "sha": "76ece6853b381b0853f52022078f9c580deb0f04",
            "filename": "src/transformers/models/blenderbot/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_blenderbot import *\n-    from .convert_blenderbot_original_pytorch_checkpoint_to_pytorch import *\n     from .modeling_blenderbot import *\n     from .modeling_flax_blenderbot import *\n     from .modeling_tf_blenderbot import *"
        },
        {
            "sha": "5443a3f6747aaa23663c46410cea5d4611d628e7",
            "filename": "src/transformers/models/blip/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,12 +19,9 @@\n \n if TYPE_CHECKING:\n     from .configuration_blip import *\n-    from .convert_blip_original_pytorch_to_hf import *\n     from .image_processing_blip import *\n     from .modeling_blip import *\n-    from .modeling_blip_text import *\n     from .modeling_tf_blip import *\n-    from .modeling_tf_blip_text import *\n     from .processing_blip import *\n else:\n     import sys"
        },
        {
            "sha": "0717e81fca606edde774ad19092472bf59b4701a",
            "filename": "src/transformers/models/blip_2/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fblip_2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fblip_2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_blip_2 import *\n-    from .convert_blip_2_original_to_pytorch import *\n     from .modeling_blip_2 import *\n     from .processing_blip_2 import *\n else:"
        },
        {
            "sha": "72d1d6e6ca4724235ce46978e5c11e84583d4033",
            "filename": "src/transformers/models/bloom/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bloom import *\n-    from .convert_bloom_original_checkpoint_to_pytorch import *\n     from .modeling_bloom import *\n     from .modeling_flax_bloom import *\n     from .tokenization_bloom_fast import *"
        },
        {
            "sha": "2178cfd03a40593bc9acb97111204aab0423860c",
            "filename": "src/transformers/models/bros/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbros%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbros%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_bros import *\n-    from .convert_bros_to_pytorch import *\n     from .modeling_bros import *\n     from .processing_bros import *\n else:"
        },
        {
            "sha": "cb726942b0f16105f8a5a5f7c661485951d7ccc7",
            "filename": "src/transformers/models/byt5/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbyt5%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fbyt5%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbyt5%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -18,7 +18,6 @@\n \n \n if TYPE_CHECKING:\n-    from .convert_byt5_original_tf_checkpoint_to_pytorch import *\n     from .tokenization_byt5 import *\n else:\n     import sys"
        },
        {
            "sha": "bb00d8fd8827035ff7b351db49d7128a54429c53",
            "filename": "src/transformers/models/canine/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcanine%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcanine%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_canine import *\n-    from .convert_canine_original_tf_checkpoint_to_pytorch import *\n     from .modeling_canine import *\n     from .tokenization_canine import *\n else:"
        },
        {
            "sha": "4332161036d516359b6a52f7df4a1a63c2c069ba",
            "filename": "src/transformers/models/chameleon/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fchameleon%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fchameleon%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_chameleon import *\n-    from .convert_chameleon_weights_to_hf import *\n     from .image_processing_chameleon import *\n     from .modeling_chameleon import *\n     from .processing_chameleon import *"
        },
        {
            "sha": "fc1f002a16ad95767704ee522c43a498d06ce0c9",
            "filename": "src/transformers/models/chinese_clip/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fchinese_clip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fchinese_clip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_chinese_clip import *\n-    from .convert_chinese_clip_original_pytorch_to_hf import *\n     from .feature_extraction_chinese_clip import *\n     from .image_processing_chinese_clip import *\n     from .modeling_chinese_clip import *"
        },
        {
            "sha": "6d54ee86aecef2cbe5b9bfdee321a0375d977880",
            "filename": "src/transformers/models/clap/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclap%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclap%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_clap import *\n-    from .convert_clap_original_pytorch_to_hf import *\n     from .feature_extraction_clap import *\n     from .modeling_clap import *\n     from .processing_clap import *"
        },
        {
            "sha": "f2c43e0b51d63b76d28aa6cb0171c827441ab501",
            "filename": "src/transformers/models/clip/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_clip import *\n-    from .convert_clip_original_pytorch_to_hf import *\n     from .feature_extraction_clip import *\n     from .image_processing_clip import *\n     from .modeling_clip import *"
        },
        {
            "sha": "55b38987fd0a3e443b8ae94ffbdd6f73c79ba619",
            "filename": "src/transformers/models/clipseg/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclipseg%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclipseg%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_clipseg import *\n-    from .convert_clipseg_original_pytorch_to_hf import *\n     from .modeling_clipseg import *\n     from .processing_clipseg import *\n else:"
        },
        {
            "sha": "986e185ff7771e5909db2b0e2ab91ba95cfa3a27",
            "filename": "src/transformers/models/clvp/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 59,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,67 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_clvp\": [\n-        \"ClvpConfig\",\n-        \"ClvpDecoderConfig\",\n-        \"ClvpEncoderConfig\",\n-    ],\n-    \"feature_extraction_clvp\": [\"ClvpFeatureExtractor\"],\n-    \"processing_clvp\": [\"ClvpProcessor\"],\n-    \"tokenization_clvp\": [\"ClvpTokenizer\"],\n-}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_clvp\"] = [\n-        \"ClvpModelForConditionalGeneration\",\n-        \"ClvpForCausalLM\",\n-        \"ClvpModel\",\n-        \"ClvpPreTrainedModel\",\n-        \"ClvpEncoder\",\n-        \"ClvpDecoder\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_clvp import (\n-        ClvpConfig,\n-        ClvpDecoderConfig,\n-        ClvpEncoderConfig,\n-    )\n-    from .feature_extraction_clvp import ClvpFeatureExtractor\n-    from .processing_clvp import ClvpProcessor\n-    from .tokenization_clvp import ClvpTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_clvp import (\n-            ClvpDecoder,\n-            ClvpEncoder,\n-            ClvpForCausalLM,\n-            ClvpModel,\n-            ClvpModelForConditionalGeneration,\n-            ClvpPreTrainedModel,\n-        )\n-\n+    from .configuration_clvp import *\n+    from .feature_extraction_clvp import *\n+    from .modeling_clvp import *\n+    from .processing_clvp import *\n+    from .tokenization_clvp import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "cffc962eb322aa45ef279bb66b9902c4760a1496",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -438,3 +438,6 @@ def from_sub_model_configs(\n             decoder_config=decoder_config.to_dict(),\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"ClvpConfig\", \"ClvpDecoderConfig\", \"ClvpEncoderConfig\"]"
        },
        {
            "sha": "2dbda430bb25e14ea14620d8415eab7b15dc5e9f",
            "filename": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -236,3 +236,6 @@ def __call__(\n             padded_inputs[\"input_features\"] = input_features\n \n         return padded_inputs.convert_to_tensors(return_tensors)\n+\n+\n+__all__ = [\"ClvpFeatureExtractor\"]"
        },
        {
            "sha": "844ca354cd1079a825cfef385a877b93215968d8",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -2021,3 +2021,13 @@ def generate(\n             text_encoder_hidden_states=text_outputs.hidden_states,\n             speech_encoder_hidden_states=speech_outputs.hidden_states,\n         )\n+\n+\n+__all__ = [\n+    \"ClvpModelForConditionalGeneration\",\n+    \"ClvpForCausalLM\",\n+    \"ClvpModel\",\n+    \"ClvpPreTrainedModel\",\n+    \"ClvpEncoder\",\n+    \"ClvpDecoder\",\n+]"
        },
        {
            "sha": "3f4d54f259032f88880b904f3d48ec6f058914c3",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -88,3 +88,6 @@ def decode(self, *args, **kwargs):\n         the docstring of this method for more information.\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n+\n+\n+__all__ = [\"ClvpProcessor\"]"
        },
        {
            "sha": "85ae1d6991ebc8dc8a98bb63d72e292ab1a19588",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -362,3 +362,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 index += 1\n \n         return vocab_file, merge_file\n+\n+\n+__all__ = [\"ClvpTokenizer\"]"
        },
        {
            "sha": "b65c4bddb4b0cd3fa8dfd6a781a3c0f58e30e5a7",
            "filename": "src/transformers/models/code_llama/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcode_llama%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcode_llama%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 MetaAI and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,45 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available, is_tokenizers_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_code_llama\"] = [\"CodeLlamaTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_code_llama_fast\"] = [\"CodeLlamaTokenizerFast\"]\n-\n if TYPE_CHECKING:\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_code_llama import CodeLlamaTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_code_llama_fast import CodeLlamaTokenizerFast\n-\n+    from .tokenization_code_llama import *\n+    from .tokenization_code_llama_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "43386ecdaee4dce9e983470242648fd7ec03a90d",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -447,3 +447,6 @@ def __setstate__(self, d):\n         self.__dict__ = d\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n+\n+\n+__all__ = [\"CodeLlamaTokenizer\"]"
        },
        {
            "sha": "3bc831cdd6a15b0af85982173e903b7462e9a2a2",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -376,3 +376,6 @@ def build_inputs_with_special_tokens(\n         if token_ids_1 is None:\n             return self.bos_token_id + token_ids_0 + self.eos_token_id\n         return self.bos_token_id + token_ids_0 + token_ids_1 + self.eos_token_id\n+\n+\n+__all__ = [\"CodeLlamaTokenizerFast\"]"
        },
        {
            "sha": "ea2d9af11150f556b2cedfb78271f174256e64b0",
            "filename": "src/transformers/models/codegen/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 51,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 Salesforce authors, The EleutherAI, and HuggingFace Teams. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,59 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_codegen\": [\"CodeGenConfig\", \"CodeGenOnnxConfig\"],\n-    \"tokenization_codegen\": [\"CodeGenTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_codegen_fast\"] = [\"CodeGenTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_codegen\"] = [\n-        \"CodeGenForCausalLM\",\n-        \"CodeGenModel\",\n-        \"CodeGenPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_codegen import CodeGenConfig, CodeGenOnnxConfig\n-    from .tokenization_codegen import CodeGenTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_codegen_fast import CodeGenTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_codegen import (\n-            CodeGenForCausalLM,\n-            CodeGenModel,\n-            CodeGenPreTrainedModel,\n-        )\n-\n+    from .configuration_codegen import *\n+    from .modeling_codegen import *\n+    from .tokenization_codegen import *\n+    from .tokenization_codegen_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6de483cb79449a0a41b4e5f51d871b4594276b43",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -225,3 +225,6 @@ def generate_dummy_inputs(\n     @property\n     def default_onnx_opset(self) -> int:\n         return 13\n+\n+\n+__all__ = [\"CodeGenConfig\", \"CodeGenOnnxConfig\"]"
        },
        {
            "sha": "44cc2a3357c6025d69a4160871a229681b9a4711",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -809,3 +809,6 @@ def _reorder_cache(\n             tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n             for layer_past in past_key_values\n         )\n+\n+\n+__all__ = [\"CodeGenForCausalLM\", \"CodeGenModel\", \"CodeGenPreTrainedModel\"]"
        },
        {
            "sha": "2b584e83b1b9ef3d4d4b9f310f69aa914897a5dc",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -414,3 +414,6 @@ def find_re(string, pattern, start_pos):\n             return completion[: min(terminals_pos)]\n         else:\n             return completion\n+\n+\n+__all__ = [\"CodeGenTokenizer\"]"
        },
        {
            "sha": "fcfe1d2795b44a14a4a7e01b0a2fadaddcca1a2a",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -270,3 +270,6 @@ def find_re(string, pattern, start_pos):\n             return completion[: min(terminals_pos)]\n         else:\n             return completion\n+\n+\n+__all__ = [\"CodeGenTokenizerFast\"]"
        },
        {
            "sha": "ad2d57500c44322b6c749a313f07c07e11f8f20f",
            "filename": "src/transformers/models/cohere/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 57,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 Cohere and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,65 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_sentencepiece_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_cohere\": [\"CohereConfig\"],\n-}\n-\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_cohere_fast\"] = [\"CohereTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_cohere\"] = [\n-        \"CohereForCausalLM\",\n-        \"CohereModel\",\n-        \"CoherePreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_cohere import CohereConfig\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_cohere_fast import CohereTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_cohere import (\n-            CohereForCausalLM,\n-            CohereModel,\n-            CoherePreTrainedModel,\n-        )\n-\n+    from .configuration_cohere import *\n+    from .modeling_cohere import *\n+    from .tokenization_cohere_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "9312e2764d33deb6cea964058732523afeb9dcf5",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -198,3 +198,6 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"CohereConfig\"]"
        },
        {
            "sha": "a035e77cf8f6763215ad0686e5642dae78963284",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1145,3 +1145,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"CohereForCausalLM\", \"CohereModel\", \"CoherePreTrainedModel\"]"
        },
        {
            "sha": "e99df5c609c89509f2376790ef292bd93aeb7df4",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -510,3 +510,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n             output = output + bos_token_id + token_ids_1 + eos_token_id\n \n         return output\n+\n+\n+__all__ = [\"CohereTokenizerFast\"]"
        },
        {
            "sha": "46bd017d2b6b198860c873bd82efc3be64c1e28a",
            "filename": "src/transformers/models/conditional_detr/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 61,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,71 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\n-    \"configuration_conditional_detr\": [\n-        \"ConditionalDetrConfig\",\n-        \"ConditionalDetrOnnxConfig\",\n-    ]\n-}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_conditional_detr\"] = [\"ConditionalDetrFeatureExtractor\"]\n-    _import_structure[\"image_processing_conditional_detr\"] = [\"ConditionalDetrImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_conditional_detr\"] = [\n-        \"ConditionalDetrForObjectDetection\",\n-        \"ConditionalDetrForSegmentation\",\n-        \"ConditionalDetrModel\",\n-        \"ConditionalDetrPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_conditional_detr import (\n-        ConditionalDetrConfig,\n-        ConditionalDetrOnnxConfig,\n-    )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_conditional_detr import ConditionalDetrFeatureExtractor\n-        from .image_processing_conditional_detr import ConditionalDetrImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_conditional_detr import (\n-            ConditionalDetrForObjectDetection,\n-            ConditionalDetrForSegmentation,\n-            ConditionalDetrModel,\n-            ConditionalDetrPreTrainedModel,\n-        )\n-\n+    from .configuration_conditional_detr import *\n+    from .feature_extraction_conditional_detr import *\n+    from .image_processing_conditional_detr import *\n+    from .modeling_conditional_detr import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8dae72edff0896d99af636e5a4c8e6bd330328d5",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -273,3 +273,6 @@ def atol_for_validation(self) -> float:\n     @property\n     def default_onnx_opset(self) -> int:\n         return 12\n+\n+\n+__all__ = [\"ConditionalDetrConfig\", \"ConditionalDetrOnnxConfig\"]"
        },
        {
            "sha": "8fe92eec42f6a10ccd604976c9a055348dee43d2",
            "filename": "src/transformers/models/conditional_detr/feature_extraction_conditional_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Ffeature_extraction_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Ffeature_extraction_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Ffeature_extraction_conditional_detr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -41,3 +41,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"ConditionalDetrFeatureExtractor\"]"
        },
        {
            "sha": "effb8c3b058168f1fbfa50d52b5a7e6f852e7e0d",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1851,3 +1851,6 @@ def post_process_panoptic_segmentation(\n \n             results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n         return results\n+\n+\n+__all__ = [\"ConditionalDetrImageProcessor\"]"
        },
        {
            "sha": "0aa4b2afa6bf96c5594c267a08597803a8173f20",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -2105,3 +2105,11 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n         weights = self.dropout(weights)\n         return weights\n+\n+\n+__all__ = [\n+    \"ConditionalDetrForObjectDetection\",\n+    \"ConditionalDetrForSegmentation\",\n+    \"ConditionalDetrModel\",\n+    \"ConditionalDetrPreTrainedModel\",\n+]"
        },
        {
            "sha": "670a7d6f47647ff58d72cc461618bbb2c14f9565",
            "filename": "src/transformers/models/convbert/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 106,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,114 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_convbert\": [\"ConvBertConfig\", \"ConvBertOnnxConfig\"],\n-    \"tokenization_convbert\": [\"ConvBertTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_convbert_fast\"] = [\"ConvBertTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_convbert\"] = [\n-        \"ConvBertForMaskedLM\",\n-        \"ConvBertForMultipleChoice\",\n-        \"ConvBertForQuestionAnswering\",\n-        \"ConvBertForSequenceClassification\",\n-        \"ConvBertForTokenClassification\",\n-        \"ConvBertLayer\",\n-        \"ConvBertModel\",\n-        \"ConvBertPreTrainedModel\",\n-        \"load_tf_weights_in_convbert\",\n-    ]\n-\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_convbert\"] = [\n-        \"TFConvBertForMaskedLM\",\n-        \"TFConvBertForMultipleChoice\",\n-        \"TFConvBertForQuestionAnswering\",\n-        \"TFConvBertForSequenceClassification\",\n-        \"TFConvBertForTokenClassification\",\n-        \"TFConvBertLayer\",\n-        \"TFConvBertModel\",\n-        \"TFConvBertPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_convbert import ConvBertConfig, ConvBertOnnxConfig\n-    from .tokenization_convbert import ConvBertTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_convbert_fast import ConvBertTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_convbert import (\n-            ConvBertForMaskedLM,\n-            ConvBertForMultipleChoice,\n-            ConvBertForQuestionAnswering,\n-            ConvBertForSequenceClassification,\n-            ConvBertForTokenClassification,\n-            ConvBertLayer,\n-            ConvBertModel,\n-            ConvBertPreTrainedModel,\n-            load_tf_weights_in_convbert,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_convbert import (\n-            TFConvBertForMaskedLM,\n-            TFConvBertForMultipleChoice,\n-            TFConvBertForQuestionAnswering,\n-            TFConvBertForSequenceClassification,\n-            TFConvBertForTokenClassification,\n-            TFConvBertLayer,\n-            TFConvBertModel,\n-            TFConvBertPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_convbert import *\n+    from .modeling_convbert import *\n+    from .modeling_tf_convbert import *\n+    from .tokenization_convbert import *\n+    from .tokenization_convbert_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "558ef5638cd4fed2946416c3c0a2b1639226e431",
            "filename": "src/transformers/models/convbert/configuration_convbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -155,3 +155,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"token_type_ids\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"ConvBertConfig\", \"ConvBertOnnxConfig\"]"
        },
        {
            "sha": "19eacfe2ac6a9e4eac45ee858c577e1158579fcc",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1331,3 +1331,16 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"ConvBertForMaskedLM\",\n+    \"ConvBertForMultipleChoice\",\n+    \"ConvBertForQuestionAnswering\",\n+    \"ConvBertForSequenceClassification\",\n+    \"ConvBertForTokenClassification\",\n+    \"ConvBertLayer\",\n+    \"ConvBertModel\",\n+    \"ConvBertPreTrainedModel\",\n+    \"load_tf_weights_in_convbert\",\n+]"
        },
        {
            "sha": "9b2696a7e2b0090007bc98ddda3995413d11fa5f",
            "filename": "src/transformers/models/convbert/modeling_tf_convbert.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1462,3 +1462,15 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFConvBertForMaskedLM\",\n+    \"TFConvBertForMultipleChoice\",\n+    \"TFConvBertForQuestionAnswering\",\n+    \"TFConvBertForSequenceClassification\",\n+    \"TFConvBertForTokenClassification\",\n+    \"TFConvBertLayer\",\n+    \"TFConvBertModel\",\n+    \"TFConvBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "c2d68428389c5f2c1095593b5fd886a89e93438d",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -507,3 +507,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"ConvBertTokenizer\"]"
        },
        {
            "sha": "59262d976e97a9079b4f7422c2b3fb3a68542283",
            "filename": "src/transformers/models/convbert/tokenization_convbert_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -171,3 +171,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"ConvBertTokenizerFast\"]"
        },
        {
            "sha": "796b9a48926fe6239195588496012842cf355299",
            "filename": "src/transformers/models/convnext/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 78,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,86 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_torch_available,\n-    is_vision_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_convnext\": [\"ConvNextConfig\", \"ConvNextOnnxConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_convnext\"] = [\"ConvNextFeatureExtractor\"]\n-    _import_structure[\"image_processing_convnext\"] = [\"ConvNextImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_convnext\"] = [\n-        \"ConvNextForImageClassification\",\n-        \"ConvNextModel\",\n-        \"ConvNextPreTrainedModel\",\n-        \"ConvNextBackbone\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_convnext\"] = [\n-        \"TFConvNextForImageClassification\",\n-        \"TFConvNextModel\",\n-        \"TFConvNextPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_convnext import ConvNextConfig, ConvNextOnnxConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_convnext import ConvNextFeatureExtractor\n-        from .image_processing_convnext import ConvNextImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_convnext import (\n-            ConvNextBackbone,\n-            ConvNextForImageClassification,\n-            ConvNextModel,\n-            ConvNextPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_convnext import TFConvNextForImageClassification, TFConvNextModel, TFConvNextPreTrainedModel\n-\n-\n+    from .configuration_convnext import *\n+    from .feature_extraction_convnext import *\n+    from .image_processing_convnext import *\n+    from .modeling_convnext import *\n+    from .modeling_tf_convnext import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "9f9ed3bfd469df4a8ea03966de43afb9fb67623c",
            "filename": "src/transformers/models/convnext/configuration_convnext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -137,3 +137,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n     @property\n     def atol_for_validation(self) -> float:\n         return 1e-5\n+\n+\n+__all__ = [\"ConvNextConfig\", \"ConvNextOnnxConfig\"]"
        },
        {
            "sha": "6b2208e5b1134fd998e767a15e1639a097b5fa41",
            "filename": "src/transformers/models/convnext/feature_extraction_convnext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Ffeature_extraction_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Ffeature_extraction_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Ffeature_extraction_convnext.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"ConvNextFeatureExtractor\"]"
        },
        {
            "sha": "90fc0bb1ff179b48c961810bf0ef463ca0327357",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -318,3 +318,6 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"ConvNextImageProcessor\"]"
        },
        {
            "sha": "155f466ac4ae68034752d9cf11d9e0d5fdfdee69",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -546,3 +546,6 @@ def forward(\n             hidden_states=hidden_states if output_hidden_states else None,\n             attentions=None,\n         )\n+\n+\n+__all__ = [\"ConvNextForImageClassification\", \"ConvNextModel\", \"ConvNextPreTrainedModel\", \"ConvNextBackbone\"]"
        },
        {
            "sha": "af1bae81db495f17d5fe1696efa020a8b4d28af2",
            "filename": "src/transformers/models/convnext/modeling_tf_convnext.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -664,3 +664,6 @@ def build(self, input_shape=None):\n             if hasattr(self.classifier, \"name\"):\n                 with tf.name_scope(self.classifier.name):\n                     self.classifier.build([None, None, self.config.hidden_sizes[-1]])\n+\n+\n+__all__ = [\"TFConvNextForImageClassification\", \"TFConvNextModel\", \"TFConvNextPreTrainedModel\"]"
        },
        {
            "sha": "0fd1293963b233da99850c67212dc2998102b126",
            "filename": "src/transformers/models/convnextv2/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 69,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,8 +1,4 @@\n-# flake8: noqa\n-# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n-# module, but to preserve other warnings. So, don't check this module at all.\n-\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -17,73 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-# rely on isort to merge the imports\n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-    is_tf_available,\n-)\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {\"configuration_convnextv2\": [\"ConvNextV2Config\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_convnextv2\"] = [\n-        \"ConvNextV2ForImageClassification\",\n-        \"ConvNextV2Model\",\n-        \"ConvNextV2PreTrainedModel\",\n-        \"ConvNextV2Backbone\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_convnextv2\"] = [\n-        \"TFConvNextV2ForImageClassification\",\n-        \"TFConvNextV2Model\",\n-        \"TFConvNextV2PreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_convnextv2 import (\n-        ConvNextV2Config,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_convnextv2 import (\n-            ConvNextV2Backbone,\n-            ConvNextV2ForImageClassification,\n-            ConvNextV2Model,\n-            ConvNextV2PreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_convnextv2 import (\n-            TFConvNextV2ForImageClassification,\n-            TFConvNextV2Model,\n-            TFConvNextV2PreTrainedModel,\n-        )\n-\n+    from .configuration_convnextv2 import *\n+    from .modeling_convnextv2 import *\n+    from .modeling_tf_convnextv2 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "60b631a340c05115e57a65af00475cb6bb63549d",
            "filename": "src/transformers/models/convnextv2/configuration_convnextv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -113,3 +113,6 @@ def __init__(\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n         )\n+\n+\n+__all__ = [\"ConvNextV2Config\"]"
        },
        {
            "sha": "c0490eead21c88c792676cd60d6302f22fa3ebc0",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -569,3 +569,6 @@ def forward(\n             hidden_states=hidden_states if output_hidden_states else None,\n             attentions=None,\n         )\n+\n+\n+__all__ = [\"ConvNextV2ForImageClassification\", \"ConvNextV2Model\", \"ConvNextV2PreTrainedModel\", \"ConvNextV2Backbone\"]"
        },
        {
            "sha": "c27ba2da453039c5afe9f9ebc78cbf6bbd73605a",
            "filename": "src/transformers/models/convnextv2/modeling_tf_convnextv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -678,3 +678,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"classifier\", None) is not None:\n             with tf.name_scope(self.classifier.name):\n                 self.classifier.build([None, None, self.config.hidden_sizes[-1]])\n+\n+\n+__all__ = [\"TFConvNextV2ForImageClassification\", \"TFConvNextV2Model\", \"TFConvNextV2PreTrainedModel\"]"
        },
        {
            "sha": "aaf4524671fdfe7aa54adab84bda0a4703b3b892",
            "filename": "src/transformers/models/cpm/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 39,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,49 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available, is_tokenizers_available\n-\n-\n-_import_structure = {}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_cpm\"] = [\"CpmTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_cpm_fast\"] = [\"CpmTokenizerFast\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_cpm import CpmTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_cpm_fast import CpmTokenizerFast\n-\n+    from .tokenization_cpm import *\n+    from .tokenization_cpm_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "884068f1a1571037a1502c94c6dbb6ea5bdc22ae",
            "filename": "src/transformers/models/cpm/tokenization_cpm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -343,3 +343,6 @@ def _decode(self, *args, **kwargs):\n         text = super()._decode(*args, **kwargs)\n         text = text.replace(\" \", \"\").replace(\"\\u2582\", \" \").replace(\"\\u2583\", \"\\n\")\n         return text\n+\n+\n+__all__ = [\"CpmTokenizer\"]"
        },
        {
            "sha": "ef933e084ddb2b375df12cc27aa03a27de55db43",
            "filename": "src/transformers/models/cpm/tokenization_cpm_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -236,3 +236,6 @@ def _decode(self, *args, **kwargs):\n         text = super()._decode(*args, **kwargs)\n         text = text.replace(\" \", \"\").replace(\"\\u2582\", \" \").replace(\"\\u2583\", \"\\n\")\n         return text\n+\n+\n+__all__ = [\"CpmTokenizerFast\"]"
        },
        {
            "sha": "d92eea75693e728f4c1e9afc199d40751ef6af7a",
            "filename": "src/transformers/models/cpmant/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 42,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,8 +1,4 @@\n-# flake8: noqa\n-# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n-# module, but to preserve other warnings. So, don't check this module at all.\n-\n-# Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -17,46 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-# rely on isort to merge the imports\n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_cpmant\": [\"CpmAntConfig\"],\n-    \"tokenization_cpmant\": [\"CpmAntTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_cpmant\"] = [\n-        \"CpmAntForCausalLM\",\n-        \"CpmAntModel\",\n-        \"CpmAntPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_cpmant import CpmAntConfig\n-    from .tokenization_cpmant import CpmAntTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_cpmant import (\n-            CpmAntForCausalLM,\n-            CpmAntModel,\n-            CpmAntPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_cpmant import *\n+    from .modeling_cpmant import *\n+    from .tokenization_cpmant import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c3368d67af7ab7db1278bcecdd01f6bf0b6ca59c",
            "filename": "src/transformers/models/cpmant/configuration_cpmant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2Fconfiguration_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2Fconfiguration_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fconfiguration_cpmant.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -117,3 +117,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.vocab_size = vocab_size\n         self.init_std = init_std\n+\n+\n+__all__ = [\"CpmAntConfig\"]"
        },
        {
            "sha": "df0aebe3cbf850d5ce022352c1f52e9b2d1f8d0e",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -855,3 +855,6 @@ def _reorder_cache(self, past_key_values, beam_idx):\n             key_value_layer[0] = key_value_layer[0][beam_idx]\n             key_value_layer[1] = key_value_layer[1][beam_idx]\n         return past_key_values\n+\n+\n+__all__ = [\"CpmAntForCausalLM\", \"CpmAntModel\", \"CpmAntPreTrainedModel\"]"
        },
        {
            "sha": "2da1d6286c5e8b3a39750ae705cc1486ddd63416",
            "filename": "src/transformers/models/cpmant/tokenization_cpmant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -265,3 +265,6 @@ def get_special_tokens_mask(\n         if token_ids_1 is not None:\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n         return [1] + ([0] * len(token_ids_0))\n+\n+\n+__all__ = [\"CpmAntTokenizer\"]"
        },
        {
            "sha": "ea62163babef934550a87c17577891614b071642",
            "filename": "src/transformers/models/ctrl/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 65,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,75 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_ctrl\": [\"CTRLConfig\"],\n-    \"tokenization_ctrl\": [\"CTRLTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_ctrl\"] = [\n-        \"CTRLForSequenceClassification\",\n-        \"CTRLLMHeadModel\",\n-        \"CTRLModel\",\n-        \"CTRLPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_ctrl\"] = [\n-        \"TFCTRLForSequenceClassification\",\n-        \"TFCTRLLMHeadModel\",\n-        \"TFCTRLModel\",\n-        \"TFCTRLPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_ctrl import CTRLConfig\n-    from .tokenization_ctrl import CTRLTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_ctrl import (\n-            CTRLForSequenceClassification,\n-            CTRLLMHeadModel,\n-            CTRLModel,\n-            CTRLPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_ctrl import (\n-            TFCTRLForSequenceClassification,\n-            TFCTRLLMHeadModel,\n-            TFCTRLModel,\n-            TFCTRLPreTrainedModel,\n-        )\n-\n+    from .configuration_ctrl import *\n+    from .modeling_ctrl import *\n+    from .modeling_tf_ctrl import *\n+    from .tokenization_ctrl import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7a812f0b55650fab8ab9a4d1b4c6bad4cd9446f4",
            "filename": "src/transformers/models/ctrl/configuration_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fconfiguration_ctrl.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -111,3 +111,6 @@ def __init__(\n         self.use_cache = use_cache\n \n         super().__init__(**kwargs)\n+\n+\n+__all__ = [\"CTRLConfig\"]"
        },
        {
            "sha": "10c325dbee838bbceab12421e941bbe83f368c3f",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -839,3 +839,6 @@ def forward(\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )\n+\n+\n+__all__ = [\"CTRLForSequenceClassification\", \"CTRLLMHeadModel\", \"CTRLModel\", \"CTRLPreTrainedModel\"]"
        },
        {
            "sha": "26609cc671649a9ca74eebf4078882fe8306524d",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -926,3 +926,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"transformer\", None) is not None:\n             with tf.name_scope(self.transformer.name):\n                 self.transformer.build(None)\n+\n+\n+__all__ = [\"TFCTRLForSequenceClassification\", \"TFCTRLLMHeadModel\", \"TFCTRLModel\", \"TFCTRLPreTrainedModel\"]"
        },
        {
            "sha": "66dae2b05fa620152757fc4df3b64e2ad7a3bea5",
            "filename": "src/transformers/models/ctrl/tokenization_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -246,3 +246,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n     #     tokens_generated_so_far = re.sub('(@@ )', '', string=filtered_tokens)\n     #     tokens_generated_so_far = re.sub('(@@ ?$)', '', string=tokens_generated_so_far)\n     #     return ''.join(tokens_generated_so_far)\n+\n+\n+__all__ = [\"CTRLTokenizer\"]"
        },
        {
            "sha": "756aded9e6ad2964ffa5add89af2c4af56071e88",
            "filename": "src/transformers/models/cvt/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 57,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,65 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_cvt\": [\"CvtConfig\"]}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_cvt\"] = [\n-        \"CvtForImageClassification\",\n-        \"CvtModel\",\n-        \"CvtPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_cvt\"] = [\n-        \"TFCvtForImageClassification\",\n-        \"TFCvtModel\",\n-        \"TFCvtPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_cvt import CvtConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_cvt import (\n-            CvtForImageClassification,\n-            CvtModel,\n-            CvtPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_cvt import (\n-            TFCvtForImageClassification,\n-            TFCvtModel,\n-            TFCvtPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_cvt import *\n+    from .modeling_cvt import *\n+    from .modeling_tf_cvt import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "38cba6874f6860e1270aa4aea8166df309b4f014",
            "filename": "src/transformers/models/cvt/configuration_cvt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2Fconfiguration_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2Fconfiguration_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fconfiguration_cvt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -141,3 +141,6 @@ def __init__(\n         self.stride_q = stride_q\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n+\n+\n+__all__ = [\"CvtConfig\"]"
        },
        {
            "sha": "cd68b391ba1ff414b96fa57f91e2450284adf6e9",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -720,3 +720,6 @@ def forward(\n             return ((loss,) + output) if loss is not None else output\n \n         return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\n+\n+\n+__all__ = [\"CvtForImageClassification\", \"CvtModel\", \"CvtPreTrainedModel\"]"
        },
        {
            "sha": "fa9a4d9a3a4450c51610a47632dbca6c9faf9a79",
            "filename": "src/transformers/models/cvt/modeling_tf_cvt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1091,3 +1091,6 @@ def build(self, input_shape=None):\n             if hasattr(self.classifier, \"name\"):\n                 with tf.name_scope(self.classifier.name):\n                     self.classifier.build([None, None, self.config.embed_dim[-1]])\n+\n+\n+__all__ = [\"TFCvtForImageClassification\", \"TFCvtModel\", \"TFCvtPreTrainedModel\"]"
        },
        {
            "sha": "40f84ccb59be91ad440ba413366dd726e555ca26",
            "filename": "src/transformers/models/dac/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 40,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,5 +1,4 @@\n-# coding=utf-8\n-# Copyright 2024 Descript and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -14,47 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_dac\": [\"DacConfig\"],\n-    \"feature_extraction_dac\": [\"DacFeatureExtractor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_dac\"] = [\n-        \"DacModel\",\n-        \"DacPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_dac import (\n-        DacConfig,\n-    )\n-    from .feature_extraction_dac import DacFeatureExtractor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_dac import (\n-            DacModel,\n-            DacPreTrainedModel,\n-        )\n-\n+    from .configuration_dac import *\n+    from .feature_extraction_dac import *\n+    from .modeling_dac import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "dbbefd3ccb59d774f063d2bf85d637fa31896c92",
            "filename": "src/transformers/models/dac/configuration_dac.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2Fconfiguration_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2Fconfiguration_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fconfiguration_dac.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -109,3 +109,6 @@ def __init__(\n     def frame_rate(self) -> int:\n         hop_length = np.prod(self.upsampling_ratios)\n         return math.ceil(self.sampling_rate / hop_length)\n+\n+\n+__all__ = [\"DacConfig\"]"
        },
        {
            "sha": "c22a7603f0598de162977c430ec5d9611ed03b71",
            "filename": "src/transformers/models/dac/feature_extraction_dac.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2Ffeature_extraction_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2Ffeature_extraction_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Ffeature_extraction_dac.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -168,3 +168,6 @@ def __call__(\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n         return padded_inputs\n+\n+\n+__all__ = [\"DacFeatureExtractor\"]"
        },
        {
            "sha": "f05f5d35bf34a4ce17ef4b40caf39fd62d9feb54",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -719,3 +719,6 @@ def forward(\n             return (loss, audio_values, quantized_representation, audio_codes, projected_latents)\n \n         return DacOutput(loss, audio_values, quantized_representation, audio_codes, projected_latents)\n+\n+\n+__all__ = [\"DacModel\", \"DacPreTrainedModel\"]"
        },
        {
            "sha": "7000ac3d353bf4eba157b07e350b9ac5f7552a98",
            "filename": "src/transformers/models/data2vec/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 105,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,115 +11,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {\n-    \"configuration_data2vec_audio\": [\"Data2VecAudioConfig\"],\n-    \"configuration_data2vec_text\": [\n-        \"Data2VecTextConfig\",\n-        \"Data2VecTextOnnxConfig\",\n-    ],\n-    \"configuration_data2vec_vision\": [\n-        \"Data2VecVisionConfig\",\n-        \"Data2VecVisionOnnxConfig\",\n-    ],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_data2vec_audio\"] = [\n-        \"Data2VecAudioForAudioFrameClassification\",\n-        \"Data2VecAudioForCTC\",\n-        \"Data2VecAudioForSequenceClassification\",\n-        \"Data2VecAudioForXVector\",\n-        \"Data2VecAudioModel\",\n-        \"Data2VecAudioPreTrainedModel\",\n-    ]\n-    _import_structure[\"modeling_data2vec_text\"] = [\n-        \"Data2VecTextForCausalLM\",\n-        \"Data2VecTextForMaskedLM\",\n-        \"Data2VecTextForMultipleChoice\",\n-        \"Data2VecTextForQuestionAnswering\",\n-        \"Data2VecTextForSequenceClassification\",\n-        \"Data2VecTextForTokenClassification\",\n-        \"Data2VecTextModel\",\n-        \"Data2VecTextPreTrainedModel\",\n-    ]\n-    _import_structure[\"modeling_data2vec_vision\"] = [\n-        \"Data2VecVisionForImageClassification\",\n-        \"Data2VecVisionForMaskedImageModeling\",\n-        \"Data2VecVisionForSemanticSegmentation\",\n-        \"Data2VecVisionModel\",\n-        \"Data2VecVisionPreTrainedModel\",\n-    ]\n-\n-if is_tf_available():\n-    _import_structure[\"modeling_tf_data2vec_vision\"] = [\n-        \"TFData2VecVisionForImageClassification\",\n-        \"TFData2VecVisionForSemanticSegmentation\",\n-        \"TFData2VecVisionModel\",\n-        \"TFData2VecVisionPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_data2vec_audio import Data2VecAudioConfig\n-    from .configuration_data2vec_text import (\n-        Data2VecTextConfig,\n-        Data2VecTextOnnxConfig,\n-    )\n-    from .configuration_data2vec_vision import (\n-        Data2VecVisionConfig,\n-        Data2VecVisionOnnxConfig,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_data2vec_audio import (\n-            Data2VecAudioForAudioFrameClassification,\n-            Data2VecAudioForCTC,\n-            Data2VecAudioForSequenceClassification,\n-            Data2VecAudioForXVector,\n-            Data2VecAudioModel,\n-            Data2VecAudioPreTrainedModel,\n-        )\n-        from .modeling_data2vec_text import (\n-            Data2VecTextForCausalLM,\n-            Data2VecTextForMaskedLM,\n-            Data2VecTextForMultipleChoice,\n-            Data2VecTextForQuestionAnswering,\n-            Data2VecTextForSequenceClassification,\n-            Data2VecTextForTokenClassification,\n-            Data2VecTextModel,\n-            Data2VecTextPreTrainedModel,\n-        )\n-        from .modeling_data2vec_vision import (\n-            Data2VecVisionForImageClassification,\n-            Data2VecVisionForMaskedImageModeling,\n-            Data2VecVisionForSemanticSegmentation,\n-            Data2VecVisionModel,\n-            Data2VecVisionPreTrainedModel,\n-        )\n-    if is_tf_available():\n-        from .modeling_tf_data2vec_vision import (\n-            TFData2VecVisionForImageClassification,\n-            TFData2VecVisionForSemanticSegmentation,\n-            TFData2VecVisionModel,\n-            TFData2VecVisionPreTrainedModel,\n-        )\n-\n+    from .configuration_data2vec_audio import *\n+    from .configuration_data2vec_text import *\n+    from .configuration_data2vec_vision import *\n+    from .modeling_data2vec_audio import *\n+    from .modeling_data2vec_text import *\n+    from .modeling_data2vec_vision import *\n+    from .modeling_tf_data2vec_vision import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8066829027fb6e62e1f3bf4e15f229da81e92106",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -283,3 +283,6 @@ def __init__(\n     @property\n     def inputs_to_logits_ratio(self):\n         return math.prod(self.conv_stride)\n+\n+\n+__all__ = [\"Data2VecAudioConfig\"]"
        },
        {
            "sha": "3aa9a6b7bf22a8b0bbe9665ede242fa6e895cf7b",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -149,3 +149,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"attention_mask\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"Data2VecTextConfig\", \"Data2VecTextOnnxConfig\"]"
        },
        {
            "sha": "b822b03ef3eb2028bbb22ab31ac50f3cb6dd4173",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -189,3 +189,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n     @property\n     def atol_for_validation(self) -> float:\n         return 1e-4\n+\n+\n+__all__ = [\"Data2VecVisionConfig\", \"Data2VecVisionOnnxConfig\"]"
        },
        {
            "sha": "b1be8ab1966073375d8d7c8eeda0fb430889eacf",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1763,3 +1763,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"Data2VecAudioForAudioFrameClassification\",\n+    \"Data2VecAudioForCTC\",\n+    \"Data2VecAudioForSequenceClassification\",\n+    \"Data2VecAudioForXVector\",\n+    \"Data2VecAudioModel\",\n+    \"Data2VecAudioPreTrainedModel\",\n+]"
        },
        {
            "sha": "806a1b0edb5737b9c6d08246cc7769c95c11610e",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1539,3 +1539,15 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n     mask = input_ids.ne(padding_idx).int()\n     incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n     return incremental_indices.long() + padding_idx\n+\n+\n+__all__ = [\n+    \"Data2VecTextForCausalLM\",\n+    \"Data2VecTextForMaskedLM\",\n+    \"Data2VecTextForMultipleChoice\",\n+    \"Data2VecTextForQuestionAnswering\",\n+    \"Data2VecTextForSequenceClassification\",\n+    \"Data2VecTextForTokenClassification\",\n+    \"Data2VecTextModel\",\n+    \"Data2VecTextPreTrainedModel\",\n+]"
        },
        {
            "sha": "d0a3b4dd59e7901e0d9d71dc24c457a240975816",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1444,3 +1444,11 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"Data2VecVisionForImageClassification\",\n+    \"Data2VecVisionForSemanticSegmentation\",\n+    \"Data2VecVisionModel\",\n+    \"Data2VecVisionPreTrainedModel\",\n+]"
        },
        {
            "sha": "71595b4a43ce8e336d6eb95f2f24f02cfd8b9ccc",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1714,3 +1714,11 @@ def build(self, input_shape=None):\n         if getattr(self, \"fpn2\", None) is not None:\n             with tf.name_scope(self.fpn2[0].name):\n                 self.fpn2[0].build([None, None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFData2VecVisionForImageClassification\",\n+    \"TFData2VecVisionForSemanticSegmentation\",\n+    \"TFData2VecVisionModel\",\n+    \"TFData2VecVisionPreTrainedModel\",\n+]"
        },
        {
            "sha": "cce0f34c778d21a81d03940e7a7951707d898c86",
            "filename": "src/transformers/models/dbrx/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 30,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdbrx%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdbrx%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -13,39 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_dbrx\": [\"DbrxConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_dbrx\"] = [\n-        \"DbrxForCausalLM\",\n-        \"DbrxModel\",\n-        \"DbrxPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_dbrx import DbrxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_dbrx import DbrxForCausalLM, DbrxModel, DbrxPreTrainedModel\n-\n-\n+    from .configuration_dbrx import *\n+    from .modeling_dbrx import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7935b1d1beb7a740b2b1d7193270bbdb1ea6ace5",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -227,3 +227,6 @@ def __init__(\n             raise ValueError(\"tie_word_embeddings is not supported for DBRX models.\")\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+\n+\n+__all__ = [\"DbrxConfig\"]"
        },
        {
            "sha": "a2373d345412c1b6d1973ca9a0c5a28c70171681",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1374,3 +1374,6 @@ def forward(\n             attentions=outputs.attentions,\n             router_logits=outputs.router_logits,\n         )\n+\n+\n+__all__ = [\"DbrxForCausalLM\", \"DbrxModel\", \"DbrxPreTrainedModel\"]"
        },
        {
            "sha": "f70972237964208b461eee8141a4f8b1377154f6",
            "filename": "src/transformers/models/deberta/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 96,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,106 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_deberta\": [\"DebertaConfig\", \"DebertaOnnxConfig\"],\n-    \"tokenization_deberta\": [\"DebertaTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_deberta_fast\"] = [\"DebertaTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_deberta\"] = [\n-        \"DebertaForMaskedLM\",\n-        \"DebertaForQuestionAnswering\",\n-        \"DebertaForSequenceClassification\",\n-        \"DebertaForTokenClassification\",\n-        \"DebertaModel\",\n-        \"DebertaPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_deberta\"] = [\n-        \"TFDebertaForMaskedLM\",\n-        \"TFDebertaForQuestionAnswering\",\n-        \"TFDebertaForSequenceClassification\",\n-        \"TFDebertaForTokenClassification\",\n-        \"TFDebertaModel\",\n-        \"TFDebertaPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_deberta import DebertaConfig, DebertaOnnxConfig\n-    from .tokenization_deberta import DebertaTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_deberta_fast import DebertaTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_deberta import (\n-            DebertaForMaskedLM,\n-            DebertaForQuestionAnswering,\n-            DebertaForSequenceClassification,\n-            DebertaForTokenClassification,\n-            DebertaModel,\n-            DebertaPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_deberta import (\n-            TFDebertaForMaskedLM,\n-            TFDebertaForQuestionAnswering,\n-            TFDebertaForSequenceClassification,\n-            TFDebertaForTokenClassification,\n-            TFDebertaModel,\n-            TFDebertaPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_deberta import *\n+    from .modeling_deberta import *\n+    from .modeling_tf_deberta import *\n+    from .tokenization_deberta import *\n+    from .tokenization_deberta_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "835c16080a17efa0ea6dc5f4570a97e495fa9931",
            "filename": "src/transformers/models/deberta/configuration_deberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -194,3 +194,6 @@ def generate_dummy_inputs(\n         if self._config.type_vocab_size == 0 and \"token_type_ids\" in dummy_inputs:\n             del dummy_inputs[\"token_type_ids\"]\n         return dummy_inputs\n+\n+\n+__all__ = [\"DebertaConfig\", \"DebertaOnnxConfig\"]"
        },
        {
            "sha": "415c03fa9685891adbb19bd1ef0f2db5b87b361d",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1332,3 +1332,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"DebertaForMaskedLM\",\n+    \"DebertaForQuestionAnswering\",\n+    \"DebertaForSequenceClassification\",\n+    \"DebertaForTokenClassification\",\n+    \"DebertaModel\",\n+    \"DebertaPreTrainedModel\",\n+]"
        },
        {
            "sha": "6a8b233978ff7eec4722cf95e15a0a2f58d750f4",
            "filename": "src/transformers/models/deberta/modeling_tf_deberta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1640,3 +1640,13 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFDebertaForMaskedLM\",\n+    \"TFDebertaForQuestionAnswering\",\n+    \"TFDebertaForSequenceClassification\",\n+    \"TFDebertaForTokenClassification\",\n+    \"TFDebertaModel\",\n+    \"TFDebertaPreTrainedModel\",\n+]"
        },
        {
            "sha": "63933c1d2a32fbf1050697a660d0ad83f2b1c2d8",
            "filename": "src/transformers/models/deberta/tokenization_deberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -391,3 +391,6 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n         if (is_split_into_words or add_prefix_space) and (len(text) > 0 and not text[0].isspace()):\n             text = \" \" + text\n         return (text, kwargs)\n+\n+\n+__all__ = [\"DebertaTokenizer\"]"
        },
        {
            "sha": "39c64d90e533a2966d4b278d9e957ba7573e7e7e",
            "filename": "src/transformers/models/deberta/tokenization_deberta_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -245,3 +245,6 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"DebertaTokenizerFast\"]"
        },
        {
            "sha": "7c42c9c502862416b8942b9150567a33b6ec9301",
            "filename": "src/transformers/models/deberta_v2/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 102,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,112 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_deberta_v2\": [\"DebertaV2Config\", \"DebertaV2OnnxConfig\"],\n-    \"tokenization_deberta_v2\": [\"DebertaV2Tokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_deberta_v2_fast\"] = [\"DebertaV2TokenizerFast\"]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_deberta_v2\"] = [\n-        \"TFDebertaV2ForMaskedLM\",\n-        \"TFDebertaV2ForQuestionAnswering\",\n-        \"TFDebertaV2ForMultipleChoice\",\n-        \"TFDebertaV2ForSequenceClassification\",\n-        \"TFDebertaV2ForTokenClassification\",\n-        \"TFDebertaV2Model\",\n-        \"TFDebertaV2PreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_deberta_v2\"] = [\n-        \"DebertaV2ForMaskedLM\",\n-        \"DebertaV2ForMultipleChoice\",\n-        \"DebertaV2ForQuestionAnswering\",\n-        \"DebertaV2ForSequenceClassification\",\n-        \"DebertaV2ForTokenClassification\",\n-        \"DebertaV2Model\",\n-        \"DebertaV2PreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_deberta_v2 import (\n-        DebertaV2Config,\n-        DebertaV2OnnxConfig,\n-    )\n-    from .tokenization_deberta_v2 import DebertaV2Tokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_deberta_v2 import (\n-            TFDebertaV2ForMaskedLM,\n-            TFDebertaV2ForMultipleChoice,\n-            TFDebertaV2ForQuestionAnswering,\n-            TFDebertaV2ForSequenceClassification,\n-            TFDebertaV2ForTokenClassification,\n-            TFDebertaV2Model,\n-            TFDebertaV2PreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_deberta_v2 import (\n-            DebertaV2ForMaskedLM,\n-            DebertaV2ForMultipleChoice,\n-            DebertaV2ForQuestionAnswering,\n-            DebertaV2ForSequenceClassification,\n-            DebertaV2ForTokenClassification,\n-            DebertaV2Model,\n-            DebertaV2PreTrainedModel,\n-        )\n-\n+    from .configuration_deberta_v2 import *\n+    from .modeling_deberta_v2 import *\n+    from .modeling_tf_deberta_v2 import *\n+    from .tokenization_deberta_v2 import *\n+    from .tokenization_deberta_v2_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "3b1aaa3f04668300f41380c946755726aac0236f",
            "filename": "src/transformers/models/deberta_v2/configuration_deberta_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -193,3 +193,6 @@ def generate_dummy_inputs(\n         if self._config.type_vocab_size == 0 and \"token_type_ids\" in dummy_inputs:\n             del dummy_inputs[\"token_type_ids\"]\n         return dummy_inputs\n+\n+\n+__all__ = [\"DebertaV2Config\", \"DebertaV2OnnxConfig\"]"
        },
        {
            "sha": "177ca3dca551fae0fb780504a26155fffc9aaccf",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1506,3 +1506,14 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"DebertaV2ForMaskedLM\",\n+    \"DebertaV2ForMultipleChoice\",\n+    \"DebertaV2ForQuestionAnswering\",\n+    \"DebertaV2ForSequenceClassification\",\n+    \"DebertaV2ForTokenClassification\",\n+    \"DebertaV2Model\",\n+    \"DebertaV2PreTrainedModel\",\n+]"
        },
        {
            "sha": "b7b5a01d170fe38dbc15b98613bbedd31b127a5c",
            "filename": "src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1868,3 +1868,14 @@ def build(self, input_shape=None):\n         if getattr(self, \"classifier\", None) is not None:\n             with tf.name_scope(self.classifier.name):\n                 self.classifier.build([None, None, self.output_dim])\n+\n+\n+__all__ = [\n+    \"TFDebertaV2ForMaskedLM\",\n+    \"TFDebertaV2ForQuestionAnswering\",\n+    \"TFDebertaV2ForMultipleChoice\",\n+    \"TFDebertaV2ForSequenceClassification\",\n+    \"TFDebertaV2ForTokenClassification\",\n+    \"TFDebertaV2Model\",\n+    \"TFDebertaV2PreTrainedModel\",\n+]"
        },
        {
            "sha": "4440cc2e1c4840f172ba16e2d8bd35e4df745697",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -519,3 +519,6 @@ def convert_to_unicode(text):\n         return text.decode(\"utf-8\", \"ignore\")\n     else:\n         raise TypeError(f\"Unsupported string type: {type(text)}\")\n+\n+\n+__all__ = [\"DebertaV2Tokenizer\"]"
        },
        {
            "sha": "784e82995419029d1b560aafaacab1aa87ddd0a9",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -218,3 +218,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n             copyfile(self.vocab_file, out_vocab_file)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"DebertaV2TokenizerFast\"]"
        },
        {
            "sha": "455f7ffec5dee0567039cfd844588cb991c15622",
            "filename": "src/transformers/models/decision_transformer/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 39,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,47 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_decision_transformer\": [\"DecisionTransformerConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_decision_transformer\"] = [\n-        \"DecisionTransformerGPT2Model\",\n-        \"DecisionTransformerGPT2PreTrainedModel\",\n-        \"DecisionTransformerModel\",\n-        \"DecisionTransformerPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_decision_transformer import (\n-        DecisionTransformerConfig,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_decision_transformer import (\n-            DecisionTransformerGPT2Model,\n-            DecisionTransformerGPT2PreTrainedModel,\n-            DecisionTransformerModel,\n-            DecisionTransformerPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_decision_transformer import *\n+    from .modeling_decision_transformer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e677206aa089ce6befe5255f37868267f5b5bbc2",
            "filename": "src/transformers/models/decision_transformer/configuration_decision_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -152,3 +152,6 @@ def __init__(\n         self.eos_token_id = eos_token_id\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+\n+__all__ = [\"DecisionTransformerConfig\"]"
        },
        {
            "sha": "08c0f918c43578dd43405e9d68eb807cab0d8144",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -953,3 +953,11 @@ def forward(\n             hidden_states=encoder_outputs.hidden_states,\n             attentions=encoder_outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"DecisionTransformerGPT2Model\",\n+    \"DecisionTransformerGPT2PreTrainedModel\",\n+    \"DecisionTransformerModel\",\n+    \"DecisionTransformerPreTrainedModel\",\n+]"
        },
        {
            "sha": "994580e8152027bf26bee40fb3ff3953fd371ba3",
            "filename": "src/transformers/models/deit/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 89,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,97 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_torch_available,\n-    is_vision_available,\n-)\n-\n-\n-_import_structure = {\"configuration_deit\": [\"DeiTConfig\", \"DeiTOnnxConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_deit\"] = [\"DeiTFeatureExtractor\"]\n-    _import_structure[\"image_processing_deit\"] = [\"DeiTImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_deit\"] = [\n-        \"DeiTForImageClassification\",\n-        \"DeiTForImageClassificationWithTeacher\",\n-        \"DeiTForMaskedImageModeling\",\n-        \"DeiTModel\",\n-        \"DeiTPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_deit\"] = [\n-        \"TFDeiTForImageClassification\",\n-        \"TFDeiTForImageClassificationWithTeacher\",\n-        \"TFDeiTForMaskedImageModeling\",\n-        \"TFDeiTModel\",\n-        \"TFDeiTPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_deit import DeiTConfig, DeiTOnnxConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_deit import DeiTFeatureExtractor\n-        from .image_processing_deit import DeiTImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_deit import (\n-            DeiTForImageClassification,\n-            DeiTForImageClassificationWithTeacher,\n-            DeiTForMaskedImageModeling,\n-            DeiTModel,\n-            DeiTPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_deit import (\n-            TFDeiTForImageClassification,\n-            TFDeiTForImageClassificationWithTeacher,\n-            TFDeiTForMaskedImageModeling,\n-            TFDeiTModel,\n-            TFDeiTPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_deit import *\n+    from .feature_extraction_deit import *\n+    from .image_processing_deit import *\n+    from .modeling_deit import *\n+    from .modeling_tf_deit import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "d135144a2a40d264618c1747ee2756e9c9367183",
            "filename": "src/transformers/models/deit/configuration_deit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -137,3 +137,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n     @property\n     def atol_for_validation(self) -> float:\n         return 1e-4\n+\n+\n+__all__ = [\"DeiTConfig\", \"DeiTOnnxConfig\"]"
        },
        {
            "sha": "813c115fce553dfbf6d22efa41b123c50e6113c7",
            "filename": "src/transformers/models/deit/feature_extraction_deit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Ffeature_extraction_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Ffeature_extraction_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Ffeature_extraction_deit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"DeiTFeatureExtractor\"]"
        },
        {
            "sha": "7b5f981b004984848453447f317aae73e0a56ae7",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -294,3 +294,6 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"DeiTImageProcessor\"]"
        },
        {
            "sha": "dfb7753d6f9dd01c028f73748d04c894126c6c0b",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1010,3 +1010,12 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"DeiTForImageClassification\",\n+    \"DeiTForImageClassificationWithTeacher\",\n+    \"DeiTForMaskedImageModeling\",\n+    \"DeiTModel\",\n+    \"DeiTPreTrainedModel\",\n+]"
        },
        {
            "sha": "7723f8fc3478728b21d71da878416aa30dd450c8",
            "filename": "src/transformers/models/deit/modeling_tf_deit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1222,3 +1222,12 @@ def build(self, input_shape=None):\n         if getattr(self, \"distillation_classifier\", None) is not None:\n             with tf.name_scope(self.distillation_classifier.name):\n                 self.distillation_classifier.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFDeiTForImageClassification\",\n+    \"TFDeiTForImageClassificationWithTeacher\",\n+    \"TFDeiTForMaskedImageModeling\",\n+    \"TFDeiTModel\",\n+    \"TFDeiTPreTrainedModel\",\n+]"
        },
        {
            "sha": "7425e37e0399c792155a88488045176fb3b5e7a5",
            "filename": "src/transformers/models/depth_anything/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdepth_anything%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdepth_anything%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -13,40 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...file_utils import _LazyModule, is_torch_available\n-from ...utils import OptionalDependencyNotAvailable\n-\n-\n-_import_structure = {\"configuration_depth_anything\": [\"DepthAnythingConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_depth_anything\"] = [\n-        \"DepthAnythingForDepthEstimation\",\n-        \"DepthAnythingPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_depth_anything import DepthAnythingConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_depth_anything import (\n-            DepthAnythingForDepthEstimation,\n-            DepthAnythingPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_depth_anything import *\n+    from .modeling_depth_anything import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "3bbe621a44310c2b217fe00fd2fba653de3489d9",
            "filename": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -163,3 +163,6 @@ def to_dict(self):\n \n         output[\"model_type\"] = self.__class__.model_type\n         return output\n+\n+\n+__all__ = [\"DepthAnythingConfig\"]"
        },
        {
            "sha": "98a6ccde8c17d4a43b5abfb3e7a44f745f8a6ff5",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -463,3 +463,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"DepthAnythingForDepthEstimation\", \"DepthAnythingPreTrainedModel\"]"
        },
        {
            "sha": "b64cdbb3c7eb0467f6112225b8c0d9e1f65f9e99",
            "filename": "src/transformers/models/dinat/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 34,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,42 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_dinat\": [\"DinatConfig\"]}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_dinat\"] = [\n-        \"DinatForImageClassification\",\n-        \"DinatModel\",\n-        \"DinatPreTrainedModel\",\n-        \"DinatBackbone\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_dinat import DinatConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_dinat import (\n-            DinatBackbone,\n-            DinatForImageClassification,\n-            DinatModel,\n-            DinatPreTrainedModel,\n-        )\n-\n+    from .configuration_dinat import *\n+    from .modeling_dinat import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7b432e37c851395e98ff9c0dff859294b3e016f4",
            "filename": "src/transformers/models/dinat/configuration_dinat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinat%2Fconfiguration_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinat%2Fconfiguration_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fconfiguration_dinat.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -147,3 +147,6 @@ def __init__(\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n         )\n+\n+\n+__all__ = [\"DinatConfig\"]"
        },
        {
            "sha": "69677e406410be03cbdf9de44a268cc36a79383d",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -955,3 +955,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"DinatForImageClassification\", \"DinatModel\", \"DinatPreTrainedModel\", \"DinatBackbone\"]"
        },
        {
            "sha": "3cc316957eac509573bf44785209d0729ea13bb6",
            "filename": "src/transformers/models/dinov2/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 62,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,70 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_dinov2\": [\"Dinov2Config\", \"Dinov2OnnxConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_dinov2\"] = [\n-        \"Dinov2ForImageClassification\",\n-        \"Dinov2Model\",\n-        \"Dinov2PreTrainedModel\",\n-        \"Dinov2Backbone\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_dinov2\"] = [\n-        \"FlaxDinov2ForImageClassification\",\n-        \"FlaxDinov2Model\",\n-        \"FlaxDinov2PreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_dinov2 import Dinov2Config, Dinov2OnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_dinov2 import (\n-            Dinov2Backbone,\n-            Dinov2ForImageClassification,\n-            Dinov2Model,\n-            Dinov2PreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_dinov2 import (\n-            FlaxDinov2ForImageClassification,\n-            FlaxDinov2Model,\n-            FlaxDinov2PreTrainedModel,\n-        )\n-\n+    from .configuration_dinov2 import *\n+    from .modeling_dinov2 import *\n+    from .modeling_flax_dinov2 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "dfc339f49da7de20c5eb7544e1d65741a3aa14fd",
            "filename": "src/transformers/models/dinov2/configuration_dinov2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -170,3 +170,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n     @property\n     def atol_for_validation(self) -> float:\n         return 1e-4\n+\n+\n+__all__ = [\"Dinov2Config\", \"Dinov2OnnxConfig\"]"
        },
        {
            "sha": "71e0029d22d8351621041cfb50fac449a56f79c7",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -913,3 +913,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions if output_attentions else None,\n         )\n+\n+\n+__all__ = [\"Dinov2ForImageClassification\", \"Dinov2Model\", \"Dinov2PreTrainedModel\", \"Dinov2Backbone\"]"
        },
        {
            "sha": "82d1bf95fa400c34ab949a52ac2d668f1bfd147e",
            "filename": "src/transformers/models/dinov2/modeling_flax_dinov2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -793,3 +793,6 @@ class FlaxDinov2ForImageClassification(FlaxDinov2PreTrainedModel):\n append_replace_return_docstrings(\n     FlaxDinov2ForImageClassification, output_type=FlaxSequenceClassifierOutput, config_class=Dinov2Config\n )\n+\n+\n+__all__ = [\"FlaxDinov2ForImageClassification\", \"FlaxDinov2Model\", \"FlaxDinov2PreTrainedModel\"]"
        },
        {
            "sha": "4d6fae2e0236e7619988f0cfa3502ed49d0f90b0",
            "filename": "src/transformers/models/distilbert/__init__.py",
            "status": "modified",
            "additions": 11,
            "deletions": 140,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,150 +11,21 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_distilbert\": [\n-        \"DistilBertConfig\",\n-        \"DistilBertOnnxConfig\",\n-    ],\n-    \"tokenization_distilbert\": [\"DistilBertTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_distilbert_fast\"] = [\"DistilBertTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_distilbert\"] = [\n-        \"DistilBertForMaskedLM\",\n-        \"DistilBertForMultipleChoice\",\n-        \"DistilBertForQuestionAnswering\",\n-        \"DistilBertForSequenceClassification\",\n-        \"DistilBertForTokenClassification\",\n-        \"DistilBertModel\",\n-        \"DistilBertPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_distilbert\"] = [\n-        \"TFDistilBertForMaskedLM\",\n-        \"TFDistilBertForMultipleChoice\",\n-        \"TFDistilBertForQuestionAnswering\",\n-        \"TFDistilBertForSequenceClassification\",\n-        \"TFDistilBertForTokenClassification\",\n-        \"TFDistilBertMainLayer\",\n-        \"TFDistilBertModel\",\n-        \"TFDistilBertPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_distilbert\"] = [\n-        \"FlaxDistilBertForMaskedLM\",\n-        \"FlaxDistilBertForMultipleChoice\",\n-        \"FlaxDistilBertForQuestionAnswering\",\n-        \"FlaxDistilBertForSequenceClassification\",\n-        \"FlaxDistilBertForTokenClassification\",\n-        \"FlaxDistilBertModel\",\n-        \"FlaxDistilBertPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_distilbert import (\n-        DistilBertConfig,\n-        DistilBertOnnxConfig,\n-    )\n-    from .tokenization_distilbert import DistilBertTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_distilbert_fast import DistilBertTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_distilbert import (\n-            DistilBertForMaskedLM,\n-            DistilBertForMultipleChoice,\n-            DistilBertForQuestionAnswering,\n-            DistilBertForSequenceClassification,\n-            DistilBertForTokenClassification,\n-            DistilBertModel,\n-            DistilBertPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_distilbert import (\n-            TFDistilBertForMaskedLM,\n-            TFDistilBertForMultipleChoice,\n-            TFDistilBertForQuestionAnswering,\n-            TFDistilBertForSequenceClassification,\n-            TFDistilBertForTokenClassification,\n-            TFDistilBertMainLayer,\n-            TFDistilBertModel,\n-            TFDistilBertPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_distilbert import (\n-            FlaxDistilBertForMaskedLM,\n-            FlaxDistilBertForMultipleChoice,\n-            FlaxDistilBertForQuestionAnswering,\n-            FlaxDistilBertForSequenceClassification,\n-            FlaxDistilBertForTokenClassification,\n-            FlaxDistilBertModel,\n-            FlaxDistilBertPreTrainedModel,\n-        )\n-\n+    from .configuration_distilbert import *\n+    from .modeling_distilbert import *\n+    from .modeling_flax_distilbert import *\n+    from .modeling_tf_distilbert import *\n+    from .tokenization_distilbert import *\n+    from .tokenization_distilbert_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "9a28c8e5d03d029d344c3f9f3294c9298a9fd808",
            "filename": "src/transformers/models/distilbert/configuration_distilbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -136,3 +136,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"attention_mask\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"DistilBertConfig\", \"DistilBertOnnxConfig\"]"
        },
        {
            "sha": "6aa50397d42cbe4ee1011b38a1e1b2804365a0aa",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1365,3 +1365,14 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"DistilBertForMaskedLM\",\n+    \"DistilBertForMultipleChoice\",\n+    \"DistilBertForQuestionAnswering\",\n+    \"DistilBertForSequenceClassification\",\n+    \"DistilBertForTokenClassification\",\n+    \"DistilBertModel\",\n+    \"DistilBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "f1cf0faaed3f6a423a05902b626d6206e645a62e",
            "filename": "src/transformers/models/distilbert/modeling_flax_distilbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -893,3 +893,14 @@ class FlaxDistilBertForQuestionAnswering(FlaxDistilBertPreTrainedModel):\n     FlaxQuestionAnsweringModelOutput,\n     _CONFIG_FOR_DOC,\n )\n+\n+\n+__all__ = [\n+    \"FlaxDistilBertForMaskedLM\",\n+    \"FlaxDistilBertForMultipleChoice\",\n+    \"FlaxDistilBertForQuestionAnswering\",\n+    \"FlaxDistilBertForSequenceClassification\",\n+    \"FlaxDistilBertForTokenClassification\",\n+    \"FlaxDistilBertModel\",\n+    \"FlaxDistilBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "09b14b89c563b6b34f64ada2df9fc5404121da9f",
            "filename": "src/transformers/models/distilbert/modeling_tf_distilbert.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1133,3 +1133,15 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.dim])\n+\n+\n+__all__ = [\n+    \"TFDistilBertForMaskedLM\",\n+    \"TFDistilBertForMultipleChoice\",\n+    \"TFDistilBertForQuestionAnswering\",\n+    \"TFDistilBertForSequenceClassification\",\n+    \"TFDistilBertForTokenClassification\",\n+    \"TFDistilBertMainLayer\",\n+    \"TFDistilBertModel\",\n+    \"TFDistilBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "c894211a2e0acf2bd82858186e88f7e3e99f672e",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -517,3 +517,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"DistilBertTokenizer\"]"
        },
        {
            "sha": "d3829763d5e7ab8e2a338c53a0f7dd50c3e4b737",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -174,3 +174,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"DistilBertTokenizerFast\"]"
        },
        {
            "sha": "54de054051f850e9f07c09ea39e76bd1191e91a8",
            "filename": "src/transformers/models/donut/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 52,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,60 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\n-    \"configuration_donut_swin\": [\"DonutSwinConfig\"],\n-    \"processing_donut\": [\"DonutProcessor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_donut_swin\"] = [\n-        \"DonutSwinModel\",\n-        \"DonutSwinPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_donut\"] = [\"DonutFeatureExtractor\"]\n-    _import_structure[\"image_processing_donut\"] = [\"DonutImageProcessor\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_donut_swin import DonutSwinConfig\n-    from .processing_donut import DonutProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_donut_swin import (\n-            DonutSwinModel,\n-            DonutSwinPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_donut import DonutFeatureExtractor\n-        from .image_processing_donut import DonutImageProcessor\n-\n+    from .configuration_donut_swin import *\n+    from .feature_extraction_donut import *\n+    from .image_processing_donut import *\n+    from .modeling_donut_swin import *\n+    from .processing_donut import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "9aac07dace7688273be0bdc57da0a12663c2fb5b",
            "filename": "src/transformers/models/donut/configuration_donut_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fconfiguration_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fconfiguration_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fconfiguration_donut_swin.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -130,3 +130,6 @@ def __init__(\n         # we set the hidden_size attribute in order to make Swin work with VisionEncoderDecoderModel\n         # this indicates the channel dimension after the last stage of the model\n         self.hidden_size = int(embed_dim * 2 ** (len(depths) - 1))\n+\n+\n+__all__ = [\"DonutSwinConfig\"]"
        },
        {
            "sha": "012b208204c5c37517406b4b474f25d7076e6825",
            "filename": "src/transformers/models/donut/feature_extraction_donut.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Ffeature_extraction_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Ffeature_extraction_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Ffeature_extraction_donut.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"DonutFeatureExtractor\"]"
        },
        {
            "sha": "0ddd0591ca371d03360d7491d00f439a5b292767",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -457,3 +457,6 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"DonutImageProcessor\"]"
        },
        {
            "sha": "1434ae41504535f19e4ad0c0d6ca53c41e58a4c8",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1006,3 +1006,6 @@ def forward(\n             attentions=encoder_outputs.attentions,\n             reshaped_hidden_states=encoder_outputs.reshaped_hidden_states,\n         )\n+\n+\n+__all__ = [\"DonutSwinModel\", \"DonutSwinPreTrainedModel\"]"
        },
        {
            "sha": "ed3112ff8dd9529bc7b6a2ecc08f96dc76a8a9a5",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -226,3 +226,6 @@ def feature_extractor(self):\n             FutureWarning,\n         )\n         return self.image_processor\n+\n+\n+__all__ = [\"DonutProcessor\"]"
        },
        {
            "sha": "9aeadbeaf416575570c280a3e15a52422a007103",
            "filename": "src/transformers/models/dpr/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 116,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,126 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_dpr\": [\"DPRConfig\"],\n-    \"tokenization_dpr\": [\n-        \"DPRContextEncoderTokenizer\",\n-        \"DPRQuestionEncoderTokenizer\",\n-        \"DPRReaderOutput\",\n-        \"DPRReaderTokenizer\",\n-    ],\n-}\n-\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_dpr_fast\"] = [\n-        \"DPRContextEncoderTokenizerFast\",\n-        \"DPRQuestionEncoderTokenizerFast\",\n-        \"DPRReaderTokenizerFast\",\n-    ]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_dpr\"] = [\n-        \"DPRContextEncoder\",\n-        \"DPRPretrainedContextEncoder\",\n-        \"DPRPreTrainedModel\",\n-        \"DPRPretrainedQuestionEncoder\",\n-        \"DPRPretrainedReader\",\n-        \"DPRQuestionEncoder\",\n-        \"DPRReader\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_dpr\"] = [\n-        \"TFDPRContextEncoder\",\n-        \"TFDPRPretrainedContextEncoder\",\n-        \"TFDPRPretrainedQuestionEncoder\",\n-        \"TFDPRPretrainedReader\",\n-        \"TFDPRQuestionEncoder\",\n-        \"TFDPRReader\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_dpr import DPRConfig\n-    from .tokenization_dpr import (\n-        DPRContextEncoderTokenizer,\n-        DPRQuestionEncoderTokenizer,\n-        DPRReaderOutput,\n-        DPRReaderTokenizer,\n-    )\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_dpr_fast import (\n-            DPRContextEncoderTokenizerFast,\n-            DPRQuestionEncoderTokenizerFast,\n-            DPRReaderTokenizerFast,\n-        )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_dpr import (\n-            DPRContextEncoder,\n-            DPRPretrainedContextEncoder,\n-            DPRPreTrainedModel,\n-            DPRPretrainedQuestionEncoder,\n-            DPRPretrainedReader,\n-            DPRQuestionEncoder,\n-            DPRReader,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_dpr import (\n-            TFDPRContextEncoder,\n-            TFDPRPretrainedContextEncoder,\n-            TFDPRPretrainedQuestionEncoder,\n-            TFDPRPretrainedReader,\n-            TFDPRQuestionEncoder,\n-            TFDPRReader,\n-        )\n-\n+    from .configuration_dpr import *\n+    from .modeling_dpr import *\n+    from .modeling_tf_dpr import *\n+    from .tokenization_dpr import *\n+    from .tokenization_dpr_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7e4b97c97a4f7f1acedb0f2e23be4fb5dd770a99",
            "filename": "src/transformers/models/dpr/configuration_dpr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fconfiguration_dpr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -126,3 +126,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.projection_dim = projection_dim\n         self.position_embedding_type = position_embedding_type\n+\n+\n+__all__ = [\"DPRConfig\"]"
        },
        {
            "sha": "79317202b8ca716f45174e94daf531452cc85738",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -655,3 +655,14 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n+\n+\n+__all__ = [\n+    \"DPRContextEncoder\",\n+    \"DPRPretrainedContextEncoder\",\n+    \"DPRPreTrainedModel\",\n+    \"DPRPretrainedQuestionEncoder\",\n+    \"DPRPretrainedReader\",\n+    \"DPRQuestionEncoder\",\n+    \"DPRReader\",\n+]"
        },
        {
            "sha": "49a750fa4ff4b82754bb365075dcb7ca5fb15e38",
            "filename": "src/transformers/models/dpr/modeling_tf_dpr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -788,3 +788,13 @@ def build(self, input_shape=None):\n         if getattr(self, \"span_predictor\", None) is not None:\n             with tf.name_scope(self.span_predictor.name):\n                 self.span_predictor.build(None)\n+\n+\n+__all__ = [\n+    \"TFDPRContextEncoder\",\n+    \"TFDPRPretrainedContextEncoder\",\n+    \"TFDPRPretrainedQuestionEncoder\",\n+    \"TFDPRPretrainedReader\",\n+    \"TFDPRQuestionEncoder\",\n+    \"TFDPRReader\",\n+]"
        },
        {
            "sha": "00b8dedfa7e4b7d190ad4fa3b9f68597996b038f",
            "filename": "src/transformers/models/dpr/tokenization_dpr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -316,3 +316,6 @@ class DPRReaderTokenizer(CustomDPRReaderTokenizerMixin, BertTokenizer):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n+\n+\n+__all__ = [\"DPRContextEncoderTokenizer\", \"DPRQuestionEncoderTokenizer\", \"DPRReaderOutput\", \"DPRReaderTokenizer\"]"
        },
        {
            "sha": "026ba1a8907dccdef75f67796093db025b4cca1c",
            "filename": "src/transformers/models/dpr/tokenization_dpr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -316,3 +316,6 @@ class DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin, BertTokenizerFast):\n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n     slow_tokenizer_class = DPRReaderTokenizer\n+\n+\n+__all__ = [\"DPRContextEncoderTokenizerFast\", \"DPRQuestionEncoderTokenizerFast\", \"DPRReaderTokenizerFast\"]"
        },
        {
            "sha": "086750423dbd93ecd6f23bf50adb8e0955c1771d",
            "filename": "src/transformers/models/dpt/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 54,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,62 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...file_utils import _LazyModule, is_tokenizers_available, is_torch_available, is_vision_available\n-from ...utils import OptionalDependencyNotAvailable\n-\n-\n-_import_structure = {\"configuration_dpt\": [\"DPTConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_dpt\"] = [\"DPTFeatureExtractor\"]\n-    _import_structure[\"image_processing_dpt\"] = [\"DPTImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_dpt\"] = [\n-        \"DPTForDepthEstimation\",\n-        \"DPTForSemanticSegmentation\",\n-        \"DPTModel\",\n-        \"DPTPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_dpt import DPTConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_dpt import DPTFeatureExtractor\n-        from .image_processing_dpt import DPTImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_dpt import (\n-            DPTForDepthEstimation,\n-            DPTForSemanticSegmentation,\n-            DPTModel,\n-            DPTPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_dpt import *\n+    from .feature_extraction_dpt import *\n+    from .image_processing_dpt import *\n+    from .modeling_dpt import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "516f8f43f0d2bf5f77541d1f09c945c7d0e06d61",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -281,3 +281,6 @@ def to_dict(self):\n \n         output[\"model_type\"] = self.__class__.model_type\n         return output\n+\n+\n+__all__ = [\"DPTConfig\"]"
        },
        {
            "sha": "8a13989676edf888d408e629f5559710b2840a4d",
            "filename": "src/transformers/models/dpt/feature_extraction_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Ffeature_extraction_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Ffeature_extraction_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Ffeature_extraction_dpt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"DPTFeatureExtractor\"]"
        },
        {
            "sha": "c4d79696f0ab62ed426ac623936ee1e1eab05cb1",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -512,3 +512,6 @@ def post_process_depth_estimation(\n             results.append({\"predicted_depth\": depth})\n \n         return results\n+\n+\n+__all__ = [\"DPTImageProcessor\"]"
        },
        {
            "sha": "a82227b45809e890c261bc71cc3e8edf8bebd49a",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1371,3 +1371,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"DPTForDepthEstimation\", \"DPTForSemanticSegmentation\", \"DPTModel\", \"DPTPreTrainedModel\"]"
        },
        {
            "sha": "68a2825c7057df49700e2d4955955cd800d38ed4",
            "filename": "src/transformers/models/efficientnet/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 60,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,8 +1,4 @@\n-# flake8: noqa\n-# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n-# module, but to preserve other warnings. So, don't check this module at all.\n-\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -17,64 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-# rely on isort to merge the imports\n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {\n-    \"configuration_efficientnet\": [\n-        \"EfficientNetConfig\",\n-        \"EfficientNetOnnxConfig\",\n-    ]\n-}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_efficientnet\"] = [\"EfficientNetImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_efficientnet\"] = [\n-        \"EfficientNetForImageClassification\",\n-        \"EfficientNetModel\",\n-        \"EfficientNetPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_efficientnet import (\n-        EfficientNetConfig,\n-        EfficientNetOnnxConfig,\n-    )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_efficientnet import EfficientNetImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_efficientnet import (\n-            EfficientNetForImageClassification,\n-            EfficientNetModel,\n-            EfficientNetPreTrainedModel,\n-        )\n-\n+    from .configuration_efficientnet import *\n+    from .image_processing_efficientnet import *\n+    from .modeling_efficientnet import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ef25447d6aef3ac5d5afa414b10611ce370e6374",
            "filename": "src/transformers/models/efficientnet/configuration_efficientnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -164,3 +164,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n     @property\n     def atol_for_validation(self) -> float:\n         return 1e-5\n+\n+\n+__all__ = [\"EfficientNetConfig\", \"EfficientNetOnnxConfig\"]"
        },
        {
            "sha": "3e2403716ce0af6aa714475d6fbc5ce40d794178",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -364,3 +364,6 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"EfficientNetImageProcessor\"]"
        },
        {
            "sha": "0ab5fa2e6aacbd367ac82982cb7a0e1f0637f371",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -642,3 +642,6 @@ def forward(\n             logits=logits,\n             hidden_states=outputs.hidden_states,\n         )\n+\n+\n+__all__ = [\"EfficientNetForImageClassification\", \"EfficientNetModel\", \"EfficientNetPreTrainedModel\"]"
        },
        {
            "sha": "a78ed5c42aea51038335efabde5b03e333592ed6",
            "filename": "src/transformers/models/electra/__init__.py",
            "status": "modified",
            "additions": 11,
            "deletions": 144,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,154 +11,21 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_electra\": [\"ElectraConfig\", \"ElectraOnnxConfig\"],\n-    \"tokenization_electra\": [\"ElectraTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_electra_fast\"] = [\"ElectraTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_electra\"] = [\n-        \"ElectraForCausalLM\",\n-        \"ElectraForMaskedLM\",\n-        \"ElectraForMultipleChoice\",\n-        \"ElectraForPreTraining\",\n-        \"ElectraForQuestionAnswering\",\n-        \"ElectraForSequenceClassification\",\n-        \"ElectraForTokenClassification\",\n-        \"ElectraModel\",\n-        \"ElectraPreTrainedModel\",\n-        \"load_tf_weights_in_electra\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_electra\"] = [\n-        \"TFElectraForMaskedLM\",\n-        \"TFElectraForMultipleChoice\",\n-        \"TFElectraForPreTraining\",\n-        \"TFElectraForQuestionAnswering\",\n-        \"TFElectraForSequenceClassification\",\n-        \"TFElectraForTokenClassification\",\n-        \"TFElectraModel\",\n-        \"TFElectraPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_electra\"] = [\n-        \"FlaxElectraForCausalLM\",\n-        \"FlaxElectraForMaskedLM\",\n-        \"FlaxElectraForMultipleChoice\",\n-        \"FlaxElectraForPreTraining\",\n-        \"FlaxElectraForQuestionAnswering\",\n-        \"FlaxElectraForSequenceClassification\",\n-        \"FlaxElectraForTokenClassification\",\n-        \"FlaxElectraModel\",\n-        \"FlaxElectraPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_electra import ElectraConfig, ElectraOnnxConfig\n-    from .tokenization_electra import ElectraTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_electra_fast import ElectraTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_electra import (\n-            ElectraForCausalLM,\n-            ElectraForMaskedLM,\n-            ElectraForMultipleChoice,\n-            ElectraForPreTraining,\n-            ElectraForQuestionAnswering,\n-            ElectraForSequenceClassification,\n-            ElectraForTokenClassification,\n-            ElectraModel,\n-            ElectraPreTrainedModel,\n-            load_tf_weights_in_electra,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_electra import (\n-            TFElectraForMaskedLM,\n-            TFElectraForMultipleChoice,\n-            TFElectraForPreTraining,\n-            TFElectraForQuestionAnswering,\n-            TFElectraForSequenceClassification,\n-            TFElectraForTokenClassification,\n-            TFElectraModel,\n-            TFElectraPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_electra import (\n-            FlaxElectraForCausalLM,\n-            FlaxElectraForMaskedLM,\n-            FlaxElectraForMultipleChoice,\n-            FlaxElectraForPreTraining,\n-            FlaxElectraForQuestionAnswering,\n-            FlaxElectraForSequenceClassification,\n-            FlaxElectraForTokenClassification,\n-            FlaxElectraModel,\n-            FlaxElectraPreTrainedModel,\n-        )\n-\n+    from .configuration_electra import *\n+    from .modeling_electra import *\n+    from .modeling_flax_electra import *\n+    from .modeling_tf_electra import *\n+    from .tokenization_electra import *\n+    from .tokenization_electra_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "20b242c0f8d65fd5a4a3109fdf3b58a3ff7b9181",
            "filename": "src/transformers/models/electra/configuration_electra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -182,3 +182,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"token_type_ids\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"ElectraConfig\", \"ElectraOnnxConfig\"]"
        },
        {
            "sha": "14fd33b683eacdbcbe8890d50d49ededbf5b5bcc",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1660,3 +1660,17 @@ def _reorder_cache(self, past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\n+    \"ElectraForCausalLM\",\n+    \"ElectraForMaskedLM\",\n+    \"ElectraForMultipleChoice\",\n+    \"ElectraForPreTraining\",\n+    \"ElectraForQuestionAnswering\",\n+    \"ElectraForSequenceClassification\",\n+    \"ElectraForTokenClassification\",\n+    \"ElectraModel\",\n+    \"ElectraPreTrainedModel\",\n+    \"load_tf_weights_in_electra\",\n+]"
        },
        {
            "sha": "4ca7d1d6dcf484afb81e927be197b40696b6702d",
            "filename": "src/transformers/models/electra/modeling_flax_electra.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1599,3 +1599,16 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     FlaxCausalLMOutputWithCrossAttentions,\n     _CONFIG_FOR_DOC,\n )\n+\n+\n+__all__ = [\n+    \"FlaxElectraForCausalLM\",\n+    \"FlaxElectraForMaskedLM\",\n+    \"FlaxElectraForMultipleChoice\",\n+    \"FlaxElectraForPreTraining\",\n+    \"FlaxElectraForQuestionAnswering\",\n+    \"FlaxElectraForSequenceClassification\",\n+    \"FlaxElectraForTokenClassification\",\n+    \"FlaxElectraModel\",\n+    \"FlaxElectraPreTrainedModel\",\n+]"
        },
        {
            "sha": "827241d0a874e0b0fc02ec7430e4b9e01c5a9f17",
            "filename": "src/transformers/models/electra/modeling_tf_electra.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1762,3 +1762,15 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFElectraForMaskedLM\",\n+    \"TFElectraForMultipleChoice\",\n+    \"TFElectraForPreTraining\",\n+    \"TFElectraForQuestionAnswering\",\n+    \"TFElectraForSequenceClassification\",\n+    \"TFElectraForTokenClassification\",\n+    \"TFElectraModel\",\n+    \"TFElectraPreTrainedModel\",\n+]"
        },
        {
            "sha": "3b21527e6cdae25e1bdc40a81a01f3b2f014ffb5",
            "filename": "src/transformers/models/electra/tokenization_electra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -506,3 +506,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"ElectraTokenizer\"]"
        },
        {
            "sha": "34ea4339b9382b28c6f9a5842f88af8807f9f928",
            "filename": "src/transformers/models/electra/tokenization_electra_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -167,3 +167,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"ElectraTokenizerFast\"]"
        },
        {
            "sha": "3adeea056604d1d31f946a5cd0bf53ea590ea3aa",
            "filename": "src/transformers/models/encodec/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 39,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,47 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_encodec\": [\"EncodecConfig\"],\n-    \"feature_extraction_encodec\": [\"EncodecFeatureExtractor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_encodec\"] = [\n-        \"EncodecModel\",\n-        \"EncodecPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_encodec import (\n-        EncodecConfig,\n-    )\n-    from .feature_extraction_encodec import EncodecFeatureExtractor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_encodec import (\n-            EncodecModel,\n-            EncodecPreTrainedModel,\n-        )\n-\n+    from .configuration_encodec import *\n+    from .feature_extraction_encodec import *\n+    from .modeling_encodec import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "77fd67727dc390e0b6a402f4b3617e6aae9e9791",
            "filename": "src/transformers/models/encodec/configuration_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -187,3 +187,6 @@ def frame_rate(self) -> int:\n     @property\n     def num_quantizers(self) -> int:\n         return int(1000 * self.target_bandwidths[-1] // (self.frame_rate * 10))\n+\n+\n+__all__ = [\"EncodecConfig\"]"
        },
        {
            "sha": "9bed59de45d8c801f3e19a1f2835f5a2eb46962f",
            "filename": "src/transformers/models/encodec/feature_extraction_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -204,3 +204,6 @@ def __call__(\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n         return padded_inputs\n+\n+\n+__all__ = [\"EncodecFeatureExtractor\"]"
        },
        {
            "sha": "9339c2645374b4e693b804d99f8ed55d938b9671",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -814,3 +814,6 @@ def forward(\n             return (audio_codes, audio_values)\n \n         return EncodecOutput(audio_codes=audio_codes, audio_values=audio_values)\n+\n+\n+__all__ = [\"EncodecModel\", \"EncodecPreTrainedModel\"]"
        },
        {
            "sha": "c786feb9213fdd31640c0fdeaead5164026ad37a",
            "filename": "src/transformers/models/encoder_decoder/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 62,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,72 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\"configuration_encoder_decoder\": [\"EncoderDecoderConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_encoder_decoder\"] = [\"EncoderDecoderModel\"]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_encoder_decoder\"] = [\"TFEncoderDecoderModel\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_encoder_decoder\"] = [\"FlaxEncoderDecoderModel\"]\n \n if TYPE_CHECKING:\n-    from .configuration_encoder_decoder import EncoderDecoderConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_encoder_decoder import EncoderDecoderModel\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_encoder_decoder import TFEncoderDecoderModel\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_encoder_decoder import FlaxEncoderDecoderModel\n-\n+    from .configuration_encoder_decoder import *\n+    from .modeling_encoder_decoder import *\n+    from .modeling_flax_encoder_decoder import *\n+    from .modeling_tf_encoder_decoder import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8b5c62363f6ad9c3ce72c61bd597dac3f9078c0a",
            "filename": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -106,3 +106,6 @@ def from_encoder_decoder_configs(\n         decoder_config.add_cross_attention = True\n \n         return cls(encoder=encoder_config.to_dict(), decoder=decoder_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"EncoderDecoderConfig\"]"
        },
        {
            "sha": "9ab4b7f2ced1674b0d51326ae2b1e8c6991aac93",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -682,3 +682,6 @@ def resize_token_embeddings(self, *args, **kwargs):\n     def _reorder_cache(self, past_key_values, beam_idx):\n         # apply decoder cache reordering here\n         return self.decoder._reorder_cache(past_key_values, beam_idx)\n+\n+\n+__all__ = [\"EncoderDecoderModel\"]"
        },
        {
            "sha": "bdc589484cda1083f42ce96e67b12771f84b6af9",
            "filename": "src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -896,3 +896,6 @@ def from_encoder_decoder_pretrained(\n         model.params[\"decoder\"] = decoder.params\n \n         return model\n+\n+\n+__all__ = [\"FlaxEncoderDecoderModel\"]"
        },
        {
            "sha": "66009fc3ef060507748e7fc5244625f80b25fed2",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -660,3 +660,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"decoder\", None) is not None:\n             with tf.name_scope(self.decoder.name):\n                 self.decoder.build(None)\n+\n+\n+__all__ = [\"TFEncoderDecoderModel\"]"
        },
        {
            "sha": "9bb8983063ddb0117e8b0d7cd6603aa6ac3056b6",
            "filename": "src/transformers/models/ernie/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 48,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fernie%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fernie%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,58 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tensorflow_text_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_ernie\": [\"ErnieConfig\", \"ErnieOnnxConfig\"],\n-}\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_ernie\"] = [\n-        \"ErnieForCausalLM\",\n-        \"ErnieForMaskedLM\",\n-        \"ErnieForMultipleChoice\",\n-        \"ErnieForNextSentencePrediction\",\n-        \"ErnieForPreTraining\",\n-        \"ErnieForQuestionAnswering\",\n-        \"ErnieForSequenceClassification\",\n-        \"ErnieForTokenClassification\",\n-        \"ErnieModel\",\n-        \"ErniePreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_ernie import ErnieConfig, ErnieOnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_ernie import (\n-            ErnieForCausalLM,\n-            ErnieForMaskedLM,\n-            ErnieForMultipleChoice,\n-            ErnieForNextSentencePrediction,\n-            ErnieForPreTraining,\n-            ErnieForQuestionAnswering,\n-            ErnieForSequenceClassification,\n-            ErnieForTokenClassification,\n-            ErnieModel,\n-            ErniePreTrainedModel,\n-        )\n-\n+    from .configuration_ernie import *\n+    from .modeling_ernie import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "655e40e163b59dac4f2cab5fe96265b2173478c1",
            "filename": "src/transformers/models/ernie/configuration_ernie.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -158,3 +158,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"task_type_ids\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"ErnieConfig\", \"ErnieOnnxConfig\"]"
        },
        {
            "sha": "ec090b712e4420723b27b9f7e19c5f6b4e440382",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1799,3 +1799,17 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"ErnieForCausalLM\",\n+    \"ErnieForMaskedLM\",\n+    \"ErnieForMultipleChoice\",\n+    \"ErnieForNextSentencePrediction\",\n+    \"ErnieForPreTraining\",\n+    \"ErnieForQuestionAnswering\",\n+    \"ErnieForSequenceClassification\",\n+    \"ErnieForTokenClassification\",\n+    \"ErnieModel\",\n+    \"ErniePreTrainedModel\",\n+]"
        },
        {
            "sha": "8eac54d6ddcbdae2b8ca3771ae5540522f6f29da",
            "filename": "src/transformers/models/esm/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 70,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 Facebook and The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,78 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_esm\": [\"EsmConfig\"],\n-    \"tokenization_esm\": [\"EsmTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_esm\"] = [\n-        \"EsmForMaskedLM\",\n-        \"EsmForSequenceClassification\",\n-        \"EsmForTokenClassification\",\n-        \"EsmModel\",\n-        \"EsmPreTrainedModel\",\n-    ]\n-    _import_structure[\"modeling_esmfold\"] = [\"EsmForProteinFolding\", \"EsmFoldPreTrainedModel\"]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_esm\"] = [\n-        \"TFEsmForMaskedLM\",\n-        \"TFEsmForSequenceClassification\",\n-        \"TFEsmForTokenClassification\",\n-        \"TFEsmModel\",\n-        \"TFEsmPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_esm import EsmConfig\n-    from .tokenization_esm import EsmTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_esm import (\n-            EsmForMaskedLM,\n-            EsmForSequenceClassification,\n-            EsmForTokenClassification,\n-            EsmModel,\n-            EsmPreTrainedModel,\n-        )\n-        from .modeling_esmfold import EsmFoldPreTrainedModel, EsmForProteinFolding\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_esm import (\n-            TFEsmForMaskedLM,\n-            TFEsmForSequenceClassification,\n-            TFEsmForTokenClassification,\n-            TFEsmModel,\n-            TFEsmPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_esm import *\n+    from .modeling_esm import *\n+    from .modeling_esmfold import *\n+    from .modeling_tf_esm import *\n+    from .tokenization_esm import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "c0a31e6958472d7a48fdead53f19b6be8764186d",
            "filename": "src/transformers/models/esm/configuration_esm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -360,3 +360,6 @@ def get_default_vocab_list():\n         \"<null_1>\",\n         \"<mask>\",\n     )\n+\n+\n+__all__ = [\"EsmConfig\"]"
        },
        {
            "sha": "a7d07904e06a94205c238aac82f849f9088e75a2",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1260,3 +1260,12 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n     mask = input_ids.ne(padding_idx).int()\n     incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n     return incremental_indices.long() + padding_idx\n+\n+\n+__all__ = [\n+    \"EsmForMaskedLM\",\n+    \"EsmForSequenceClassification\",\n+    \"EsmForTokenClassification\",\n+    \"EsmModel\",\n+    \"EsmPreTrainedModel\",\n+]"
        },
        {
            "sha": "67cee99294a89bba8dbcc4f9a529adb70dc52e3a",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -2320,3 +2320,6 @@ def infer_pdbs(self, seqs: List[str], *args, **kwargs) -> List[str]:\n         \"\"\"Returns the pdb (file) string from the model given an input sequence.\"\"\"\n         output = self.infer(seqs, *args, **kwargs)\n         return self.output_to_pdb(output)\n+\n+\n+__all__ = [\"EsmForProteinFolding\", \"EsmFoldPreTrainedModel\"]"
        },
        {
            "sha": "71698486dab0adfece1feb14ce91b4500e24382c",
            "filename": "src/transformers/models/esm/modeling_tf_esm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_tf_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_tf_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_tf_esm.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1564,3 +1564,12 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n     mask = tf.cast(input_ids != padding_idx, tf.int64)\n     incremental_indices = (tf.cumsum(mask, axis=1) + past_key_values_length) * mask\n     return incremental_indices + padding_idx\n+\n+\n+__all__ = [\n+    \"TFEsmForMaskedLM\",\n+    \"TFEsmForSequenceClassification\",\n+    \"TFEsmForTokenClassification\",\n+    \"TFEsmModel\",\n+    \"TFEsmPreTrainedModel\",\n+]"
        },
        {
            "sha": "4bc433e350e13c33fc779092baf52197d8aa5e0d",
            "filename": "src/transformers/models/esm/tokenization_esm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Ftokenization_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fesm%2Ftokenization_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Ftokenization_esm.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -142,3 +142,6 @@ def save_vocabulary(self, save_directory, filename_prefix):\n     @property\n     def vocab_size(self) -> int:\n         return len(self.all_tokens)\n+\n+\n+__all__ = [\"EsmTokenizer\"]"
        },
        {
            "sha": "f9789767f11402264660b5dec0b5cae2466ee9d8",
            "filename": "src/transformers/models/falcon/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 46,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,5 +1,4 @@\n-# coding=utf-8\n-# Copyright 2023 the Falcon authors and HuggingFace Inc. team.  All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -14,53 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_falcon\": [\"FalconConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_falcon\"] = [\n-        \"FalconForCausalLM\",\n-        \"FalconModel\",\n-        \"FalconPreTrainedModel\",\n-        \"FalconForSequenceClassification\",\n-        \"FalconForTokenClassification\",\n-        \"FalconForQuestionAnswering\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_falcon import FalconConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_falcon import (\n-            FalconForCausalLM,\n-            FalconForQuestionAnswering,\n-            FalconForSequenceClassification,\n-            FalconForTokenClassification,\n-            FalconModel,\n-            FalconPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_falcon import *\n+    from .modeling_falcon import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "2d072b14f79001ff94b7f3de969cd75e26346c4b",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -206,3 +206,6 @@ def head_dim(self):\n     @property\n     def rotary(self):\n         return not self.alibi\n+\n+\n+__all__ = [\"FalconConfig\"]"
        },
        {
            "sha": "d600da4448808adbe54bc2712c4708c1d1e169e7",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1584,3 +1584,13 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"FalconForCausalLM\",\n+    \"FalconModel\",\n+    \"FalconPreTrainedModel\",\n+    \"FalconForSequenceClassification\",\n+    \"FalconForTokenClassification\",\n+    \"FalconForQuestionAnswering\",\n+]"
        },
        {
            "sha": "202147c938465dd7dfcb7e79ecbeeb93ce632dbf",
            "filename": "src/transformers/models/falcon_mamba/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 37,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -11,48 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_falcon_mamba\": [\"FalconMambaConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_falcon_mamba\"] = [\n-        \"FalconMambaForCausalLM\",\n-        \"FalconMambaModel\",\n-        \"FalconMambaPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_falcon_mamba import FalconMambaConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_falcon_mamba import (\n-            FalconMambaForCausalLM,\n-            FalconMambaModel,\n-            FalconMambaPreTrainedModel,\n-        )\n+    from .configuration_falcon_mamba import *\n+    from .modeling_falcon_mamba import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4127551445644f4c407f71f46265298978f9d1a7",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -157,3 +157,6 @@ def __init__(\n         self.mixer_rms_eps = mixer_rms_eps\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n+\n+\n+__all__ = [\"FalconMambaConfig\"]"
        },
        {
            "sha": "22ad7af59616fbf651d143fddc568b2951b8ac87",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -867,3 +867,6 @@ def forward(\n             cache_params=falcon_mamba_outputs.cache_params,\n             hidden_states=falcon_mamba_outputs.hidden_states,\n         )\n+\n+\n+__all__ = [\"FalconMambaForCausalLM\", \"FalconMambaModel\", \"FalconMambaPreTrainedModel\"]"
        },
        {
            "sha": "44d1ec7236310774ed6b1379683c144d7f93ecce",
            "filename": "src/transformers/models/fastspeech2_conformer/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 49,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,57 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_fastspeech2_conformer\": [\n-        \"FastSpeech2ConformerConfig\",\n-        \"FastSpeech2ConformerHifiGanConfig\",\n-        \"FastSpeech2ConformerWithHifiGanConfig\",\n-    ],\n-    \"tokenization_fastspeech2_conformer\": [\"FastSpeech2ConformerTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_fastspeech2_conformer\"] = [\n-        \"FastSpeech2ConformerWithHifiGan\",\n-        \"FastSpeech2ConformerHifiGan\",\n-        \"FastSpeech2ConformerModel\",\n-        \"FastSpeech2ConformerPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_fastspeech2_conformer import (\n-        FastSpeech2ConformerConfig,\n-        FastSpeech2ConformerHifiGanConfig,\n-        FastSpeech2ConformerWithHifiGanConfig,\n-    )\n-    from .tokenization_fastspeech2_conformer import FastSpeech2ConformerTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_fastspeech2_conformer import (\n-            FastSpeech2ConformerHifiGan,\n-            FastSpeech2ConformerModel,\n-            FastSpeech2ConformerPreTrainedModel,\n-            FastSpeech2ConformerWithHifiGan,\n-        )\n-\n+    from .configuration_fastspeech2_conformer import *\n+    from .modeling_fastspeech2_conformer import *\n+    from .tokenization_fastspeech2_conformer import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e9a061ffd3733c831190b9efb14316987674b853",
            "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -475,3 +475,6 @@ def __init__(\n         self.vocoder_config = FastSpeech2ConformerHifiGanConfig(**vocoder_config)\n \n         super().__init__(**kwargs)\n+\n+\n+__all__ = [\"FastSpeech2ConformerConfig\", \"FastSpeech2ConformerHifiGanConfig\", \"FastSpeech2ConformerWithHifiGanConfig\"]"
        },
        {
            "sha": "81c1eef8959e4af855ef1f614d9c9cd05b3bfadd",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1687,3 +1687,11 @@ def forward(\n             return model_outputs + (waveform,)\n \n         return FastSpeech2ConformerWithHifiGanOutput(waveform=waveform, **model_outputs)\n+\n+\n+__all__ = [\n+    \"FastSpeech2ConformerWithHifiGan\",\n+    \"FastSpeech2ConformerHifiGan\",\n+    \"FastSpeech2ConformerModel\",\n+    \"FastSpeech2ConformerPreTrainedModel\",\n+]"
        },
        {
            "sha": "faa1420d71c4b2152e2335aa579ed15856b3ee62",
            "filename": "src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Ftokenization_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Ftokenization_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Ftokenization_fastspeech2_conformer.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -183,3 +183,6 @@ def __setstate__(self, d):\n                 \"You need to install g2p-en to use FastSpeech2ConformerTokenizer. \"\n                 \"See https://pypi.org/project/g2p-en/ for installation.\"\n             )\n+\n+\n+__all__ = [\"FastSpeech2ConformerTokenizer\"]"
        },
        {
            "sha": "e981d9cbcb1e456c206b3bec252df1598e23575a",
            "filename": "src/transformers/models/flaubert/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 79,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,89 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_flaubert\": [\"FlaubertConfig\", \"FlaubertOnnxConfig\"],\n-    \"tokenization_flaubert\": [\"FlaubertTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flaubert\"] = [\n-        \"FlaubertForMultipleChoice\",\n-        \"FlaubertForQuestionAnswering\",\n-        \"FlaubertForQuestionAnsweringSimple\",\n-        \"FlaubertForSequenceClassification\",\n-        \"FlaubertForTokenClassification\",\n-        \"FlaubertModel\",\n-        \"FlaubertWithLMHeadModel\",\n-        \"FlaubertPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_flaubert\"] = [\n-        \"TFFlaubertForMultipleChoice\",\n-        \"TFFlaubertForQuestionAnsweringSimple\",\n-        \"TFFlaubertForSequenceClassification\",\n-        \"TFFlaubertForTokenClassification\",\n-        \"TFFlaubertModel\",\n-        \"TFFlaubertPreTrainedModel\",\n-        \"TFFlaubertWithLMHeadModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_flaubert import FlaubertConfig, FlaubertOnnxConfig\n-    from .tokenization_flaubert import FlaubertTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flaubert import (\n-            FlaubertForMultipleChoice,\n-            FlaubertForQuestionAnswering,\n-            FlaubertForQuestionAnsweringSimple,\n-            FlaubertForSequenceClassification,\n-            FlaubertForTokenClassification,\n-            FlaubertModel,\n-            FlaubertPreTrainedModel,\n-            FlaubertWithLMHeadModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_flaubert import (\n-            TFFlaubertForMultipleChoice,\n-            TFFlaubertForQuestionAnsweringSimple,\n-            TFFlaubertForSequenceClassification,\n-            TFFlaubertForTokenClassification,\n-            TFFlaubertModel,\n-            TFFlaubertPreTrainedModel,\n-            TFFlaubertWithLMHeadModel,\n-        )\n-\n+    from .configuration_flaubert import *\n+    from .modeling_flaubert import *\n+    from .modeling_tf_flaubert import *\n+    from .tokenization_flaubert import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "93e4645da5651405d014389b7130433ccac18f48",
            "filename": "src/transformers/models/flaubert/configuration_flaubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -230,3 +230,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"attention_mask\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"FlaubertConfig\", \"FlaubertOnnxConfig\"]"
        },
        {
            "sha": "bc1d66f835572ba163a82f4ebf74ae770995b7b5",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1300,3 +1300,15 @@ def forward(\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"FlaubertForMultipleChoice\",\n+    \"FlaubertForQuestionAnswering\",\n+    \"FlaubertForQuestionAnsweringSimple\",\n+    \"FlaubertForSequenceClassification\",\n+    \"FlaubertForTokenClassification\",\n+    \"FlaubertModel\",\n+    \"FlaubertWithLMHeadModel\",\n+    \"FlaubertPreTrainedModel\",\n+]"
        },
        {
            "sha": "a08006815511a812e559c00fbe7477d3e14a889c",
            "filename": "src/transformers/models/flaubert/modeling_tf_flaubert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1331,3 +1331,14 @@ def build(self, input_shape=None):\n         if getattr(self, \"logits_proj\", None) is not None:\n             with tf.name_scope(self.logits_proj.name):\n                 self.logits_proj.build([None, None, self.config.num_labels])\n+\n+\n+__all__ = [\n+    \"TFFlaubertForMultipleChoice\",\n+    \"TFFlaubertForQuestionAnsweringSimple\",\n+    \"TFFlaubertForSequenceClassification\",\n+    \"TFFlaubertForTokenClassification\",\n+    \"TFFlaubertModel\",\n+    \"TFFlaubertPreTrainedModel\",\n+    \"TFFlaubertWithLMHeadModel\",\n+]"
        },
        {
            "sha": "ac9e5aa4336c667d0401c18a1b63217ce0f287a7",
            "filename": "src/transformers/models/flaubert/tokenization_flaubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Ftokenization_flaubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -563,3 +563,6 @@ def __setstate__(self, d):\n             )\n \n         self.sm = sacremoses\n+\n+\n+__all__ = [\"FlaubertTokenizer\"]"
        },
        {
            "sha": "c258a8afc8e92ab9b162453d7e6726d8bcc311aa",
            "filename": "src/transformers/models/flava/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 73,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 Meta Platforms authors and The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,81 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_flava\": [\n-        \"FlavaConfig\",\n-        \"FlavaImageCodebookConfig\",\n-        \"FlavaImageConfig\",\n-        \"FlavaMultimodalConfig\",\n-        \"FlavaTextConfig\",\n-    ],\n-}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_flava\"] = [\"FlavaFeatureExtractor\"]\n-    _import_structure[\"image_processing_flava\"] = [\"FlavaImageProcessor\"]\n-    _import_structure[\"processing_flava\"] = [\"FlavaProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flava\"] = [\n-        \"FlavaForPreTraining\",\n-        \"FlavaImageCodebook\",\n-        \"FlavaImageModel\",\n-        \"FlavaModel\",\n-        \"FlavaMultimodalModel\",\n-        \"FlavaPreTrainedModel\",\n-        \"FlavaTextModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_flava import (\n-        FlavaConfig,\n-        FlavaImageCodebookConfig,\n-        FlavaImageConfig,\n-        FlavaMultimodalConfig,\n-        FlavaTextConfig,\n-    )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_flava import FlavaFeatureExtractor\n-        from .image_processing_flava import FlavaImageProcessor\n-        from .processing_flava import FlavaProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flava import (\n-            FlavaForPreTraining,\n-            FlavaImageCodebook,\n-            FlavaImageModel,\n-            FlavaModel,\n-            FlavaMultimodalModel,\n-            FlavaPreTrainedModel,\n-            FlavaTextModel,\n-        )\n-\n+    from .configuration_flava import *\n+    from .feature_extraction_flava import *\n+    from .image_processing_flava import *\n+    from .modeling_flava import *\n+    from .processing_flava import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7a18b33ac86ea8db820b5dacd9b4b2081b785311",
            "filename": "src/transformers/models/flava/configuration_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fconfiguration_flava.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -696,3 +696,6 @@ def from_configs(\n             image_codebook_config=image_codebook_config.to_dict(),\n             **kwargs,\n         )\n+\n+\n+__all__ = [\"FlavaConfig\", \"FlavaImageCodebookConfig\", \"FlavaImageConfig\", \"FlavaMultimodalConfig\", \"FlavaTextConfig\"]"
        },
        {
            "sha": "111795d418fc1b4497a04c767e7d7d727c736522",
            "filename": "src/transformers/models/flava/feature_extraction_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Ffeature_extraction_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Ffeature_extraction_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Ffeature_extraction_flava.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"FlavaFeatureExtractor\"]"
        },
        {
            "sha": "254af58c92d78d3948512afc2da7ce1cccbec7b8",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -698,3 +698,6 @@ def preprocess(\n             data[\"bool_masked_pos\"] = masks\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"FlavaImageProcessor\"]"
        },
        {
            "sha": "c893938e42841ea6df097646732f68692a114087",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -2102,3 +2102,14 @@ def forward(\n             mmm_image_logits=mmm_image_logits,\n             mmm_text_logits=mmm_text_logits,\n         )\n+\n+\n+__all__ = [\n+    \"FlavaForPreTraining\",\n+    \"FlavaImageCodebook\",\n+    \"FlavaImageModel\",\n+    \"FlavaModel\",\n+    \"FlavaMultimodalModel\",\n+    \"FlavaPreTrainedModel\",\n+    \"FlavaTextModel\",\n+]"
        },
        {
            "sha": "0e2a98cc1038f60004456984ce04073ab832bcde",
            "filename": "src/transformers/models/flava/processing_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -163,3 +163,6 @@ def feature_extractor(self):\n             FutureWarning,\n         )\n         return self.image_processor\n+\n+\n+__all__ = [\"FlavaProcessor\"]"
        },
        {
            "sha": "756d690e72c11e8075429e3666e8579f0666d074",
            "filename": "src/transformers/models/fnet/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 85,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,93 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_sentencepiece_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\"configuration_fnet\": [\"FNetConfig\"]}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_fnet\"] = [\"FNetTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_fnet_fast\"] = [\"FNetTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_fnet\"] = [\n-        \"FNetForMaskedLM\",\n-        \"FNetForMultipleChoice\",\n-        \"FNetForNextSentencePrediction\",\n-        \"FNetForPreTraining\",\n-        \"FNetForQuestionAnswering\",\n-        \"FNetForSequenceClassification\",\n-        \"FNetForTokenClassification\",\n-        \"FNetLayer\",\n-        \"FNetModel\",\n-        \"FNetPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_fnet import FNetConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_fnet import FNetTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_fnet_fast import FNetTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_fnet import (\n-            FNetForMaskedLM,\n-            FNetForMultipleChoice,\n-            FNetForNextSentencePrediction,\n-            FNetForPreTraining,\n-            FNetForQuestionAnswering,\n-            FNetForSequenceClassification,\n-            FNetForTokenClassification,\n-            FNetLayer,\n-            FNetModel,\n-            FNetPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_fnet import *\n+    from .modeling_fnet import *\n+    from .tokenization_fnet import *\n+    from .tokenization_fnet_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "24a578328565939b821cf8f9500559b2f585ea56",
            "filename": "src/transformers/models/fnet/configuration_fnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fconfiguration_fnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -114,3 +114,6 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.use_tpu_fourier_optimizations = use_tpu_fourier_optimizations\n         self.tpu_short_seq_length = tpu_short_seq_length\n+\n+\n+__all__ = [\"FNetConfig\"]"
        },
        {
            "sha": "9d02d35210f34fba32941781ac8dbeb43c6a687e",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1183,3 +1183,17 @@ def forward(\n         return QuestionAnsweringModelOutput(\n             loss=total_loss, start_logits=start_logits, end_logits=end_logits, hidden_states=outputs.hidden_states\n         )\n+\n+\n+__all__ = [\n+    \"FNetForMaskedLM\",\n+    \"FNetForMultipleChoice\",\n+    \"FNetForNextSentencePrediction\",\n+    \"FNetForPreTraining\",\n+    \"FNetForQuestionAnswering\",\n+    \"FNetForSequenceClassification\",\n+    \"FNetForTokenClassification\",\n+    \"FNetLayer\",\n+    \"FNetModel\",\n+    \"FNetPreTrainedModel\",\n+]"
        },
        {
            "sha": "877a50cc2d1d27b521e4b2e4ad2854a7641ada2f",
            "filename": "src/transformers/models/fnet/tokenization_fnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -336,3 +336,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 fi.write(content_spiece_model)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"FNetTokenizer\"]"
        },
        {
            "sha": "ac33bc13c60cd19bbdaf3eaa43dd8290e874378c",
            "filename": "src/transformers/models/fnet/tokenization_fnet_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -184,3 +184,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n             copyfile(self.vocab_file, out_vocab_file)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"FNetTokenizerFast\"]"
        },
        {
            "sha": "5dec8135f3b3030b20691e761483e5994ba441f0",
            "filename": "src/transformers/models/focalnet/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffocalnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffocalnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,45 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-# rely on isort to merge the imports\n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_focalnet\": [\"FocalNetConfig\"]}\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_focalnet\"] = [\n-        \"FocalNetForImageClassification\",\n-        \"FocalNetForMaskedImageModeling\",\n-        \"FocalNetBackbone\",\n-        \"FocalNetModel\",\n-        \"FocalNetPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_focalnet import FocalNetConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_focalnet import (\n-            FocalNetBackbone,\n-            FocalNetForImageClassification,\n-            FocalNetForMaskedImageModeling,\n-            FocalNetModel,\n-            FocalNetPreTrainedModel,\n-        )\n-\n+    from .configuration_focalnet import *\n+    from .modeling_focalnet import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8fe5831a112215eabba1a23cc5db0dae0f1c1111",
            "filename": "src/transformers/models/focalnet/configuration_focalnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconfiguration_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconfiguration_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconfiguration_focalnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -159,3 +159,6 @@ def __init__(\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n         )\n+\n+\n+__all__ = [\"FocalNetConfig\"]"
        },
        {
            "sha": "43de96087cd7f7f47c349814bd6de19ad8d38235",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1027,3 +1027,12 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=None,\n         )\n+\n+\n+__all__ = [\n+    \"FocalNetForImageClassification\",\n+    \"FocalNetForMaskedImageModeling\",\n+    \"FocalNetBackbone\",\n+    \"FocalNetModel\",\n+    \"FocalNetPreTrainedModel\",\n+]"
        },
        {
            "sha": "f8f31762d681dbf3541d38c39fafdf5fa6b864d1",
            "filename": "src/transformers/models/fsmt/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 29,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,39 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_fsmt\": [\"FSMTConfig\"],\n-    \"tokenization_fsmt\": [\"FSMTTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_fsmt\"] = [\"FSMTForConditionalGeneration\", \"FSMTModel\", \"PretrainedFSMTModel\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_fsmt import FSMTConfig\n-    from .tokenization_fsmt import FSMTTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_fsmt import FSMTForConditionalGeneration, FSMTModel, PretrainedFSMTModel\n-\n+    from .configuration_fsmt import *\n+    from .modeling_fsmt import *\n+    from .tokenization_fsmt import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "96b617e0da512e4675fd2b2c62fe7234665a3dbd",
            "filename": "src/transformers/models/fsmt/configuration_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -213,3 +213,6 @@ def __init__(\n             early_stopping=early_stopping,\n             **common_kwargs,\n         )\n+\n+\n+__all__ = [\"FSMTConfig\"]"
        },
        {
            "sha": "9961ea4c88f00d59496b3b3dc8de94dc87600be4",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1364,3 +1364,6 @@ def forward(\n             self.make_weight(max_pos, self.embedding_dim, self.padding_idx)\n         positions = self.make_positions(input, self.padding_idx)\n         return super().forward(positions)\n+\n+\n+__all__ = [\"FSMTForConditionalGeneration\", \"FSMTModel\", \"PretrainedFSMTModel\"]"
        },
        {
            "sha": "ce28766100e3498e39541ddc79daf773ea1e1e2d",
            "filename": "src/transformers/models/fsmt/tokenization_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Ftokenization_fsmt.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -516,3 +516,6 @@ def __setstate__(self, d):\n             )\n \n         self.sm = sacremoses\n+\n+\n+__all__ = [\"FSMTTokenizer\"]"
        },
        {
            "sha": "e4e0587ce32f5e59562102b302a113f387c60130",
            "filename": "src/transformers/models/funnel/__init__.py",
            "status": "modified",
            "additions": 11,
            "deletions": 110,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,120 +11,21 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_funnel\": [\"FunnelConfig\"],\n-    \"convert_funnel_original_tf_checkpoint_to_pytorch\": [],\n-    \"tokenization_funnel\": [\"FunnelTokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_funnel_fast\"] = [\"FunnelTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_funnel\"] = [\n-        \"FunnelBaseModel\",\n-        \"FunnelForMaskedLM\",\n-        \"FunnelForMultipleChoice\",\n-        \"FunnelForPreTraining\",\n-        \"FunnelForQuestionAnswering\",\n-        \"FunnelForSequenceClassification\",\n-        \"FunnelForTokenClassification\",\n-        \"FunnelModel\",\n-        \"FunnelPreTrainedModel\",\n-        \"load_tf_weights_in_funnel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_funnel\"] = [\n-        \"TFFunnelBaseModel\",\n-        \"TFFunnelForMaskedLM\",\n-        \"TFFunnelForMultipleChoice\",\n-        \"TFFunnelForPreTraining\",\n-        \"TFFunnelForQuestionAnswering\",\n-        \"TFFunnelForSequenceClassification\",\n-        \"TFFunnelForTokenClassification\",\n-        \"TFFunnelModel\",\n-        \"TFFunnelPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_funnel import FunnelConfig\n-    from .tokenization_funnel import FunnelTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_funnel_fast import FunnelTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_funnel import (\n-            FunnelBaseModel,\n-            FunnelForMaskedLM,\n-            FunnelForMultipleChoice,\n-            FunnelForPreTraining,\n-            FunnelForQuestionAnswering,\n-            FunnelForSequenceClassification,\n-            FunnelForTokenClassification,\n-            FunnelModel,\n-            FunnelPreTrainedModel,\n-            load_tf_weights_in_funnel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_funnel import (\n-            TFFunnelBaseModel,\n-            TFFunnelForMaskedLM,\n-            TFFunnelForMultipleChoice,\n-            TFFunnelForPreTraining,\n-            TFFunnelForQuestionAnswering,\n-            TFFunnelForSequenceClassification,\n-            TFFunnelForTokenClassification,\n-            TFFunnelModel,\n-            TFFunnelPreTrainedModel,\n-        )\n-\n+    from .configuration_funnel import *\n+    from .convert_funnel_original_tf_checkpoint_to_pytorch import *\n+    from .modeling_funnel import *\n+    from .modeling_tf_funnel import *\n+    from .tokenization_funnel import *\n+    from .tokenization_funnel_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "b164f286042a74bf9908f3f9f9138f5a0d8fb84d",
            "filename": "src/transformers/models/funnel/configuration_funnel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -161,3 +161,6 @@ def num_blocks(self):\n     @num_blocks.setter\n     def num_blocks(self, value):\n         raise NotImplementedError(\"This model does not support the setting of `num_blocks`. Please set `block_sizes`.\")\n+\n+\n+__all__ = [\"FunnelConfig\"]"
        },
        {
            "sha": "37f71c0d233e5e0e7004b03791bf02fb3f01b3ef",
            "filename": "src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconvert_funnel_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconvert_funnel_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconvert_funnel_original_tf_checkpoint_to_pytorch.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -62,3 +62,6 @@ def convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, config_file, pytorch_du\n     convert_tf_checkpoint_to_pytorch(\n         args.tf_checkpoint_path, args.config_file, args.pytorch_dump_path, args.base_model\n     )\n+\n+\n+__all__ = []"
        },
        {
            "sha": "0d2f689da89307abcfd9ad8abb88fd750db82416",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1592,3 +1592,17 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"FunnelBaseModel\",\n+    \"FunnelForMaskedLM\",\n+    \"FunnelForMultipleChoice\",\n+    \"FunnelForPreTraining\",\n+    \"FunnelForQuestionAnswering\",\n+    \"FunnelForSequenceClassification\",\n+    \"FunnelForTokenClassification\",\n+    \"FunnelModel\",\n+    \"FunnelPreTrainedModel\",\n+    \"load_tf_weights_in_funnel\",\n+]"
        },
        {
            "sha": "d4efd7ba0a3aaa41f2c6df4e642271ba43f3aaf3",
            "filename": "src/transformers/models/funnel/modeling_tf_funnel.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1865,3 +1865,16 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFFunnelBaseModel\",\n+    \"TFFunnelForMaskedLM\",\n+    \"TFFunnelForMultipleChoice\",\n+    \"TFFunnelForPreTraining\",\n+    \"TFFunnelForQuestionAnswering\",\n+    \"TFFunnelForSequenceClassification\",\n+    \"TFFunnelForTokenClassification\",\n+    \"TFFunnelModel\",\n+    \"TFFunnelPreTrainedModel\",\n+]"
        },
        {
            "sha": "8cb6f1af0e815334edda71069b698681cf915d2a",
            "filename": "src/transformers/models/funnel/tokenization_funnel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -537,3 +537,6 @@ def tokenize(self, text):\n             else:\n                 output_tokens.extend(sub_tokens)\n         return output_tokens\n+\n+\n+__all__ = [\"FunnelTokenizer\"]"
        },
        {
            "sha": "c3e45ed62ac29cce91c98a9a8b1b649894ba612a",
            "filename": "src/transformers/models/funnel/tokenization_funnel_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -198,3 +198,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"FunnelTokenizerFast\"]"
        },
        {
            "sha": "c2a7d252010e00ec7e3192520ac401b200dc1da9",
            "filename": "src/transformers/models/fuyu/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 53,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 AdeptAI and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,61 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\n-    \"configuration_fuyu\": [\"FuyuConfig\"],\n-}\n-\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_fuyu\"] = [\"FuyuImageProcessor\"]\n-    _import_structure[\"processing_fuyu\"] = [\"FuyuProcessor\"]\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_fuyu\"] = [\n-        \"FuyuForCausalLM\",\n-        \"FuyuPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_fuyu import FuyuConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_fuyu import FuyuImageProcessor\n-        from .processing_fuyu import FuyuProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_fuyu import (\n-            FuyuForCausalLM,\n-            FuyuPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_fuyu import *\n+    from .image_processing_fuyu import *\n+    from .modeling_fuyu import *\n+    from .processing_fuyu import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "23c3d88a8ec1f40f227be4ad299243ef34ef1b10",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -205,3 +205,6 @@ def _rope_scaling_validation(self):\n             )\n         if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n             raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n+\n+\n+__all__ = [\"FuyuConfig\"]"
        },
        {
            "sha": "da9319d3a98aa90c12f54089a18d482a8621f373",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -719,3 +719,6 @@ def preprocess_with_tokenizer_info(\n                 \"image_patch_indices_per_subsequence\": image_patch_indices_per_subsequence,\n             }\n         )\n+\n+\n+__all__ = [\"FuyuImageProcessor\"]"
        },
        {
            "sha": "a7afb411c44805c261e810894e34996e1acdbbc2",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -386,3 +386,6 @@ def _reorder_cache(past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\"FuyuForCausalLM\", \"FuyuPreTrainedModel\"]"
        },
        {
            "sha": "768542a85cbe4697c5e02361e9d62bce567157b1",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -721,3 +721,6 @@ def decode(self, *args, **kwargs):\n         the docstring of this method for more information.\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n+\n+\n+__all__ = [\"FuyuProcessor\"]"
        },
        {
            "sha": "06e3e86927ab7901f1302a87882c4f841f35865d",
            "filename": "src/transformers/models/git/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 38,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,48 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_git\": [\"GitConfig\", \"GitVisionConfig\"],\n-    \"processing_git\": [\"GitProcessor\"],\n-}\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_git\"] = [\n-        \"GitForCausalLM\",\n-        \"GitModel\",\n-        \"GitPreTrainedModel\",\n-        \"GitVisionModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_git import GitConfig, GitVisionConfig\n-    from .processing_git import GitProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_git import (\n-            GitForCausalLM,\n-            GitModel,\n-            GitPreTrainedModel,\n-            GitVisionModel,\n-        )\n-\n+    from .configuration_git import *\n+    from .modeling_git import *\n+    from .processing_git import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6266f0f45de757d18b36b1e8a2d6820db6d1545e",
            "filename": "src/transformers/models/git/configuration_git.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fconfiguration_git.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -217,3 +217,6 @@ def __init__(\n \n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n+\n+\n+__all__ = [\"GitConfig\", \"GitVisionConfig\"]"
        },
        {
            "sha": "662ff0d1ccef402a6ae77192cc9ac04070cc9020",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1644,3 +1644,6 @@ def _reorder_cache(self, past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\"GitForCausalLM\", \"GitModel\", \"GitPreTrainedModel\", \"GitVisionModel\"]"
        },
        {
            "sha": "29f91badc85a9cbabfa42998863a7c80a1d9cf8a",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -146,3 +146,6 @@ def decode(self, *args, **kwargs):\n     @property\n     def model_input_names(self):\n         return [\"input_ids\", \"attention_mask\", \"pixel_values\"]\n+\n+\n+__all__ = [\"GitProcessor\"]"
        },
        {
            "sha": "2a5b38675c34780fd7554db92ea870121f31fd76",
            "filename": "src/transformers/models/glpn/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 53,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,61 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\"configuration_glpn\": [\"GLPNConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"feature_extraction_glpn\"] = [\"GLPNFeatureExtractor\"]\n-    _import_structure[\"image_processing_glpn\"] = [\"GLPNImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_glpn\"] = [\n-        \"GLPNForDepthEstimation\",\n-        \"GLPNLayer\",\n-        \"GLPNModel\",\n-        \"GLPNPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_glpn import GLPNConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .feature_extraction_glpn import GLPNFeatureExtractor\n-        from .image_processing_glpn import GLPNImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_glpn import (\n-            GLPNForDepthEstimation,\n-            GLPNLayer,\n-            GLPNModel,\n-            GLPNPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_glpn import *\n+    from .feature_extraction_glpn import *\n+    from .image_processing_glpn import *\n+    from .modeling_glpn import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "19eb04b19b862962c7febf2ab63a58ead8cb9c45",
            "filename": "src/transformers/models/glpn/configuration_glpn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Fconfiguration_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Fconfiguration_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fconfiguration_glpn.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -130,3 +130,6 @@ def __init__(\n         self.decoder_hidden_size = decoder_hidden_size\n         self.max_depth = max_depth\n         self.head_in_index = head_in_index\n+\n+\n+__all__ = [\"GLPNConfig\"]"
        },
        {
            "sha": "a7f1f5cc85b7c9468681155fcc754768aa78a8af",
            "filename": "src/transformers/models/glpn/feature_extraction_glpn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Ffeature_extraction_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Ffeature_extraction_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Ffeature_extraction_glpn.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -31,3 +31,6 @@ def __init__(self, *args, **kwargs) -> None:\n             FutureWarning,\n         )\n         super().__init__(*args, **kwargs)\n+\n+\n+__all__ = [\"GLPNFeatureExtractor\"]"
        },
        {
            "sha": "0ef93c21d9491eaae1f1c0ca2ce7602443ba023d",
            "filename": "src/transformers/models/glpn/image_processing_glpn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fimage_processing_glpn.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -268,3 +268,6 @@ def post_process_depth_estimation(\n             results.append({\"predicted_depth\": depth})\n \n         return results\n+\n+\n+__all__ = [\"GLPNImageProcessor\"]"
        },
        {
            "sha": "b753db2654938a7942f9c8f3a624c27d088cbd4d",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -771,3 +771,6 @@ def forward(\n             hidden_states=outputs.hidden_states if output_hidden_states else None,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"GLPNForDepthEstimation\", \"GLPNLayer\", \"GLPNModel\", \"GLPNPreTrainedModel\"]"
        },
        {
            "sha": "f01899e668e3a86548db3f59c7f42d70746385ab",
            "filename": "src/transformers/models/gpt2/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 133,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,143 +11,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_keras_nlp_available,\n-    is_tensorflow_text_available,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"configuration_gpt2\": [\"GPT2Config\", \"GPT2OnnxConfig\"],\n-    \"tokenization_gpt2\": [\"GPT2Tokenizer\"],\n-}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gpt2_fast\"] = [\"GPT2TokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt2\"] = [\n-        \"GPT2DoubleHeadsModel\",\n-        \"GPT2ForQuestionAnswering\",\n-        \"GPT2ForSequenceClassification\",\n-        \"GPT2ForTokenClassification\",\n-        \"GPT2LMHeadModel\",\n-        \"GPT2Model\",\n-        \"GPT2PreTrainedModel\",\n-        \"load_tf_weights_in_gpt2\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_gpt2\"] = [\n-        \"TFGPT2DoubleHeadsModel\",\n-        \"TFGPT2ForSequenceClassification\",\n-        \"TFGPT2LMHeadModel\",\n-        \"TFGPT2MainLayer\",\n-        \"TFGPT2Model\",\n-        \"TFGPT2PreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_keras_nlp_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gpt2_tf\"] = [\"TFGPT2Tokenizer\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_gpt2\"] = [\"FlaxGPT2LMHeadModel\", \"FlaxGPT2Model\", \"FlaxGPT2PreTrainedModel\"]\n \n if TYPE_CHECKING:\n-    from .configuration_gpt2 import GPT2Config, GPT2OnnxConfig\n-    from .tokenization_gpt2 import GPT2Tokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gpt2_fast import GPT2TokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt2 import (\n-            GPT2DoubleHeadsModel,\n-            GPT2ForQuestionAnswering,\n-            GPT2ForSequenceClassification,\n-            GPT2ForTokenClassification,\n-            GPT2LMHeadModel,\n-            GPT2Model,\n-            GPT2PreTrainedModel,\n-            load_tf_weights_in_gpt2,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_gpt2 import (\n-            TFGPT2DoubleHeadsModel,\n-            TFGPT2ForSequenceClassification,\n-            TFGPT2LMHeadModel,\n-            TFGPT2MainLayer,\n-            TFGPT2Model,\n-            TFGPT2PreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_keras_nlp_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gpt2_tf import TFGPT2Tokenizer\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_gpt2 import FlaxGPT2LMHeadModel, FlaxGPT2Model, FlaxGPT2PreTrainedModel\n-\n+    from .configuration_gpt2 import *\n+    from .modeling_flax_gpt2 import *\n+    from .modeling_gpt2 import *\n+    from .modeling_tf_gpt2 import *\n+    from .tokenization_gpt2 import *\n+    from .tokenization_gpt2_fast import *\n+    from .tokenization_gpt2_tf import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "f3ebea02496c5057f72b5f1fdb90784f79d407fa",
            "filename": "src/transformers/models/gpt2/configuration_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -268,3 +268,6 @@ def generate_dummy_inputs(\n     @property\n     def default_onnx_opset(self) -> int:\n         return 13\n+\n+\n+__all__ = [\"GPT2Config\", \"GPT2OnnxConfig\"]"
        },
        {
            "sha": "62704d203b0eddd351f9ae065e389482b2f66936",
            "filename": "src/transformers/models/gpt2/modeling_flax_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -777,3 +777,6 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     FlaxCausalLMOutputWithCrossAttentions,\n     _CONFIG_FOR_DOC,\n )\n+\n+\n+__all__ = [\"FlaxGPT2LMHeadModel\", \"FlaxGPT2Model\", \"FlaxGPT2PreTrainedModel\"]"
        },
        {
            "sha": "df3d88eda8cad9c2b75cae0d6d395e62cba49774",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1660,3 +1660,15 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"GPT2DoubleHeadsModel\",\n+    \"GPT2ForQuestionAnswering\",\n+    \"GPT2ForSequenceClassification\",\n+    \"GPT2ForTokenClassification\",\n+    \"GPT2LMHeadModel\",\n+    \"GPT2Model\",\n+    \"GPT2PreTrainedModel\",\n+    \"load_tf_weights_in_gpt2\",\n+]"
        },
        {
            "sha": "f222cdf486fd5b8b4e781efec015fee203bd34ba",
            "filename": "src/transformers/models/gpt2/modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1233,3 +1233,13 @@ def build(self, input_shape=None):\n         if getattr(self, \"transformer\", None) is not None:\n             with tf.name_scope(self.transformer.name):\n                 self.transformer.build(None)\n+\n+\n+__all__ = [\n+    \"TFGPT2DoubleHeadsModel\",\n+    \"TFGPT2ForSequenceClassification\",\n+    \"TFGPT2LMHeadModel\",\n+    \"TFGPT2MainLayer\",\n+    \"TFGPT2Model\",\n+    \"TFGPT2PreTrainedModel\",\n+]"
        },
        {
            "sha": "709bcec5b611231ea8bfc3d72322b0ebeb1a9a1e",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -329,3 +329,6 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n         if is_split_into_words or add_prefix_space:\n             text = \" \" + text\n         return (text, kwargs)\n+\n+\n+__all__ = [\"GPT2Tokenizer\"]"
        },
        {
            "sha": "07b48faad4e35cf91303806929dbf00e1bb72bb0",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -139,3 +139,6 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"GPT2TokenizerFast\"]"
        },
        {
            "sha": "0c0fdb3ae80689e5379916d194f82b1a295293d6",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_tf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -102,3 +102,6 @@ def call(self, x, max_length: int = None):\n                 )\n \n         return {\"attention_mask\": attention_mask, \"input_ids\": input_ids}\n+\n+\n+__all__ = [\"TFGPT2Tokenizer\"]"
        },
        {
            "sha": "92e985d92734550a5b0635941294669386d35749",
            "filename": "src/transformers/models/gpt_bigcode/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 43,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,53 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {\n-    \"configuration_gpt_bigcode\": [\"GPTBigCodeConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt_bigcode\"] = [\n-        \"GPTBigCodeForSequenceClassification\",\n-        \"GPTBigCodeForTokenClassification\",\n-        \"GPTBigCodeForCausalLM\",\n-        \"GPTBigCodeModel\",\n-        \"GPTBigCodePreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_gpt_bigcode import GPTBigCodeConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt_bigcode import (\n-            GPTBigCodeForCausalLM,\n-            GPTBigCodeForSequenceClassification,\n-            GPTBigCodeForTokenClassification,\n-            GPTBigCodeModel,\n-            GPTBigCodePreTrainedModel,\n-        )\n-\n-\n+    from .configuration_gpt_bigcode import *\n+    from .modeling_gpt_bigcode import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "46a3dfea44101fb07666a82bad4a127137486f99",
            "filename": "src/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fconfiguration_gpt_bigcode.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -139,3 +139,6 @@ def __init__(\n         self.eos_token_id = eos_token_id\n \n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+\n+__all__ = [\"GPTBigCodeConfig\"]"
        },
        {
            "sha": "b4237370f1c318ed0f13ed95c080cdfb1ae1b8da",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1419,3 +1419,12 @@ def forward(\n             hidden_states=transformer_outputs.hidden_states,\n             attentions=transformer_outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"GPTBigCodeForSequenceClassification\",\n+    \"GPTBigCodeForTokenClassification\",\n+    \"GPTBigCodeForCausalLM\",\n+    \"GPTBigCodeModel\",\n+    \"GPTBigCodePreTrainedModel\",\n+]"
        },
        {
            "sha": "578577f22882cdc5eea08928e274a18725cf4615",
            "filename": "src/transformers/models/gpt_neo/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 63,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,71 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_flax_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_gpt_neo\": [\"GPTNeoConfig\", \"GPTNeoOnnxConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt_neo\"] = [\n-        \"GPTNeoForCausalLM\",\n-        \"GPTNeoForQuestionAnswering\",\n-        \"GPTNeoForSequenceClassification\",\n-        \"GPTNeoForTokenClassification\",\n-        \"GPTNeoModel\",\n-        \"GPTNeoPreTrainedModel\",\n-        \"load_tf_weights_in_gpt_neo\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_gpt_neo\"] = [\n-        \"FlaxGPTNeoForCausalLM\",\n-        \"FlaxGPTNeoModel\",\n-        \"FlaxGPTNeoPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_gpt_neo import GPTNeoConfig, GPTNeoOnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt_neo import (\n-            GPTNeoForCausalLM,\n-            GPTNeoForQuestionAnswering,\n-            GPTNeoForSequenceClassification,\n-            GPTNeoForTokenClassification,\n-            GPTNeoModel,\n-            GPTNeoPreTrainedModel,\n-            load_tf_weights_in_gpt_neo,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_gpt_neo import FlaxGPTNeoForCausalLM, FlaxGPTNeoModel, FlaxGPTNeoPreTrainedModel\n-\n-\n+    from .configuration_gpt_neo import *\n+    from .modeling_flax_gpt_neo import *\n+    from .modeling_gpt_neo import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "a8f358e7218b8058919393f21b3b474bdb5b21ba",
            "filename": "src/transformers/models/gpt_neo/configuration_gpt_neo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -267,3 +267,6 @@ def generate_dummy_inputs(\n     @property\n     def default_onnx_opset(self) -> int:\n         return 13\n+\n+\n+__all__ = [\"GPTNeoConfig\", \"GPTNeoOnnxConfig\"]"
        },
        {
            "sha": "851c20dfcfefac0cca4142ce9f272f43a67f11a8",
            "filename": "src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -682,3 +682,6 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n \n \n append_call_sample_docstring(FlaxGPTNeoForCausalLM, _CHECKPOINT_FOR_DOC, FlaxCausalLMOutput, _CONFIG_FOR_DOC)\n+\n+\n+__all__ = [\"FlaxGPTNeoForCausalLM\", \"FlaxGPTNeoModel\", \"FlaxGPTNeoPreTrainedModel\"]"
        },
        {
            "sha": "3d30c9260c60910e2c2f3061dfc00c98347b8383",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1334,3 +1334,14 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"GPTNeoForCausalLM\",\n+    \"GPTNeoForQuestionAnswering\",\n+    \"GPTNeoForSequenceClassification\",\n+    \"GPTNeoForTokenClassification\",\n+    \"GPTNeoModel\",\n+    \"GPTNeoPreTrainedModel\",\n+    \"load_tf_weights_in_gpt_neo\",\n+]"
        },
        {
            "sha": "fdf2639019963511e7fed587f636aad5edf96ee9",
            "filename": "src/transformers/models/gpt_neox/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 58,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,66 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...file_utils import _LazyModule, is_tokenizers_available, is_torch_available\n-from ...utils import OptionalDependencyNotAvailable\n-\n-\n-_import_structure = {\"configuration_gpt_neox\": [\"GPTNeoXConfig\"]}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gpt_neox_fast\"] = [\"GPTNeoXTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt_neox\"] = [\n-        \"GPTNeoXForCausalLM\",\n-        \"GPTNeoXForQuestionAnswering\",\n-        \"GPTNeoXForSequenceClassification\",\n-        \"GPTNeoXForTokenClassification\",\n-        \"GPTNeoXLayer\",\n-        \"GPTNeoXModel\",\n-        \"GPTNeoXPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_gpt_neox import GPTNeoXConfig\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gpt_neox_fast import GPTNeoXTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt_neox import (\n-            GPTNeoXForCausalLM,\n-            GPTNeoXForQuestionAnswering,\n-            GPTNeoXForSequenceClassification,\n-            GPTNeoXForTokenClassification,\n-            GPTNeoXLayer,\n-            GPTNeoXModel,\n-            GPTNeoXPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_gpt_neox import *\n+    from .modeling_gpt_neox import *\n+    from .tokenization_gpt_neox_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6d73403ed1b0ce651a5cc7c1aa6d3ae82c1d2d01",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -189,3 +189,6 @@ def __init__(\n             raise ValueError(\n                 \"The hidden size is not divisble by the number of attention heads! Make sure to update them!\"\n             )\n+\n+\n+__all__ = [\"GPTNeoXConfig\"]"
        },
        {
            "sha": "1bc737379c02e5b17d14535fdea493d4c81e916b",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1465,3 +1465,14 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"GPTNeoXForCausalLM\",\n+    \"GPTNeoXForQuestionAnswering\",\n+    \"GPTNeoXForSequenceClassification\",\n+    \"GPTNeoXForTokenClassification\",\n+    \"GPTNeoXLayer\",\n+    \"GPTNeoXModel\",\n+    \"GPTNeoXPreTrainedModel\",\n+]"
        },
        {
            "sha": "1df53f3776d5b92e3baa05ff804f7e5202ac068f",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -228,3 +228,6 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"GPTNeoXTokenizerFast\"]"
        },
        {
            "sha": "94ba39d69ad638c706f6ac8491e2dea80e269929",
            "filename": "src/transformers/models/gpt_neox_japanese/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 40,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,48 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...file_utils import _LazyModule, is_torch_available\n-from ...utils import OptionalDependencyNotAvailable\n-\n-\n-_import_structure = {\n-    \"configuration_gpt_neox_japanese\": [\"GPTNeoXJapaneseConfig\"],\n-    \"tokenization_gpt_neox_japanese\": [\"GPTNeoXJapaneseTokenizer\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gpt_neox_japanese\"] = [\n-        \"GPTNeoXJapaneseForCausalLM\",\n-        \"GPTNeoXJapaneseLayer\",\n-        \"GPTNeoXJapaneseModel\",\n-        \"GPTNeoXJapanesePreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_gpt_neox_japanese import GPTNeoXJapaneseConfig\n-    from .tokenization_gpt_neox_japanese import GPTNeoXJapaneseTokenizer\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gpt_neox_japanese import (\n-            GPTNeoXJapaneseForCausalLM,\n-            GPTNeoXJapaneseLayer,\n-            GPTNeoXJapaneseModel,\n-            GPTNeoXJapanesePreTrainedModel,\n-        )\n-\n-\n+    from .configuration_gpt_neox_japanese import *\n+    from .modeling_gpt_neox_japanese import *\n+    from .tokenization_gpt_neox_japanese import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "650c6124949fff5551f40c2d6b05d90cdec36e98",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -162,3 +162,6 @@ def __init__(\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n+\n+\n+__all__ = [\"GPTNeoXJapaneseConfig\"]"
        },
        {
            "sha": "6c5f4b8240e6db172f5205bede5ab0a0507e6873",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -896,3 +896,11 @@ def _reorder_cache(self, past_key_values, beam_idx):\n                 + layer_past[2:],\n             )\n         return reordered_past\n+\n+\n+__all__ = [\n+    \"GPTNeoXJapaneseForCausalLM\",\n+    \"GPTNeoXJapaneseLayer\",\n+    \"GPTNeoXJapaneseModel\",\n+    \"GPTNeoXJapanesePreTrainedModel\",\n+]"
        },
        {
            "sha": "dbb084e930bd7d6602bf36ff1521f120f664dc85",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -349,3 +349,6 @@ def convert_id_to_token(self, index, breakline=\"\\n\"):\n             words.append(bytearray(byte_tokens).decode(\"utf-8\", errors=\"replace\"))\n         text = \"\".join(words)\n         return text\n+\n+\n+__all__ = [\"GPTNeoXJapaneseTokenizer\"]"
        },
        {
            "sha": "e477eb1d2cc2ff0e34d214ed99ad3f80afe8ab0a",
            "filename": "src/transformers/models/gpt_sw3/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 23,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,33 +11,16 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_sentencepiece_available\n-\n-\n-_import_structure = {}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_gpt_sw3\"] = [\"GPTSw3Tokenizer\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_gpt_sw3 import GPTSw3Tokenizer\n-\n+    from .tokenization_gpt_sw3 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7991988c74849ee8815ea315ebd3235f4c8d4012",
            "filename": "src/transformers/models/gpt_sw3/tokenization_gpt_sw3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Ftokenization_gpt_sw3.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -294,3 +294,6 @@ def decode_fast(self, token_ids: Union[int, List[int]]) -> str:\n         \"\"\"\n \n         return self.sp_model.decode(token_ids)\n+\n+\n+__all__ = [\"GPTSw3Tokenizer\"]"
        },
        {
            "sha": "84d99fda2e6996d80c8dd32cf1247e61fb83230b",
            "filename": "src/transformers/models/gptj/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 90,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The EleutherAI and HuggingFace Teams. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,98 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\"configuration_gptj\": [\"GPTJConfig\", \"GPTJOnnxConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_gptj\"] = [\n-        \"GPTJForCausalLM\",\n-        \"GPTJForQuestionAnswering\",\n-        \"GPTJForSequenceClassification\",\n-        \"GPTJModel\",\n-        \"GPTJPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_gptj\"] = [\n-        \"TFGPTJForCausalLM\",\n-        \"TFGPTJForQuestionAnswering\",\n-        \"TFGPTJForSequenceClassification\",\n-        \"TFGPTJModel\",\n-        \"TFGPTJPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_gptj\"] = [\n-        \"FlaxGPTJForCausalLM\",\n-        \"FlaxGPTJModel\",\n-        \"FlaxGPTJPreTrainedModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_gptj import GPTJConfig, GPTJOnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_gptj import (\n-            GPTJForCausalLM,\n-            GPTJForQuestionAnswering,\n-            GPTJForSequenceClassification,\n-            GPTJModel,\n-            GPTJPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_gptj import (\n-            TFGPTJForCausalLM,\n-            TFGPTJForQuestionAnswering,\n-            TFGPTJForSequenceClassification,\n-            TFGPTJModel,\n-            TFGPTJPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_gptj import FlaxGPTJForCausalLM, FlaxGPTJModel, FlaxGPTJPreTrainedModel\n-\n+    from .configuration_gptj import *\n+    from .modeling_flax_gptj import *\n+    from .modeling_gptj import *\n+    from .modeling_tf_gptj import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "5e76b3f4ba6ee3069e7ba84fd3dbdf96a1b2e847",
            "filename": "src/transformers/models/gptj/configuration_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -214,3 +214,6 @@ def generate_dummy_inputs(\n     @property\n     def default_onnx_opset(self) -> int:\n         return 13\n+\n+\n+__all__ = [\"GPTJConfig\", \"GPTJOnnxConfig\"]"
        },
        {
            "sha": "83abf840ac276d6bda248ca6375bee4443c3e0e7",
            "filename": "src/transformers/models/gptj/modeling_flax_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -716,3 +716,6 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n     FlaxCausalLMOutput,\n     _CONFIG_FOR_DOC,\n )\n+\n+\n+__all__ = [\"FlaxGPTJForCausalLM\", \"FlaxGPTJModel\", \"FlaxGPTJPreTrainedModel\"]"
        },
        {
            "sha": "804218d588f93b40f25eb40c56d5daa8cc8c6ffb",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1396,3 +1396,12 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"GPTJForCausalLM\",\n+    \"GPTJForQuestionAnswering\",\n+    \"GPTJForSequenceClassification\",\n+    \"GPTJModel\",\n+    \"GPTJPreTrainedModel\",\n+]"
        },
        {
            "sha": "a72e1f795d2e1b9564d40865974eabb45c5cdba5",
            "filename": "src/transformers/models/gptj/modeling_tf_gptj.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1096,3 +1096,12 @@ def build(self, input_shape=None):\n         if getattr(self, \"qa_outputs\", None) is not None:\n             with tf.name_scope(self.qa_outputs.name):\n                 self.qa_outputs.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFGPTJForCausalLM\",\n+    \"TFGPTJForQuestionAnswering\",\n+    \"TFGPTJForSequenceClassification\",\n+    \"TFGPTJModel\",\n+    \"TFGPTJPreTrainedModel\",\n+]"
        },
        {
            "sha": "c85333f70b5ee330c9798da1ad624c44c65fc8d4",
            "filename": "src/transformers/models/granitemoe/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgranitemoe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgranitemoe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2024 EleutherAI and The HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,45 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_granitemoe\": [\"GraniteMoeConfig\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_granitemoe\"] = [\n-        \"GraniteMoeForCausalLM\",\n-        \"GraniteMoeModel\",\n-        \"GraniteMoePreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_granitemoe import GraniteMoeConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_granitemoe import (\n-            GraniteMoeForCausalLM,\n-            GraniteMoeModel,\n-            GraniteMoePreTrainedModel,\n-        )\n-\n+    from .configuration_granitemoe import *\n+    from .modeling_granitemoe import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "9ef029f95c3e0cff89492fd1befc42b3c6417c8e",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -189,3 +189,6 @@ def __init__(\n         )\n \n         rope_config_validation(self)\n+\n+\n+__all__ = [\"GraniteMoeConfig\"]"
        },
        {
            "sha": "0acddecdd59082bd2af5383981e05df18ca1b8ef",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1403,3 +1403,6 @@ def _reorder_cache(past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\"GraniteMoeForCausalLM\", \"GraniteMoeModel\", \"GraniteMoePreTrainedModel\"]"
        },
        {
            "sha": "15169ed7f8dee4602e47a9eb975730d63b6ef082",
            "filename": "src/transformers/models/grounding_dino/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 54,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -11,65 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n-\n-\n-_import_structure = {\n-    \"configuration_grounding_dino\": [\"GroundingDinoConfig\"],\n-    \"processing_grounding_dino\": [\"GroundingDinoProcessor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_grounding_dino\"] = [\n-        \"GroundingDinoForObjectDetection\",\n-        \"GroundingDinoModel\",\n-        \"GroundingDinoPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_grounding_dino\"] = [\"GroundingDinoImageProcessor\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_grounding_dino import (\n-        GroundingDinoConfig,\n-    )\n-    from .processing_grounding_dino import GroundingDinoProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_grounding_dino import (\n-            GroundingDinoForObjectDetection,\n-            GroundingDinoModel,\n-            GroundingDinoPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_grounding_dino import GroundingDinoImageProcessor\n-\n+    from .configuration_grounding_dino import *\n+    from .image_processing_grounding_dino import *\n+    from .modeling_grounding_dino import *\n+    from .processing_grounding_dino import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ca8960ee9a96ecf212467c0921d66fe8530f5185",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -293,3 +293,6 @@ def num_attention_heads(self) -> int:\n     @property\n     def hidden_size(self) -> int:\n         return self.d_model\n+\n+\n+__all__ = [\"GroundingDinoConfig\"]"
        },
        {
            "sha": "23f4b719698cb90d915b5c3babfc480317de4494",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1586,3 +1586,6 @@ def post_process_object_detection(\n             results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n \n         return results\n+\n+\n+__all__ = [\"GroundingDinoImageProcessor\"]"
        },
        {
            "sha": "4a101b1d93b4f7909aa81bc4f2c316990c300d42",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -2671,3 +2671,6 @@ def forward(\n         )\n \n         return dict_outputs\n+\n+\n+__all__ = [\"GroundingDinoForObjectDetection\", \"GroundingDinoModel\", \"GroundingDinoPreTrainedModel\"]"
        },
        {
            "sha": "9dbcea643280b85ab78826bcd1f490fbb57c9619",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -235,3 +235,6 @@ def post_process_grounded_object_detection(\n             results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n \n         return results\n+\n+\n+__all__ = [\"GroundingDinoProcessor\"]"
        },
        {
            "sha": "ab7fa27d09d16590d6ba25185c9ef9c4974e2ea1",
            "filename": "src/transformers/models/groupvit/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 71,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,79 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_groupvit\": [\n-        \"GroupViTConfig\",\n-        \"GroupViTOnnxConfig\",\n-        \"GroupViTTextConfig\",\n-        \"GroupViTVisionConfig\",\n-    ],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_groupvit\"] = [\n-        \"GroupViTModel\",\n-        \"GroupViTPreTrainedModel\",\n-        \"GroupViTTextModel\",\n-        \"GroupViTVisionModel\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_groupvit\"] = [\n-        \"TFGroupViTModel\",\n-        \"TFGroupViTPreTrainedModel\",\n-        \"TFGroupViTTextModel\",\n-        \"TFGroupViTVisionModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_groupvit import (\n-        GroupViTConfig,\n-        GroupViTOnnxConfig,\n-        GroupViTTextConfig,\n-        GroupViTVisionConfig,\n-    )\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_groupvit import (\n-            GroupViTModel,\n-            GroupViTPreTrainedModel,\n-            GroupViTTextModel,\n-            GroupViTVisionModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_groupvit import (\n-            TFGroupViTModel,\n-            TFGroupViTPreTrainedModel,\n-            TFGroupViTTextModel,\n-            TFGroupViTVisionModel,\n-        )\n-\n+    from .configuration_groupvit import *\n+    from .modeling_groupvit import *\n+    from .modeling_tf_groupvit import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e55346f503546b8aa8ce2d9eb7e9413858d887a4",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -413,3 +413,6 @@ def generate_dummy_inputs(\n     @property\n     def default_onnx_opset(self) -> int:\n         return 14\n+\n+\n+__all__ = [\"GroupViTConfig\", \"GroupViTOnnxConfig\", \"GroupViTTextConfig\", \"GroupViTVisionConfig\"]"
        },
        {
            "sha": "c2f38ef3dcbe59029daa35548bcb13de44fb2e6c",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1584,3 +1584,6 @@ def forward(\n             text_model_output=text_outputs,\n             vision_model_output=vision_outputs,\n         )\n+\n+\n+__all__ = [\"GroupViTModel\", \"GroupViTPreTrainedModel\", \"GroupViTTextModel\", \"GroupViTVisionModel\"]"
        },
        {
            "sha": "7c6b3d05f32574dfeb313af3b9af510f1b25b3bd",
            "filename": "src/transformers/models/groupvit/modeling_tf_groupvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -2136,3 +2136,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"groupvit\", None) is not None:\n             with tf.name_scope(self.groupvit.name):\n                 self.groupvit.build(None)\n+\n+\n+__all__ = [\"TFGroupViTModel\", \"TFGroupViTPreTrainedModel\", \"TFGroupViTTextModel\", \"TFGroupViTVisionModel\"]"
        },
        {
            "sha": "e0d0794a06e8cfbe4178a72e2b09d5292ddbc4fb",
            "filename": "src/transformers/models/herbert/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 25,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fherbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fherbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,35 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available\n-\n-\n-_import_structure = {\"tokenization_herbert\": [\"HerbertTokenizer\"]}\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_herbert_fast\"] = [\"HerbertTokenizerFast\"]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .tokenization_herbert import HerbertTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_herbert_fast import HerbertTokenizerFast\n-\n+    from .tokenization_herbert import *\n+    from .tokenization_herbert_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "806bf95af8d299047d8aabc6dc1a35d7eaa222fa",
            "filename": "src/transformers/models/herbert/tokenization_herbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -642,3 +642,6 @@ def __setstate__(self, d):\n             )\n \n         self.sm = sacremoses\n+\n+\n+__all__ = [\"HerbertTokenizer\"]"
        },
        {
            "sha": "6b2569307fe71d3c5f691e97f481e0e178c82441",
            "filename": "src/transformers/models/herbert/tokenization_herbert_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert_fast.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -156,3 +156,6 @@ def create_token_type_ids_from_sequences(\n     def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n+\n+\n+__all__ = [\"HerbertTokenizerFast\"]"
        },
        {
            "sha": "841f13be4c0d2f48f54eecc916acd826395449af",
            "filename": "src/transformers/models/hiera/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 38,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhiera%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhiera%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -13,47 +13,15 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_hiera\": [\"HieraConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_hiera\"] = [\n-        \"HieraForImageClassification\",\n-        \"HieraForPreTraining\",\n-        \"HieraBackbone\",\n-        \"HieraModel\",\n-        \"HieraPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_hiera import HieraConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_hiera import (\n-            HieraBackbone,\n-            HieraForImageClassification,\n-            HieraForPreTraining,\n-            HieraModel,\n-            HieraPreTrainedModel,\n-        )\n-\n+    from .configuration_hiera import *\n+    from .modeling_hiera import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "ebd6fe594c0bd70e1817800d70cb70423222e2a9",
            "filename": "src/transformers/models/hiera/configuration_hiera.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhiera%2Fconfiguration_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhiera%2Fconfiguration_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fconfiguration_hiera.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -189,3 +189,6 @@ def __init__(\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n         )\n+\n+\n+__all__ = [\"HieraConfig\"]"
        },
        {
            "sha": "dd602e9f048a517e02c6f4b3906490a703921ceb",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1568,3 +1568,6 @@ def forward(\n             hidden_states=outputs[1] if output_hidden_states else None,\n             attentions=outputs[2] if output_attentions else None,\n         )\n+\n+\n+__all__ = [\"HieraForImageClassification\", \"HieraForPreTraining\", \"HieraBackbone\", \"HieraModel\", \"HieraPreTrainedModel\"]"
        },
        {
            "sha": "d975dabc689a73c83818ced8bed5ad86072df9b2",
            "filename": "src/transformers/models/hubert/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 59,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,67 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tf_available, is_torch_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_hubert\": [\"HubertConfig\"]}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_hubert\"] = [\n-        \"HubertForCTC\",\n-        \"HubertForSequenceClassification\",\n-        \"HubertModel\",\n-        \"HubertPreTrainedModel\",\n-    ]\n-\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_hubert\"] = [\n-        \"TFHubertForCTC\",\n-        \"TFHubertModel\",\n-        \"TFHubertPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_hubert import HubertConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_hubert import (\n-            HubertForCTC,\n-            HubertForSequenceClassification,\n-            HubertModel,\n-            HubertPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_hubert import (\n-            TFHubertForCTC,\n-            TFHubertModel,\n-            TFHubertPreTrainedModel,\n-        )\n-\n-\n+    from .configuration_hubert import *\n+    from .modeling_hubert import *\n+    from .modeling_tf_hubert import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bb5dae7f2cb2e094301c4af1909512beb7473843",
            "filename": "src/transformers/models/hubert/configuration_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -260,3 +260,6 @@ def __init__(\n     @property\n     def inputs_to_logits_ratio(self):\n         return functools.reduce(operator.mul, self.conv_stride, 1)\n+\n+\n+__all__ = [\"HubertConfig\"]"
        },
        {
            "sha": "b986ab863680386debc42a33d73465990b1c2be2",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1650,3 +1650,6 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\"HubertForCTC\", \"HubertForSequenceClassification\", \"HubertModel\", \"HubertPreTrainedModel\"]"
        },
        {
            "sha": "8971656365b7020896b0f14a81cbd0bc26066949",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1670,3 +1670,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"lm_head\", None) is not None:\n             with tf.name_scope(self.lm_head.name):\n                 self.lm_head.build([None, None, self.output_hidden_size])\n+\n+\n+__all__ = [\"TFHubertForCTC\", \"TFHubertModel\", \"TFHubertPreTrainedModel\"]"
        },
        {
            "sha": "cf34ec43ac1014d8c153b3aa259e394fc7b73570",
            "filename": "src/transformers/models/ibert/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 40,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fibert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fibert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,50 +11,17 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n-\n-\n-_import_structure = {\"configuration_ibert\": [\"IBertConfig\", \"IBertOnnxConfig\"]}\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_ibert\"] = [\n-        \"IBertForMaskedLM\",\n-        \"IBertForMultipleChoice\",\n-        \"IBertForQuestionAnswering\",\n-        \"IBertForSequenceClassification\",\n-        \"IBertForTokenClassification\",\n-        \"IBertModel\",\n-        \"IBertPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_ibert import IBertConfig, IBertOnnxConfig\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_ibert import (\n-            IBertForMaskedLM,\n-            IBertForMultipleChoice,\n-            IBertForQuestionAnswering,\n-            IBertForSequenceClassification,\n-            IBertForTokenClassification,\n-            IBertModel,\n-            IBertPreTrainedModel,\n-        )\n-\n+    from .configuration_ibert import *\n+    from .modeling_ibert import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6ddc344b9e10ca945d5a75b8ab9eaf672881e14e",
            "filename": "src/transformers/models/ibert/configuration_ibert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -137,3 +137,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"attention_mask\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"IBertConfig\", \"IBertOnnxConfig\"]"
        },
        {
            "sha": "a1470d6a3b7632b371a1f3237fd15451f994e4b7",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1353,3 +1353,14 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n     mask = input_ids.ne(padding_idx).int()\n     incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n     return incremental_indices.long() + padding_idx\n+\n+\n+__all__ = [\n+    \"IBertForMaskedLM\",\n+    \"IBertForMultipleChoice\",\n+    \"IBertForQuestionAnswering\",\n+    \"IBertForSequenceClassification\",\n+    \"IBertForTokenClassification\",\n+    \"IBertModel\",\n+    \"IBertPreTrainedModel\",\n+]"
        },
        {
            "sha": "4adb66825445f25a1c34bd7b3b86e60eed7be85f",
            "filename": "src/transformers/models/idefics/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 79,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2022 The HuggingFace Team. All rights reserved.\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,87 +13,18 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_tf_available,\n-    is_torch_available,\n-    is_vision_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_idefics\": [\"IdeficsConfig\"]}\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_idefics\"] = [\"IdeficsImageProcessor\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_idefics\"] = [\n-        \"IdeficsForVisionText2Text\",\n-        \"IdeficsModel\",\n-        \"IdeficsPreTrainedModel\",\n-    ]\n-    _import_structure[\"processing_idefics\"] = [\"IdeficsProcessor\"]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_idefics\"] = [\n-        \"TFIdeficsForVisionText2Text\",\n-        \"TFIdeficsModel\",\n-        \"TFIdeficsPreTrainedModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_idefics import IdeficsConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_idefics import IdeficsImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_idefics import (\n-            IdeficsForVisionText2Text,\n-            IdeficsModel,\n-            IdeficsPreTrainedModel,\n-        )\n-        from .processing_idefics import IdeficsProcessor\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_idefics import (\n-            TFIdeficsForVisionText2Text,\n-            TFIdeficsModel,\n-            TFIdeficsPreTrainedModel,\n-        )\n-\n+    from .configuration_idefics import *\n+    from .image_processing_idefics import *\n+    from .modeling_idefics import *\n+    from .modeling_tf_idefics import *\n+    from .processing_idefics import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e8320b98725d0b02c85783464efe11dbb0e8f0a8",
            "filename": "src/transformers/models/idefics/configuration_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fconfiguration_idefics.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -320,3 +320,6 @@ def __init__(\n         # updates the config object with `kwargs` from from_pretrained, so during the instantiation\n         # of this object many attributes have default values and haven't yet been overridden.\n         # Do any required checks inside `from_pretrained` once the superclass' `from_pretrained` was run.\n+\n+\n+__all__ = [\"IdeficsConfig\"]"
        },
        {
            "sha": "2b317da05d498b7a3306c46b642935328babaecd",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -166,3 +166,6 @@ def preprocess(\n         images = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)[\"pixel_values\"]\n \n         return images\n+\n+\n+__all__ = [\"IdeficsImageProcessor\"]"
        },
        {
            "sha": "e917efbc73d7dcb00c2b4c2c3fb841bd47e7e985",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1747,3 +1747,6 @@ def _reorder_cache(past, beam_idx):\n         for layer_past in past:\n             reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n         return reordered_past\n+\n+\n+__all__ = [\"IdeficsForVisionText2Text\", \"IdeficsModel\", \"IdeficsPreTrainedModel\"]"
        },
        {
            "sha": "f412d28aa80f693e07b2781c62297655cd989421",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1810,3 +1810,6 @@ def build(self, input_shape=None):\n         if getattr(self, \"lm_head\", None) is not None:\n             with tf.name_scope(self.lm_head.name):\n                 self.lm_head.build(None)\n+\n+\n+__all__ = [\"TFIdeficsForVisionText2Text\", \"TFIdeficsModel\", \"TFIdeficsPreTrainedModel\"]"
        },
        {
            "sha": "444a970e5a3606d938cd7efe2f61fd0dc9509334",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -539,3 +539,6 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"IdeficsProcessor\"]"
        },
        {
            "sha": "b76a53a081fe9ae1f21bf673c921bbfb595e3f15",
            "filename": "src/transformers/models/idefics2/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 51,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -13,60 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_idefics2\": [\"Idefics2Config\"]}\n-\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_idefics2\"] = [\"Idefics2ImageProcessor\"]\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_idefics2\"] = [\n-        \"Idefics2ForConditionalGeneration\",\n-        \"Idefics2PreTrainedModel\",\n-        \"Idefics2Model\",\n-    ]\n-    _import_structure[\"processing_idefics2\"] = [\"Idefics2Processor\"]\n-\n if TYPE_CHECKING:\n-    from .configuration_idefics2 import Idefics2Config\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_idefics2 import Idefics2ImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_idefics2 import (\n-            Idefics2ForConditionalGeneration,\n-            Idefics2Model,\n-            Idefics2PreTrainedModel,\n-        )\n-        from .processing_idefics2 import Idefics2Processor\n-\n-\n+    from .configuration_idefics2 import *\n+    from .image_processing_idefics2 import *\n+    from .modeling_idefics2 import *\n+    from .processing_idefics2 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "2f0376a8952db16b41d568e6c00dcba4e2584528",
            "filename": "src/transformers/models/idefics2/configuration_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconfiguration_idefics2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -259,3 +259,6 @@ def __init__(\n             )\n \n         super().__init__(**kwargs, tie_word_embeddings=tie_word_embeddings)\n+\n+\n+__all__ = [\"Idefics2Config\"]"
        },
        {
            "sha": "22ef9c2eac6e4812ff06e05084cb1dcd561ab57b",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -595,3 +595,6 @@ def preprocess(\n             data[\"pixel_attention_mask\"] = np.array(pixel_attention_mask) if do_pad else pixel_attention_mask\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Idefics2ImageProcessor\"]"
        },
        {
            "sha": "bdae2c70d5f28b1c8fd43a2af20b7ef52dcf08ef",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1752,3 +1752,6 @@ def _reorder_cache(past_key_values, beam_idx):\n                 tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n         return reordered_past\n+\n+\n+__all__ = [\"Idefics2ForConditionalGeneration\", \"Idefics2PreTrainedModel\", \"Idefics2Model\"]"
        },
        {
            "sha": "8c1647cdff41a77115d40541812a50b80403f21a",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -278,3 +278,6 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"Idefics2Processor\"]"
        },
        {
            "sha": "34bf9c962b65420b6e101091549b8245ad394f9b",
            "filename": "src/transformers/models/idefics3/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 53,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -13,62 +13,17 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\"configuration_idefics3\": [\"Idefics3Config\", \"Idefics3VisionConfig\"]}\n-\n-\n-try:\n-    if not is_vision_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"image_processing_idefics3\"] = [\"Idefics3ImageProcessor\"]\n-\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_idefics3\"] = [\n-        \"Idefics3ForConditionalGeneration\",\n-        \"Idefics3PreTrainedModel\",\n-        \"Idefics3Model\",\n-        \"Idefics3VisionTransformer\",\n-    ]\n-    _import_structure[\"processing_idefics3\"] = [\"Idefics3Processor\"]\n-\n if TYPE_CHECKING:\n-    from .configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n-\n-    try:\n-        if not is_vision_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .image_processing_idefics3 import Idefics3ImageProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_idefics3 import (\n-            Idefics3ForConditionalGeneration,\n-            Idefics3Model,\n-            Idefics3PreTrainedModel,\n-            Idefics3VisionTransformer,\n-        )\n-        from .processing_idefics3 import Idefics3Processor\n-\n-\n+    from .configuration_idefics3 import *\n+    from .image_processing_idefics3 import *\n+    from .modeling_idefics3 import *\n+    from .processing_idefics3 import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "01c96afcaa47bb2621ceba6d9487c2b3484c14a4",
            "filename": "src/transformers/models/idefics3/configuration_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconfiguration_idefics3.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -185,3 +185,6 @@ def __init__(\n         self.scale_factor = scale_factor\n \n         super().__init__(**kwargs, pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings)\n+\n+\n+__all__ = [\"Idefics3Config\", \"Idefics3VisionConfig\"]"
        },
        {
            "sha": "9005c38af6c07246724ea5a5978a853294cd51b0",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -883,3 +883,6 @@ def preprocess(\n             encoding[\"cols\"] = images_list_cols\n \n         return encoding\n+\n+\n+__all__ = [\"Idefics3ImageProcessor\"]"
        },
        {
            "sha": "e4ab65e7b777d5a5aa3868fc5997c58c0bfd1fd1",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -1315,3 +1315,6 @@ def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_\n         # Get the precomputed image_hidden_states\n         model_kwargs[\"image_hidden_states\"] = outputs.image_hidden_states\n         return model_kwargs\n+\n+\n+__all__ = [\"Idefics3ForConditionalGeneration\", \"Idefics3PreTrainedModel\", \"Idefics3Model\", \"Idefics3VisionTransformer\"]"
        },
        {
            "sha": "40c8829fe76eae816e7cbc2aad24f109b881bf6e",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2f2977533445c4f62bf58e10b1360e6856e78ce/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=b2f2977533445c4f62bf58e10b1360e6856e78ce",
            "patch": "@@ -355,3 +355,6 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"Idefics3Processor\"]"
        }
    ],
    "stats": {
        "total": 18480,
        "additions": 5236,
        "deletions": 13244
    }
}