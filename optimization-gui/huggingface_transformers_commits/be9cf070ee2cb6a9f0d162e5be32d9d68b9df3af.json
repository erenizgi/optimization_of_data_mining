{
    "author": "Isotr0py",
    "message": "Fix Llava conversion for LlavaQwen2ForCausalLM with Clip vision tower (#33613)\n\nfix llavaqwen2 model conversion",
    "sha": "be9cf070ee2cb6a9f0d162e5be32d9d68b9df3af",
    "files": [
        {
            "sha": "b8d936e8cc4473083c2b6f9aec43e79861aa2c7d",
            "filename": "src/transformers/models/llava/convert_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be9cf070ee2cb6a9f0d162e5be32d9d68b9df3af/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be9cf070ee2cb6a9f0d162e5be32d9d68b9df3af/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py?ref=be9cf070ee2cb6a9f0d162e5be32d9d68b9df3af",
            "patch": "@@ -76,7 +76,9 @@ def load_original_state_dict(model_id):\n     if \"lm_head.weight\" not in original_state_dict:\n         original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n \n-    del original_state_dict[\"model.image_newline\"]  # not used in the original implementation because \"merge_type=flat\"\n+    if \"model.image_newline\" in original_state_dict:\n+        # not used in the original implementation because \"merge_type=flat\"\n+        del original_state_dict[\"model.image_newline\"]\n     return original_state_dict\n \n \n@@ -107,7 +109,7 @@ def convert_llava_llama_to_hf(text_model_id, vision_model_id, output_hub_path, o\n     image_processor = AutoImageProcessor.from_pretrained(vision_model_id)\n     processor = LlavaProcessor(tokenizer=tokenizer, image_processor=image_processor)\n \n-    if \"Qwen\" in text_model_id:\n+    if \"siglip\" in vision_model_id:\n         vision_config = SiglipVisionConfig(\n             hidden_size=1152,\n             image_size=384,\n@@ -128,8 +130,9 @@ def convert_llava_llama_to_hf(text_model_id, vision_model_id, output_hub_path, o\n     # llms-lab interleeave models do not use any selection startegy except for last hidden state\n     if \"Qwen\" in text_model_id:\n         config.image_token_index = 151646\n-        config.vision_feature_select_strategy = \"full\"\n-        config.vision_feature_layer = -1\n+        if \"siglip\" in vision_model_id:\n+            config.vision_feature_select_strategy = \"full\"\n+            config.vision_feature_layer = -1\n     else:\n         config.pad_token_id = 32001\n         config.image_token_index = 32000"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 7,
        "deletions": 4
    }
}