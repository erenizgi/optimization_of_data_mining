{
    "author": "loadams",
    "message": "Update checks for torch.distributed.tensor to require torch >= 2.5 (#34816)\n\n* Update checks for torch.distributed.tensor\r\n\r\n* Update PR with feedback\r\n\r\n* Formatting fix for import order\r\n\r\n* Remove unused function",
    "sha": "1867be666d4e18c69874d4bf35f55f4cde86040c",
    "files": [
        {
            "sha": "4703c415e42fbbb87ec8692197ae0876a54c5b52",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1867be666d4e18c69874d4bf35f55f4cde86040c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1867be666d4e18c69874d4bf35f55f4cde86040c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1867be666d4e18c69874d4bf35f55f4cde86040c",
            "patch": "@@ -52,7 +52,6 @@\n     find_pruneable_heads_and_indices,\n     id_tensor_storage,\n     is_torch_greater_or_equal_than_1_13,\n-    is_torch_greater_or_equal_than_2_4,\n     prune_conv1d_layer,\n     prune_layer,\n     prune_linear_layer,\n@@ -90,6 +89,7 @@\n     is_peft_available,\n     is_remote_url,\n     is_safetensors_available,\n+    is_torch_greater_or_equal,\n     is_torch_sdpa_available,\n     is_torch_xla_available,\n     logging,\n@@ -5032,7 +5032,7 @@ def tensor_parallel(self, device_mesh):\n             device_mesh (`torch.distributed.DeviceMesh`):\n                 The device mesh to use for tensor parallelism.\n         \"\"\"\n-        if not is_torch_greater_or_equal_than_2_4:\n+        if not is_torch_greater_or_equal(\"2.5\"):\n             raise EnvironmentError(\"tensor parallel is only supported for `torch>=2.5`.\")\n \n         # Tensor parallelize a nn.Module based on the `_tp_plan` attribute of the module."
        },
        {
            "sha": "5bdf8a355ddfaad03189b368f2e30af6cca31daa",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1867be666d4e18c69874d4bf35f55f4cde86040c/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1867be666d4e18c69874d4bf35f55f4cde86040c/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=1867be666d4e18c69874d4bf35f55f4cde86040c",
            "patch": "@@ -21,7 +21,7 @@\n from safetensors.torch import storage_ptr, storage_size\n from torch import nn\n \n-from .utils import is_torch_xla_available, logging\n+from .utils import is_torch_greater_or_equal, is_torch_xla_available, logging\n \n \n ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n@@ -39,7 +39,7 @@\n is_torch_greater_or_equal_than_1_12 = parsed_torch_version_base >= version.parse(\"1.12\")\n \n \n-if is_torch_greater_or_equal_than_2_4:\n+if is_torch_greater_or_equal(\"2.5\"):\n     from torch.distributed.tensor import Replicate\n     from torch.distributed.tensor.parallel import (\n         ColwiseParallel,"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}