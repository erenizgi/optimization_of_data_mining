{
    "author": "yonigozlan",
    "message": "Support having multiple sub-processors (of any kind) in the same processor (#42667)\n\n* support saving/loading multiple sub_processor of the same kind\n\n* standardize all processors\n\n* remove tokenizer_class from lasr\n\n* fix modular\n\n* fix kwargs logic\n\n* override _load_tokenizer_from_pretrained in pixtral and fuyu\n\n* fix missing subfolder loading primary tokenizer\n\n* remove Fast suffix in tokenization auto\n\n* revert to ParakeetTokenizerFast (not yet updated)\n\n* Set TestMistralCommonBackend as flaky",
    "sha": "dd24a80666b72c85f02c6cf9df18164cc174ab74",
    "files": [
        {
            "sha": "b53dcd16546448fac1b2e36ed97ef8e4d7d14686",
            "filename": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -74,10 +74,6 @@ class AudioFlamingo3Processor(ProcessorMixin):\n             Special token used to represent audio inputs in the chat template.\n     \"\"\"\n \n-    attributes = [\"feature_extractor\", \"tokenizer\"]\n-    feature_extractor_class = \"WhisperFeatureExtractor\"\n-    tokenizer_class = \"Qwen2TokenizerFast\"\n-\n     def __init__(\n         self,\n         feature_extractor,"
        },
        {
            "sha": "edec34a56fca2e442468d4e46f37774c74c1f932",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -38,6 +38,7 @@\n FEATURE_EXTRACTOR_MAPPING_NAMES = OrderedDict(\n     [\n         (\"audio-spectrogram-transformer\", \"ASTFeatureExtractor\"),\n+        (\"audioflamingo3\", \"WhisperFeatureExtractor\"),\n         (\"clap\", \"ClapFeatureExtractor\"),\n         (\"clvp\", \"ClvpFeatureExtractor\"),\n         (\"csm\", \"EncodecFeatureExtractor\"),"
        },
        {
            "sha": "d84c2f2c6be80e2990c11062a62f024d05bdf425",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -93,6 +93,8 @@\n         (\"kosmos-2\", \"Kosmos2Processor\"),\n         (\"kosmos-2.5\", \"Kosmos2_5Processor\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextProcessor\"),\n+        (\"lasr_ctc\", \"LasrProcessor\"),\n+        (\"lasr_encoder\", \"LasrProcessor\"),\n         (\"layoutlmv2\", \"LayoutLMv2Processor\"),\n         (\"layoutlmv3\", \"LayoutLMv3Processor\"),\n         (\"layoutxlm\", \"LayoutXLMProcessor\"),"
        },
        {
            "sha": "e18047f527f680befe91cff4ef6c8e339609a886",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -68,6 +68,7 @@\n         (\"align\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"arcee\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"aria\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n+        (\"audioflamingo3\", \"Qwen2Tokenizer\" if is_tokenizers_available() else None),\n         (\"aya_vision\", \"CohereTokenizer\" if is_tokenizers_available() else None),\n         (\"bark\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"bart\", \"RobertaTokenizer\" if is_tokenizers_available() else None),\n@@ -182,6 +183,8 @@\n         (\"jetmoe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"kosmos-2\", \"XLMRobertaTokenizer\" if is_tokenizers_available() else None),\n         (\"kosmos-2.5\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"lasr_ctc\", \"ParakeetTokenizerFast\" if is_tokenizers_available() else None),\n+        (\"lasr_encoder\", \"ParakeetTokenizerFast\" if is_tokenizers_available() else None),\n         (\"layoutlm\", \"BertTokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlmv2\", \"LayoutLMv2Tokenizer\" if is_tokenizers_available() else None),\n         (\"layoutlmv3\", \"LayoutLMv3Tokenizer\" if is_tokenizers_available() else None),"
        },
        {
            "sha": "fecd9d28fcf3c91c4663534c22121fcbe45dabd3",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -347,6 +347,22 @@ class FuyuProcessor(ProcessorMixin):\n             The tokenizer is a required input.\n     \"\"\"\n \n+    @classmethod\n+    def _load_tokenizer_from_pretrained(\n+        cls, sub_processor_type, pretrained_model_name_or_path, subfolder=\"\", **kwargs\n+    ):\n+        \"\"\"\n+        Override for BC. Fuyu uses TokenizersBackend and requires token_type_ids to be removed from model_input_names\n+        because Fuyu uses mm_token_type_ids instead for multimodal token identification.    `\n+        \"\"\"\n+        from ...tokenization_utils_tokenizers import TokenizersBackend\n+\n+        tokenizer = TokenizersBackend.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+        # Remove token_type_ids as Fuyu uses mm_token_type_ids instead\n+        if \"token_type_ids\" in tokenizer.model_input_names:\n+            tokenizer.model_input_names.remove(\"token_type_ids\")\n+        return tokenizer\n+\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor=image_processor, tokenizer=tokenizer)\n         self.image_processor = image_processor"
        },
        {
            "sha": "dfe7ec6e1b694e5876d5a71e690348c9a488589d",
            "filename": "src/transformers/models/lasr/modular_lasr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fmodular_lasr.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -97,7 +97,7 @@ def _decode(\n \n \n class LasrProcessor(ParakeetProcessor):\n-    tokenizer_class = \"ParakeetTokenizerFast\"\n+    pass\n \n \n class LasrEncoderConfig(ParakeetEncoderConfig):"
        },
        {
            "sha": "7a4661c6a6ce8d1d09a5aee5979a919b7af805e7",
            "filename": "src/transformers/models/lasr/processing_lasr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -47,8 +47,6 @@ class LasrProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class LasrProcessor(ProcessorMixin):\n-    tokenizer_class = \"ParakeetTokenizerFast\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "cde0898218784f260c98840d4b0a2ab03dc94e59",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -58,8 +58,6 @@ class Phi4MultimodalProcessor(ProcessorMixin):\n             The fake audio token pattern.\n     \"\"\"\n \n-    audio_processor_class = \"Phi4MultimodalFeatureExtractor\"\n-\n     def __init__(\n         self,\n         image_processor,"
        },
        {
            "sha": "3ce09bf9d7fc2d7b0141740a82cdb11cf8d1df64",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -61,10 +61,6 @@ class Pix2StructProcessor(ProcessorMixin):\n             An instance of ['T5Tokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"Pix2StructImageProcessor\"\n-    tokenizer_class = (\"T5Tokenizer\",)\n-\n     def __init__(self, image_processor, tokenizer):\n         tokenizer.return_token_type_ids = False\n         super().__init__(image_processor, tokenizer)"
        },
        {
            "sha": "7b898242102dd31a14299189955b717f889927d9",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -87,6 +87,25 @@ class PixtralProcessor(ProcessorMixin):\n             Special token used to denote the end of an image input.\n     \"\"\"\n \n+    @classmethod\n+    def _load_tokenizer_from_pretrained(\n+        cls, sub_processor_type, pretrained_model_name_or_path, subfolder=\"\", **kwargs\n+    ):\n+        \"\"\"\n+        Override for BC. Pixtral requires a modified pre_tokenizer with ByteLevel prepended to handle\n+        the specific tokenization format expected by pretrained Pixtral models.\n+        \"\"\"\n+        from tokenizers import pre_tokenizers\n+\n+        from ...models.llama import LlamaTokenizer\n+\n+        tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n+        # Add ByteLevel pre_tokenizer before the existing one\n+        tokenizer._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [pre_tokenizers.ByteLevel(False), tokenizer._tokenizer.pre_tokenizer]\n+        )\n+        return tokenizer\n+\n     def __init__(\n         self,\n         image_processor=None,"
        },
        {
            "sha": "f1ed964704f1ba161d62c7d6cf08c036569cdf7f",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 118,
            "deletions": 43,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -129,6 +129,26 @@ def keys(self):\n     \"video_processor\": \"BaseVideoProcessor\",\n }\n \n+\n+def _get_modality_for_attribute(attribute_name: str) -> str:\n+    \"\"\"\n+    Get the canonical modality type for a given attribute name.\n+\n+    For example:\n+    - \"image_processor\" -> \"image_processor\"\n+    - \"encoder_image_processor\" -> \"image_processor\"\n+    - \"text_tokenizer\" -> \"tokenizer\"\n+    - \"my_feature_extractor\" -> \"feature_extractor\"\n+    \"\"\"\n+    for modality in MODALITY_TO_AUTOPROCESSOR_MAPPING.keys():\n+        if modality in attribute_name:\n+            return modality\n+    raise ValueError(\n+        f\"Cannot determine modality for attribute '{attribute_name}'. \"\n+        f\"Attribute name must contain one of: {list(MODALITY_TO_AUTOPROCESSOR_MAPPING.keys())}\"\n+    )\n+\n+\n if sys.version_info >= (3, 11):\n     Unpack = typing.Unpack\n else:\n@@ -663,8 +683,10 @@ def check_argument_for_proper_class(self, argument_name, argument):\n         mismatch between expected and actual class, an error is raise. Otherwise, the proper retrieved class\n         is returned.\n         \"\"\"\n-        if argument_name not in MODALITY_TO_BASE_CLASS_MAPPING and \"tokenizer\" in argument_name:\n-            argument_name = \"tokenizer\"\n+        # If the exact attribute name is not in the mapping, use its canonical modality\n+        # (e.g., \"encoder_tokenizer\" -> \"tokenizer\")\n+        if argument_name not in MODALITY_TO_BASE_CLASS_MAPPING:\n+            argument_name = _get_modality_for_attribute(argument_name)\n         class_name = MODALITY_TO_BASE_CLASS_MAPPING.get(argument_name)\n         if isinstance(class_name, tuple):\n             proper_class = tuple(self.get_possibly_dynamic_module(n) for n in class_name if n is not None)\n@@ -695,9 +717,13 @@ def to_dict(self) -> dict[str, Any]:\n         # extra attributes to be kept\n         attrs_to_save += [\"auto_map\"]\n \n+        # Remove tokenizers from output - they have their own vocab files and are saved separately.\n+        # All other sub-processors (image_processor, feature_extractor, etc.) are kept in processor_config.json.\n         for attribute in self.__class__.get_attributes():\n-            if \"tokenizer\" in attribute and attribute in output:\n-                del output[attribute]\n+            if attribute in output:\n+                modality = _get_modality_for_attribute(attribute)\n+                if modality == \"tokenizer\":\n+                    del output[attribute]\n \n         if \"chat_template\" in output:\n             del output[\"chat_template\"]\n@@ -805,15 +831,16 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         for attribute_name in self.get_attributes():\n             attribute = getattr(self, attribute_name)\n \n-            # Save the tokenizer in its own vocab file. The other attributes are saved as part of `processor_config.json`\n-            if attribute_name == \"tokenizer\":\n+            modality = _get_modality_for_attribute(attribute_name)\n+            is_primary = attribute_name == modality\n+            if modality == \"tokenizer\":\n                 attribute._set_processor_class(self.__class__.__name__)\n-                attribute.save_pretrained(save_directory)\n-            # if a model has multiple tokenizers, save the additional tokenizers in their own folders.\n-            # Note that the additional tokenizers must have \"tokenizer\" in their attribute name.\n-            elif \"tokenizer\" in attribute_name:\n-                attribute._set_processor_class(self.__class__.__name__)\n-                attribute.save_pretrained(os.path.join(save_directory, attribute_name))\n+                # Save the tokenizer in its own vocab file. The other attributes are saved as part of `processor_config.json`\n+                if is_primary:\n+                    attribute.save_pretrained(save_directory)\n+                else:\n+                    # if a model has multiple tokenizers, save the additional tokenizers in their own folders.\n+                    attribute.save_pretrained(os.path.join(save_directory, attribute_name))\n             elif attribute._auto_class is not None:\n                 custom_object_save(attribute, save_directory, config=attribute)\n \n@@ -1381,9 +1408,10 @@ def from_pretrained(\n         if token is not None:\n             kwargs[\"token\"] = token\n \n-        args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n-        processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n-        return cls.from_args_and_dict(args, processor_dict, **kwargs)\n+        # Get processor_dict first so we can use it to instantiate non-tokenizer sub-processors\n+        processor_dict, instantiation_kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n+        args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, processor_dict, **kwargs)\n+        return cls.from_args_and_dict(args, processor_dict, **instantiation_kwargs)\n \n     @classmethod\n     def get_attributes(cls):\n@@ -1393,7 +1421,7 @@ def get_attributes(cls):\n             # don't treat audio_tokenizer as an attribute\n             if sub_processor_type == \"audio_tokenizer\":\n                 continue\n-            if sub_processor_type in MODALITY_TO_AUTOPROCESSOR_MAPPING or \"tokenizer\" in sub_processor_type:\n+            if any(modality in sub_processor_type for modality in MODALITY_TO_AUTOPROCESSOR_MAPPING.keys()):\n                 attributes.append(sub_processor_type)\n \n         # Legacy processors may not override `__init__` and instead expose modality\n@@ -1407,7 +1435,7 @@ def get_attributes(cls):\n                 inferred_attribute = attribute_name[: -len(\"_class\")]\n                 if inferred_attribute == \"audio_tokenizer\":\n                     continue\n-                if inferred_attribute in MODALITY_TO_AUTOPROCESSOR_MAPPING or \"tokenizer\" in inferred_attribute:\n+                if any(modality in inferred_attribute for modality in MODALITY_TO_AUTOPROCESSOR_MAPPING.keys()):\n                     attributes.append(inferred_attribute)\n \n         return attributes\n@@ -1435,49 +1463,96 @@ def register_for_auto_class(cls, auto_class=\"AutoProcessor\"):\n         cls._auto_class = auto_class\n \n     @classmethod\n-    def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n+    def _load_tokenizer_from_pretrained(\n+        cls, sub_processor_type, pretrained_model_name_or_path, subfolder=\"\", **kwargs\n+    ):\n+        auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[\"tokenizer\"]\n+        is_primary = sub_processor_type == \"tokenizer\"\n+\n+        if is_primary:\n+            # Primary tokenizer: load from root\n+            tokenizer = auto_processor_class.from_pretrained(\n+                pretrained_model_name_or_path, subfolder=subfolder, **kwargs\n+            )\n+        else:\n+            # Additional tokenizer: load from subfolder (e.g., \"decoder_tokenizer\")\n+            tokenizer_subfolder = os.path.join(subfolder, sub_processor_type) if subfolder else sub_processor_type\n+            tokenizer = auto_processor_class.from_pretrained(\n+                pretrained_model_name_or_path, subfolder=tokenizer_subfolder, **kwargs\n+            )\n+        return tokenizer\n+\n+    @classmethod\n+    def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, processor_dict=None, **kwargs):\n         \"\"\"\n         Identify and instantiate the subcomponents of Processor classes, such as image processors, tokenizers,\n         and feature extractors. This method inspects the processor's `__init__` signature to identify parameters\n         that correspond to known modality types (image_processor, tokenizer, feature_extractor, etc.) or contain\n-        \"tokenizer\" in their name. It then uses the appropriate Auto class (AutoImageProcessor, AutoTokenizer, etc.)\n-        from `MODALITY_TO_AUTOPROCESSOR_MAPPING` to load each subcomponent via `.from_pretrained()`. For tokenizer-like\n-        parameters not explicitly in the mapping, the method uses AutoTokenizer with a subfolder argument.\n+        modality names in their attribute name.\n+\n+        For tokenizers: Uses the appropriate Auto class (AutoTokenizer) to load via `.from_pretrained()`.\n+        Additional tokenizers (e.g., \"decoder_tokenizer\") are loaded from subfolders.\n+\n+        For other sub-processors (image_processor, feature_extractor, etc.): Primary ones are loaded via\n+        Auto class. Additional ones are instantiated from the config stored in processor_config.json\n+        (passed as processor_dict).\n+\n+        Args:\n+            pretrained_model_name_or_path: Path or model id to load from.\n+            processor_dict: Optional dict containing processor config (from processor_config.json).\n+                Required when loading additional non-tokenizer sub-processors.\n         \"\"\"\n         args = []\n+        processor_dict = processor_dict if processor_dict is not None else {}\n+        # Remove subfolder from kwargs to avoid duplicate keyword arguments\n+        subfolder = kwargs.pop(\"subfolder\", \"\")\n+\n         # get args from processor init signature\n         sub_processors = cls.get_attributes()\n         for sub_processor_type in sub_processors:\n-            if \"FuyuProcessor\" in cls.__name__ and \"tokenizer\" in sub_processor_type:\n-                from .tokenization_utils_tokenizers import TokenizersBackend\n+            modality = _get_modality_for_attribute(sub_processor_type)\n+            is_primary = sub_processor_type == modality\n \n-                tokenizer = TokenizersBackend.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-                if \"token_type_ids\" in tokenizer.model_input_names:\n-                    tokenizer.model_input_names.remove(\"token_type_ids\")\n-                args.append(tokenizer)\n-            elif \"PixtralProcessor\" in cls.__name__ and \"tokenizer\" in sub_processor_type:\n-                from tokenizers import pre_tokenizers\n-\n-                from .models.llama import LlamaTokenizer\n-\n-                tokenizer = LlamaTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-                tokenizer._tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n-                    [pre_tokenizers.ByteLevel(False), tokenizer._tokenizer.pre_tokenizer]\n+            if \"tokenizer\" in sub_processor_type:\n+                tokenizer = cls._load_tokenizer_from_pretrained(\n+                    sub_processor_type, pretrained_model_name_or_path, subfolder=subfolder, **kwargs\n                 )\n                 args.append(tokenizer)\n-            elif sub_processor_type in MODALITY_TO_AUTOPROCESSOR_MAPPING:\n+            elif is_primary:\n+                # Primary non-tokenizer sub-processor: load via Auto class\n                 auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[sub_processor_type]\n-                sub_processor = auto_processor_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n-                args.append(sub_processor)\n-            elif \"tokenizer\" in sub_processor_type:\n-                # Special case: tokenizer-like parameters not in the mapping (e.g., \"protein_tokenizer\")\n-                # Load using AutoTokenizer with subfolder\n-                auto_processor_class = MODALITY_TO_AUTOPROCESSOR_MAPPING[\"tokenizer\"]\n                 sub_processor = auto_processor_class.from_pretrained(\n-                    pretrained_model_name_or_path, subfolder=sub_processor_type, **kwargs\n+                    pretrained_model_name_or_path, subfolder=subfolder, **kwargs\n                 )\n                 args.append(sub_processor)\n \n+            elif sub_processor_type in processor_dict:\n+                # Additional non-tokenizer sub-processor: instantiate from config in processor_dict\n+                sub_processor_config = processor_dict[sub_processor_type]\n+                if isinstance(sub_processor_config, dict):\n+                    # Determine the class to instantiate\n+                    # Image processors have 'image_processor_type', feature extractors have 'feature_extractor_type'\n+                    type_key = f\"{modality}_type\"\n+                    class_name = sub_processor_config.get(type_key)\n+                    if class_name is None:\n+                        raise ValueError(\n+                            f\"Cannot instantiate {sub_processor_type}: missing '{type_key}' in config. \"\n+                            f\"Config keys: {list(sub_processor_config.keys())}\"\n+                        )\n+                    processor_class = cls.get_possibly_dynamic_module(class_name)\n+                    sub_processor = processor_class(**sub_processor_config)\n+                    args.append(sub_processor)\n+                else:\n+                    raise ValueError(\n+                        f\"Expected dict for {sub_processor_type} in processor_config.json, \"\n+                        f\"got {type(sub_processor_config)}\"\n+                    )\n+            else:\n+                raise ValueError(\n+                    f\"Cannot find config for {sub_processor_type} in processor_config.json. \"\n+                    f\"Available keys: {list(processor_dict.keys())}\"\n+                )\n+\n         return args\n \n     @staticmethod"
        },
        {
            "sha": "a177ebc941e39b5067c0286e60011299e868f356",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 113,
            "deletions": 0,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74",
            "patch": "@@ -35,13 +35,15 @@\n     AutoTokenizer,\n     BaseVideoProcessor,\n     BertTokenizer,\n+    CLIPImageProcessorFast,\n     FeatureExtractionMixin,\n     ImageProcessingMixin,\n     LlamaTokenizer,\n     LlavaOnevisionVideoProcessor,\n     LlavaProcessor,\n     ProcessorMixin,\n     SiglipImageProcessor,\n+    SiglipImageProcessorFast,\n     Wav2Vec2Config,\n     Wav2Vec2FeatureExtractor,\n     Wav2Vec2Processor,\n@@ -430,6 +432,117 @@ def test_auto_processor_save_load(self):\n             second_processor = AutoProcessor.from_pretrained(tmp_dir)\n             self.assertEqual(second_processor.__class__.__name__, processor.__class__.__name__)\n \n+    def test_processor_with_multiple_tokenizers_save_load(self):\n+        \"\"\"Test that processors with multiple tokenizers save and load correctly.\"\"\"\n+\n+        class DualTokenizerProcessor(ProcessorMixin):\n+            \"\"\"A processor with two tokenizers and an image processor.\"\"\"\n+\n+            def __init__(self, tokenizer, decoder_tokenizer, image_processor):\n+                super().__init__(tokenizer, decoder_tokenizer, image_processor)\n+\n+        # Create processor with multiple tokenizers\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-BertForMaskedLM\")\n+        decoder_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+        image_processor = SiglipImageProcessor()\n+\n+        processor = DualTokenizerProcessor(\n+            tokenizer=tokenizer,\n+            decoder_tokenizer=decoder_tokenizer,\n+            image_processor=image_processor,\n+        )\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            processor.save_pretrained(tmp_dir)\n+\n+            # Verify directory structure: primary tokenizer in root, additional in subfolder\n+            self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"tokenizer_config.json\")))\n+            self.assertTrue(os.path.isdir(os.path.join(tmp_dir, \"decoder_tokenizer\")))\n+            self.assertTrue(os.path.exists(os.path.join(tmp_dir, \"decoder_tokenizer\", \"tokenizer_config.json\")))\n+\n+            # Verify processor_config.json contains image_processor but not tokenizers\n+            with open(os.path.join(tmp_dir, \"processor_config.json\")) as f:\n+                processor_config = json.load(f)\n+            self.assertIn(\"image_processor\", processor_config)\n+            self.assertNotIn(\"tokenizer\", processor_config)\n+            self.assertNotIn(\"decoder_tokenizer\", processor_config)\n+\n+            # Reload the full processor and verify all attributes\n+            loaded_processor = DualTokenizerProcessor.from_pretrained(tmp_dir)\n+\n+            # Verify the processor has all expected attributes\n+            self.assertTrue(hasattr(loaded_processor, \"tokenizer\"))\n+            self.assertTrue(hasattr(loaded_processor, \"decoder_tokenizer\"))\n+            self.assertTrue(hasattr(loaded_processor, \"image_processor\"))\n+\n+            # Verify tokenizers loaded correctly\n+            self.assertEqual(loaded_processor.tokenizer.vocab_size, tokenizer.vocab_size)\n+            self.assertEqual(loaded_processor.decoder_tokenizer.vocab_size, decoder_tokenizer.vocab_size)\n+\n+            # Verify image processor loaded correctly\n+            self.assertEqual(loaded_processor.image_processor.size, image_processor.size)\n+\n+    def test_processor_with_multiple_image_processors_save_load(self):\n+        \"\"\"Test that processors with multiple image processors save and load correctly.\"\"\"\n+\n+        class DualImageProcessorProcessor(ProcessorMixin):\n+            \"\"\"A processor with two image processors and a tokenizer.\"\"\"\n+\n+            def __init__(self, tokenizer, image_processor, encoder_image_processor):\n+                super().__init__(tokenizer, image_processor, encoder_image_processor)\n+\n+        # Create processor with multiple image processors\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-BertForMaskedLM\")\n+        image_processor = SiglipImageProcessorFast(size={\"height\": 224, \"width\": 224})\n+        encoder_image_processor = CLIPImageProcessorFast(size={\"height\": 384, \"width\": 384})\n+\n+        processor = DualImageProcessorProcessor(\n+            tokenizer=tokenizer,\n+            image_processor=image_processor,\n+            encoder_image_processor=encoder_image_processor,\n+        )\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            processor.save_pretrained(tmp_dir)\n+\n+            # Verify processor_config.json contains both image processors\n+            with open(os.path.join(tmp_dir, \"processor_config.json\")) as f:\n+                processor_config = json.load(f)\n+            self.assertIn(\"image_processor\", processor_config)\n+            self.assertIn(\"encoder_image_processor\", processor_config)\n+            self.assertNotIn(\"tokenizer\", processor_config)\n+\n+            # Verify both image processors have the correct type key for instantiation\n+            self.assertIn(\"image_processor_type\", processor_config[\"image_processor\"])\n+            self.assertIn(\"image_processor_type\", processor_config[\"encoder_image_processor\"])\n+            self.assertEqual(processor_config[\"image_processor\"][\"image_processor_type\"], \"SiglipImageProcessorFast\")\n+            self.assertEqual(\n+                processor_config[\"encoder_image_processor\"][\"image_processor_type\"], \"CLIPImageProcessorFast\"\n+            )\n+\n+            # Verify the sizes are different (to ensure they're separate configs)\n+            self.assertEqual(processor_config[\"image_processor\"][\"size\"], {\"height\": 224, \"width\": 224})\n+            self.assertEqual(processor_config[\"encoder_image_processor\"][\"size\"], {\"height\": 384, \"width\": 384})\n+\n+            # Reload the full processor and verify all attributes\n+            loaded_processor = DualImageProcessorProcessor.from_pretrained(tmp_dir)\n+\n+            # Verify the processor has all expected attributes\n+            self.assertTrue(hasattr(loaded_processor, \"tokenizer\"))\n+            self.assertTrue(hasattr(loaded_processor, \"image_processor\"))\n+            self.assertTrue(hasattr(loaded_processor, \"encoder_image_processor\"))\n+\n+            # Verify tokenizer loaded correctly\n+            self.assertEqual(loaded_processor.tokenizer.vocab_size, tokenizer.vocab_size)\n+\n+            # Verify image processors loaded correctly with their distinct sizes\n+            self.assertEqual(loaded_processor.image_processor.size, {\"height\": 224, \"width\": 224})\n+            self.assertEqual(loaded_processor.encoder_image_processor.size, {\"height\": 384, \"width\": 384})\n+\n+            # Verify they are different types\n+            self.assertIsInstance(loaded_processor.image_processor, SiglipImageProcessorFast)\n+            self.assertIsInstance(loaded_processor.encoder_image_processor, CLIPImageProcessorFast)\n+\n \n @is_staging_test\n class ProcessorPushToHubTester(unittest.TestCase):"
        },
        {
            "sha": "ea108896cab5198136996a5f98bae45371d803bf",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd24a80666b72c85f02c6cf9df18164cc174ab74/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd24a80666b72c85f02c6cf9df18164cc174ab74/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=dd24a80666b72c85f02c6cf9df18164cc174ab74"
        }
    ],
    "stats": {
        "total": 335,
        "additions": 278,
        "deletions": 57
    }
}