{
    "author": "cyyever",
    "message": "Enable more ruff UP rules (#40579)\n\n* Import Sequence from collections.abc\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Apply ruff UP rules\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "a470f213961f9ef39e70bb9b6fd91817f096a2b8",
    "files": [
        {
            "sha": "7784580e033ce19688f2bcb697d2ac72a75a7b58",
            "filename": "examples/pytorch/text-generation/run_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -63,7 +63,7 @@\n )\n logger = logging.getLogger(__name__)\n \n-MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n+MAX_LENGTH = 10000  # Hardcoded max length to avoid infinite loop\n \n MODEL_CLASSES = {\n     \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),"
        },
        {
            "sha": "5d3a9436eb3f0dcacba98317b36bc765f4821107",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -22,17 +22,23 @@ line-length = 119\n # SIM300: Yoda condition detected\n # SIM212: Checks for if expressions that check against a negated condition.\n # SIM905: Consider using a list literal instead of `str.split`\n-ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\", \"SIM1\", \"SIM300\", \"SIM212\", \"SIM905\"]\n+# UP009: UTF-8 encoding declaration is unnecessary\n+# UP015: Unnecessary mode argument\n+# UP031: Use format specifiers instead of percent format\n+# UP004: Class `XXX` inherits from `object`\n+# UP028: Checks for for loops that can be replaced with yield from expressions\n+ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\", \"SIM1\", \"SIM300\", \"SIM212\", \"SIM905\", \"UP009\", \"UP015\", \"UP031\", \"UP028\", \"UP004\"]\n # RUF013: Checks for the use of implicit Optional\n #  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"UP006\", \"PERF102\", \"PLC1802\", \"PLC0208\",\"SIM\"]\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"PERF102\", \"PLC1802\", \"PLC0208\", \"SIM\", \"UP\"]\n extend-safe-fixes = [\"UP006\"]\n \n # Ignore import violations in all `__init__.py` files.\n [tool.ruff.lint.per-file-ignores]\n \"__init__.py\" = [\"E402\", \"F401\", \"F403\", \"F811\"]\n \"src/transformers/file_utils.py\" = [\"F401\"]\n \"src/transformers/utils/dummy_*.py\" = [\"F401\"]\n+\"examples/legacy/**/*.py\" = [\"UP\"]\n \n [tool.ruff.lint.isort]\n lines-after-imports = 2"
        },
        {
            "sha": "e848f558738ca2164503a7aacb90ff3f392334bb",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -21,8 +21,9 @@\n import io\n import os\n import warnings\n+from collections.abc import Sequence\n from io import BytesIO\n-from typing import Any, Optional, Sequence, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import requests"
        },
        {
            "sha": "48197bac0e606068b56673b8f174db199cb49720",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -734,7 +734,7 @@ def get_all_models():\n \n         uvicorn.run(app, host=self.args.host, port=self.args.port, log_level=self.args.log_level)\n \n-    @functools.lru_cache(maxsize=None)\n+    @functools.cache\n     def get_gen_models(self) -> list[dict[str, any]]:\n         \"\"\"\n         This is by no means a limit to which models may be instantiated with `transformers serve`: any chat-based"
        },
        {
            "sha": "4f4a599693dd07178d6c6a82aa84c9c966402c63",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -95,7 +95,7 @@ def __call__(self, *args, **kwargs):\n \n \n @auto_docstring\n-class GenericForSequenceClassification(object):\n+class GenericForSequenceClassification:\n     base_model_prefix = \"model\"\n \n     def __init__(self, config):\n@@ -170,7 +170,7 @@ def forward(\n \n \n @auto_docstring\n-class GenericForQuestionAnswering(object):\n+class GenericForQuestionAnswering:\n     base_model_prefix = \"model\"\n \n     def __init__(self, config):\n@@ -231,7 +231,7 @@ def forward(\n \n \n @auto_docstring\n-class GenericForTokenClassification(object):\n+class GenericForTokenClassification:\n     base_model_prefix = \"model\"\n \n     def __init__(self, config):"
        },
        {
            "sha": "f11dd2a9137e2b544a914d0786d93ace8f523146",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -994,7 +994,7 @@ def __call__(self, text=None, sequence=None, image_size=None, parse_tasks=None)\n                     instances = self.parse_description_with_bboxes_from_text_and_spans(text, image_size=image_size)\n                 parsed_dict[\"description_with_bboxes_or_polygons\"] = instances\n             else:\n-                raise ValueError(\"task {} is not supported\".format(task))\n+                raise ValueError(f\"task {task} is not supported\")\n \n         return parsed_dict\n "
        },
        {
            "sha": "9e56d2e3d8fc6032bcdce985fccbe3784457c202",
            "filename": "src/transformers/models/florence2/processing_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -795,7 +795,7 @@ def __call__(self, text=None, sequence=None, image_size=None, parse_tasks=None)\n                     instances = self.parse_description_with_bboxes_from_text_and_spans(text, image_size=image_size)\n                 parsed_dict[\"description_with_bboxes_or_polygons\"] = instances\n             else:\n-                raise ValueError(\"task {} is not supported\".format(task))\n+                raise ValueError(f\"task {task} is not supported\")\n \n         return parsed_dict\n "
        },
        {
            "sha": "6b77bbf766c15da591d6f41d8878ebedae7147c7",
            "filename": "src/transformers/models/gemma3n/convert_gemma3n_weights.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -519,7 +519,7 @@ def convert_vision_weights(\n     weights: np.ndarray,\n ) -> Iterable[tuple[str, np.ndarray]]:\n     def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]]:\n-        re_str = r\"{}(\\d+)/\".format(block_type)\n+        re_str = rf\"{block_type}(\\d+)/\"\n         re_pattern = re.compile(re_str)\n         match = re.search(re_pattern, path).group(1)\n         idx = abs(int(match)) - 1"
        },
        {
            "sha": "4bce174cc9e914cd60d3f3d1520075f6eaac52f3",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -1487,9 +1487,7 @@ def forward(\n         if input_points is not None and input_boxes is not None:\n             if input_points.shape[1] != input_boxes.shape[1]:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        input_points.shape[1], input_boxes.shape[1]\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {input_points.shape[1]} and {input_boxes.shape[1]}.\"\n                 )\n \n         image_positional_embeddings = self.get_image_wide_positional_embeddings()"
        },
        {
            "sha": "3d856ad411888ec62c2029e037c5c7534563a653",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -1384,9 +1384,7 @@ def forward(\n         if input_points is not None and input_boxes is not None:\n             if input_points.shape[1] != input_boxes.shape[1]:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        input_points.shape[1], input_boxes.shape[1]\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {input_points.shape[1]} and {input_boxes.shape[1]}.\"\n                 )\n \n         image_positional_embeddings = self.get_image_wide_positional_embeddings()"
        },
        {
            "sha": "0a0a308c1fe9de812246f575ad4f70124e49f8fc",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -21,8 +21,9 @@\n \n import math\n from collections import OrderedDict\n+from collections.abc import Iterator\n from dataclasses import dataclass\n-from typing import Any, Callable, Iterator, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -988,7 +989,7 @@ def __init__(self, config: Sam2VideoConfig):\n         )  # pointwise/1x1 convs, implemented with linear layers\n         self.pointwise_conv2 = nn.Linear(config.memory_fuser_intermediate_dim, config.memory_fuser_embed_dim)\n         self.scale = nn.Parameter(\n-            config.memory_fuser_layer_scale_init_value * torch.ones((config.memory_fuser_embed_dim)),\n+            config.memory_fuser_layer_scale_init_value * torch.ones(config.memory_fuser_embed_dim),\n             requires_grad=True,\n         )\n \n@@ -1923,9 +1924,7 @@ def _single_frame_forward(\n         if input_points is not None and input_boxes is not None:\n             if input_points.shape[1] != input_boxes.shape[1]:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        input_points.shape[1], input_boxes.shape[1]\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {input_points.shape[1]} and {input_boxes.shape[1]}.\"\n                 )\n         elif input_points is not None:\n             num_objects = input_points.shape[1]"
        },
        {
            "sha": "0b6cef4e910a3cc7397cd323232f98932d9436a8",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -16,8 +16,9 @@\n \n import math\n from collections import OrderedDict\n+from collections.abc import Iterator\n from dataclasses import dataclass\n-from typing import Any, Callable, Iterator, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -1326,7 +1327,7 @@ def __init__(self, config: Sam2VideoConfig):\n         )  # pointwise/1x1 convs, implemented with linear layers\n         self.pointwise_conv2 = nn.Linear(config.memory_fuser_intermediate_dim, config.memory_fuser_embed_dim)\n         self.scale = nn.Parameter(\n-            config.memory_fuser_layer_scale_init_value * torch.ones((config.memory_fuser_embed_dim)),\n+            config.memory_fuser_layer_scale_init_value * torch.ones(config.memory_fuser_embed_dim),\n             requires_grad=True,\n         )\n \n@@ -1634,9 +1635,7 @@ def _single_frame_forward(\n         if input_points is not None and input_boxes is not None:\n             if input_points.shape[1] != input_boxes.shape[1]:\n                 raise ValueError(\n-                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n-                        input_points.shape[1], input_boxes.shape[1]\n-                    )\n+                    f\"You should provide as many bounding boxes as input points per box. Got {input_points.shape[1]} and {input_boxes.shape[1]}.\"\n                 )\n         elif input_points is not None:\n             num_objects = input_points.shape[1]"
        },
        {
            "sha": "11afd3d4326c81fac9a1020dedd03fe43e009b66",
            "filename": "src/transformers/pipelines/keypoint_matching.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fpipelines%2Fkeypoint_matching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fpipelines%2Fkeypoint_matching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fkeypoint_matching.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Sequence, TypedDict, Union\n+from collections.abc import Sequence\n+from typing import Any, TypedDict, Union\n \n from typing_extensions import TypeAlias, overload\n \n@@ -29,8 +30,16 @@\n \n ImagePair: TypeAlias = Sequence[Union[\"Image.Image\", str]]\n \n-Keypoint = TypedDict(\"Keypoint\", {\"x\": float, \"y\": float})\n-Match = TypedDict(\"Match\", {\"keypoint_image_0\": Keypoint, \"keypoint_image_1\": Keypoint, \"score\": float})\n+\n+class Keypoint(TypedDict):\n+    x: float\n+    y: float\n+\n+\n+class Match(TypedDict):\n+    keypoint_image_0: Keypoint\n+    keypoint_image_1: Keypoint\n+    score: float\n \n \n def validate_image_pairs(images: Any) -> Sequence[Sequence[ImagePair]]:"
        },
        {
            "sha": "8b72069e48073cebc35813949721a74a85c871b8",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a470f213961f9ef39e70bb9b6fd91817f096a2b8/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=a470f213961f9ef39e70bb9b6fd91817f096a2b8",
            "patch": "@@ -694,8 +694,8 @@ def get_video_processor_dict(\n                     _raise_exceptions_for_missing_entries=False,\n                 )\n                 resolved_video_processor_file = resolved_video_processor_files[0]\n-            except EnvironmentError:\n-                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n+            except OSError:\n+                # Raise any OS error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n                 raise\n             except Exception:"
        }
    ],
    "stats": {
        "total": 74,
        "additions": 42,
        "deletions": 32
    }
}