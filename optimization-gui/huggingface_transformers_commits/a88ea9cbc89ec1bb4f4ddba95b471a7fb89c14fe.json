{
    "author": "sbucaille",
    "message": "Add EfficientLoFTR model (#36355)\n\n* initial commit\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* fix: various typos, typehints, refactors from suggestions\n\n* fix: fine_matching method\n\n* Added EfficientLoFTRModel and AutoModelForKeypointMatching class\n\n* fix: got rid of compilation breaking instructions\n\n* docs: added todo for plot\n\n* fix: used correct hub repo\n\n* docs: added comments\n\n* fix: run modular\n\n* doc: added PyTorch badge\n\n* fix: model repo typo in config\n\n* fix: make modular\n\n* fix: removed mask values from outputs\n\n* feat: added plot_keypoint_matching to EfficientLoFTRImageProcessor\n\n* feat: added SuperGlueForKeypointMatching to AutoModelForKeypointMatching list\n\n* fix: reformat\n\n* refactor: renamed aggregation_sizes config parameter into q, kv aggregation kernel size and stride\n\n* doc: added q, kv aggregation kernel size and stride doc to config\n\n* refactor: converted efficientloftr implementation from modular to copied from mechanism\n\n* tests: overwrote batching_equivalence for \"keypoints\" specific tests\n\n* fix: changed EfficientLoFTRConfig import in test_modeling_rope_utils\n\n* fix: make fix-copies\n\n* fix: make style\n\n* fix: update rope function to make meta tests pass\n\n* fix: rename plot_keypoint_matching to visualize_output for clarity\n\n* refactor: optimize image pair processing by removing redundant target size calculations\n\n* feat: add EfficientLoFTRImageProcessor to image processor mapping\n\n* refactor: removed logger and updated attention forward\n\n* refactor: added auto_docstring and can_return_tuple decorators\n\n* refactor: update type imports\n\n* refactor: update type hints from List/Dict to list/dict for consistency\n\n* refactor: update MODEL_MAPPING_NAMES and __all__ to include LightGlue and AutoModelForKeypointMatching\n\n* fix: change type hint for size parameter in EfficientLoFTRImageProcessor to Optional[dict]\n\n* fix typing\n\n* fix some typing issues\n\n* nit\n\n* a few more typehint fixes\n\n* Remove output_attentions and output_hidden_states from modeling code\n\n* else -> elif to support efficientloftr\n\n* nit\n\n* tests: added EfficientLoFTR image processor tests\n\n* refactor: reorder functions\n\n* chore: update copyright year in EfficientLoFTR test file\n\n* Use default rope\n\n* Add docs\n\n* Update visualization method\n\n* fix doc order\n\n* remove 2d rope test\n\n* Update src/transformers/models/efficientloftr/modeling_efficientloftr.py\n\n* fix docs\n\n* Update src/transformers/models/efficientloftr/image_processing_efficientloftr.py\n\n* update gradient\n\n* refactor: removed unused codepath\n\n* Add motivation to keep postprocessing in modeling code\n\n* refactor: removed unnecessary variable declarations\n\n* docs: use load_image from image_utils\n\n* refactor: moved stage in and out channels computation to configuration\n\n* refactor: set an intermediate_size parameter to be more explicit\n\n* refactor: removed all mentions of attention masks as they are not used\n\n* refactor: moved position_embeddings to be computed once in the model instead of every layer\n\n* refactor: removed unnecessary hidden expansion parameter from config\n\n* refactor: removed completely hidden expansions\n\n* refactor: removed position embeddings slice function\n\n* tests: fixed broken tests because of previous commit\n\n* fix is_grayscale typehint\n\n* not refactoring\n\n* not renaming\n\n* move h/w to embeddings class\n\n* Precompute embeddings in init\n\n* fix: replaced cuda device in convert script to accelerate device\n\n* fix: replaced stevenbucaille repo to zju-community\n\n* Remove accelerator.device from conversion script\n\n* refactor: moved parameter computation in configuration instead of figuring it out when instantiating a Module\n\n* fix: removed unused attributes in configuration\n\n* fix: missing self\n\n* fix: refactoring and tests\n\n* fix: make style\n\n---------\n\nCo-authored-by: steven <steven.bucaille@buawei.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
    "files": [
        {
            "sha": "37b6fa60d28984aaee74ef7b21699445b8b1469d",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -747,6 +747,8 @@\n         title: DPT\n       - local: model_doc/efficientformer\n         title: EfficientFormer\n+      - local: model_doc/efficientloftr\n+        title: EfficientLoFTR\n       - local: model_doc/efficientnet\n         title: EfficientNet\n       - local: model_doc/eomt"
        },
        {
            "sha": "326d8e2d2d3517ef538356c9d550673828920e7f",
            "filename": "docs/source/en/model_doc/auto.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -258,6 +258,10 @@ The following auto classes are available for the following computer vision tasks\n \n [[autodoc]] AutoModelForKeypointDetection\n \n+### AutoModelForKeypointMatching\n+\n+[[autodoc]] AutoModelForKeypointMatching\n+\n ### AutoModelForMaskedImageModeling\n \n [[autodoc]] AutoModelForMaskedImageModeling"
        },
        {
            "sha": "0e84d94c10fb29303acbe803f9e8234722e73005",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "added",
            "additions": 114,
            "deletions": 0,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,114 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the MIT License; you may not use this file except in compliance with\n+the License.\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+\n+-->\n+\n+# EfficientLoFTR\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+The EfficientLoFTR model was proposed in [Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed](https://arxiv.org/abs/2403.04765) by Yifan Wang, Xingyi He, Sida Peng, Dongli Tan and Xiaowei Zhou.\n+\n+This model consists of matching two images together by finding pixel correspondences. It can be used to estimate the pose between them. \n+This model is useful for tasks such as image matching, homography estimation, etc.\n+\n+The abstract from the paper is the following:\n+\n+*We present a novel method for efficiently producing semidense matches across images. Previous detector-free matcher \n+LoFTR has shown remarkable matching capability in handling large-viewpoint change and texture-poor scenarios but suffers\n+from low efficiency. We revisit its design choices and derive multiple improvements for both efficiency and accuracy. \n+One key observation is that performing the transformer over the entire feature map is redundant due to shared local \n+information, therefore we propose an aggregated attention mechanism with adaptive token selection for efficiency. \n+Furthermore, we find spatial variance exists in LoFTR’s fine correlation module, which is adverse to matching accuracy. \n+A novel two-stage correlation layer is proposed to achieve accurate subpixel correspondences for accuracy improvement. \n+Our efficiency optimized model is ∼ 2.5× faster than LoFTR which can even surpass state-of-the-art efficient sparse \n+matching pipeline SuperPoint + LightGlue. Moreover, extensive experiments show that our method can achieve higher \n+accuracy compared with competitive semi-dense matchers, with considerable efficiency benefits. This opens up exciting \n+prospects for large-scale or latency-sensitive applications such as image retrieval and 3D reconstruction. \n+Project page: [https://zju3dv.github.io/efficientloftr/](https://zju3dv.github.io/efficientloftr/).*\n+\n+## How to use\n+\n+Here is a quick example of using the model. \n+```python\n+import torch\n+\n+from transformers import AutoImageProcessor, AutoModelForKeypointMatching\n+from transformers.image_utils import load_image\n+\n+\n+image1 = load_image(\"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\")\n+image2 = load_image(\"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\")\n+\n+images = [image1, image2]\n+\n+processor = AutoImageProcessor.from_pretrained(\"stevenbucaille/efficientloftr\")\n+model = AutoModelForKeypointMatching.from_pretrained(\"stevenbucaille/efficientloftr\")\n+\n+inputs = processor(images, return_tensors=\"pt\")\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+```\n+\n+You can use the `post_process_keypoint_matching` method from the `ImageProcessor` to get the keypoints and matches in a more readable format:\n+\n+```python\n+image_sizes = [[(image.height, image.width) for image in images]]\n+outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+for i, output in enumerate(outputs):\n+    print(\"For the image pair\", i)\n+    for keypoint0, keypoint1, matching_score in zip(\n+            output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n+    ):\n+        print(\n+            f\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n+        )\n+```\n+\n+From the post processed outputs, you can visualize the matches between the two images using the following code:\n+```python\n+images_with_matching = processor.visualize_keypoint_matching(images, outputs)\n+```\n+\n+![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/2nJZQlFToCYp_iLurvcZ4.png)\n+\n+This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n+The original code can be found [here](https://github.com/zju3dv/EfficientLoFTR).\n+\n+## EfficientLoFTRConfig\n+\n+[[autodoc]] EfficientLoFTRConfig\n+\n+## EfficientLoFTRImageProcessor\n+\n+[[autodoc]] EfficientLoFTRImageProcessor\n+\n+- preprocess\n+- post_process_keypoint_matching\n+- visualize_keypoint_matching\n+\n+## EfficientLoFTRModel\n+\n+[[autodoc]] EfficientLoFTRModel\n+\n+- forward\n+\n+## EfficientLoFTRForKeypointMatching\n+\n+[[autodoc]] EfficientLoFTRForKeypointMatching\n+\n+- forward\n\\ No newline at end of file"
        },
        {
            "sha": "738d2ab83ccd7003c3c6b0f6e5900b2df2b3aa08",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -102,6 +102,7 @@\n     from .dots1 import *\n     from .dpr import *\n     from .dpt import *\n+    from .efficientloftr import *\n     from .efficientnet import *\n     from .electra import *\n     from .emu3 import *"
        },
        {
            "sha": "78c2ad034b1dae33a2574857b01c64cb5132f761",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -121,6 +121,7 @@\n         (\"dpr\", \"DPRConfig\"),\n         (\"dpt\", \"DPTConfig\"),\n         (\"efficientformer\", \"EfficientFormerConfig\"),\n+        (\"efficientloftr\", \"EfficientLoFTRConfig\"),\n         (\"efficientnet\", \"EfficientNetConfig\"),\n         (\"electra\", \"ElectraConfig\"),\n         (\"emu3\", \"Emu3Config\"),\n@@ -515,6 +516,7 @@\n         (\"dpr\", \"DPR\"),\n         (\"dpt\", \"DPT\"),\n         (\"efficientformer\", \"EfficientFormer\"),\n+        (\"efficientloftr\", \"EfficientLoFTR\"),\n         (\"efficientnet\", \"EfficientNet\"),\n         (\"electra\", \"ELECTRA\"),\n         (\"emu3\", \"Emu3\"),"
        },
        {
            "sha": "775d94b25b91a847766e3157604ff7934668dddd",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -85,6 +85,7 @@\n             (\"donut-swin\", (\"DonutImageProcessor\", \"DonutImageProcessorFast\")),\n             (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n+            (\"efficientloftr\", (\"EfficientLoFTRImageProcessor\",)),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),"
        },
        {
            "sha": "20f039b22b53934e848148b19be60a9981509de9",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -114,6 +114,7 @@\n         (\"dpr\", \"DPRQuestionEncoder\"),\n         (\"dpt\", \"DPTModel\"),\n         (\"efficientformer\", \"EfficientFormerModel\"),\n+        (\"efficientloftr\", \"EfficientLoFTRModel\"),\n         (\"efficientnet\", \"EfficientNetModel\"),\n         (\"electra\", \"ElectraModel\"),\n         (\"emu3\", \"Emu3Model\"),\n@@ -322,7 +323,6 @@\n         (\"squeezebert\", \"SqueezeBertModel\"),\n         (\"stablelm\", \"StableLmModel\"),\n         (\"starcoder2\", \"Starcoder2Model\"),\n-        (\"superglue\", \"SuperGlueForKeypointMatching\"),\n         (\"swiftformer\", \"SwiftFormerModel\"),\n         (\"swin\", \"SwinModel\"),\n         (\"swin2sr\", \"Swin2SRModel\"),\n@@ -1607,6 +1607,13 @@\n     ]\n )\n \n+MODEL_FOR_KEYPOINT_MATCHING_MAPPING_NAMES = OrderedDict(\n+    [\n+        (\"efficientloftr\", \"EfficientLoFTRForKeypointMatching\"),\n+        (\"lightglue\", \"LightGlueForKeypointMatching\"),\n+        (\"superglue\", \"SuperGlueForKeypointMatching\"),\n+    ]\n+)\n \n MODEL_FOR_TEXT_ENCODING_MAPPING_NAMES = OrderedDict(\n     [\n@@ -1768,6 +1775,8 @@\n     CONFIG_MAPPING_NAMES, MODEL_FOR_KEYPOINT_DETECTION_MAPPING_NAMES\n )\n \n+MODEL_FOR_KEYPOINT_MATCHING_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_KEYPOINT_MATCHING_MAPPING_NAMES)\n+\n MODEL_FOR_TEXT_ENCODING_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_TEXT_ENCODING_MAPPING_NAMES)\n \n MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING = _LazyAutoMapping(\n@@ -1795,6 +1804,10 @@ class AutoModelForKeypointDetection(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_KEYPOINT_DETECTION_MAPPING\n \n \n+class AutoModelForKeypointMatching(_BaseAutoModelClass):\n+    _model_mapping = MODEL_FOR_KEYPOINT_MATCHING_MAPPING\n+\n+\n class AutoModelForTextEncoding(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_TEXT_ENCODING_MAPPING\n \n@@ -2151,6 +2164,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     \"MODEL_FOR_IMAGE_SEGMENTATION_MAPPING\",\n     \"MODEL_FOR_IMAGE_TO_IMAGE_MAPPING\",\n     \"MODEL_FOR_KEYPOINT_DETECTION_MAPPING\",\n+    \"MODEL_FOR_KEYPOINT_MATCHING_MAPPING\",\n     \"MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING\",\n     \"MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\",\n     \"MODEL_FOR_MASKED_LM_MAPPING\",\n@@ -2196,6 +2210,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     \"AutoModelForImageToImage\",\n     \"AutoModelForInstanceSegmentation\",\n     \"AutoModelForKeypointDetection\",\n+    \"AutoModelForKeypointMatching\",\n     \"AutoModelForMaskGeneration\",\n     \"AutoModelForTextEncoding\",\n     \"AutoModelForMaskedImageModeling\","
        },
        {
            "sha": "5ded8084c320035afa9f1ba7108c74f2721f0d8d",
            "filename": "src/transformers/models/efficientloftr/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2F__init__.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_efficientloftr import *\n+    from .image_processing_efficientloftr import *\n+    from .modeling_efficientloftr import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "d3e4acde4fa486d0b4f720e21e6f42fc7ef23de4",
            "filename": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "status": "added",
            "additions": 203,
            "deletions": 0,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,203 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class EfficientLoFTRConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`EffientLoFTRFromKeypointMatching`].\n+    It is used to instantiate a EfficientLoFTR model according to the specified arguments, defining the model\n+    architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n+    EfficientLoFTR [zju-community/efficientloftr](https://huggingface.co/zju-community/efficientloftr) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        stage_num_blocks (`List`, *optional*, defaults to [1, 2, 4, 14]):\n+            The number of blocks in each stages\n+        out_features (`List`, *optional*, defaults to [64, 64, 128, 256]):\n+            The number of channels in each stage\n+        stage_stride (`List`, *optional*, defaults to [2, 1, 2, 2]):\n+            The stride used in each stage\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            The dimension of the descriptors.\n+        activation_function (`str`, *optional*, defaults to `\"relu\"`):\n+            The activation function used in the backbone\n+        q_aggregation_kernel_size (`int`, *optional*, defaults to 4):\n+            The kernel size of the aggregation of query states in the fusion network\n+        kv_aggregation_kernel_size (`int`, *optional*, defaults to 4):\n+            The kernel size of the aggregation of key and value states in the fusion network\n+        q_aggregation_stride (`int`, *optional*, defaults to 4):\n+            The stride of the aggregation of query states in the fusion network\n+        kv_aggregation_stride (`int`, *optional*, defaults to 4):\n+            The stride of the aggregation of key and value states in the fusion network\n+        num_attention_layers (`int`, *optional*, defaults to 4):\n+            Number of attention layers in the LocalFeatureTransformer\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            The number of heads in the GNN layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during attention.\n+        mlp_activation_function (`str`, *optional*, defaults to `\"leaky_relu\"`):\n+            Activation function used in the attention mlp layer.\n+        coarse_matching_skip_softmax (`bool`, *optional*, defaults to `False`):\n+            Whether to skip softmax or not at the coarse matching step.\n+        coarse_matching_threshold (`float`, *optional*, defaults to 0.2):\n+            The threshold for the minimum score required for a match.\n+        coarse_matching_temperature (`float`, *optional*, defaults to 0.1):\n+            The temperature to apply to the coarse similarity matrix\n+        coarse_matching_border_removal (`int`, *optional*, defaults to 2):\n+            The size of the border to remove during coarse matching\n+        fine_kernel_size (`int`, *optional*, defaults to 8):\n+            Kernel size used for the fine feature matching\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the batch normalization layers.\n+        embedding_size (`List`, *optional*, defaults to [15, 20]):\n+            The size (height, width) of the embedding for the position embeddings.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        partial_rotary_factor (`float`, *optional*, defaults to 4.0):\n+            Dim factor for the RoPE embeddings, in EfficientLoFTR, frequencies should be generated for\n+            the whole hidden_size, so this factor is used to compensate.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3', '2d'], with 'default' being the original RoPE implementation.\n+                `dim` (`int`): The dimension of the RoPE embeddings.\n+        fine_matching_slice_dim (`int`, *optional*, defaults to 8):\n+            The size of the slice used to divide the fine features for the first and second fine matching stages.\n+        fine_matching_regress_temperature (`float`, *optional*, defaults to 10.0):\n+            The temperature to apply to the fine similarity matrix\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+    Examples:\n+        ```python\n+        >>> from transformers import EfficientLoFTRConfig, EfficientLoFTRForKeypointMatching\n+\n+        >>> # Initializing a EfficientLoFTR configuration\n+        >>> configuration = EfficientLoFTRConfig()\n+\n+        >>> # Initializing a model from the EfficientLoFTR configuration\n+        >>> model = EfficientLoFTRForKeypointMatching(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    \"\"\"\n+\n+    model_type = \"efficientloftr\"\n+\n+    def __init__(\n+        self,\n+        stage_num_blocks: Optional[list[int]] = None,\n+        out_features: Optional[list[int]] = None,\n+        stage_stride: Optional[list[int]] = None,\n+        hidden_size: int = 256,\n+        activation_function: str = \"relu\",\n+        q_aggregation_kernel_size: int = 4,\n+        kv_aggregation_kernel_size: int = 4,\n+        q_aggregation_stride: int = 4,\n+        kv_aggregation_stride: int = 4,\n+        num_attention_layers: int = 4,\n+        num_attention_heads: int = 8,\n+        attention_dropout: float = 0.0,\n+        attention_bias: bool = False,\n+        mlp_activation_function: str = \"leaky_relu\",\n+        coarse_matching_skip_softmax: bool = False,\n+        coarse_matching_threshold: float = 0.2,\n+        coarse_matching_temperature: float = 0.1,\n+        coarse_matching_border_removal: int = 2,\n+        fine_kernel_size: int = 8,\n+        batch_norm_eps: float = 1e-5,\n+        embedding_size: Optional[list[int]] = None,\n+        rope_theta: float = 10000.0,\n+        partial_rotary_factor: float = 4.0,\n+        rope_scaling: Optional[dict] = None,\n+        fine_matching_slice_dim: int = 8,\n+        fine_matching_regress_temperature: float = 10.0,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        # Stage level of RepVGG\n+        self.stage_num_blocks = stage_num_blocks if stage_num_blocks is not None else [1, 2, 4, 14]\n+        self.stage_stride = stage_stride if stage_stride is not None else [2, 1, 2, 2]\n+        self.out_features = out_features if out_features is not None else [64, 64, 128, 256]\n+        self.stage_in_channels = [1] + self.out_features[:-1]\n+\n+        # Block level of RepVGG\n+        self.stage_block_stride = [\n+            [stride] + [1] * (num_blocks - 1) for stride, num_blocks in zip(self.stage_stride, self.stage_num_blocks)\n+        ]\n+        self.stage_block_out_channels = [\n+            [self.out_features[stage_idx]] * num_blocks for stage_idx, num_blocks in enumerate(self.stage_num_blocks)\n+        ]\n+        self.stage_block_in_channels = [\n+            [self.stage_in_channels[stage_idx]] + self.stage_block_out_channels[stage_idx][:-1]\n+            for stage_idx in range(len(self.stage_num_blocks))\n+        ]\n+\n+        # Fine matching level of EfficientLoFTR\n+        self.fine_fusion_dims = list(reversed(self.out_features))[:-1]\n+\n+        self.hidden_size = hidden_size\n+        if self.hidden_size != self.out_features[-1]:\n+            raise ValueError(\n+                f\"hidden_size should be equal to the last value in out_features. hidden_size = {self.hidden_size}, out_features = {self.stage_out_channels}\"\n+            )\n+\n+        self.activation_function = activation_function\n+        self.q_aggregation_kernel_size = q_aggregation_kernel_size\n+        self.kv_aggregation_kernel_size = kv_aggregation_kernel_size\n+        self.q_aggregation_stride = q_aggregation_stride\n+        self.kv_aggregation_stride = kv_aggregation_stride\n+        self.num_attention_layers = num_attention_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_dropout = attention_dropout\n+        self.attention_bias = attention_bias\n+        self.intermediate_size = self.hidden_size * 2\n+        self.mlp_activation_function = mlp_activation_function\n+        self.coarse_matching_skip_softmax = coarse_matching_skip_softmax\n+        self.coarse_matching_threshold = coarse_matching_threshold\n+        self.coarse_matching_temperature = coarse_matching_temperature\n+        self.coarse_matching_border_removal = coarse_matching_border_removal\n+        self.fine_kernel_size = fine_kernel_size\n+        self.batch_norm_eps = batch_norm_eps\n+        self.fine_matching_slice_dim = fine_matching_slice_dim\n+        self.fine_matching_regress_temperature = fine_matching_regress_temperature\n+\n+        self.num_key_value_heads = num_attention_heads\n+        self.embedding_size = embedding_size if embedding_size is not None else [15, 20]\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling if rope_scaling is not None else {\"rope_type\": \"default\"}\n+\n+        # for compatibility with \"default\" rope type\n+        self.partial_rotary_factor = partial_rotary_factor\n+        rope_config_validation(self)\n+\n+        self.initializer_range = initializer_range\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"EfficientLoFTRConfig\"]"
        },
        {
            "sha": "d15d07dbb8f638bebe2fbfc54ef1a3720e775bf0",
            "filename": "src/transformers/models/efficientloftr/convert_efficientloftr_to_hf.py",
            "status": "added",
            "additions": 257,
            "deletions": 0,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconvert_efficientloftr_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconvert_efficientloftr_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconvert_efficientloftr_to_hf.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,257 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import os\n+import re\n+\n+import torch\n+from datasets import load_dataset\n+from huggingface_hub import hf_hub_download\n+\n+from transformers.models.efficientloftr.image_processing_efficientloftr import EfficientLoFTRImageProcessor\n+from transformers.models.efficientloftr.modeling_efficientloftr import (\n+    EfficientLoFTRConfig,\n+    EfficientLoFTRForKeypointMatching,\n+)\n+\n+\n+DEFAULT_MODEL_REPO = \"stevenbucaille/efficient_loftr_pth\"\n+DEFAULT_FILE = \"eloftr.pth\"\n+\n+\n+def prepare_imgs():\n+    dataset = load_dataset(\"hf-internal-testing/image-matching-test-dataset\", split=\"train\")\n+    image0 = dataset[0][\"image\"]\n+    image2 = dataset[2][\"image\"]\n+    return [[image2, image0]]\n+\n+\n+def verify_model_outputs(model, device):\n+    images = prepare_imgs()\n+    preprocessor = EfficientLoFTRImageProcessor()\n+    inputs = preprocessor(images=images, return_tensors=\"pt\").to(device)\n+    model.to(device)\n+    model.eval()\n+    with torch.no_grad():\n+        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+    predicted_number_of_matches = outputs.matches.shape[-1]\n+    predicted_top10 = torch.topk(outputs.matching_scores[0, 0], k=10)\n+    predicted_top10_matches_indices = predicted_top10.indices\n+    predicted_top10_matching_scores = predicted_top10.values\n+\n+    expected_number_of_matches = 4800\n+    expected_matches_shape = torch.Size((len(images), 2, expected_number_of_matches))\n+    expected_matching_scores_shape = torch.Size((len(images), 2, expected_number_of_matches))\n+\n+    expected_top10_matches_indices = torch.tensor(\n+        [1798, 1639, 1401, 1559, 2596, 2362, 2441, 2605, 1643, 2607], dtype=torch.int64\n+    ).to(device)\n+    expected_top10_matching_scores = torch.tensor(\n+        [0.9563, 0.9355, 0.9265, 0.9091, 0.9071, 0.9062, 0.9000, 0.8978, 0.8908, 0.8853]\n+    ).to(device)\n+\n+    assert outputs.matches.shape == expected_matches_shape\n+    assert outputs.matching_scores.shape == expected_matching_scores_shape\n+\n+    torch.testing.assert_close(predicted_top10_matches_indices, expected_top10_matches_indices, rtol=5e-3, atol=5e-3)\n+    torch.testing.assert_close(predicted_top10_matching_scores, expected_top10_matching_scores, rtol=5e-3, atol=5e-3)\n+\n+    assert predicted_number_of_matches == expected_number_of_matches\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"matcher.backbone.layer(\\d+).rbr_dense.conv\": r\"efficientloftr.backbone.stages.\\1.blocks.0.conv1.conv\",\n+    r\"matcher.backbone.layer(\\d+).rbr_dense.bn\": r\"efficientloftr.backbone.stages.\\1.blocks.0.conv1.norm\",\n+    r\"matcher.backbone.layer(\\d+).rbr_1x1.conv\": r\"efficientloftr.backbone.stages.\\1.blocks.0.conv2.conv\",\n+    r\"matcher.backbone.layer(\\d+).rbr_1x1.bn\": r\"efficientloftr.backbone.stages.\\1.blocks.0.conv2.norm\",\n+    r\"matcher.backbone.layer(\\d+).(\\d+).rbr_dense.conv\": r\"efficientloftr.backbone.stages.\\1.blocks.\\2.conv1.conv\",\n+    r\"matcher.backbone.layer(\\d+).(\\d+).rbr_dense.bn\": r\"efficientloftr.backbone.stages.\\1.blocks.\\2.conv1.norm\",\n+    r\"matcher.backbone.layer(\\d+).(\\d+).rbr_1x1.conv\": r\"efficientloftr.backbone.stages.\\1.blocks.\\2.conv2.conv\",\n+    r\"matcher.backbone.layer(\\d+).(\\d+).rbr_1x1.bn\": r\"efficientloftr.backbone.stages.\\1.blocks.\\2.conv2.norm\",\n+    r\"matcher.backbone.layer(\\d+).(\\d+).rbr_identity\": r\"efficientloftr.backbone.stages.\\1.blocks.\\2.identity\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).aggregate\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.aggregation.q_aggregation\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).norm1\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.aggregation.norm\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).q_proj\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.attention.q_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).k_proj\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.attention.k_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).v_proj\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.attention.v_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).merge\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.attention.o_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).mlp.(\\d+)\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.mlp.fc{1 if m.group(2) == '0' else 2}\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[02468]).norm2\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.self_attention.mlp.layer_norm\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).aggregate\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.aggregation.q_aggregation\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).norm1\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.aggregation.norm\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).q_proj\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.attention.q_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).k_proj\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.attention.k_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).v_proj\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.attention.v_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).merge\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.attention.o_proj\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).mlp.(\\d+)\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.mlp.fc{1 if m.group(2) == '0' else 2}\",\n+    r\"matcher.loftr_coarse.layers.(\\d*[13579]).norm2\": lambda m: f\"efficientloftr.local_feature_transformer.layers.{int(m.group(1)) // 2}.cross_attention.mlp.layer_norm\",\n+    r\"matcher.fine_preprocess.layer3_outconv\": \"refinement_layer.out_conv\",\n+    r\"matcher.fine_preprocess.layer(\\d+)_outconv.weight\": lambda m: f\"refinement_layer.out_conv_layers.{0 if int(m.group(1)) == 2 else m.group(1)}.out_conv1.weight\",\n+    r\"matcher.fine_preprocess.layer(\\d+)_outconv2\\.0\": lambda m: f\"refinement_layer.out_conv_layers.{0 if int(m.group(1)) == 2 else m.group(1)}.out_conv2\",\n+    r\"matcher.fine_preprocess.layer(\\d+)_outconv2\\.1\": lambda m: f\"refinement_layer.out_conv_layers.{0 if int(m.group(1)) == 2 else m.group(1)}.batch_norm\",\n+    r\"matcher.fine_preprocess.layer(\\d+)_outconv2\\.3\": lambda m: f\"refinement_layer.out_conv_layers.{0 if int(m.group(1)) == 2 else m.group(1)}.out_conv3\",\n+}\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: list[str]):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+@torch.no_grad()\n+def write_model(\n+    model_path,\n+    model_repo,\n+    file_name,\n+    organization,\n+    safe_serialization=True,\n+    push_to_hub=False,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+    # ------------------------------------------------------------\n+    # EfficientLoFTR config\n+    # ------------------------------------------------------------\n+\n+    config = EfficientLoFTRConfig()\n+    config.architectures = [\"EfficientLoFTRForKeypointMatching\"]\n+    config.save_pretrained(model_path)\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(f\"Fetching all parameters from the checkpoint at {model_repo}/{file_name}...\")\n+    checkpoint_path = hf_hub_download(repo_id=model_repo, filename=file_name)\n+    original_state_dict = torch.load(checkpoint_path, weights_only=True, map_location=\"cpu\")[\"state_dict\"]\n+\n+    print(\"Converting model...\")\n+    all_keys = list(original_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        state_dict[new_key] = original_state_dict.pop(key).contiguous().clone()\n+\n+    del original_state_dict\n+    gc.collect()\n+\n+    print(\"Loading the checkpoint in a EfficientLoFTR model...\")\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    with torch.device(device):\n+        model = EfficientLoFTRForKeypointMatching(config)\n+    model.load_state_dict(state_dict)\n+    print(\"Checkpoint loaded successfully...\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model...\")\n+    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = EfficientLoFTRForKeypointMatching.from_pretrained(model_path)\n+    print(\"Model reloaded successfully.\")\n+\n+    model_name = \"efficientloftr\"\n+    if model_repo == DEFAULT_MODEL_REPO:\n+        print(\"Checking the model outputs...\")\n+        verify_model_outputs(model, device)\n+    print(\"Model outputs verified successfully.\")\n+\n+    if push_to_hub:\n+        print(\"Pushing model to the hub...\")\n+        model.push_to_hub(\n+            repo_id=f\"{organization}/{model_name}\",\n+            commit_message=\"Add model\",\n+        )\n+        config.push_to_hub(repo_id=f\"{organization}/{model_name}\", commit_message=\"Add config\")\n+\n+    write_image_processor(model_path, model_name, organization, push_to_hub=push_to_hub)\n+\n+\n+def write_image_processor(save_dir, model_name, organization, push_to_hub=False):\n+    image_processor = EfficientLoFTRImageProcessor()\n+    image_processor.save_pretrained(save_dir)\n+\n+    if push_to_hub:\n+        print(\"Pushing image processor to the hub...\")\n+        image_processor.push_to_hub(\n+            repo_id=f\"{organization}/{model_name}\",\n+            commit_message=\"Add image processor\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--repo_id\",\n+        default=DEFAULT_MODEL_REPO,\n+        type=str,\n+        help=\"Model repo ID of the original EfficientLoFTR checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--file_name\",\n+        default=DEFAULT_FILE,\n+        type=str,\n+        help=\"File name of the original EfficientLoFTR checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=None,\n+        type=str,\n+        required=True,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\"--save_model\", action=\"store_true\", help=\"Save model to local\")\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Push model and image preprocessor to the hub\",\n+    )\n+    parser.add_argument(\n+        \"--organization\",\n+        default=\"zju-community\",\n+        type=str,\n+        help=\"Hub organization in which you want the model to be uploaded.\",\n+    )\n+\n+    args = parser.parse_args()\n+    write_model(\n+        args.pytorch_dump_folder_path,\n+        args.repo_id,\n+        args.file_name,\n+        args.organization,\n+        safe_serialization=True,\n+        push_to_hub=args.push_to_hub,\n+    )"
        },
        {
            "sha": "a1bed128daf7ca8a192aba13bd77fbd32cc18f58",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "added",
            "additions": 461,
            "deletions": 0,
            "changes": 461,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,461 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for SuperPoint.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ... import is_torch_available, is_vision_available\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import resize, to_channel_dimension_format\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    is_pil_image,\n+    is_scaled_image,\n+    is_valid_image,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, logging, requires_backends\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    import PIL\n+    from PIL import Image, ImageDraw\n+\n+    from .modeling_efficientloftr import KeypointMatchingOutput\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale\n+def is_grayscale(\n+    image: np.ndarray,\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+):\n+    if input_data_format == ChannelDimension.FIRST:\n+        if image.shape[0] == 1:\n+            return True\n+        return np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...])\n+    elif input_data_format == ChannelDimension.LAST:\n+        if image.shape[-1] == 1:\n+            return True\n+        return np.all(image[..., 0] == image[..., 1]) and np.all(image[..., 1] == image[..., 2])\n+\n+\n+# Copied from transformers.models.superpoint.image_processing_superpoint.convert_to_grayscale\n+def convert_to_grayscale(\n+    image: ImageInput,\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+) -> ImageInput:\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n+    and tensorflow grayscale conversion\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (Image):\n+            The image to convert.\n+        input_data_format (`ChannelDimension` or `str`, *optional*):\n+            The channel dimension format for the input image.\n+    \"\"\"\n+    requires_backends(convert_to_grayscale, [\"vision\"])\n+\n+    if isinstance(image, np.ndarray):\n+        if is_grayscale(image, input_data_format=input_data_format):\n+            return image\n+        if input_data_format == ChannelDimension.FIRST:\n+            gray_image = image[0, ...] * 0.2989 + image[1, ...] * 0.5870 + image[2, ...] * 0.1140\n+            gray_image = np.stack([gray_image] * 3, axis=0)\n+        elif input_data_format == ChannelDimension.LAST:\n+            gray_image = image[..., 0] * 0.2989 + image[..., 1] * 0.5870 + image[..., 2] * 0.1140\n+            gray_image = np.stack([gray_image] * 3, axis=-1)\n+        return gray_image\n+\n+    if not isinstance(image, PIL.Image.Image):\n+        return image\n+\n+    image = image.convert(\"L\")\n+    return image\n+\n+\n+# Copied from transformers.models.superglue.image_processing_superglue.validate_and_format_image_pairs\n+def validate_and_format_image_pairs(images: ImageInput):\n+    error_message = (\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+    def _is_valid_image(image):\n+        \"\"\"images is a PIL Image or a 3D array.\"\"\"\n+        return is_pil_image(image) or (\n+            is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+        )\n+\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image)) for image in images):\n+            return images\n+        if all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            return [image for image_pair in images for image in image_pair]\n+    raise ValueError(error_message)\n+\n+\n+class EfficientLoFTRImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a EfficientLoFTR image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            by `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n+            Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n+            `True`. Can be overriden by `size` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overriden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_grayscale (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to grayscale. Can be overriden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255,\n+        do_grayscale: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 480, \"width\": 640}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_grayscale = do_grayscale\n+\n+    # Copied from transformers.models.superpoint.image_processing_superpoint.SuperPointImageProcessor.resize\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: dict[str, int],\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Resize an image.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`dict[str, int]`):\n+                Dictionary of the form `{\"height\": int, \"width\": int}`, specifying the size of the output image.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the output image. If not provided, it will be inferred from the input\n+                image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        return resize(\n+            image,\n+            size=(size[\"height\"], size[\"width\"]),\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    # Copied from transformers.models.superglue.image_processing_superglue.SuperGlueImageProcessor.preprocess\n+    def preprocess(\n+        self,\n+        images,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_grayscale: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to preprocess. Expects either a list of 2 images or a list of list of 2 images list with\n+                pixel values ranging from 0 to 255. If passing in images with pixel values between 0 and 1, set\n+                `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the output image after `resize` has been applied. If `size[\"shortest_edge\"]` >= 384, the image\n+                is resized to `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Otherwise, the smaller edge of the\n+                image will be matched to `int(size[\"shortest_edge\"]/ crop_pct)`, after which the image is cropped to\n+                `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Only has an effect if `do_resize` is set to `True`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of `PILImageResampling`, filters. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_grayscale (`bool`, *optional*, defaults to `self.do_grayscale`):\n+                Whether to convert the image to grayscale.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_grayscale = do_grayscale if do_grayscale is not None else self.do_grayscale\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        # Validate and convert the input images into a flattened list of images for all subsequent processing steps.\n+        images = validate_and_format_image_pairs(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+        )\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_grayscale:\n+                image = convert_to_grayscale(image, input_data_format=input_data_format)\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            all_images.append(image)\n+\n+        # Convert back the flattened list of images into a list of pairs of images.\n+        image_pairs = [all_images[i : i + 2] for i in range(0, len(all_images), 2)]\n+\n+        data = {\"pixel_values\": image_pairs}\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images: ImageInput,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            outputs (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = np.zeros((max(height0, height1), width0 + width1, 3), dtype=np.uint8)\n+            plot_image[:height0, :width0] = image_pair[0]\n+            plot_image[:height1, width0:] = image_pair[1]\n+\n+            plot_image_pil = Image.fromarray(plot_image)\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return (r, g, b)\n+\n+\n+__all__ = [\"EfficientLoFTRImageProcessor\"]"
        },
        {
            "sha": "a934c1bb2700b49238b23715d93d8731e962c9da",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "added",
            "additions": 1302,
            "deletions": 0,
            "changes": 1302,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,1302 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2CLS, ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BackboneOutput\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    ModelOutput,\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    torch_int,\n+)\n+from ...utils.generic import check_model_inputs\n+from .configuration_efficientloftr import EfficientLoFTRConfig\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for outputs of keypoint matching models. Due to the nature of keypoint detection and matching, the number\n+    of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the batch of\n+    images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask tensor is\n+    used to indicate which values in the keypoints, matches and matching_scores tensors are keypoint matching\n+    information.\n+    \"\"\"\n+)\n+class KeypointMatchingOutput(ModelOutput):\n+    r\"\"\"\n+    matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+        Index of keypoint matched in the other image.\n+    matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+        Scores of predicted matches.\n+    keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n+        Absolute (x, y) coordinates of predicted keypoints in a given image.\n+    hidden_states (`tuple[torch.FloatTensor, ...]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n+        num_keypoints)`, returned when `output_hidden_states=True` is passed or when\n+        `config.output_hidden_states=True`)\n+    attentions (`tuple[torch.FloatTensor, ...]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n+        num_keypoints)`, returned when `output_attentions=True` is passed or when `config.output_attentions=True`)\n+    \"\"\"\n+\n+    matches: Optional[torch.FloatTensor] = None\n+    matching_scores: Optional[torch.FloatTensor] = None\n+    keypoints: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+class EfficientLoFTRRotaryEmbedding(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig, device=None):\n+        super().__init__()\n+        self.config = config\n+        self.rope_type = config.rope_scaling[\"rope_type\"]\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, _ = self.rope_init_fn(self.config, device)\n+        inv_freq_expanded = inv_freq[None, None, None, :].float().expand(1, 1, 1, -1)\n+\n+        embed_height, embed_width = config.embedding_size\n+        i_indices = torch.ones(embed_height, embed_width).cumsum(0).float().unsqueeze(-1)\n+        j_indices = torch.ones(embed_height, embed_width).cumsum(1).float().unsqueeze(-1)\n+\n+        emb = torch.zeros(1, embed_height, embed_width, self.config.hidden_size // 2)\n+        emb[:, :, :, 0::2] = i_indices * inv_freq_expanded\n+        emb[:, :, :, 1::2] = j_indices * inv_freq_expanded\n+\n+        self.register_buffer(\"inv_freq\", emb, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(\n+        self, x: torch.Tensor, position_ids: Optional[tuple[torch.LongTensor, torch.LongTensor]] = None\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            emb = self.inv_freq\n+            sin = emb.sin()\n+            cos = emb.cos()\n+\n+        sin = sin.repeat_interleave(2, dim=-1)\n+        cos = cos.repeat_interleave(2, dim=-1)\n+\n+        sin = sin.to(device=x.device, dtype=x.dtype)\n+        cos = cos.to(device=x.device, dtype=x.dtype)\n+\n+        return cos, sin\n+\n+\n+# Copied from transformers.models.rt_detr_v2.modeling_rt_detr_v2.RTDetrV2ConvNormLayer with RTDetrV2->EfficientLoFTR\n+class EfficientLoFTRConvNormLayer(nn.Module):\n+    def __init__(self, config, in_channels, out_channels, kernel_size, stride, padding=None, activation=None):\n+        super().__init__()\n+        self.conv = nn.Conv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size,\n+            stride,\n+            padding=(kernel_size - 1) // 2 if padding is None else padding,\n+            bias=False,\n+        )\n+        self.norm = nn.BatchNorm2d(out_channels, config.batch_norm_eps)\n+        self.activation = nn.Identity() if activation is None else ACT2CLS[activation]()\n+\n+    def forward(self, hidden_state):\n+        hidden_state = self.conv(hidden_state)\n+        hidden_state = self.norm(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        return hidden_state\n+\n+\n+class EfficientLoFTRRepVGGBlock(GradientCheckpointingLayer):\n+    \"\"\"\n+    RepVGG architecture block introduced by the work \"RepVGG: Making VGG-style ConvNets Great Again\".\n+    \"\"\"\n+\n+    def __init__(self, config: EfficientLoFTRConfig, stage_idx: int, block_idx: int):\n+        super().__init__()\n+        in_channels = config.stage_block_in_channels[stage_idx][block_idx]\n+        out_channels = config.stage_block_out_channels[stage_idx][block_idx]\n+        stride = config.stage_block_stride[stage_idx][block_idx]\n+        activation = config.activation_function\n+        self.conv1 = EfficientLoFTRConvNormLayer(\n+            config, in_channels, out_channels, kernel_size=3, stride=stride, padding=1\n+        )\n+        self.conv2 = EfficientLoFTRConvNormLayer(\n+            config, in_channels, out_channels, kernel_size=1, stride=stride, padding=0\n+        )\n+        self.identity = nn.BatchNorm2d(in_channels) if in_channels == out_channels and stride == 1 else None\n+        self.activation = nn.Identity() if activation is None else ACT2FN[activation]\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        if self.identity is not None:\n+            identity_out = self.identity(hidden_states)\n+        else:\n+            identity_out = 0\n+        hidden_states = self.conv1(hidden_states) + self.conv2(hidden_states) + identity_out\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class EfficientLoFTRRepVGGStage(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig, stage_idx: int):\n+        super().__init__()\n+        self.blocks = nn.ModuleList([])\n+        for block_idx in range(config.stage_num_blocks[stage_idx]):\n+            self.blocks.append(\n+                EfficientLoFTRRepVGGBlock(\n+                    config,\n+                    stage_idx,\n+                    block_idx,\n+                )\n+            )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        for block in self.blocks:\n+            hidden_states = block(hidden_states)\n+        return hidden_states\n+\n+\n+class EfficientLoFTRepVGG(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__()\n+\n+        self.stages = nn.ModuleList([])\n+\n+        for stage_idx in range(len(config.stage_stride)):\n+            stage = EfficientLoFTRRepVGGStage(config, stage_idx)\n+            self.stages.append(stage)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> list[torch.Tensor]:\n+        outputs = []\n+        for stage in self.stages:\n+            hidden_states = stage(hidden_states)\n+            outputs.append(hidden_states)\n+\n+        # Exclude first stage in outputs\n+        outputs = outputs[1:]\n+        return outputs\n+\n+\n+class EfficientLoFTRAggregationLayer(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__()\n+\n+        hidden_size = config.hidden_size\n+\n+        self.q_aggregation = nn.Conv2d(\n+            hidden_size,\n+            hidden_size,\n+            kernel_size=config.q_aggregation_kernel_size,\n+            padding=0,\n+            stride=config.q_aggregation_stride,\n+            bias=False,\n+            groups=hidden_size,\n+        )\n+        self.kv_aggregation = torch.nn.MaxPool2d(\n+            kernel_size=config.kv_aggregation_kernel_size, stride=config.kv_aggregation_stride\n+        )\n+        self.norm = nn.LayerNorm(hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        query_states = hidden_states\n+        is_cross_attention = encoder_hidden_states is not None\n+        kv_states = encoder_hidden_states if is_cross_attention else hidden_states\n+\n+        query_states = self.q_aggregation(query_states)\n+        kv_states = self.kv_aggregation(kv_states)\n+        query_states = query_states.permute(0, 2, 3, 1)\n+        kv_states = kv_states.permute(0, 2, 3, 1)\n+        hidden_states = self.norm(query_states)\n+        encoder_hidden_states = self.norm(kv_states)\n+        return hidden_states, encoder_hidden_states\n+\n+\n+# Copied from transformers.models.cohere.modeling_cohere.rotate_half\n+def rotate_half(x):\n+    # Split and rotate. Note that this function is different from e.g. Llama.\n+    x1 = x[..., ::2]\n+    x2 = x[..., 1::2]\n+    rot_x = torch.stack([-x2, x1], dim=-1).flatten(-2)\n+    return rot_x\n+\n+\n+# Copied from transformers.models.cohere.modeling_cohere.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    dtype = q.dtype\n+    q = q.float()\n+    k = k.float()\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n+\n+\n+# Copied from transformers.models.cohere.modeling_cohere.repeat_kv\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class EfficientLoFTRAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    # Copied from transformers.models.llama.modeling_llama.LlamaAttention.__init__ with Llama->EfficientLoFTR\n+    def __init__(self, config: EfficientLoFTRConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        batch_size, seq_len, dim = hidden_states.shape\n+        input_shape = hidden_states.shape[:-1]\n+\n+        query_states = self.q_proj(hidden_states).view(batch_size, seq_len, -1, dim)\n+\n+        is_cross_attention = encoder_hidden_states is not None\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+\n+        key_states = self.k_proj(current_states).view(batch_size, seq_len, -1, dim)\n+        value_states = self.v_proj(current_states).view(batch_size, seq_len, -1, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is not None:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, unsqueeze_dim=2)\n+\n+        query_states = query_states.view(batch_size, seq_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, seq_len, -1, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class EfficientLoFTRMLP(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        intermediate_size = config.intermediate_size\n+        self.fc1 = nn.Linear(hidden_size * 2, intermediate_size, bias=False)\n+        self.activation = ACT2FN[config.mlp_activation_function]\n+        self.fc2 = nn.Linear(intermediate_size, hidden_size, bias=False)\n+        self.layer_norm = nn.LayerNorm(hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = self.layer_norm(hidden_states)\n+        return hidden_states\n+\n+\n+class EfficientLoFTRAggregatedAttention(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig, layer_idx: int):\n+        super().__init__()\n+\n+        self.q_aggregation_kernel_size = config.q_aggregation_kernel_size\n+        self.aggregation = EfficientLoFTRAggregationLayer(config)\n+        self.attention = EfficientLoFTRAttention(config, layer_idx)\n+        self.mlp = EfficientLoFTRMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        batch_size, embed_dim, _, _ = hidden_states.shape\n+\n+        # Aggregate features\n+        aggregated_hidden_states, aggregated_encoder_hidden_states = self.aggregation(\n+            hidden_states, encoder_hidden_states\n+        )\n+        _, aggregated_h, aggregated_w, _ = aggregated_hidden_states.shape\n+\n+        # Multi-head attention\n+        aggregated_hidden_states = aggregated_hidden_states.reshape(batch_size, -1, embed_dim)\n+        aggregated_encoder_hidden_states = aggregated_encoder_hidden_states.reshape(batch_size, -1, embed_dim)\n+        attn_output, _ = self.attention(\n+            aggregated_hidden_states,\n+            aggregated_encoder_hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        # Upsample features\n+        # (batch_size, seq_len, embed_dim) -> (batch_size, embed_dim, h, w) with seq_len = h * w\n+        attn_output = attn_output.permute(0, 2, 1)\n+        attn_output = attn_output.reshape(batch_size, embed_dim, aggregated_h, aggregated_w)\n+        attn_output = torch.nn.functional.interpolate(\n+            attn_output, scale_factor=self.q_aggregation_kernel_size, mode=\"bilinear\", align_corners=False\n+        )\n+        intermediate_states = torch.cat([hidden_states, attn_output], dim=1)\n+        intermediate_states = intermediate_states.permute(0, 2, 3, 1)\n+        output_states = self.mlp(intermediate_states)\n+        output_states = output_states.permute(0, 3, 1, 2)\n+\n+        hidden_states = hidden_states + output_states\n+\n+        return hidden_states\n+\n+\n+class EfficientLoFTRLocalFeatureTransformerLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: EfficientLoFTRConfig, layer_idx: int):\n+        super().__init__()\n+\n+        self.self_attention = EfficientLoFTRAggregatedAttention(config, layer_idx)\n+        self.cross_attention = EfficientLoFTRAggregatedAttention(config, layer_idx)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        batch_size, _, embed_dim, height, width = hidden_states.shape\n+\n+        hidden_states = hidden_states.reshape(-1, embed_dim, height, width)\n+        hidden_states = self.self_attention(hidden_states, position_embeddings=position_embeddings, **kwargs)\n+\n+        encoder_hidden_states = hidden_states.reshape(-1, 2, embed_dim, height, width)\n+        encoder_hidden_states = encoder_hidden_states.flip(1)\n+        encoder_hidden_states = encoder_hidden_states.reshape(-1, embed_dim, height, width)\n+\n+        hidden_states = self.cross_attention(hidden_states, encoder_hidden_states, **kwargs)\n+        hidden_states = hidden_states.reshape(batch_size, -1, embed_dim, height, width)\n+\n+        return hidden_states\n+\n+\n+class EfficientLoFTRLocalFeatureTransformer(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__()\n+        self.layers = nn.ModuleList(\n+            [\n+                EfficientLoFTRLocalFeatureTransformerLayer(config, layer_idx=i)\n+                for i in range(config.num_attention_layers)\n+            ]\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        for layer in self.layers:\n+            hidden_states = layer(hidden_states, position_embeddings=position_embeddings, **kwargs)\n+        return hidden_states\n+\n+\n+class EfficientLoFTROutConvBlock(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig, hidden_size: int, intermediate_size: int):\n+        super().__init__()\n+\n+        self.out_conv1 = nn.Conv2d(hidden_size, intermediate_size, kernel_size=1, stride=1, padding=0, bias=False)\n+        self.out_conv2 = nn.Conv2d(\n+            intermediate_size, intermediate_size, kernel_size=3, stride=1, padding=1, bias=False\n+        )\n+        self.batch_norm = nn.BatchNorm2d(intermediate_size)\n+        self.activation = ACT2CLS[config.mlp_activation_function]()\n+        self.out_conv3 = nn.Conv2d(intermediate_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor, residual_states: torch.Tensor) -> torch.Tensor:\n+        residual_states = self.out_conv1(residual_states)\n+        residual_states = residual_states + hidden_states\n+        residual_states = self.out_conv2(residual_states)\n+        residual_states = self.batch_norm(residual_states)\n+        residual_states = self.activation(residual_states)\n+        residual_states = self.out_conv3(residual_states)\n+        residual_states = nn.functional.interpolate(\n+            residual_states, scale_factor=2.0, mode=\"bilinear\", align_corners=False\n+        )\n+        return residual_states\n+\n+\n+class EfficientLoFTRFineFusionLayer(nn.Module):\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__()\n+\n+        self.fine_kernel_size = config.fine_kernel_size\n+\n+        fine_fusion_dims = config.fine_fusion_dims\n+        self.out_conv = nn.Conv2d(\n+            fine_fusion_dims[0], fine_fusion_dims[0], kernel_size=1, stride=1, padding=0, bias=False\n+        )\n+        self.out_conv_layers = nn.ModuleList()\n+        for i in range(1, len(fine_fusion_dims)):\n+            out_conv = EfficientLoFTROutConvBlock(config, fine_fusion_dims[i], fine_fusion_dims[i - 1])\n+            self.out_conv_layers.append(out_conv)\n+\n+    def forward_pyramid(\n+        self,\n+        hidden_states: torch.Tensor,\n+        residual_states: list[torch.Tensor],\n+    ) -> torch.Tensor:\n+        hidden_states = self.out_conv(hidden_states)\n+        hidden_states = nn.functional.interpolate(\n+            hidden_states, scale_factor=2.0, mode=\"bilinear\", align_corners=False\n+        )\n+        for i, layer in enumerate(self.out_conv_layers):\n+            hidden_states = layer(hidden_states, residual_states[i])\n+\n+        return hidden_states\n+\n+    def forward(\n+        self,\n+        coarse_features: torch.Tensor,\n+        residual_features: list[torch.Tensor],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        For each image pair, compute the fine features of pixels.\n+        In both images, compute a patch of fine features center cropped around each coarse pixel.\n+        In the first image, the feature patch is kernel_size large and long.\n+        In the second image, it is (kernel_size + 2) large and long.\n+        \"\"\"\n+        batch_size, _, embed_dim, coarse_height, coarse_width = coarse_features.shape\n+\n+        coarse_features = coarse_features.reshape(-1, embed_dim, coarse_height, coarse_width)\n+        residual_features = list(reversed(residual_features))\n+\n+        # 1. Fine feature extraction\n+        fine_features = self.forward_pyramid(coarse_features, residual_features)\n+        _, fine_embed_dim, fine_height, fine_width = fine_features.shape\n+\n+        fine_features = fine_features.reshape(batch_size, 2, fine_embed_dim, fine_height, fine_width)\n+        fine_features_0 = fine_features[:, 0]\n+        fine_features_1 = fine_features[:, 1]\n+\n+        # 2. Unfold all local windows in crops\n+        stride = int(fine_height // coarse_height)\n+        fine_features_0 = nn.functional.unfold(\n+            fine_features_0, kernel_size=self.fine_kernel_size, stride=stride, padding=0\n+        )\n+        _, _, seq_len = fine_features_0.shape\n+        fine_features_0 = fine_features_0.reshape(batch_size, -1, self.fine_kernel_size**2, seq_len)\n+        fine_features_0 = fine_features_0.permute(0, 3, 2, 1)\n+\n+        fine_features_1 = nn.functional.unfold(\n+            fine_features_1, kernel_size=self.fine_kernel_size + 2, stride=stride, padding=1\n+        )\n+        fine_features_1 = fine_features_1.reshape(batch_size, -1, (self.fine_kernel_size + 2) ** 2, seq_len)\n+        fine_features_1 = fine_features_1.permute(0, 3, 2, 1)\n+\n+        return fine_features_0, fine_features_1\n+\n+\n+@auto_docstring\n+class EfficientLoFTRPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = EfficientLoFTRConfig\n+    base_model_prefix = \"efficientloftr\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _can_record_outputs = {\n+        \"hidden_states\": EfficientLoFTRRepVGGBlock,\n+        \"attentions\": EfficientLoFTRAttention,\n+    }\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d, nn.BatchNorm2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+    # Copied from transformers.models.superpoint.modeling_superpoint.SuperPointPreTrainedModel.extract_one_channel_pixel_values with SuperPoint->EfficientLoFTR\n+    def extract_one_channel_pixel_values(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:\n+        \"\"\"\n+        Assuming pixel_values has shape (batch_size, 3, height, width), and that all channels values are the same,\n+        extract the first channel value to get a tensor of shape (batch_size, 1, height, width) for EfficientLoFTR. This is\n+        a workaround for the issue discussed in :\n+        https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+        Args:\n+            pixel_values: torch.FloatTensor of shape (batch_size, 3, height, width)\n+\n+        Returns:\n+            pixel_values: torch.FloatTensor of shape (batch_size, 1, height, width)\n+\n+        \"\"\"\n+        return pixel_values[:, 0, :, :][:, None, :, :]\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    EfficientLoFTR model taking images as inputs and outputting the features of the images.\n+    \"\"\"\n+)\n+class EfficientLoFTRModel(EfficientLoFTRPreTrainedModel):\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.backbone = EfficientLoFTRepVGG(config)\n+        self.local_feature_transformer = EfficientLoFTRLocalFeatureTransformer(config)\n+        self.rotary_emb = EfficientLoFTRRotaryEmbedding(config=config)\n+\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BackboneOutput:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoModel\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg?raw=true\"\n+        >>> image1 = Image.open(requests.get(url, stream=True).raw)\n+        >>> url = \"https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_19481797_2295892421.jpg?raw=true\"\n+        >>> image2 = Image.open(requests.get(url, stream=True).raw)\n+        >>> images = [image1, image2]\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"zju-community/efficient_loftr\")\n+        >>> model = AutoModel.from_pretrained(\"zju-community/efficient_loftr\")\n+\n+        >>> with torch.no_grad():\n+        >>>     inputs = processor(images, return_tensors=\"pt\")\n+        >>>     outputs = model(**inputs)\n+        ```\"\"\"\n+        if labels is not None:\n+            raise ValueError(\"EfficientLoFTR is not trainable, no labels should be provided.\")\n+\n+        if pixel_values.ndim != 5 or pixel_values.size(1) != 2:\n+            raise ValueError(\"Input must be a 5D tensor of shape (batch_size, 2, num_channels, height, width)\")\n+\n+        batch_size, _, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * 2, channels, height, width)\n+        pixel_values = self.extract_one_channel_pixel_values(pixel_values)\n+\n+        # 1. Local Feature CNN\n+        features = self.backbone(pixel_values)\n+        # Last stage outputs are coarse outputs\n+        coarse_features = features[-1]\n+        # Rest is residual features used in EfficientLoFTRFineFusionLayer\n+        residual_features = features[:-1]\n+        coarse_embed_dim, coarse_height, coarse_width = coarse_features.shape[-3:]\n+\n+        # 2. Coarse-level LoFTR module\n+        cos, sin = self.rotary_emb(coarse_features)\n+        cos = cos.expand(batch_size * 2, -1, -1, -1).reshape(batch_size * 2, -1, coarse_embed_dim)\n+        sin = sin.expand(batch_size * 2, -1, -1, -1).reshape(batch_size * 2, -1, coarse_embed_dim)\n+        position_embeddings = (cos, sin)\n+\n+        coarse_features = coarse_features.reshape(batch_size, 2, coarse_embed_dim, coarse_height, coarse_width)\n+        coarse_features = self.local_feature_transformer(\n+            coarse_features, position_embeddings=position_embeddings, **kwargs\n+        )\n+\n+        features = (coarse_features,) + tuple(residual_features)\n+\n+        return BackboneOutput(feature_maps=features)\n+\n+\n+def mask_border(tensor: torch.Tensor, border_margin: int, value: Union[bool, float, int]) -> torch.Tensor:\n+    \"\"\"\n+    Mask a tensor border with a given value\n+\n+    Args:\n+        tensor (`torch.Tensor` of shape `(batch_size, height_0, width_0, height_1, width_1)`):\n+            The tensor to mask\n+        border_margin (`int`) :\n+            The size of the border\n+        value (`Union[bool, int, float]`):\n+            The value to place in the tensor's borders\n+\n+    Returns:\n+        tensor (`torch.Tensor` of shape `(batch_size, height_0, width_0, height_1, width_1)`):\n+            The masked tensor\n+    \"\"\"\n+    if border_margin <= 0:\n+        return tensor\n+\n+    tensor[:, :border_margin, :border_margin, :border_margin, :border_margin] = value\n+    tensor[:, -border_margin:, -border_margin:, -border_margin:, -border_margin:] = value\n+    return tensor\n+\n+\n+def create_meshgrid(\n+    height: Union[int, torch.Tensor],\n+    width: Union[int, torch.Tensor],\n+    normalized_coordinates: bool = False,\n+    device: Optional[torch.device] = None,\n+    dtype: Optional[torch.dtype] = None,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Copied from kornia library : kornia/kornia/utils/grid.py:26\n+\n+    Generate a coordinate grid for an image.\n+\n+    When the flag ``normalized_coordinates`` is set to True, the grid is\n+    normalized to be in the range :math:`[-1,1]` to be consistent with the pytorch\n+    function :py:func:`torch.nn.functional.grid_sample`.\n+\n+    Args:\n+        height (`int`):\n+            The image height (rows).\n+        width (`int`):\n+            The image width (cols).\n+        normalized_coordinates (`bool`):\n+            Whether to normalize coordinates in the range :math:`[-1,1]` in order to be consistent with the\n+            PyTorch function :py:func:`torch.nn.functional.grid_sample`.\n+        device (`torch.device`):\n+            The device on which the grid will be generated.\n+        dtype (`torch.dtype`):\n+            The data type of the generated grid.\n+\n+    Return:\n+        grid (`torch.Tensor` of shape `(1, height, width, 2)`):\n+            The grid tensor.\n+\n+    Example:\n+        >>> create_meshgrid(2, 2)\n+        tensor([[[[-1., -1.],\n+                  [ 1., -1.]],\n+        <BLANKLINE>\n+                 [[-1.,  1.],\n+                  [ 1.,  1.]]]])\n+\n+        >>> create_meshgrid(2, 2, normalized_coordinates=False)\n+        tensor([[[[0., 0.],\n+                  [1., 0.]],\n+        <BLANKLINE>\n+                 [[0., 1.],\n+                  [1., 1.]]]])\n+\n+    \"\"\"\n+    xs = torch.linspace(0, width - 1, width, device=device, dtype=dtype)\n+    ys = torch.linspace(0, height - 1, height, device=device, dtype=dtype)\n+    if normalized_coordinates:\n+        xs = (xs / (width - 1) - 0.5) * 2\n+        ys = (ys / (height - 1) - 0.5) * 2\n+    grid = torch.stack(torch.meshgrid(ys, xs, indexing=\"ij\"), dim=-1)\n+    grid = grid.permute(1, 0, 2).unsqueeze(0)\n+    return grid\n+\n+\n+def spatial_expectation2d(input: torch.Tensor, normalized_coordinates: bool = True) -> torch.Tensor:\n+    r\"\"\"\n+    Copied from kornia library : kornia/geometry/subpix/dsnt.py:76\n+    Compute the expectation of coordinate values using spatial probabilities.\n+\n+    The input heatmap is assumed to represent a valid spatial probability distribution,\n+    which can be achieved using :func:`~kornia.geometry.subpixel.spatial_softmax2d`.\n+\n+    Args:\n+        input (`torch.Tensor` of shape `(batch_size, embed_dim, height, width)`):\n+            The input tensor representing dense spatial probabilities.\n+        normalized_coordinates (`bool`):\n+            Whether to return the coordinates normalized in the range of :math:`[-1, 1]`. Otherwise, it will return\n+            the coordinates in the range of the input shape.\n+\n+    Returns:\n+        output (`torch.Tensor` of shape `(batch_size, embed_dim, 2)`)\n+            Expected value of the 2D coordinates. Output order of the coordinates is (x, y).\n+\n+    Examples:\n+        >>> heatmaps = torch.tensor([[[\n+        ... [0., 0., 0.],\n+        ... [0., 0., 0.],\n+        ... [0., 1., 0.]]]])\n+        >>> spatial_expectation2d(heatmaps, False)\n+        tensor([[[1., 2.]]])\n+\n+    \"\"\"\n+    batch_size, embed_dim, height, width = input.shape\n+\n+    # Create coordinates grid.\n+    grid = create_meshgrid(height, width, normalized_coordinates, input.device)\n+    grid = grid.to(input.dtype)\n+\n+    pos_x = grid[..., 0].reshape(-1)\n+    pos_y = grid[..., 1].reshape(-1)\n+\n+    input_flat = input.view(batch_size, embed_dim, -1)\n+\n+    # Compute the expectation of the coordinates.\n+    expected_y = torch.sum(pos_y * input_flat, -1, keepdim=True)\n+    expected_x = torch.sum(pos_x * input_flat, -1, keepdim=True)\n+\n+    output = torch.cat([expected_x, expected_y], -1)\n+\n+    return output.view(batch_size, embed_dim, 2)\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    EfficientLoFTR model taking images as inputs and outputting the matching of them.\n+    \"\"\"\n+)\n+class EfficientLoFTRForKeypointMatching(EfficientLoFTRPreTrainedModel):\n+    \"\"\"EfficientLoFTR dense image matcher\n+\n+    Given two images, we determine the correspondences by:\n+      1. Extracting coarse and fine features through a backbone\n+      2. Transforming coarse features through self and cross attention\n+      3. Matching coarse features to obtain coarse coordinates of matches\n+      4. Obtaining full resolution fine features by fusing transformed and backbone coarse features\n+      5. Refining the coarse matches using fine feature patches centered at each coarse match in a two-stage refinement\n+\n+    Yifan Wang, Xingyi He, Sida Peng, Dongli Tan and Xiaowei Zhou.\n+    Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed\n+    In CVPR, 2024. https://arxiv.org/abs/2403.04765\n+    \"\"\"\n+\n+    def __init__(self, config: EfficientLoFTRConfig):\n+        super().__init__(config)\n+\n+        self.config = config\n+        self.efficientloftr = EfficientLoFTRModel(config)\n+        self.refinement_layer = EfficientLoFTRFineFusionLayer(config)\n+\n+        self.post_init()\n+\n+    def _get_matches_from_scores(self, scores: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Based on a keypoint score matrix, compute the best keypoint matches between the first and second image.\n+        Since each image pair can have different number of matches, the matches are concatenated together for all pair\n+        in the batch and a batch_indices tensor is returned to specify which match belong to which element in the batch.\n+\n+        Note:\n+            This step can be done as a postprocessing step, because does not involve any model weights/params.\n+            However, we keep it in the modeling code for consistency with other keypoint matching models AND for\n+            easier torch.compile/torch.export (all ops are in torch).\n+\n+        Args:\n+            scores (`torch.Tensor` of shape `(batch_size, height_0, width_0, height_1, width_1)`):\n+                Scores of keypoints\n+\n+        Returns:\n+            matched_indices (`torch.Tensor` of shape `(2, num_matches)`):\n+                Indices representing which pixel in the first image matches which pixel in the second image\n+            matching_scores (`torch.Tensor` of shape `(num_matches,)`):\n+                Scores of each match\n+        \"\"\"\n+        batch_size, height0, width0, height1, width1 = scores.shape\n+\n+        scores = scores.view(batch_size, height0 * width0, height1 * width1)\n+\n+        # For each keypoint, get the best match\n+        max_0 = scores.max(2, keepdim=True).values\n+        max_1 = scores.max(1, keepdim=True).values\n+\n+        # 1. Thresholding\n+        mask = scores > self.config.coarse_matching_threshold\n+\n+        # 2. Border removal\n+        mask = mask.reshape(batch_size, height0, width0, height1, width1)\n+        mask = mask_border(mask, self.config.coarse_matching_border_removal, False)\n+        mask = mask.reshape(batch_size, height0 * width0, height1 * width1)\n+\n+        # 3. Mutual nearest neighbors\n+        mask = mask * (scores == max_0) * (scores == max_1)\n+\n+        # 4. Fine coarse matches\n+        masked_scores = scores * mask\n+        matching_scores_0, max_indices_0 = masked_scores.max(1)\n+        matching_scores_1, max_indices_1 = masked_scores.max(2)\n+\n+        matching_indices = torch.cat([max_indices_0, max_indices_1]).reshape(batch_size, 2, -1)\n+        matching_scores = torch.stack([matching_scores_0, matching_scores_1], dim=1)\n+\n+        # For the keypoints not meeting the threshold score, set the indices to -1 which corresponds to no matches found\n+        matching_indices = torch.where(matching_scores > 0, matching_indices, -1)\n+\n+        return matching_indices, matching_scores\n+\n+    def _coarse_matching(\n+        self, coarse_features: torch.Tensor, coarse_scale: float\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        For each image pair, compute the matching confidence between each coarse element (by default (image_height / 8)\n+        * (image_width / 8 elements)) from the first image to the second image.\n+\n+        Note:\n+            This step can be done as a postprocessing step, because does not involve any model weights/params.\n+            However, we keep it in the modeling code for consistency with other keypoint matching models AND for\n+            easier torch.compile/torch.export (all ops are in torch).\n+\n+        Args:\n+            coarse_features (`torch.Tensor` of shape `(batch_size, 2, hidden_size, coarse_height, coarse_width)`):\n+                Coarse features\n+            coarse_scale (`float`): Scale between the image size and the coarse size\n+\n+        Returns:\n+            keypoints (`torch.Tensor` of shape `(batch_size, 2, num_matches, 2)`):\n+                Keypoints coordinates.\n+            matching_scores (`torch.Tensor` of shape `(batch_size, 2, num_matches)`):\n+                The confidence matching score of each keypoint.\n+            matched_indices (`torch.Tensor` of shape `(batch_size, 2, num_matches)`):\n+                Indices which indicates which keypoint in an image matched with which keypoint in the other image. For\n+                both image in the pair.\n+        \"\"\"\n+        batch_size, _, embed_dim, height, width = coarse_features.shape\n+\n+        # (batch_size, 2, embed_dim, height, width) -> (batch_size, 2, height * width, embed_dim)\n+        coarse_features = coarse_features.permute(0, 1, 3, 4, 2)\n+        coarse_features = coarse_features.reshape(batch_size, 2, -1, embed_dim)\n+\n+        coarse_features = coarse_features / coarse_features.shape[-1] ** 0.5\n+        coarse_features_0 = coarse_features[:, 0]\n+        coarse_features_1 = coarse_features[:, 1]\n+\n+        similarity = coarse_features_0 @ coarse_features_1.transpose(-1, -2)\n+        similarity = similarity / self.config.coarse_matching_temperature\n+\n+        if self.config.coarse_matching_skip_softmax:\n+            confidence = similarity\n+        else:\n+            confidence = nn.functional.softmax(similarity, 1) * nn.functional.softmax(similarity, 2)\n+\n+        confidence = confidence.view(batch_size, height, width, height, width)\n+        matched_indices, matching_scores = self._get_matches_from_scores(confidence)\n+\n+        keypoints = torch.stack([matched_indices % width, matched_indices // width], dim=-1) * coarse_scale\n+\n+        return keypoints, matching_scores, matched_indices\n+\n+    def _get_first_stage_fine_matching(\n+        self,\n+        fine_confidence: torch.Tensor,\n+        coarse_matched_keypoints: torch.Tensor,\n+        fine_window_size: int,\n+        fine_scale: float,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        For each coarse pixel, retrieve the highest fine confidence score and index.\n+        The index represents the matching between a pixel position in the fine window in the first image and a pixel\n+        position in the fine window of the second image.\n+        For example, for a fine_window_size of 64 (8 * 8), the index 2474 represents the matching between the index 38\n+        (2474 // 64) in the fine window of the first image, and the index 42 in the second image. This means that 38\n+        which corresponds to the position (4, 6) (4 // 8 and 4 % 8) is matched with the position (5, 2). In this example\n+        the coarse matched coordinate will be shifted to the matched fine coordinates in the first and second image.\n+\n+        Note:\n+            This step can be done as a postprocessing step, because does not involve any model weights/params.\n+            However, we keep it in the modeling code for consistency with other keypoint matching models AND for\n+            easier torch.compile/torch.export (all ops are in torch).\n+\n+        Args:\n+            fine_confidence (`torch.Tensor` of shape `(num_matches, fine_window_size, fine_window_size)`):\n+                First stage confidence of matching fine features between the first and the second image\n+            coarse_matched_keypoints (`torch.Tensor` of shape `(2, num_matches, 2)`):\n+                Coarse matched keypoint between the first and the second image.\n+            fine_window_size (`int`):\n+                Size of the window used to refine matches\n+            fine_scale (`float`):\n+                Scale between the size of fine features and coarse features\n+\n+        Returns:\n+            indices (`torch.Tensor` of shape `(2, num_matches, 1)`):\n+                Indices of the fine coordinate matched in the fine window\n+            fine_matches (`torch.Tensor` of shape `(2, num_matches, 2)`):\n+                Coordinates of matched keypoints after the first fine stage\n+        \"\"\"\n+        batch_size, num_keypoints, _, _ = fine_confidence.shape\n+        fine_kernel_size = torch_int(fine_window_size**0.5)\n+\n+        fine_confidence = fine_confidence.reshape(batch_size, num_keypoints, -1)\n+        values, indices = torch.max(fine_confidence, dim=-1)\n+        indices = indices[..., None]\n+        indices_0 = indices // fine_window_size\n+        indices_1 = indices % fine_window_size\n+\n+        grid = create_meshgrid(\n+            fine_kernel_size,\n+            fine_kernel_size,\n+            normalized_coordinates=False,\n+            device=fine_confidence.device,\n+            dtype=fine_confidence.dtype,\n+        )\n+        grid = grid - (fine_kernel_size // 2) + 0.5\n+        grid = grid.reshape(1, 1, -1, 2).expand(batch_size, num_keypoints, -1, -1)\n+        delta_0 = torch.gather(grid, 1, indices_0.unsqueeze(-1).expand(-1, -1, -1, 2)).squeeze(2)\n+        delta_1 = torch.gather(grid, 1, indices_1.unsqueeze(-1).expand(-1, -1, -1, 2)).squeeze(2)\n+\n+        fine_matches_0 = coarse_matched_keypoints[:, 0] + delta_0 * fine_scale\n+        fine_matches_1 = coarse_matched_keypoints[:, 1] + delta_1 * fine_scale\n+\n+        indices = torch.stack([indices_0, indices_1], dim=1)\n+        fine_matches = torch.stack([fine_matches_0, fine_matches_1], dim=1)\n+\n+        return indices, fine_matches\n+\n+    def _get_second_stage_fine_matching(\n+        self,\n+        indices: torch.Tensor,\n+        fine_matches: torch.Tensor,\n+        fine_confidence: torch.Tensor,\n+        fine_window_size: int,\n+        fine_scale: float,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        For the given position in their respective fine windows, retrieve the 3x3 fine confidences around this position.\n+        After applying softmax to these confidences, compute the 2D spatial expected coordinates.\n+        Shift the first stage fine matching with these expected coordinates.\n+\n+        Note:\n+            This step can be done as a postprocessing step, because does not involve any model weights/params.\n+            However, we keep it in the modeling code for consistency with other keypoint matching models AND for\n+            easier torch.compile/torch.export (all ops are in torch).\n+\n+        Args:\n+            indices (`torch.Tensor` of shape `(batch_size, 2, num_keypoints)`):\n+                Indices representing the position of each keypoint in the fine window\n+            fine_matches (`torch.Tensor` of shape `(2, num_matches, 2)`):\n+                Coordinates of matched keypoints after the first fine stage\n+            fine_confidence (`torch.Tensor` of shape `(num_matches, fine_window_size, fine_window_size)`):\n+                Second stage confidence of matching fine features between the first and the second image\n+            fine_window_size (`int`):\n+                Size of the window used to refine matches\n+            fine_scale (`float`):\n+                Scale between the size of fine features and coarse features\n+\n+        Returns:\n+            fine_matches (`torch.Tensor` of shape `(2, num_matches, 2)`):\n+                Coordinates of matched keypoints after the second fine stage\n+        \"\"\"\n+        batch_size, num_keypoints, _, _ = fine_confidence.shape\n+        fine_kernel_size = torch_int(fine_window_size**0.5)\n+\n+        indices_0 = indices[:, 0]\n+        indices_1 = indices[:, 1]\n+        indices_1_i = indices_1 // fine_kernel_size\n+        indices_1_j = indices_1 % fine_kernel_size\n+\n+        # matches_indices, indices_0, indices_1_i, indices_1_j of shape (num_matches, 3, 3)\n+        batch_indices = torch.arange(batch_size, device=indices_0.device).reshape(batch_size, 1, 1, 1)\n+        matches_indices = torch.arange(num_keypoints, device=indices_0.device).reshape(1, num_keypoints, 1, 1)\n+        indices_0 = indices_0[..., None]\n+        indices_1_i = indices_1_i[..., None]\n+        indices_1_j = indices_1_j[..., None]\n+\n+        delta = create_meshgrid(3, 3, normalized_coordinates=True, device=indices_0.device).to(torch.long)\n+        delta = delta[None, ...]\n+\n+        indices_1_i = indices_1_i + delta[..., 1]\n+        indices_1_j = indices_1_j + delta[..., 0]\n+\n+        fine_confidence = fine_confidence.reshape(\n+            batch_size, num_keypoints, fine_window_size, fine_kernel_size + 2, fine_kernel_size + 2\n+        )\n+        # (batch_size, seq_len, fine_window_size, fine_kernel_size + 2, fine_kernel_size + 2) -> (batch_size, seq_len, 3, 3)\n+        fine_confidence = fine_confidence[batch_indices, matches_indices, indices_0, indices_1_i, indices_1_j]\n+        fine_confidence = fine_confidence.reshape(batch_size, num_keypoints, 9)\n+        fine_confidence = nn.functional.softmax(\n+            fine_confidence / self.config.fine_matching_regress_temperature, dim=-1\n+        )\n+\n+        heatmap = fine_confidence.reshape(batch_size, num_keypoints, 3, 3)\n+        fine_coordinates_normalized = spatial_expectation2d(heatmap, True)[0]\n+\n+        fine_matches_0 = fine_matches[:, 0]\n+        fine_matches_1 = fine_matches[:, 1] + (fine_coordinates_normalized * (3 // 2) * fine_scale)\n+\n+        fine_matches = torch.stack([fine_matches_0, fine_matches_1], dim=1)\n+\n+        return fine_matches\n+\n+    def _fine_matching(\n+        self,\n+        fine_features_0: torch.Tensor,\n+        fine_features_1: torch.Tensor,\n+        coarse_matched_keypoints: torch.Tensor,\n+        fine_scale: float,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        For each coarse pixel with a corresponding window of fine features, compute the matching confidence between fine\n+        features in the first image and the second image.\n+\n+        Fine features are sliced in two part :\n+        - The first part used for the first stage are the first fine_hidden_size - config.fine_matching_slicedim (64 - 8\n+         = 56 by default) features.\n+        - The second part used for the second stage are the last config.fine_matching_slicedim (8 by default) features.\n+\n+        Each part is used to compute a fine confidence tensor of the following shape :\n+        (batch_size, (coarse_height * coarse_width), fine_window_size, fine_window_size)\n+        They correspond to the score between each fine pixel in the first image and each fine pixel in the second image.\n+\n+        Args:\n+            fine_features_0 (`torch.Tensor` of shape `(num_matches, fine_kernel_size ** 2, fine_kernel_size ** 2)`):\n+                Fine features from the first image\n+            fine_features_1 (`torch.Tensor` of shape `(num_matches, (fine_kernel_size + 2) ** 2, (fine_kernel_size + 2)\n+            ** 2)`):\n+                Fine features from the second image\n+            coarse_matched_keypoints (`torch.Tensor` of shape `(2, num_matches, 2)`):\n+                Keypoint coordinates found in coarse matching for the first and second image\n+            fine_scale (`int`):\n+                Scale between the size of fine features and coarse features\n+\n+        Returns:\n+            fine_coordinates (`torch.Tensor` of shape `(2, num_matches, 2)`):\n+                Matched keypoint between the first and the second image. All matched keypoints are concatenated in the\n+                second dimension.\n+\n+        \"\"\"\n+        batch_size, num_keypoints, fine_window_size, fine_embed_dim = fine_features_0.shape\n+        fine_matching_slice_dim = self.config.fine_matching_slice_dim\n+\n+        fine_kernel_size = torch_int(fine_window_size**0.5)\n+\n+        # Split fine features into first and second stage features\n+        split_fine_features_0 = torch.split(fine_features_0, fine_embed_dim - fine_matching_slice_dim, -1)\n+        split_fine_features_1 = torch.split(fine_features_1, fine_embed_dim - fine_matching_slice_dim, -1)\n+\n+        # Retrieve first stage fine features\n+        fine_features_0 = split_fine_features_0[0]\n+        fine_features_1 = split_fine_features_1[0]\n+\n+        # Normalize first stage fine features\n+        fine_features_0 = fine_features_0 / fine_features_0.shape[-1] ** 0.5\n+        fine_features_1 = fine_features_1 / fine_features_1.shape[-1] ** 0.5\n+\n+        # Compute first stage confidence\n+        fine_confidence = fine_features_0 @ fine_features_1.transpose(-1, -2)\n+        fine_confidence = nn.functional.softmax(fine_confidence, 1) * nn.functional.softmax(fine_confidence, 2)\n+        fine_confidence = fine_confidence.reshape(\n+            batch_size, num_keypoints, fine_window_size, fine_kernel_size + 2, fine_kernel_size + 2\n+        )\n+        fine_confidence = fine_confidence[..., 1:-1, 1:-1]\n+        first_stage_fine_confidence = fine_confidence.reshape(\n+            batch_size, num_keypoints, fine_window_size, fine_window_size\n+        )\n+\n+        fine_indices, fine_matches = self._get_first_stage_fine_matching(\n+            first_stage_fine_confidence,\n+            coarse_matched_keypoints,\n+            fine_window_size,\n+            fine_scale,\n+        )\n+\n+        # Retrieve second stage fine features\n+        fine_features_0 = split_fine_features_0[1]\n+        fine_features_1 = split_fine_features_1[1]\n+\n+        # Normalize second stage fine features\n+        fine_features_1 = fine_features_1 / fine_matching_slice_dim**0.5\n+\n+        # Compute second stage fine confidence\n+        second_stage_fine_confidence = fine_features_0 @ fine_features_1.transpose(-1, -2)\n+\n+        fine_coordinates = self._get_second_stage_fine_matching(\n+            fine_indices,\n+            fine_matches,\n+            second_stage_fine_confidence,\n+            fine_window_size,\n+            fine_scale,\n+        )\n+\n+        return fine_coordinates\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> KeypointMatchingOutput:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoModel\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg?raw=true\"\n+        >>> image1 = Image.open(requests.get(url, stream=True).raw)\n+        >>> url = \"https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_19481797_2295892421.jpg?raw=true\"\n+        >>> image2 = Image.open(requests.get(url, stream=True).raw)\n+        >>> images = [image1, image2]\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"zju-community/efficient_loftr\")\n+        >>> model = AutoModel.from_pretrained(\"zju-community/efficient_loftr\")\n+\n+        >>> with torch.no_grad():\n+        >>>     inputs = processor(images, return_tensors=\"pt\")\n+        >>>     outputs = model(**inputs)\n+        ```\"\"\"\n+        if labels is not None:\n+            raise ValueError(\"SuperGlue is not trainable, no labels should be provided.\")\n+\n+        # 1. Extract coarse and residual features\n+        model_outputs: BackboneOutput = self.efficientloftr(pixel_values, **kwargs)\n+        features = model_outputs.feature_maps\n+\n+        # 2. Compute coarse-level matching\n+        coarse_features = features[0]\n+        coarse_embed_dim, coarse_height, coarse_width = coarse_features.shape[-3:]\n+        batch_size, _, channels, height, width = pixel_values.shape\n+        coarse_scale = height / coarse_height\n+        coarse_keypoints, coarse_matching_scores, coarse_matched_indices = self._coarse_matching(\n+            coarse_features, coarse_scale\n+        )\n+\n+        # 3. Fine-level refinement\n+        residual_features = features[1:]\n+        fine_features_0, fine_features_1 = self.refinement_layer(coarse_features, residual_features)\n+\n+        # Filter fine features with coarse matches indices\n+        _, _, num_keypoints = coarse_matching_scores.shape\n+        batch_indices = torch.arange(batch_size)[..., None]\n+        fine_features_0 = fine_features_0[batch_indices, coarse_matched_indices[:, 0]]\n+        fine_features_1 = fine_features_1[batch_indices, coarse_matched_indices[:, 1]]\n+\n+        # 4. Computer fine-level matching\n+        fine_height = torch_int(coarse_height * coarse_scale)\n+        fine_scale = height / fine_height\n+        matching_keypoints = self._fine_matching(fine_features_0, fine_features_1, coarse_keypoints, fine_scale)\n+\n+        matching_keypoints[:, :, :, 0] = matching_keypoints[:, :, :, 0] / width\n+        matching_keypoints[:, :, :, 1] = matching_keypoints[:, :, :, 1] / height\n+\n+        return KeypointMatchingOutput(\n+            matches=coarse_matched_indices,\n+            matching_scores=coarse_matching_scores,\n+            keypoints=matching_keypoints,\n+            hidden_states=model_outputs.hidden_states,\n+            attentions=model_outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"EfficientLoFTRPreTrainedModel\", \"EfficientLoFTRModel\", \"EfficientLoFTRForKeypointMatching\"]"
        },
        {
            "sha": "124e4b04d502547c2f87bb6aa47b1ad24a7234ca",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -51,7 +51,7 @@\n \n \n def is_grayscale(\n-    image: ImageInput,\n+    image: np.ndarray,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:"
        },
        {
            "sha": "7fd31a905ef2b19dcb2cfd260b74d48aeeb635e6",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -53,7 +53,7 @@\n \n # Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale\n def is_grayscale(\n-    image: ImageInput,\n+    image: np.ndarray,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:"
        },
        {
            "sha": "9759e751b2d2b0f454a9cb7739610768541deddb",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -45,7 +45,7 @@\n \n \n def is_grayscale(\n-    image: ImageInput,\n+    image: np.ndarray,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:"
        },
        {
            "sha": "4a007c830f05fa0f9bf646febeeab64ff6860a78",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -1075,7 +1075,7 @@ def wrapped_forward(*args, **kwargs):\n             if key == \"hidden_states\":\n                 if hasattr(outputs, \"vision_hidden_states\"):\n                     collected_outputs[key] += (outputs.vision_hidden_states,)\n-                else:\n+                elif hasattr(outputs, \"last_hidden_state\"):\n                     collected_outputs[key] += (outputs.last_hidden_state,)\n                 outputs[key] = collected_outputs[key]\n             elif key == \"attentions\":"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/efficientloftr/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/tests%2Fmodels%2Fefficientloftr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/tests%2Fmodels%2Fefficientloftr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2F__init__.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe"
        },
        {
            "sha": "ba325aa9c233ef841433e3cb6541e99c22a3c872",
            "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
            "status": "added",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,90 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import unittest\n+\n+from tests.models.superglue.test_image_processing_superglue import (\n+    SuperGlueImageProcessingTest,\n+    SuperGlueImageProcessingTester,\n+)\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+\n+if is_torch_available():\n+    import numpy as np\n+    import torch\n+\n+    from transformers.models.efficientloftr.modeling_efficientloftr import KeypointMatchingOutput\n+\n+if is_vision_available():\n+    from transformers import EfficientLoFTRImageProcessor\n+\n+\n+def random_array(size):\n+    return np.random.randint(255, size=size)\n+\n+\n+def random_tensor(size):\n+    return torch.rand(size)\n+\n+\n+class EfficientLoFTRImageProcessingTester(SuperGlueImageProcessingTester):\n+    \"\"\"Tester for EfficientLoFTRImageProcessor\"\"\"\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=6,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_grayscale=True,\n+    ):\n+        super().__init__(\n+            parent, batch_size, num_channels, image_size, min_resolution, max_resolution, do_resize, size, do_grayscale\n+        )\n+\n+    def prepare_keypoint_matching_output(self, pixel_values):\n+        \"\"\"Prepare a fake output for the keypoint matching model with random matches between 50 keypoints per image.\"\"\"\n+        max_number_keypoints = 50\n+        batch_size = len(pixel_values)\n+        keypoints = torch.zeros((batch_size, 2, max_number_keypoints, 2))\n+        matches = torch.full((batch_size, 2, max_number_keypoints), -1, dtype=torch.int)\n+        scores = torch.zeros((batch_size, 2, max_number_keypoints))\n+        for i in range(batch_size):\n+            random_number_keypoints0 = np.random.randint(10, max_number_keypoints)\n+            random_number_keypoints1 = np.random.randint(10, max_number_keypoints)\n+            random_number_matches = np.random.randint(5, min(random_number_keypoints0, random_number_keypoints1))\n+            keypoints[i, 0, :random_number_keypoints0] = torch.rand((random_number_keypoints0, 2))\n+            keypoints[i, 1, :random_number_keypoints1] = torch.rand((random_number_keypoints1, 2))\n+            random_matches_indices0 = torch.randperm(random_number_keypoints1, dtype=torch.int)[:random_number_matches]\n+            random_matches_indices1 = torch.randperm(random_number_keypoints0, dtype=torch.int)[:random_number_matches]\n+            matches[i, 0, random_matches_indices1] = random_matches_indices0\n+            matches[i, 1, random_matches_indices0] = random_matches_indices1\n+            scores[i, 0, random_matches_indices1] = torch.rand((random_number_matches,))\n+            scores[i, 1, random_matches_indices0] = torch.rand((random_number_matches,))\n+        return KeypointMatchingOutput(keypoints=keypoints, matches=matches, matching_scores=scores)\n+\n+\n+@require_torch\n+@require_vision\n+class EfficientLoFTRImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n+    image_processing_class = EfficientLoFTRImageProcessor if is_vision_available() else None\n+\n+    def setUp(self) -> None:\n+        super().setUp()\n+        self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)"
        },
        {
            "sha": "50c24c41fa03fc9c4bfec14f6a7acecd0281b013",
            "filename": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "status": "added",
            "additions": 453,
            "deletions": 0,
            "changes": 453,
            "blob_url": "https://github.com/huggingface/transformers/blob/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py?ref=a88ea9cbc89ec1bb4f4ddba95b471a7fb89c14fe",
            "patch": "@@ -0,0 +1,453 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import inspect\n+import unittest\n+from functools import reduce\n+\n+from datasets import load_dataset\n+\n+from transformers.models.efficientloftr import EfficientLoFTRConfig, EfficientLoFTRModel\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_vision,\n+    set_config_for_less_flaky_test,\n+    set_model_for_less_flaky_test,\n+    set_model_tester_for_less_flaky_test,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import EfficientLoFTRForKeypointMatching\n+\n+if is_vision_available():\n+    from transformers import AutoImageProcessor\n+\n+\n+class EfficientLoFTRModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        image_width=80,\n+        image_height=60,\n+        stage_num_blocks: list[int] = [1, 1, 1],\n+        out_features: list[int] = [32, 32, 64],\n+        stage_stride: list[int] = [2, 1, 2],\n+        q_aggregation_kernel_size: int = 1,\n+        kv_aggregation_kernel_size: int = 1,\n+        q_aggregation_stride: int = 1,\n+        kv_aggregation_stride: int = 1,\n+        num_attention_layers: int = 2,\n+        num_attention_heads: int = 8,\n+        hidden_size: int = 64,\n+        coarse_matching_threshold: float = 0.0,\n+        fine_kernel_size: int = 2,\n+        coarse_matching_border_removal: int = 0,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_width = image_width\n+        self.image_height = image_height\n+\n+        self.stage_num_blocks = stage_num_blocks\n+        self.out_features = out_features\n+        self.stage_stride = stage_stride\n+        self.q_aggregation_kernel_size = q_aggregation_kernel_size\n+        self.kv_aggregation_kernel_size = kv_aggregation_kernel_size\n+        self.q_aggregation_stride = q_aggregation_stride\n+        self.kv_aggregation_stride = kv_aggregation_stride\n+        self.num_attention_layers = num_attention_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.hidden_size = hidden_size\n+        self.coarse_matching_threshold = coarse_matching_threshold\n+        self.coarse_matching_border_removal = coarse_matching_border_removal\n+        self.fine_kernel_size = fine_kernel_size\n+\n+    def prepare_config_and_inputs(self):\n+        # EfficientLoFTR expects a grayscale image as input\n+        pixel_values = floats_tensor([self.batch_size, 2, 3, self.image_height, self.image_width])\n+        config = self.get_config()\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return EfficientLoFTRConfig(\n+            stage_num_blocks=self.stage_num_blocks,\n+            out_features=self.out_features,\n+            stage_stride=self.stage_stride,\n+            q_aggregation_kernel_size=self.q_aggregation_kernel_size,\n+            kv_aggregation_kernel_size=self.kv_aggregation_kernel_size,\n+            q_aggregation_stride=self.q_aggregation_stride,\n+            kv_aggregation_stride=self.kv_aggregation_stride,\n+            num_attention_layers=self.num_attention_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            hidden_size=self.hidden_size,\n+            coarse_matching_threshold=self.coarse_matching_threshold,\n+            coarse_matching_border_removal=self.coarse_matching_border_removal,\n+            fine_kernel_size=self.fine_kernel_size,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = EfficientLoFTRForKeypointMatching(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        maximum_num_matches = result.matches.shape[-1]\n+        self.parent.assertEqual(\n+            result.keypoints.shape,\n+            (self.batch_size, 2, maximum_num_matches, 2),\n+        )\n+        self.parent.assertEqual(\n+            result.matches.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+        self.parent.assertEqual(\n+            result.matching_scores.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class EfficientLoFTRModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (EfficientLoFTRForKeypointMatching, EfficientLoFTRModel) if is_torch_available() else ()\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    has_attentions = True\n+\n+    def setUp(self):\n+        self.model_tester = EfficientLoFTRModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=EfficientLoFTRConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.create_and_test_config_to_json_string()\n+        self.config_tester.create_and_test_config_to_json_file()\n+        self.config_tester.create_and_test_config_from_and_save_pretrained()\n+        self.config_tester.create_and_test_config_with_num_labels()\n+        self.config_tester.check_config_can_be_init_without_params()\n+        self.config_tester.check_config_arguments_init()\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching does not use feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching is not trainable\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTRForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EfficientLoFTR does not output any loss term in the forward pass\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.hidden_states\n+\n+            expected_num_hidden_states = len(self.model_tester.stage_num_blocks)\n+            self.assertEqual(len(hidden_states), expected_num_hidden_states)\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [self.model_tester.image_height // 2, self.model_tester.image_width // 2],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_attention_outputs(self):\n+        def check_attention_output(inputs_dict, config, model_class):\n+            config._attn_implementation = \"eager\"\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.attentions\n+            total_stride = reduce(lambda a, b: a * b, config.stage_stride)\n+            hidden_size = (\n+                self.model_tester.image_height // total_stride * self.model_tester.image_width // total_stride\n+            )\n+\n+            expected_attention_shape = [\n+                self.model_tester.num_attention_heads,\n+                hidden_size,\n+                hidden_size,\n+            ]\n+\n+            for i, attention in enumerate(attentions):\n+                self.assertListEqual(\n+                    list(attention.shape[-3:]),\n+                    expected_attention_shape,\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            check_attention_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+\n+            check_attention_output(inputs_dict, config, model_class)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        from_pretrained_ids = [\"stevenbucaille/efficientloftr\"]\n+        for model_name in from_pretrained_ids:\n+            model = EfficientLoFTRForKeypointMatching.from_pretrained(model_name)\n+            self.assertIsNotNone(model)\n+\n+    def test_forward_labels_should_be_none(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                model_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                # Provide an arbitrary sized Tensor as labels to model inputs\n+                model_inputs[\"labels\"] = torch.rand((128, 128))\n+\n+                with self.assertRaises(ValueError) as cm:\n+                    model(**model_inputs)\n+                self.assertEqual(ValueError, cm.exception.__class__)\n+\n+    def test_batching_equivalence(self, atol=1e-5, rtol=1e-5):\n+        \"\"\"\n+        This test is overwritten because the model outputs do not contain only regressive values but also keypoint\n+        locations.\n+        Similarly to the problem discussed about SuperGlue implementation\n+        [here](https://github.com/huggingface/transformers/pull/29886#issuecomment-2482752787), the consequence of\n+        having different scores for matching, makes the maximum indices differ. These indices are being used to compute\n+        the keypoint coordinates. The keypoint coordinates, in the model outputs, are floating point tensors, so the\n+        original implementation of this test cover this case. But the resulting tensors may have differences exceeding\n+        the relative and absolute tolerance.\n+        Therefore, similarly to SuperGlue integration test, for the key \"keypoints\" in the model outputs, we check the\n+        number of differences in keypoint coordinates being less than a TODO given number\n+        \"\"\"\n+\n+        def recursive_check(batched_object, single_row_object, model_name, key):\n+            if isinstance(batched_object, (list, tuple)):\n+                for batched_object_value, single_row_object_value in zip(batched_object, single_row_object):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            elif isinstance(batched_object, dict):\n+                for batched_object_value, single_row_object_value in zip(\n+                    batched_object.values(), single_row_object.values()\n+                ):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            # do not compare returned loss (0-dim tensor) / codebook ids (int) / caching objects\n+            elif batched_object is None or not isinstance(batched_object, torch.Tensor):\n+                return\n+            elif batched_object.dim() == 0:\n+                return\n+            # do not compare int or bool outputs as they are mostly computed with max/argmax/topk methods which are\n+            # very sensitive to the inputs (e.g. tiny differences may give totally different results)\n+            elif not torch.is_floating_point(batched_object):\n+                return\n+            else:\n+                # indexing the first element does not always work\n+                # e.g. models that output similarity scores of size (N, M) would need to index [0, 0]\n+                slice_ids = [slice(0, index) for index in single_row_object.shape]\n+                batched_row = batched_object[slice_ids]\n+                if key == \"keypoints\":\n+                    batched_row = torch.sum(batched_row, dim=-1)\n+                    single_row_object = torch.sum(single_row_object, dim=-1)\n+                    tolerance = 0.02 * single_row_object.shape[-1]\n+                    self.assertTrue(\n+                        torch.sum(~torch.isclose(batched_row, single_row_object, rtol=rtol, atol=atol)) < tolerance\n+                    )\n+                else:\n+                    self.assertFalse(\n+                        torch.isnan(batched_row).any(), f\"Batched output has `nan` in {model_name} for key={key}\"\n+                    )\n+                    self.assertFalse(\n+                        torch.isinf(batched_row).any(), f\"Batched output has `inf` in {model_name} for key={key}\"\n+                    )\n+                    self.assertFalse(\n+                        torch.isnan(single_row_object).any(),\n+                        f\"Single row output has `nan` in {model_name} for key={key}\",\n+                    )\n+                    self.assertFalse(\n+                        torch.isinf(single_row_object).any(),\n+                        f\"Single row output has `inf` in {model_name} for key={key}\",\n+                    )\n+                    try:\n+                        torch.testing.assert_close(batched_row, single_row_object, atol=atol, rtol=rtol)\n+                    except AssertionError as e:\n+                        msg = f\"Batched and Single row outputs are not equal in {model_name} for key={key}.\\n\\n\"\n+                        msg += str(e)\n+                        raise AssertionError(msg)\n+\n+        set_model_tester_for_less_flaky_test(self)\n+\n+        config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n+        set_config_for_less_flaky_test(config)\n+\n+        for model_class in self.all_model_classes:\n+            config.output_hidden_states = True\n+\n+            model_name = model_class.__name__\n+            if hasattr(self.model_tester, \"prepare_config_and_inputs_for_model_class\"):\n+                config, batched_input = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)\n+            batched_input_prepared = self._prepare_for_class(batched_input, model_class)\n+            model = model_class(config).to(torch_device).eval()\n+            set_model_for_less_flaky_test(model)\n+\n+            batch_size = self.model_tester.batch_size\n+            single_row_input = {}\n+            for key, value in batched_input_prepared.items():\n+                if isinstance(value, torch.Tensor) and value.shape[0] % batch_size == 0:\n+                    # e.g. musicgen has inputs of size (bs*codebooks). in most cases value.shape[0] == batch_size\n+                    single_batch_shape = value.shape[0] // batch_size\n+                    single_row_input[key] = value[:single_batch_shape]\n+                else:\n+                    single_row_input[key] = value\n+\n+            with torch.no_grad():\n+                model_batched_output = model(**batched_input_prepared)\n+                model_row_output = model(**single_row_input)\n+\n+            if isinstance(model_batched_output, torch.Tensor):\n+                model_batched_output = {\"model_output\": model_batched_output}\n+                model_row_output = {\"model_output\": model_row_output}\n+\n+            for key in model_batched_output:\n+                # DETR starts from zero-init queries to decoder, leading to cos_similarity = `nan`\n+                if hasattr(self, \"zero_init_hidden_state\") and \"decoder_hidden_states\" in key:\n+                    model_batched_output[key] = model_batched_output[key][1:]\n+                    model_row_output[key] = model_row_output[key][1:]\n+                recursive_check(model_batched_output[key], model_row_output[key], model_name, key)\n+\n+\n+def prepare_imgs():\n+    dataset = load_dataset(\"hf-internal-testing/image-matching-test-dataset\", split=\"train\")\n+    image1 = dataset[0][\"image\"]\n+    image2 = dataset[1][\"image\"]\n+    image3 = dataset[2][\"image\"]\n+    return [[image1, image2], [image3, image2]]\n+\n+\n+@require_torch\n+@require_vision\n+class EfficientLoFTRModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return AutoImageProcessor.from_pretrained(\"stevenbucaille/efficientloftr\") if is_vision_available() else None\n+\n+    @slow\n+    def test_inference(self):\n+        model = EfficientLoFTRForKeypointMatching.from_pretrained(\n+            \"stevenbucaille/efficientloftr\", attn_implementation=\"eager\"\n+        ).to(torch_device)\n+        preprocessor = self.default_image_processor\n+        images = prepare_imgs()\n+        inputs = preprocessor(images=images, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+        predicted_top10 = torch.topk(outputs.matching_scores[0, 0], k=10)\n+        predicted_top10_matches_indices = predicted_top10.indices\n+        predicted_top10_matching_scores = predicted_top10.values\n+\n+        expected_number_of_matches = 4800\n+        expected_matches_shape = torch.Size((len(images), 2, expected_number_of_matches))\n+        expected_matching_scores_shape = torch.Size((len(images), 2, expected_number_of_matches))\n+\n+        expected_top10_matches_indices = torch.tensor(\n+            [3145, 3065, 3143, 3066, 3144, 1397, 1705, 3151, 2342, 2422], dtype=torch.int64, device=torch_device\n+        )\n+        expected_top10_matching_scores = torch.tensor(\n+            [0.9997, 0.9996, 0.9996, 0.9995, 0.9995, 0.9995, 0.9994, 0.9994, 0.9994, 0.9994], device=torch_device\n+        )\n+\n+        self.assertEqual(outputs.matches.shape, expected_matches_shape)\n+        self.assertEqual(outputs.matching_scores.shape, expected_matching_scores_shape)\n+\n+        torch.testing.assert_close(\n+            predicted_top10_matches_indices, expected_top10_matches_indices, rtol=5e-3, atol=5e-3\n+        )\n+        torch.testing.assert_close(\n+            predicted_top10_matching_scores, expected_top10_matching_scores, rtol=5e-3, atol=5e-3\n+        )"
        }
    ],
    "stats": {
        "total": 2943,
        "additions": 2938,
        "deletions": 5
    }
}