{
    "author": "zucchini-nlp",
    "message": "Add new model LFM2-VL (#40624)\n\n* Add LFM2-VL support\n\n* add tests\n\n* linting, formatting, misc review changes\n\n* add siglip2 to auto config and instantiate it in lfm2-vl configuration\n\n* decouple image processor from processor\n\n* remove torch import from configuration\n\n* replace | with Optional\n\n* remove layer truncation from modeling file\n\n* fix copies\n\n* update everything\n\n* fix test case to use tiny model\n\n* update the test cases\n\n* fix finally the image processor and add slow tests\n\n* fixup\n\n* typo in docs\n\n* fix tests\n\n* the doc name uses underscore\n\n* address comments from Yoni\n\n* delete tests and unsuffling\n\n* relative import\n\n* do we really handle imports better now?\n\n* fix test\n\n* slow tests\n\n* found a bug in ordering + slow tests\n\n* fix copies\n\n* dont run compile test\n\n---------\n\nCo-authored-by: Anna <anna@liquid.ai>\nCo-authored-by: Anna Banaszak <48625325+ankke@users.noreply.github.com>",
    "sha": "c532575795e4ccbf3c912a457528a9a60c0b94de",
    "files": [
        {
            "sha": "3d1b0b169636df4070f9f9bbbbee87ffb6a9e41b",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -555,6 +555,8 @@\n         title: LED\n       - local: model_doc/lfm2\n         title: LFM2\n+      - local: model_doc/lfm2_vl\n+        title: LFM2-VL\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2"
        },
        {
            "sha": "1607e3066905b19bb0e19616e16ef86cd771a557",
            "filename": "docs/source/en/model_doc/lfm2_vl.md",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,96 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+# LFM2-VL   \n+\n+## Overview\n+\n+[LFM2-VL](https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models) first series of vision-language foundation models developed by [Liquid AI](https://liquid.ai/). These multimodal models are designed for low-latency and device-aware deployment. LFM2-VL extends the LFM2 family of open-weight Liquid Foundation Models (LFMs) into the vision-language space, supporting both text and image inputs with variable resolutions.\n+\n+## Architecture\n+\n+LFM2-VL consists of three main components: a language model backbone, a vision encoder, and a multimodal projector. LFM2-VL builds upon the LFM2 backbone, inheriting from either LFM2-1.2B (for LFM2-VL-1.6B) or LFM2-350M (for LFM2-VL-450M). For the vision tower, LFM2-VL uses SigLIP2 NaFlex encoders to convert input images into token sequences. Two variants are implemented:\n+* Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\n+* Base (86M) for fast image processing for LFM2-VL-450M\n+\n+The encoder processes images at their native resolution up to 512Ã—512 pixels, efficiently handling smaller images without upscaling and supporting non-standard aspect ratios without distortion. Larger images are split into non-overlapping square patches of 512Ã—512 each, preserving detail. In LFM2-VL-1.6B, the model also receives a thumbnail (a small, downscaled version of the original image capturing the overall scene) to enhance global context understanding and alignment. Special tokens mark each patchâ€™s position and indicate the thumbnailâ€™s start. The multimodal connector is a 2-layer MLP connector with pixel unshuffle to reduce image token count. \n+\n+## Example\n+\n+The following example shows how to generate an answer using the `AutoModelForImageTextToText` class.\n+\n+```python\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+\\\n+# Load model and processor\n+model_id = \"LiquidAI/LFM2-VL-1.6B\"\n+model = AutoModelForImageTextToText.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=\"bfloat16\",\n+)\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+# Load image and create conversation\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"image\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is in this image?\"},\n+        ],\n+    },\n+]\n+\n+# Generate snswer\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    return_dict=True,\n+    tokenize=True,\n+).to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=64)\n+processor.batch_decode(outputs, skip_special_tokens=True)[0]\n+\n+```\n+\n+## Lfm2VlImageProcessorFast\n+\n+[[autodoc]] Lfm2VlImageProcessorFast\n+\n+## Lfm2VlProcessor\n+\n+[[autodoc]] Lfm2VlProcessor\n+\n+## Lfm2VlConfig\n+\n+[[autodoc]] Lfm2VlConfig\n+\n+## Lfm2VlModel\n+\n+[[autodoc]] Lfm2VlModel\n+    - forward\n+\n+## Lfm2VlForConditionalGeneration\n+\n+[[autodoc]] Lfm2VlForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "2412e497556fe2a454ffed36bbc6e55ca0f59dee",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -607,6 +607,8 @@\n         title: LED\n       - local: in_translation\n         title: LFM2\n+      - local: in_translation\n+        title: LFM2-VL\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2"
        },
        {
            "sha": "7962482bb6d0b9652c79502fdc69a9b176772760",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -1903,6 +1903,7 @@ def _supports_default_dynamic_cache(cls) -> bool:\n                 \"minimax\",\n                 \"xlnet\",\n                 \"lfm2\",\n+                \"lfm2-vl\",\n             ]\n         )\n "
        },
        {
            "sha": "c32c8a795488fc60227ff8bc9a27aa6da1f8d903",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -183,6 +183,7 @@\n     from .led import *\n     from .levit import *\n     from .lfm2 import *\n+    from .lfm2_vl import *\n     from .lightglue import *\n     from .lilt import *\n     from .llama import *"
        },
        {
            "sha": "06023f09c9d8757b986878ff5de77a113ef6d4ac",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -222,6 +222,7 @@\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n         (\"lfm2\", \"Lfm2Config\"),\n+        (\"lfm2_vl\", \"Lfm2VlConfig\"),\n         (\"lightglue\", \"LightGlueConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n         (\"llama\", \"LlamaConfig\"),\n@@ -366,6 +367,7 @@\n         (\"shieldgemma2\", \"ShieldGemma2Config\"),\n         (\"siglip\", \"SiglipConfig\"),\n         (\"siglip2\", \"Siglip2Config\"),\n+        (\"siglip2_vision_model\", \"Siglip2VisionConfig\"),\n         (\"siglip_vision_model\", \"SiglipVisionConfig\"),\n         (\"smollm3\", \"SmolLM3Config\"),\n         (\"smolvlm\", \"SmolVLMConfig\"),\n@@ -657,6 +659,7 @@\n         (\"led\", \"LED\"),\n         (\"levit\", \"LeViT\"),\n         (\"lfm2\", \"Lfm2\"),\n+        (\"lfm2_vl\", \"Lfm2Vl\"),\n         (\"lightglue\", \"LightGlue\"),\n         (\"lilt\", \"LiLT\"),\n         (\"llama\", \"LLaMA\"),\n@@ -958,6 +961,7 @@\n         (\"glm4v_moe_text\", \"glm4v_moe\"),\n         (\"idefics3_vision\", \"idefics3\"),\n         (\"siglip_vision_model\", \"siglip\"),\n+        (\"siglip2_vision_model\", \"siglip2\"),\n         (\"aimv2_vision_model\", \"aimv2\"),\n         (\"smolvlm_vision\", \"smolvlm\"),\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),"
        },
        {
            "sha": "aa16ac3555eb28cb05cd797915e99f6593f7acb3",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -120,6 +120,7 @@\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"levit\", (\"LevitImageProcessor\", \"LevitImageProcessorFast\")),\n+            (\"lfm2_vl\", (None, \"Lfm2VlImageProcessorFast\")),\n             (\"lightglue\", (\"LightGlueImageProcessor\", None)),\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),"
        },
        {
            "sha": "025a7a1f90a036ac36f6aa0609e1ffe6e4469301",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -222,6 +222,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"led\", \"LEDModel\"),\n         (\"levit\", \"LevitModel\"),\n         (\"lfm2\", \"Lfm2Model\"),\n+        (\"lfm2_vl\", \"Lfm2VlModel\"),\n         (\"lightglue\", \"LightGlueForKeypointMatching\"),\n         (\"lilt\", \"LiltModel\"),\n         (\"llama\", \"LlamaModel\"),\n@@ -356,6 +357,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"sew-d\", \"SEWDModel\"),\n         (\"siglip\", \"SiglipModel\"),\n         (\"siglip2\", \"Siglip2Model\"),\n+        (\"siglip2_vision_model\", \"Siglip2VisionModel\"),\n         (\"siglip_vision_model\", \"SiglipVisionModel\"),\n         (\"smollm3\", \"SmolLM3Model\"),\n         (\"smolvlm\", \"SmolVLMModel\"),\n@@ -1026,6 +1028,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"janus\", \"JanusForConditionalGeneration\"),\n         (\"kosmos-2\", \"Kosmos2ForConditionalGeneration\"),\n         (\"kosmos-2.5\", \"Kosmos2_5ForConditionalGeneration\"),\n+        (\"lfm2_vl\", \"Lfm2VlForConditionalGeneration\"),\n         (\"llama4\", \"Llama4ForConditionalGeneration\"),\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),"
        },
        {
            "sha": "c455c68508445481ac0c1b1854388bd5cc24ee37",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -93,6 +93,7 @@\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextProcessor\"),\n         (\"layoutlmv2\", \"LayoutLMv2Processor\"),\n         (\"layoutlmv3\", \"LayoutLMv3Processor\"),\n+        (\"lfm2_vl\", \"Lfm2VlProcessor\"),\n         (\"llama4\", \"Llama4Processor\"),\n         (\"llava\", \"LlavaProcessor\"),\n         (\"llava_next\", \"LlavaNextProcessor\"),"
        },
        {
            "sha": "7d0357ffbaa69bd379fb48e884ed4d9a13f943d9",
            "filename": "src/transformers/models/lfm2_vl/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2F__init__.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lfm2_vl import *\n+    from .image_processing_lfm2_vl_fast import *\n+    from .modeling_lfm2_vl import *\n+    from .processing_lfm2_vl import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "1378fbe6dc8c54fe8e6aa05d8b3ee473c93d4c46",
            "filename": "src/transformers/models/lfm2_vl/configuration_lfm2_vl.py",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,91 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch LFM2-VL model.\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2VlConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Lfm2VlForConditionalGeneration`]. It is used to instantiate an\n+    Lfm2Vl model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Lfm2-VL-1.6B.\n+\n+    e.g. [LiquidAI/LFM2-VL-1.6B](https://huggingface.co/LiquidAI/LFM2-VL-1.6B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`AutoConfig | dict`,  *optional*, defaults to `Siglip2ImageConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`AutoConfig | dict`, *optional*, defaults to `Lfm2Config`):\n+            The config object or dictionary of the text backbone.\n+        image_token_id (`int`, *optional*, defaults to 396):\n+            The image token index to encode the image prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function used by the multimodal projector.\n+        projector_hidden_size (`int`, *optional*, defaults to 2560):\n+            The hidden size of the multimodal projector.\n+        projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n+        downsample_factor (`int`, *optional*, defaults to 2):\n+            The downsample_factor factor of the vision backbone.\n+    \"\"\"\n+\n+    model_type = \"lfm2-vl\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_id=396,\n+        projector_hidden_act=\"gelu\",\n+        projector_hidden_size=2560,\n+        projector_bias=True,\n+        downsample_factor=2,\n+        **kwargs,\n+    ):\n+        self.image_token_id = image_token_id\n+        self.projector_hidden_act = projector_hidden_act\n+        self.projector_hidden_size = projector_hidden_size\n+        self.projector_bias = projector_bias\n+        self.downsample_factor = downsample_factor\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"siglip2_vision_model\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"siglip2_vision_model\"]()\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"lfm2\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"lfm2\"]()\n+\n+        self.vision_config = vision_config\n+        self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"Lfm2VlConfig\"]"
        },
        {
            "sha": "c709a01dca4196d520d6bea1d4aa52ffd97aa1e1",
            "filename": "src/transformers/models/lfm2_vl/image_processing_lfm2_vl_fast.py",
            "status": "added",
            "additions": 546,
            "deletions": 0,
            "changes": 546,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,546 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from functools import lru_cache\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import (\n+    Unpack,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+else:\n+    from torchvision.transforms import functional as F\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def round_by_factor(number: float, factor: int) -> int:\n+    \"\"\"Returns the closest integer to 'number' that is divisible by 'factor'.\"\"\"\n+    return round(number / factor) * factor\n+\n+\n+def find_closest_aspect_ratio(\n+    aspect_ratio: float,\n+    target_ratios: list[tuple[int, int]],\n+    width: int,\n+    height: int,\n+    image_size: int,\n+) -> tuple[int, int]:\n+    \"\"\"Find the closest aspect ratio from target_ratios to match the input aspect ratio.\n+\n+    Args:\n+        aspect_ratio: The aspect ratio to match (width/height).\n+        target_ratios: List of possible aspect ratios as tuples of (width, height) integers.\n+        width: Original image width in pixels.\n+        height: Original image height in pixels.\n+        image_size: Base size for calculating target area.\n+\n+    Returns:\n+        tuple[int, int]: The best matching ratio as (width, height) integers.\n+    \"\"\"\n+    best_ratio_diff = float(\"inf\")\n+    best_ratio = (1, 1)\n+    area = width * height\n+\n+    for ratio in target_ratios:\n+        target_aspect_ratio = ratio[0] / ratio[1]\n+        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n+\n+        # update best ratio if we found a closer match\n+        if ratio_diff < best_ratio_diff:\n+            best_ratio_diff = ratio_diff\n+            best_ratio = ratio\n+        # if equally close, prefer the ratio that better matches the original image area\n+        elif ratio_diff == best_ratio_diff:\n+            target_area = image_size * image_size * ratio[0] * ratio[1]\n+            if area > 0.5 * target_area:\n+                best_ratio = ratio\n+\n+    return best_ratio\n+\n+\n+# copied from Siglip2ImageProcessor\n+@lru_cache(maxsize=256)\n+def get_image_size_for_max_num_patches(\n+    image_height: int, image_width: int, patch_size: int, max_num_patches: int, eps: float = 1e-5\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Determine image size based on max number of patches, ensure dimensions are divisible by patch size and image is at least 1 patch.\n+\n+    Args:\n+        image_height (`int`):\n+            Original image height.\n+        image_width (`int`):\n+            Original image width.\n+        patch_size (`int`):\n+            Patch size for processing.\n+        max_num_patches (`int`):\n+            Maximum number of patches.\n+        eps (`float`):\n+            Small threshold for binary search.\n+\n+    Returns:\n+        Tuple: (target_height, target_width)\n+    \"\"\"\n+\n+    def get_scaled_image_size(scale: float, size: int, patch_size: int) -> int:\n+        scaled_size = size * scale\n+        scaled_size = math.ceil(scaled_size / patch_size) * patch_size  # make divisible by patch_size\n+        scaled_size = max(patch_size, scaled_size)  # ensure at least 1 patch\n+        return int(scaled_size)\n+\n+    # Binary search for optimal scale\n+    scale_min, scale_max = eps / 10, 100.0\n+    while (scale_max - scale_min) >= eps:\n+        scale = (scale_min + scale_max) / 2\n+        target_height = get_scaled_image_size(scale, image_height, patch_size)\n+        target_width = get_scaled_image_size(scale, image_width, patch_size)\n+        num_patches = (target_height / patch_size) * (target_width / patch_size)\n+\n+        if num_patches <= max_num_patches:\n+            scale_min = scale\n+        else:\n+            scale_max = scale\n+\n+    scale = scale_min\n+    target_height = get_scaled_image_size(scale, image_height, patch_size)\n+    target_width = get_scaled_image_size(scale, image_width, patch_size)\n+    return target_height, target_width\n+\n+\n+def convert_image_to_patches(images: \"torch.Tensor\", patch_size: int) -> \"torch.Tensor\":\n+    \"\"\"\n+    Convert 3D array image of shape (image_height, image_width, num_channels) into 2D array of patches of shape\n+    (num_patches_height * num_patches_width, patch_size * patch_size * num_channels).\n+    \"\"\"\n+    batch_size, num_channels, image_height, image_width = images.shape\n+    num_patches_height = image_height // patch_size\n+    num_patches_width = image_width // patch_size\n+    patched_image = images.reshape(\n+        batch_size, num_channels, num_patches_height, patch_size, num_patches_width, patch_size\n+    )\n+    patched_image = patched_image.permute(0, 2, 4, 3, 5, 1)\n+    patched_image = patched_image.reshape(batch_size, num_patches_height * num_patches_width, -1)\n+    return patched_image\n+\n+\n+def pad_along_first_dim(\n+    images: \"torch.Tensor\", target_length: int, pad_value: int = 0\n+) -> tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n+    \"\"\"\n+    Pad the array along the first dimension.\n+    \"\"\"\n+    current_length = images.shape[1]\n+    padding_length = target_length - current_length\n+    pixel_mask = torch.ones((target_length,), dtype=torch.int32)\n+    if padding_length > 0:\n+        paddings = (0, 0, 0, padding_length, 0, 0)\n+        images = torch.nn.functional.pad(images, paddings, mode=\"constant\", value=pad_value)\n+        pixel_mask[-padding_length:] = 0\n+    return images, pixel_mask\n+\n+\n+class Lfm2VlFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    downsample_factor (`int`, *optional*, defaults to `2`):\n+        The downsampling factor for images used when resizing the image.\n+    \"\"\"\n+\n+    downsample_factor: Optional[int]\n+    do_image_splitting: Optional[bool]\n+    min_tiles: Optional[int]\n+    max_tiles: Optional[int]\n+    use_thumbnail: Optional[bool]\n+    min_image_tokens: Optional[int]\n+    max_image_tokens: Optional[int]\n+    encoder_patch_size: Optional[int]\n+    tile_size: Optional[int]\n+    max_pixels_tolerance: Optional[float]\n+    do_pad: Optional[bool]\n+    return_row_col_info: Optional[bool]\n+\n+\n+@auto_docstring\n+class Lfm2VlImageProcessorFast(BaseImageProcessorFast):\n+    downsample_factor = 2\n+    do_image_splitting = True\n+    min_tiles = 2\n+    max_tiles = 10\n+    use_thumbnail = True\n+    min_image_tokens = 64\n+    max_image_tokens = 256\n+    encoder_patch_size = 16\n+    tile_size = 512\n+    max_pixels_tolerance = 2.0\n+    do_resize = True\n+    size = {\"height\": 512, \"width\": 512}\n+    resample = PILImageResampling.BILINEAR\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_pad = True\n+    return_row_col_info = False\n+    image_mean = IMAGENET_STANDARD_STD\n+    image_std = IMAGENET_STANDARD_MEAN\n+    valid_kwargs = Lfm2VlFastImageProcessorKwargs\n+    model_input_names = [\"pixel_values\", \"pixel_attention_mask\", \"spatial_shapes\"]\n+\n+    def __init__(self, **kwargs: Unpack[Lfm2VlFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+        max_thumbnail_image_patches = self.max_image_tokens * self.downsample_factor**2\n+        tile_size_patches = (self.tile_size // self.encoder_patch_size) ** 2 if self.do_image_splitting else 0\n+        self.max_num_patches = max(\n+            max_thumbnail_image_patches,\n+            tile_size_patches,\n+        )\n+\n+    @lru_cache(maxsize=256)\n+    def _target_ratios(self, min_tiles: int, max_tiles: int) -> list[tuple[int, int]]:\n+        ratios = [\n+            (w, h)\n+            for n in range(min_tiles, max_tiles + 1)\n+            for w in range(1, n + 1)\n+            for h in range(1, n + 1)\n+            if min_tiles <= w * h <= max_tiles\n+        ]\n+        return sorted(set(ratios), key=lambda x: x[0] * x[1])\n+\n+    def _get_grid_layout(\n+        self,\n+        height: int,\n+        width: int,\n+        min_tiles: int,\n+        max_tiles: int,\n+        tile_size: int,\n+    ) -> tuple[int, int]:\n+        aspect_ratio = width / height\n+        target_ratios = self._target_ratios(min_tiles, max_tiles)\n+\n+        # find best matching grid configuration\n+        grid_width, grid_height = find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, tile_size)\n+\n+        target_width = tile_size * grid_width\n+        target_height = tile_size * grid_height\n+        total_patches = grid_width * grid_height\n+\n+        return grid_width, grid_height, target_width, target_height, total_patches\n+\n+    def crop_image_to_patches(\n+        self,\n+        image: \"torch.Tensor\",\n+        min_tiles: int,\n+        max_tiles: int,\n+        tile_size: int,\n+        use_thumbnail: bool,\n+        thumbnail_size: tuple[int],\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Processes a high resolution image into patches.\n+        This method splits a high resolution image into a grid of smaller patches while trying to maintain\n+        the original aspect ratio. It finds the optimal grid configuration within the specified tile constraints.\n+        \"\"\"\n+        batch_size, num_channels, height, width = image.shape\n+        grid_width, grid_height, target_width, target_height, total_patches = self._get_grid_layout(\n+            height, width, min_tiles=min_tiles, max_tiles=max_tiles, tile_size=tile_size\n+        )\n+        resized_image = F.resize(\n+            image, (target_height, target_width), interpolation=interpolation, antialias=antialias\n+        )\n+\n+        # split the image into patches\n+        processed_images = (\n+            resized_image.unfold(2, size=tile_size, step=tile_size)\n+            .unfold(3, size=tile_size, step=tile_size)\n+            .contiguous()\n+            .view(batch_size, num_channels, -1, tile_size, tile_size)\n+            .permute(2, 0, 1, 3, 4)\n+            .reshape(batch_size, -1, num_channels, tile_size, tile_size)\n+        )\n+\n+        # Re-order processed images to a nested image structure, so it can be reordered back correctly\n+        # Note that the images can't be stacked because the thumbnail image is of bigger size than patches\n+        # Each image in sublist will be of shape (1, C, H, W)\n+        processed_images = list(processed_images)\n+\n+        if use_thumbnail and grid_width * grid_height != 1:\n+            total_patches += 1\n+            thumbnail_image = F.resize(image, thumbnail_size, interpolation=interpolation, antialias=antialias)\n+            for i in range(batch_size):\n+                processed_images[i] = list(processed_images[i]) + list(thumbnail_image[i][None, ...])\n+\n+        return processed_images, grid_width, grid_height\n+\n+    # Adapted from Qwen-VL with minor differences\n+    def smart_resize(\n+        self,\n+        height: int,\n+        width: int,\n+        downsample_factor: int,\n+        min_image_tokens: int,\n+        max_image_tokens: int,\n+        encoder_patch_size: int,\n+    ) -> tuple[int, int]:\n+        \"\"\"\n+        Rescales the image so that the following conditions are met:\n+        1. Both dimensions (height and width) are divisible by 'encoder_patch_size' * 'downsample_factor'.\n+           This ensures no padding is needed in the downsampling step.\n+        2. The total number of pixels is within the range ['smart_resize_min_pixels', 'smart_resize_max_pixels'].\n+        3. The aspect ratio of the image is maintained as closely as possible.\n+        \"\"\"\n+        total_factor = encoder_patch_size * downsample_factor\n+        smart_resize_min_pixels = min_image_tokens * encoder_patch_size**2 * downsample_factor**2\n+        smart_resize_max_pixels = max_image_tokens * encoder_patch_size**2 * downsample_factor**2\n+\n+        h_bar = max(total_factor, round_by_factor(height, total_factor))\n+        w_bar = max(total_factor, round_by_factor(width, total_factor))\n+\n+        if h_bar * w_bar > smart_resize_max_pixels:\n+            beta = math.sqrt((height * width) / smart_resize_max_pixels)\n+            math.floor(height / beta / total_factor) * total_factor\n+            h_bar = max(total_factor, math.floor(height / beta / total_factor) * total_factor)\n+            w_bar = max(total_factor, math.floor(width / beta / total_factor) * total_factor)\n+        elif h_bar * w_bar < smart_resize_min_pixels:\n+            beta = math.sqrt(smart_resize_min_pixels / (height * width))\n+            h_bar = math.ceil(height * beta / total_factor) * total_factor\n+            w_bar = math.ceil(width * beta / total_factor) * total_factor\n+\n+        return w_bar, h_bar\n+\n+    def _is_image_too_large(\n+        self,\n+        height: int,\n+        width: int,\n+        max_image_tokens: int,\n+        encoder_patch_size: int,\n+        downsample_factor: int,\n+        max_pixels_tolerance: float,\n+    ) -> bool:\n+        \"\"\"Check if the image is too large to be processed as one tile.\"\"\"\n+        total_factor = encoder_patch_size * downsample_factor\n+\n+        h_bar = max(encoder_patch_size, round_by_factor(height, total_factor))\n+        w_bar = max(encoder_patch_size, round_by_factor(width, total_factor))\n+        return h_bar * w_bar > max_image_tokens * encoder_patch_size**2 * downsample_factor**2 * max_pixels_tolerance\n+\n+    def resize_and_split(\n+        self,\n+        images: \"torch.Tensor\",\n+        downsample_factor: int,\n+        min_tiles: int,\n+        max_tiles: int,\n+        use_thumbnail: bool,\n+        min_image_tokens: int,\n+        max_image_tokens: int,\n+        encoder_patch_size: int,\n+        tile_size: int,\n+        max_pixels_tolerance: float,\n+        interpolation: \"F.InterpolationMode\",\n+    ) -> \"torch.Tensor\":\n+        batch_size, _, height, width = images.shape\n+        do_image_splitting = not min_tiles == max_tiles == 1\n+        is_image_large = self._is_image_too_large(\n+            height=height,\n+            width=width,\n+            max_image_tokens=max_image_tokens,\n+            encoder_patch_size=encoder_patch_size,\n+            downsample_factor=downsample_factor,\n+            max_pixels_tolerance=max_pixels_tolerance,\n+        )\n+\n+        new_width, new_height = self.smart_resize(\n+            height=height,\n+            width=width,\n+            downsample_factor=downsample_factor,\n+            min_image_tokens=min_image_tokens,\n+            max_image_tokens=max_image_tokens,\n+            encoder_patch_size=encoder_patch_size,\n+        )\n+\n+        # Big image will be cropped into patches and small images are just resized\n+        if is_image_large and do_image_splitting:\n+            images, num_rows, num_cols = self.crop_image_to_patches(\n+                images,\n+                min_tiles=min_tiles,\n+                max_tiles=max_tiles,\n+                tile_size=tile_size,\n+                thumbnail_size=(new_height, new_width),\n+                use_thumbnail=use_thumbnail,\n+                interpolation=interpolation,\n+            )\n+        else:\n+            num_rows = num_cols = 1\n+            images = F.resize(images, (new_height, new_width), interpolation=interpolation)\n+            # Make a list and treat it as single crop per image so it can be re-grouped back correctly\n+            images = [[image] for image in images]\n+\n+        num_rows = [num_rows] * batch_size\n+        num_cols = [num_cols] * batch_size\n+        image_sizes = [[new_height, new_width]] * batch_size\n+        return images, num_rows, num_cols, image_sizes\n+\n+    def _preprocess(\n+        self,\n+        images: ImageInput,\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\",\n+        do_resize: bool,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, list[float]],\n+        image_std: Union[float, list[float]],\n+        downsample_factor: int,\n+        do_image_splitting: bool,\n+        min_tiles: int,\n+        max_tiles: int,\n+        use_thumbnail: bool,\n+        min_image_tokens: int,\n+        max_image_tokens: int,\n+        encoder_patch_size: int,\n+        tile_size: int,\n+        max_pixels_tolerance: float,\n+        return_tensors: Union[str, TensorType],\n+        disable_grouping: bool,\n+        do_pad: bool,\n+        return_row_col_info: bool,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        if not do_image_splitting:\n+            min_tiles = 1\n+            max_tiles = 1\n+            logger.debug(\n+                \"Image splitting is disabled, setting min_tiles and max_tiles to 1. Set do_image_splitting=True to enable splitting.\"\n+            )\n+\n+        if do_image_splitting and min_tiles > max_tiles:\n+            raise ValueError(\"min_tiles must be less than or equal to max_tiles\")\n+\n+        max_thumbnail_image_patches = max_image_tokens * downsample_factor**2\n+        tile_size_patches = (tile_size // encoder_patch_size) ** 2 if do_image_splitting else 0\n+        max_num_patches = max(\n+            max_thumbnail_image_patches,\n+            tile_size_patches,\n+        )\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        resized_image_sizes = {}\n+        rows_grouped, cols_grouped = {}, {}\n+        for shape, stacked_images in grouped_images.items():\n+            num_rows = [1] * stacked_images.shape[0]\n+            num_cols = [1] * stacked_images.shape[0]\n+            height, width = stacked_images.shape[-2:]\n+            image_sizes = [[height, width]] * stacked_images.shape[0]\n+            do_resize = True\n+\n+            if do_resize:\n+                stacked_images, num_rows, num_cols, image_sizes = self.resize_and_split(\n+                    stacked_images,\n+                    downsample_factor=downsample_factor,\n+                    min_tiles=min_tiles,\n+                    max_tiles=max_tiles,\n+                    use_thumbnail=use_thumbnail,\n+                    min_image_tokens=min_image_tokens,\n+                    max_image_tokens=max_image_tokens,\n+                    encoder_patch_size=encoder_patch_size,\n+                    tile_size=tile_size,\n+                    max_pixels_tolerance=max_pixels_tolerance,\n+                    interpolation=interpolation,\n+                )\n+\n+            rows_grouped[shape] = num_rows\n+            cols_grouped[shape] = num_cols\n+            resized_image_sizes[shape] = image_sizes\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+        batch_rows = reorder_images(rows_grouped, grouped_images_index)\n+        batch_cols = reorder_images(cols_grouped, grouped_images_index)\n+        resized_image_sizes = reorder_images(resized_image_sizes, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+\n+        processed_images_grouped = {}\n+        processed_masks, processed_spatial_shapes = {}, {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            batch_size, *_, height, width = stacked_images.shape\n+            num_patches_height = height // encoder_patch_size\n+            num_patches_width = width // encoder_patch_size\n+\n+            stacked_images = convert_image_to_patches(stacked_images, encoder_patch_size)\n+            processed_spatial_shapes[shape] = [[num_patches_height, num_patches_width]] * batch_size\n+\n+            if do_pad:\n+                stacked_images, pixel_mask = pad_along_first_dim(stacked_images, max_num_patches)\n+                processed_masks[shape] = [pixel_mask] * batch_size\n+\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+        data = {\"pixel_values\": torch.cat([torch.stack(images) for images in processed_images])}\n+\n+        if do_pad:\n+            processed_masks = reorder_images(processed_masks, grouped_images_index, is_nested=True)\n+            processed_spatial_shapes = reorder_images(processed_spatial_shapes, grouped_images_index, is_nested=True)\n+            processed_masks = torch.cat([torch.stack(masks) for masks in processed_masks])\n+            processed_spatial_shapes = torch.cat(\n+                [torch.tensor(spatial_shape) for spatial_shape in processed_spatial_shapes]\n+            )\n+            data.update({\"pixel_attention_mask\": processed_masks, \"spatial_shapes\": processed_spatial_shapes})\n+\n+        if return_row_col_info:\n+            data[\"image_rows\"] = batch_rows\n+            data[\"image_cols\"] = batch_cols\n+            data[\"image_sizes\"] = resized_image_sizes\n+\n+        encoding = BatchFeature(data=data, tensor_type=return_tensors)\n+        return encoding\n+\n+\n+__all__ = [\"Lfm2VlImageProcessorFast\"]"
        },
        {
            "sha": "deee35394ee1fc64e7e10e6f96a4a9a892158b64",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "added",
            "additions": 497,
            "deletions": 0,
            "changes": 497,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,497 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/lfm2_vl/modular_lfm2_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lfm2_vl.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ..auto import AutoModel\n+from .configuration_lfm2_vl import Lfm2VlConfig\n+\n+\n+class Lfm2VlMultiModalProjector(nn.Module):\n+    def __init__(self, config: Lfm2VlConfig):\n+        super().__init__()\n+        in_channels = config.vision_config.hidden_size * (config.downsample_factor**2)\n+        self.factor = config.downsample_factor\n+        self.layer_norm = nn.LayerNorm(in_channels)\n+        self.linear_1 = nn.Linear(\n+            in_channels,\n+            config.projector_hidden_size,\n+            bias=config.projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.projector_hidden_size,\n+            config.text_config.hidden_size,\n+            bias=config.projector_bias,\n+        )\n+\n+    def forward(self, image_features: torch.Tensor):\n+        image_features = self.pixel_unshuffle(image_features)\n+        image_features = self.layer_norm(image_features)\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+    def pixel_unshuffle(self, hidden_states: torch.Tensor):\n+        batch_size, width, height, channels = hidden_states.size()\n+        hidden_states = hidden_states.reshape(batch_size, width, height // self.factor, channels * self.factor)\n+        hidden_states = hidden_states.permute(0, 2, 1, 3)\n+        hidden_states = hidden_states.reshape(\n+            batch_size, height // self.factor, width // self.factor, channels * self.factor**2\n+        )\n+        hidden_states = hidden_states.permute(0, 2, 1, 3)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Lfm2VlPreTrainedModel(PreTrainedModel):\n+    config: Lfm2VlConfig\n+    base_model_prefix = \"\"\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _can_compile_fullgraph = False\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Lfm2Vl causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Lfm2VlCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Cache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Lfm2Vl outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class Lfm2VlModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Lfm2Vl model which consists of a vision backbone and a language model, without a language modeling head.\n+    \"\"\"\n+)\n+class Lfm2VlModel(Lfm2VlPreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: Lfm2VlConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+\n+        self.multi_modal_projector = Lfm2VlMultiModalProjector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        spatial_shapes: torch.Tensor,\n+        pixel_attention_mask: torch.Tensor,\n+        **kwargs,\n+    ) -> list[torch.Tensor]:\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            spatial_shapes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                The spatial shapes of the input images.\n+            pixel_attention_mask (`torch.Tensor` of shape `(batch_size, height, width)`):\n+                The pixel attention mask of the input images.\n+        Returns:\n+            image_features (`list[torch.Tensor]`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(\n+            pixel_values=pixel_values,\n+            spatial_shapes=spatial_shapes,\n+            pixel_attention_mask=pixel_attention_mask,\n+        ).last_hidden_state\n+\n+        img_feature_lengths = pixel_attention_mask.sum(dim=1)\n+        image_features = []\n+\n+        for img_idx in range(image_outputs.size(0)):\n+            feature = image_outputs[img_idx]\n+            # unpad the image representation\n+            feature = feature[: img_feature_lengths[img_idx], :].unsqueeze(0)\n+\n+            # reshape to original height and width\n+            feature_org_h, feature_org_w = spatial_shapes[img_idx]\n+            feature = feature.reshape(1, feature_org_h, feature_org_w, -1)\n+\n+            # project the image representation\n+            img_embedding = self.multi_modal_projector(feature)\n+\n+            # flatten here to handle variable length in naflex\n+            img_embedding = img_embedding.reshape(-1, img_embedding.size(-1))\n+            image_features.append(img_embedding)\n+\n+        return image_features\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        spatial_shapes: Optional[torch.Tensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Lfm2VlModelOutputWithPast]:\n+        r\"\"\"\n+        spatial_shapes (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n+            The spatial shapes of the input images.\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, height, width)`, *optional*):\n+            The pixel attention mask of the input images.\n+        \"\"\"\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                spatial_shapes=spatial_shapes,\n+                pixel_attention_mask=pixel_attention_mask,\n+            )\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids=input_ids,\n+                inputs_embeds=inputs_embeds,\n+                image_features=image_features,\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return Lfm2VlModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The LFM2_VL model which consists of a vision backbone and a language model.\n+    \"\"\"\n+)\n+class Lfm2VlForConditionalGeneration(Lfm2VlPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: Lfm2VlConfig):\n+        super().__init__(config)\n+        self.model = Lfm2VlModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        spatial_shapes: torch.Tensor,\n+        pixel_attention_mask: torch.Tensor,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            spatial_shapes=spatial_shapes,\n+            pixel_attention_mask=pixel_attention_mask,\n+            **kwargs,\n+        )\n+\n+    # Make modules available through conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        spatial_shapes: Optional[torch.Tensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Lfm2VlCausalLMOutputWithPast]:\n+        r\"\"\"\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, channels, height, width)`, *optional*):\n+            The input image tensors.\n+        spatial_shapes (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n+            The spatial shapes of the input images.\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, height, width)`, *optional*):\n+            The pixel attention mask of the input images.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+        >>> from transformers.image_utils import load_image\n+\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\n+        ...     \"LiquidAI/LFM2-VL-1.6B\",\n+        ... )\n+        >>> processor = AutoProcessor.from_pretrained(\n+        ...     \"LiquidAI/LFM2-VL-1.6B\",\n+        ... )\n+\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = load_image(url)\n+\n+        >>> conversation = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"image\", \"image\": image},\n+        ...             {\"type\": \"text\", \"text\": \"What is in this image?\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     conversation,\n+        ...     add_generation_prompt=True,\n+        ...     tokenize=True,\n+        ...     return_dict=True,\n+        ...     return_tensors=\"pt\"\n+        ... )\n+\n+        >>> # Generate\n+        >>> outputs = model.generate(**inputs, max_new_tokens=45)\n+        >>> processor.batch_decode(outputs, skip_special_tokens=True)[0]\n+        'This image depicts a vibrant street scene in what appears to be a Chinatown or similar cultural area. The focal point is a large red stop sign with white lettering, mounted on a pole.'\n+        ```\"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            spatial_shapes=spatial_shapes,\n+            pixel_attention_mask=pixel_attention_mask,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits,\n+                labels=labels,\n+                vocab_size=self.config.text_config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        return Lfm2VlCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"Lfm2VlForConditionalGeneration\", \"Lfm2VlPreTrainedModel\", \"Lfm2VlModel\"]"
        },
        {
            "sha": "68367464c3cf75a093c0469a3e44c65e4a3bbd27",
            "filename": "src/transformers/models/lfm2_vl/modular_lfm2_vl.py",
            "status": "added",
            "additions": 352,
            "deletions": 0,
            "changes": 352,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,352 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Lfm2-VL model.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ..llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaModelOutputWithPast,\n+    LlavaPreTrainedModel,\n+)\n+from .configuration_lfm2_vl import Lfm2VlConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2VlMultiModalProjector(nn.Module):\n+    def __init__(self, config: Lfm2VlConfig):\n+        super().__init__()\n+        in_channels = config.vision_config.hidden_size * (config.downsample_factor**2)\n+        self.factor = config.downsample_factor\n+        self.layer_norm = nn.LayerNorm(in_channels)\n+        self.linear_1 = nn.Linear(\n+            in_channels,\n+            config.projector_hidden_size,\n+            bias=config.projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.projector_hidden_size,\n+            config.text_config.hidden_size,\n+            bias=config.projector_bias,\n+        )\n+\n+    def forward(self, image_features: torch.Tensor):\n+        image_features = self.pixel_unshuffle(image_features)\n+        image_features = self.layer_norm(image_features)\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+    def pixel_unshuffle(self, hidden_states: torch.Tensor):\n+        batch_size, width, height, channels = hidden_states.size()\n+        hidden_states = hidden_states.reshape(batch_size, width, height // self.factor, channels * self.factor)\n+        hidden_states = hidden_states.permute(0, 2, 1, 3)\n+        hidden_states = hidden_states.reshape(\n+            batch_size, height // self.factor, width // self.factor, channels * self.factor**2\n+        )\n+        hidden_states = hidden_states.permute(0, 2, 1, 3)\n+        return hidden_states\n+\n+\n+class Lfm2VlPreTrainedModel(LlavaPreTrainedModel):\n+    _can_compile_fullgraph = False\n+\n+\n+class Lfm2VlCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+class Lfm2VlModelOutputWithPast(LlavaModelOutputWithPast):\n+    pass\n+\n+\n+class Lfm2VlModel(LlavaModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: Lfm2VlConfig):\n+        super().__init__(config)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        spatial_shapes: torch.Tensor,\n+        pixel_attention_mask: torch.Tensor,\n+        **kwargs,\n+    ) -> list[torch.Tensor]:\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            spatial_shapes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                The spatial shapes of the input images.\n+            pixel_attention_mask (`torch.Tensor` of shape `(batch_size, height, width)`):\n+                The pixel attention mask of the input images.\n+        Returns:\n+            image_features (`list[torch.Tensor]`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(\n+            pixel_values=pixel_values,\n+            spatial_shapes=spatial_shapes,\n+            pixel_attention_mask=pixel_attention_mask,\n+        ).last_hidden_state\n+\n+        img_feature_lengths = pixel_attention_mask.sum(dim=1)\n+        image_features = []\n+\n+        for img_idx in range(image_outputs.size(0)):\n+            feature = image_outputs[img_idx]\n+            # unpad the image representation\n+            feature = feature[: img_feature_lengths[img_idx], :].unsqueeze(0)\n+\n+            # reshape to original height and width\n+            feature_org_h, feature_org_w = spatial_shapes[img_idx]\n+            feature = feature.reshape(1, feature_org_h, feature_org_w, -1)\n+\n+            # project the image representation\n+            img_embedding = self.multi_modal_projector(feature)\n+\n+            # flatten here to handle variable length in naflex\n+            img_embedding = img_embedding.reshape(-1, img_embedding.size(-1))\n+            image_features.append(img_embedding)\n+\n+        return image_features\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        spatial_shapes: Optional[torch.Tensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Lfm2VlModelOutputWithPast]:\n+        r\"\"\"\n+        spatial_shapes (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n+            The spatial shapes of the input images.\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, height, width)`, *optional*):\n+            The pixel attention mask of the input images.\n+        \"\"\"\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                spatial_shapes=spatial_shapes,\n+                pixel_attention_mask=pixel_attention_mask,\n+            )\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids=input_ids,\n+                inputs_embeds=inputs_embeds,\n+                image_features=image_features,\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return Lfm2VlModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+class Lfm2VlForConditionalGeneration(LlavaForConditionalGeneration):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        spatial_shapes: torch.Tensor,\n+        pixel_attention_mask: torch.Tensor,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            spatial_shapes=spatial_shapes,\n+            pixel_attention_mask=pixel_attention_mask,\n+            **kwargs,\n+        )\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        spatial_shapes: Optional[torch.Tensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, Lfm2VlCausalLMOutputWithPast]:\n+        r\"\"\"\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, channels, height, width)`, *optional*):\n+            The input image tensors.\n+        spatial_shapes (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n+            The spatial shapes of the input images.\n+        pixel_attention_mask (`torch.Tensor` of shape `(batch_size, height, width)`, *optional*):\n+            The pixel attention mask of the input images.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+        >>> from transformers.image_utils import load_image\n+\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\n+        ...     \"LiquidAI/LFM2-VL-1.6B\",\n+        ... )\n+        >>> processor = AutoProcessor.from_pretrained(\n+        ...     \"LiquidAI/LFM2-VL-1.6B\",\n+        ... )\n+\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = load_image(url)\n+\n+        >>> conversation = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"image\", \"image\": image},\n+        ...             {\"type\": \"text\", \"text\": \"What is in this image?\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     conversation,\n+        ...     add_generation_prompt=True,\n+        ...     tokenize=True,\n+        ...     return_dict=True,\n+        ...     return_tensors=\"pt\"\n+        ... )\n+\n+        >>> # Generate\n+        >>> outputs = model.generate(**inputs, max_new_tokens=45)\n+        >>> processor.batch_decode(outputs, skip_special_tokens=True)[0]\n+        'This image depicts a vibrant street scene in what appears to be a Chinatown or similar cultural area. The focal point is a large red stop sign with white lettering, mounted on a pole.'\n+        ```\"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            spatial_shapes=spatial_shapes,\n+            pixel_attention_mask=pixel_attention_mask,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits,\n+                labels=labels,\n+                vocab_size=self.config.text_config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        return Lfm2VlCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+\n+__all__ = [\"Lfm2VlForConditionalGeneration\", \"Lfm2VlPreTrainedModel\", \"Lfm2VlModel\"]"
        },
        {
            "sha": "12f289c266a14ae1ab84e07f748b6a84c72ea5be",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "added",
            "additions": 269,
            "deletions": 0,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,269 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput, make_nested_list_of_images\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+)\n+from ...tokenization_utils_base import BatchEncoding, TextInput\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2VlImagesKwargs(ImagesKwargs, total=False):\n+    downsample_factor: Optional[int]\n+    do_image_splitting: Optional[bool]\n+    min_tiles: Optional[int]\n+    max_tiles: Optional[int]\n+    use_thumbnail: Optional[bool]\n+    min_image_tokens: Optional[int]\n+    max_image_tokens: Optional[int]\n+    encoder_patch_size: Optional[int]\n+    tile_size: Optional[int]\n+    max_pixels_tolerance: Optional[float]\n+    patch_size: Optional[int]\n+    do_pad: Optional[bool]\n+    return_row_col_info: Optional[bool]\n+\n+\n+class Lfm2VlProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Lfm2VlImagesKwargs\n+\n+    _defaults = {\n+        \"images_kwargs\": {\n+            \"return_row_col_info\": True,\n+        },\n+        \"text_kwargs\": {\n+            \"use_image_special_tokens\": True,\n+            \"add_special_tokens\": False,\n+            \"padding\": False,\n+            \"is_split_into_words\": False,\n+        },\n+    }\n+\n+\n+class Lfm2VlProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Lfm2Vl processor which wraps a Lfm2Tokenizer tokenizer and Lfm2VlImageProcessor into a single processor.\n+\n+    [`Lfm2VlProcessor`] offers all the functionalities of [`Lfm2ImageProcessor`] and [`Lfm2Tokenizer`].\n+\n+    Args:\n+        image_processor (`Lfm2VlImageProcessor`):\n+             An instance of [`Lfm2VlImageProcessor`]. The image processor is a required input.\n+        tokenizer (`PreTrainedTokenizerBase`):\n+            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n+        use_image_special_tokens (`bool`, *optional*, defaults to `True`):\n+            Whether to use image special tokens or not when processing.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"Lfm2VlImageProcessorFast\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor,\n+        tokenizer,\n+        chat_template: Optional[str] = None,\n+        use_image_special_tokens: Optional[bool] = True,\n+        **kwargs,\n+    ):\n+        self.image_token = tokenizer.image_token\n+        self.image_token_id = tokenizer.image_token_id\n+        self.use_image_special_tokens = use_image_special_tokens\n+        self.image_start_token = tokenizer.image_start_token\n+        self.image_end_token = tokenizer.image_end_token\n+        self.image_thumbnail_token = tokenizer.image_thumbnail\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n+\n+    def __call__(\n+        self,\n+        images: Optional[Union[ImageInput, list[ImageInput], list[list[ImageInput]]]] = None,\n+        text: Optional[Union[TextInput, list[TextInput]]] = None,\n+        **kwargs: Unpack[Lfm2VlProcessorKwargs],\n+    ) -> BatchEncoding:\n+        \"\"\"\n+        Processes the input prompts and returns a BatchFeature.\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, *optional*):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. If is of type `list[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n+            text (`TextInput`, *optional*):\n+                The sequence or batch of sequences to be encoded.\n+                Wherever an image token, `<image>` is encountered it is expanded to a proper sequence of image tokens.\n+            return_tensors (`Optional[str, TensorType]`, *optional*):\n+                If set, will return tensors of a particular framework. See [`PreTrainedTokenizerFast.__call__`] for more\n+                information.\n+        \"\"\"\n+        if text is None and images is None:\n+            raise ValueError(\"You must provide one of `text` or `images`.\")\n+\n+        if images is not None and text is None:\n+            raise ValueError(\n+                \"You must provide `text` when `images` is provided. Minimal text consists of a single image token.\"\n+            )\n+\n+        output_kwargs = self._merge_kwargs(\n+            Lfm2VlProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        n_images_in_text = [sample.count(self.image_token) for sample in text]\n+        if sum(n_images_in_text) > 0 and images is None:\n+            raise ValueError(f\"We detected {sum(n_images_in_text)} tokens in the text but no images were passed\")\n+\n+        inputs = {}\n+        use_image_special_tokens = output_kwargs[\"text_kwargs\"].pop(\"use_image_special_tokens\")\n+\n+        if images is not None:\n+            images = self.image_processor.fetch_images(images)\n+            batched_images = make_nested_list_of_images(images)\n+            vision_inputs = self.image_processor(batched_images, **output_kwargs[\"images_kwargs\"])\n+\n+            n_images_in_images = [len(sublist) for sublist in batched_images]\n+            if n_images_in_images != n_images_in_text:\n+                raise ValueError(\n+                    f\"The number of images in the text {n_images_in_text} and images {n_images_in_images} should be the same.\"\n+                )\n+\n+            text = self.expand_text_with_placeholders(\n+                text,\n+                batched_images,\n+                image_rows=vision_inputs.pop(\"image_rows\"),\n+                image_cols=vision_inputs.pop(\"image_cols\"),\n+                image_sizes=vision_inputs.pop(\"image_sizes\"),\n+                use_image_special_tokens=use_image_special_tokens,\n+                **output_kwargs[\"images_kwargs\"],\n+            )\n+            inputs.update(vision_inputs)\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        inputs.update(text_inputs)\n+\n+        return BatchFeature(inputs, tensor_type=return_tensors)\n+\n+    def expand_text_with_placeholders(\n+        self,\n+        text: list[str],\n+        images: list[list[ImageInput]],\n+        image_rows: list[list[int]],\n+        image_cols: list[list[int]],\n+        image_sizes: list[list[int]],\n+        use_image_special_tokens: bool,\n+        **images_kwargs,\n+    ):\n+        prompt_strings = []\n+\n+        image_data = iter(zip(*[image_rows, image_cols, image_sizes]))\n+        for sample_text, sample_images in zip(text, images):\n+            split_sample = sample_text.split(self.image_token)\n+            sample_text_with_image_tokens = \"\"\n+            for i, image in enumerate(sample_images):\n+                sample_text_with_image_tokens += split_sample[i]\n+                if use_image_special_tokens:\n+                    sample_text_with_image_tokens += self.image_start_token\n+\n+                rows, cols, image_size = next(image_data)\n+                num_thumbnail_tokens, num_tokens_per_tile = self._get_image_num_tokens(image_size, **images_kwargs)\n+\n+                if rows > 1 or cols > 1:\n+                    for row in range(rows):\n+                        for col in range(cols):\n+                            if use_image_special_tokens:\n+                                sample_text_with_image_tokens += f\"<|img_row_{row + 1}_col_{col + 1}|>\"\n+                            sample_text_with_image_tokens += self.image_token * num_tokens_per_tile\n+\n+                    if num_thumbnail_tokens > 0:\n+                        if use_image_special_tokens:\n+                            sample_text_with_image_tokens += self.image_thumbnail_token\n+                        sample_text_with_image_tokens += self.image_token * num_thumbnail_tokens\n+                else:\n+                    sample_text_with_image_tokens += self.image_token * num_thumbnail_tokens\n+\n+                if use_image_special_tokens:\n+                    sample_text_with_image_tokens += self.image_end_token\n+\n+                sample_text_with_image_tokens += split_sample[i + 1]\n+            prompt_strings.append(sample_text_with_image_tokens)\n+\n+        return prompt_strings\n+\n+    def _get_image_num_tokens(self, image_size: list[int], **images_kwargs) -> tuple[int, int]:\n+        tile_size = images_kwargs.get(\"tile_size\", self.image_processor.tile_size)\n+        downsample_factor = images_kwargs.get(\"downsample_factor\", self.image_processor.downsample_factor)\n+        encoder_patch_size = images_kwargs.get(\"encoder_patch_size\", self.image_processor.encoder_patch_size)\n+        use_thumbnail = images_kwargs.get(\"use_thumbnail\", self.image_processor.use_thumbnail)\n+\n+        thumbnail_tokens = 0\n+        if use_thumbnail:\n+            image_height, image_width = image_size\n+            num_patches_height = image_height // encoder_patch_size\n+            num_patches_width = image_width // encoder_patch_size\n+            dwn_num_patches_height = math.ceil(num_patches_height / downsample_factor)\n+            dwn_num_patches_width = math.ceil(num_patches_width / downsample_factor)\n+            thumbnail_tokens = dwn_num_patches_height * dwn_num_patches_width\n+\n+        num_patches_tile = tile_size // encoder_patch_size\n+        dwn_num_patches_tile = math.ceil(num_patches_tile / downsample_factor)\n+        tile_tokens = dwn_num_patches_tile * dwn_num_patches_tile\n+\n+        return thumbnail_tokens, tile_tokens\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LFM2Tokeniser's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        batched_decode_output = self.tokenizer.batch_decode(*args, **kwargs)\n+        return batched_decode_output\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LFM2Tokeniser's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        decode_output = self.tokenizer.decode(*args, **kwargs)\n+        return decode_output\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+\n+        # LFM2-VL has no dedicated tokenizer class and uses the Base class with default model input names\n+        tokenizer_input_names = [name for name in tokenizer_input_names if name != \"token_type_ids\"]\n+        return list(tokenizer_input_names + image_processor_input_names)\n+\n+\n+__all__ = [\"Lfm2VlProcessor\"]"
        },
        {
            "sha": "094c5861ab1052a35f46d6314ab3e380cf4fcf7d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -2387,6 +2387,7 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n             \"zamba\",\n             \"zamba2\",\n             \"lfm2\",\n+            \"lfm2-vl\",\n         )\n         has_standard_cache = not any(\n             model_name in config.__class__.__name__.lower() for model_name in models_without_standard_cache"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/lfm2_vl/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2F__init__.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de"
        },
        {
            "sha": "8edf59ac78e08e5e786678205cd72dba1fb66625",
            "filename": "tests/models/lfm2_vl/test_image_processing_lfm2_vl.py",
            "status": "added",
            "additions": 289,
            "deletions": 0,
            "changes": 289,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2Ftest_image_processing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2Ftest_image_processing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_image_processing_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,289 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    if is_torchvision_available():\n+        from transformers import Lfm2VlImageProcessorFast\n+        from transformers.models.lfm2_vl.image_processing_lfm2_vl_fast import find_closest_aspect_ratio\n+\n+\n+class Lfm2VlImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        num_images=1,\n+        min_resolution=256,\n+        max_resolution=1024,\n+        downsample_factor=2,\n+        do_image_splitting=False,\n+        min_tiles=2,\n+        max_tiles=10,\n+        use_thumbnail=True,\n+        min_image_tokens=64,\n+        max_image_tokens=256,\n+        encoder_patch_size=16,\n+        tile_size=512,\n+        max_pixels_tolerance=2.0,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.num_images = num_images\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+\n+        self.downsample_factor = downsample_factor\n+        self.do_image_splitting = do_image_splitting\n+        self.min_tiles = min_tiles\n+        self.max_tiles = max_tiles\n+        self.use_thumbnail = use_thumbnail\n+        self.min_image_tokens = min_image_tokens\n+        self.max_image_tokens = max_image_tokens\n+        self.encoder_patch_size = encoder_patch_size\n+        self.tile_size = tile_size\n+        self.max_pixels_tolerance = max_pixels_tolerance\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"downsample_factor\": self.downsample_factor,\n+            \"do_image_splitting\": self.do_image_splitting,\n+            \"min_tiles\": self.min_tiles,\n+            \"max_tiles\": self.max_tiles,\n+            \"use_thumbnail\": self.use_thumbnail,\n+            \"min_image_tokens\": self.min_image_tokens,\n+            \"max_image_tokens\": self.max_image_tokens,\n+            \"encoder_patch_size\": self.encoder_patch_size,\n+            \"tile_size\": self.tile_size,\n+            \"max_pixels_tolerance\": self.max_pixels_tolerance,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        images = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+        return [[image] for image in images]\n+\n+\n+@require_torch\n+@require_vision\n+class Lfm2VlImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    test_slow_image_processor = False\n+    fast_image_processing_class = Lfm2VlImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Lfm2VlImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"downsample_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"min_tiles\"))\n+            self.assertTrue(hasattr(image_processing, \"max_tiles\"))\n+            self.assertTrue(hasattr(image_processing, \"use_thumbnail\"))\n+            self.assertTrue(hasattr(image_processing, \"min_image_tokens\"))\n+            self.assertTrue(hasattr(image_processing, \"max_image_tokens\"))\n+            self.assertTrue(hasattr(image_processing, \"encoder_patch_size\"))\n+            self.assertTrue(hasattr(image_processing, \"tile_size\"))\n+            self.assertTrue(hasattr(image_processing, \"max_pixels_tolerance\"))\n+\n+    @require_vision\n+    def test_smart_resize(self):\n+        # verify that smart resize output dims are divisible by encoder_patch_size * downsample_factor\n+        image_processing = self.fast_image_processing_class(**self.image_processor_dict)\n+        width, height = image_processing.smart_resize(\n+            height=500,\n+            width=300,\n+            downsample_factor=image_processing.downsample_factor,\n+            min_image_tokens=image_processing.min_image_tokens,\n+            max_image_tokens=image_processing.max_image_tokens,\n+            encoder_patch_size=image_processing.encoder_patch_size,\n+        )\n+        mod = image_processing.encoder_patch_size * image_processing.downsample_factor\n+        self.assertEqual(width % mod, 0)\n+        self.assertEqual(height % mod, 0)\n+\n+    @require_vision\n+    def test_get_grid_layout(self):\n+        # splitting a 512Ã—512 image into tiles of size processor.image_processor.tile_size\n+        image_processing = self.fast_image_processing_class(**self.image_processor_dict)\n+        rows, cols, _, _, num_patches = image_processing._get_grid_layout(\n+            height=1024,\n+            width=1024,\n+            min_tiles=image_processing.min_tiles,\n+            max_tiles=image_processing.max_tiles,\n+            tile_size=image_processing.tile_size,\n+        )\n+        self.assertEqual(num_patches, 4)\n+        self.assertEqual(num_patches, rows * cols)\n+\n+        rows, cols, _, _, num_patches = image_processing._get_grid_layout(\n+            height=1024,\n+            width=1024,\n+            min_tiles=8,\n+            max_tiles=8,\n+            tile_size=image_processing.tile_size,\n+        )\n+        self.assertEqual(num_patches, 8)\n+        self.assertEqual(num_patches, rows * cols)\n+\n+    def test_find_closest_aspect_ratio(self):\n+        # should pick (1,1) over (2,1) for a square image\n+        result = find_closest_aspect_ratio(1.0, [(1, 1), (2, 1)], width=100, height=100, image_size=100)\n+        self.assertEqual(result, (1, 1))\n+\n+        result = find_closest_aspect_ratio(0.5, [(1, 1), (1, 2)], width=100, height=200, image_size=200)\n+        self.assertEqual(result, (1, 2))\n+\n+    def test_call_numpy(self):\n+        # Initialize image_processing\n+        image_processing = self.fast_image_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+        for sample_images in image_inputs:\n+            for image in sample_images:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (1, image_processing.max_num_patches, 3 * image_processing.encoder_patch_size**2),\n+        )\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (\n+                self.image_processor_tester.batch_size,\n+                image_processing.max_num_patches,\n+                3 * image_processing.encoder_patch_size**2,\n+            ),\n+        )\n+\n+    def test_call_numpy_4_channels(self):\n+        # Lfm2Vl always processes images as RGB, so it always returns images with 3 channels\n+        # Initialize image_processing\n+        image_processor_dict = self.image_processor_dict\n+        image_processing = self.fast_image_processing_class(**image_processor_dict)\n+        # create random numpy tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+\n+        for sample_images in image_inputs:\n+            for image in sample_images:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (1, image_processing.max_num_patches, 3 * image_processing.encoder_patch_size**2),\n+        )\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (\n+                self.image_processor_tester.batch_size,\n+                image_processing.max_num_patches,\n+                3 * image_processing.encoder_patch_size**2,\n+            ),\n+        )\n+\n+    def test_call_pil(self):\n+        # Initialize image_processing\n+        image_processing = self.fast_image_processing_class(**self.image_processor_dict)\n+        # create random PIL images\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+        for images in image_inputs:\n+            for image in images:\n+                self.assertIsInstance(image, Image.Image)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (1, image_processing.max_num_patches, 3 * image_processing.encoder_patch_size**2),\n+        )\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (\n+                self.image_processor_tester.batch_size,\n+                image_processing.max_num_patches,\n+                3 * image_processing.encoder_patch_size**2,\n+            ),\n+        )\n+\n+    def test_call_pytorch(self):\n+        # Initialize image_processing\n+        image_processing = self.fast_image_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+        for images in image_inputs:\n+            for image in images:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (1, image_processing.max_num_patches, 3 * image_processing.encoder_patch_size**2),\n+        )\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (\n+                self.image_processor_tester.batch_size,\n+                image_processing.max_num_patches,\n+                3 * image_processing.encoder_patch_size**2,\n+            ),\n+        )"
        },
        {
            "sha": "42c732887af00029e048a2837dfc3bd6834c897c",
            "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
            "status": "added",
            "additions": 296,
            "deletions": 0,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,296 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the LFM2-VL model.\"\"\"\n+\n+import math\n+import unittest\n+from io import BytesIO\n+\n+import pytest\n+import requests\n+\n+from transformers import AutoProcessor, is_torch_available\n+from transformers.models.lfm2_vl.modeling_lfm2_vl import Lfm2VlForConditionalGeneration\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_vision_available\n+\n+from ...causal_lm_tester import CausalLMModelTester\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Lfm2VlConfig, Lfm2VlForConditionalGeneration, Lfm2VlModel\n+\n+\n+class Lfm2VlModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Lfm2VlConfig\n+        base_model_class = Lfm2VlModel\n+        causal_lm_class = Lfm2VlForConditionalGeneration\n+\n+    def __init__(\n+        self,\n+        parent,\n+        is_training=True,\n+        batch_size=2,\n+        scale_factor=2,\n+        num_images=2,\n+        vision_config={\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 37,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 2,\n+            \"num_channels\": 3,\n+            \"num_patches\": 16,\n+            \"patch_size\": 4,\n+            \"hidden_act\": \"gelu_pytorch_tanh\",\n+            \"layer_norm_eps\": 1e-6,\n+            \"attention_dropout\": 0.0,\n+        },\n+        text_config={\n+            \"vocab_size\": 100,\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 37,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"max_position_embeddings\": 100,\n+            \"pad_token_id\": 0,\n+            \"bos_token_id\": 1,\n+            \"eos_token_id\": 2,\n+            \"tie_word_embeddings\": True,\n+            \"rope_theta\": 1000000.0,\n+            \"conv_bias\": False,\n+            \"conv_L_cache\": 3,\n+            \"block_multiple_of\": 2,\n+            \"full_attn_idxs\": [0],\n+        },\n+        image_token_id=4,\n+        downsample_factor=4,\n+        projector_hidden_size=32,\n+    ):\n+        super().__init__(parent)\n+        self.vision_config = vision_config\n+        self.text_config = text_config\n+        self.image_token_id = image_token_id\n+        self.is_training = is_training\n+        self.batch_size = batch_size\n+        self.scale_factor = scale_factor\n+        self.num_images = num_images\n+        self.downsample_factor = downsample_factor\n+        self.projector_hidden_size = projector_hidden_size\n+        self.image_seq_length = 4\n+\n+    def get_config(self):\n+        return Lfm2VlConfig(\n+            vision_config=self.vision_config,\n+            text_config=self.text_config,\n+            image_token_id=self.image_token_id,\n+            downsample_factor=self.downsample_factor,\n+            projector_hidden_size=self.projector_hidden_size,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        # Create dummy pixel values: [num_images, num_patches, channels * patch_size^2]\n+        patch_size = self.vision_config[\"patch_size\"]\n+        pixel_values = floats_tensor([self.num_images, 64, 3 * patch_size * patch_size])\n+\n+        # Spatial shapes: one (height_patches, width_patches) per image\n+        patches = int(math.sqrt(64))\n+        spatial_shapes = torch.tensor([[patches, patches]] * self.num_images, dtype=torch.long, device=torch_device)\n+\n+        # Pixel attention mask: mark all patches as valid (no padding)\n+        pixel_attention_mask = torch.ones((self.num_images, 64), dtype=torch.long, device=torch_device)\n+        config = self.get_config()\n+        return config, pixel_values, spatial_shapes, pixel_attention_mask\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, spatial_shapes, pixel_attention_mask = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 1\n+\n+        # For simplicity just set the last n tokens to the image token\n+        input_ids[input_ids == self.image_token_id] = self.text_config[\"pad_token_id\"]\n+        input_ids[:, -self.image_seq_length :] = self.image_token_id\n+\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"spatial_shapes\": spatial_shapes,\n+            \"pixel_attention_mask\": pixel_attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Lfm2VlModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    all_model_classes = (Lfm2VlModel, Lfm2VlForConditionalGeneration) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Lfm2VlModel,\n+            \"text-generation\": Lfm2VlForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    model_tester_class = Lfm2VlModelTester\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Lfm2VlModelTester(self)\n+        common_properties = [\"image_token_id\", \"projector_hidden_size\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Lfm2VlConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(\n+        \"Lfm2 backbone alternates between attention and conv layers, so attention are only returned for attention layers\"\n+    )\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Lfm2 backbone has a special cache format as it alternates between attention and conv layers\")\n+    def test_past_key_values_format(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Lfm2 backbone has a special cache format which is not compatible with compile as it has static address for conv cache\"\n+    )\n+    @pytest.mark.torch_compile_test\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Backbone Siglip2VisionModel does not support standalone training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Backbone Siglip2VisionModel does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Backbone Siglip2VisionModel does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Siglip2 backbone has a non-standard initialization scheme, that this test cannot handle easily\"\n+    )\n+    def test_initialization(self):\n+        pass\n+\n+\n+@require_torch_accelerator\n+@require_read_token\n+@slow\n+class Lfm2VlForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"LiquidAI/LFM2-VL-1.6B\")\n+        self.processor.tokenizer.padding_side = \"left\"\n+        self.image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        self.image2 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+                ).content\n+            )\n+        )\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_integration_test(self):\n+        model = Lfm2VlForConditionalGeneration.from_pretrained(\n+            \"LiquidAI/LFM2-VL-1.6B\",\n+            dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+        )\n+\n+        # Create inputs\n+        text = \"<image>In this image, we see\"\n+        images = self.image\n+        inputs = self.processor(text=text, images=images, return_tensors=\"pt\")\n+        inputs.to(device=torch_device, dtype=torch.bfloat16)\n+\n+        generated_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        expected_generated_text = \"In this image, we see a cat and a dog lying on a pink blanket. They are both sleeping peacefully. They are\"\n+        self.assertEqual(generated_texts[0], expected_generated_text)\n+\n+    def test_integration_test_high_resolution(self):\n+        model = Lfm2VlForConditionalGeneration.from_pretrained(\n+            \"LiquidAI/LFM2-VL-1.6B\",\n+            dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+        )\n+\n+        # Create inputs\n+        text = \"<image>In this image, we see\"\n+        images = self.image2\n+        inputs = self.processor(text=text, images=images, return_tensors=\"pt\")\n+        inputs.to(device=torch_device, dtype=torch.bfloat16)\n+\n+        generated_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        expected_generated_text = (\n+            \"In this image, we see the Statue of Liberty, standing tall on its pedestal. The statue is made of metal,\"\n+        )\n+        self.assertEqual(generated_texts[0], expected_generated_text)\n+\n+    def test_integration_test_batched(self):\n+        model = Lfm2VlForConditionalGeneration.from_pretrained(\n+            \"LiquidAI/LFM2-VL-450M\",\n+            dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+        )\n+\n+        # Create inputs\n+        text = [\"<image>In this image, we see\", \"<image>In this image, there is a cat on\"]\n+        images = [[self.image2], [self.image]]\n+        inputs = self.processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n+        inputs.to(device=torch_device, dtype=torch.bfloat16)\n+\n+        generated_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        expected_generated_text = [\n+            \"In this image, we see a panoramic view of the New York City skyline. The iconic Statics and the New York\",\n+            \"In this image, there is a cat on a bed with a cat on a bed with a cat on a bed with a cat on a bed\",\n+        ]\n+        self.assertListEqual(generated_texts, expected_generated_text)"
        },
        {
            "sha": "f2c33e40e3f6cd1ed7ed3e2553b125d3c42abfe5",
            "filename": "tests/models/lfm2_vl/test_processing_lfm2_vl.py",
            "status": "added",
            "additions": 467,
            "deletions": 0,
            "changes": 467,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -0,0 +1,467 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import AutoTokenizer, Lfm2VlProcessor\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    if is_torchvision_available():\n+        from transformers import Lfm2VlImageProcessorFast\n+\n+\n+@require_torch\n+@require_vision\n+class Lfm2VlProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Lfm2VlProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor_kwargs = cls.prepare_processor_dict()\n+        image_processor = Lfm2VlImageProcessorFast(\n+            tile_size=14,\n+            min_image_tokens=2,\n+            max_image_tokens=10,\n+            encoder_patch_size=2,\n+            do_image_splitting=False,\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-VL-1.6B\", **processor_kwargs)\n+\n+        processor = Lfm2VlProcessor(tokenizer=tokenizer, image_processor=image_processor, **processor_kwargs)\n+        processor.save_pretrained(cls.tmpdirname)\n+\n+        # Create images with different sizes\n+        cls.small_image = Image.new(\"RGB\", (256, 256))\n+        cls.large_image = Image.new(\"RGB\", (512, 1024))\n+        cls.high_res_image = Image.new(\"RGB\", (1024, 1024))\n+\n+        cls.bos_token = processor.tokenizer.bos_token\n+        cls.image_token = processor.image_token\n+\n+        cls.bos_token_id = processor.tokenizer.convert_tokens_to_ids(cls.bos_token)\n+        cls.image_token_id = processor.image_token_id\n+        cls.image_start_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_start_token)\n+        cls.image_end_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_end_token)\n+        cls.padding_token_id = processor.tokenizer.pad_token_id\n+        cls.image_thumbnail_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_thumbnail_token)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return Lfm2VlProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return Lfm2VlProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_processor(self, **kwargs):\n+        return Lfm2VlProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        chat_template = (\n+            \"{{bos_token}}{% for message in messages %}\"\n+            \"{{'<|im_start|>' + message['role'] + '\\n'}}\"\n+            \"{% if message['content'] is string %}\"\n+            \"{{ message['content'] }}\"\n+            \"{% else %}\"\n+            \"{% for content in message['content'] %}\"\n+            \"{% if content['type'] == 'image' %}\"\n+            \"{{ '<image>' }}\"\n+            \"{% elif content['type'] == 'text' %}\"\n+            \"{{ content['text'] }}\"\n+            \"{% endif %}\"\n+            \"{% endfor %}\"\n+            \"{% endif %}\"\n+            \"{{'<|im_end|>\\n'}}\"\n+            \"{% endfor %}\"\n+            \"{% if add_generation_prompt %}\"\n+            \"{{'<|im_start|>assistant\\n' }}\"\n+            \"{% endif %}\"\n+        )\n+        return {\"chat_template\": chat_template, \"use_image_special_tokens\": True}\n+\n+    # Override as Lfm2VL needs images/video to be an explicitly nested batch\n+    def prepare_image_inputs(self, batch_size=None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        images = super().prepare_image_inputs(batch_size)\n+        if isinstance(images, (list, tuple)):\n+            images = [[image] for image in images]\n+        return images\n+\n+    def get_split_image_expected_tokens(self, processor, image_rows, image_cols, add_thumbnail, image_seq_len):\n+        text_split_images = [self.image_start_token_id]\n+        num_patches_tile = processor.image_processor.tile_size // processor.image_processor.encoder_patch_size\n+        tile_seq_len = math.ceil(num_patches_tile / processor.image_processor.downsample_factor) ** 2\n+        for n_h in range(image_rows):\n+            for n_w in range(image_cols):\n+                text_split_images += (\n+                    processor.tokenizer(f\"<|img_row_{n_h + 1}_col_{n_w + 1}|>\", add_special_tokens=False)[\"input_ids\"]\n+                    + [self.image_token_id] * tile_seq_len\n+                )\n+        if add_thumbnail:\n+            text_split_images += [self.image_thumbnail_token_id] + [self.image_token_id] * image_seq_len\n+        text_split_images += [self.image_end_token_id]\n+        return text_split_images\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    def test_process_interleaved_images_prompts_no_image_splitting_single_image(self):\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\", do_image_splitting=False)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        image_str = \"<image>\"\n+\n+        # Test that a single image is processed correctly\n+        inputs = processor(images=self.small_image, text=image_str)\n+        encoder_feature_dims = (\n+            3 * processor.image_processor.encoder_patch_size * processor.image_processor.encoder_patch_size\n+        )\n+        self.assertEqual(\n+            np.array(inputs[\"pixel_values\"]).shape,\n+            (1, processor.image_processor.max_num_patches, encoder_feature_dims),\n+        )\n+        self.assertEqual(\n+            np.array(inputs[\"pixel_attention_mask\"]).shape, (1, processor.image_processor.max_num_patches)\n+        )\n+        self.assertListEqual(inputs[\"spatial_shapes\"].tolist(), [[6, 6]])\n+        # fmt: on\n+\n+    def test_process_interleaved_images_prompts_no_image_splitting_single_image_with_text(self):\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\", do_image_splitting=False)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+\n+        image_str = \"<image>\"\n+        text_str = \"In this image, we see\"\n+        text = image_str + text_str\n+        inputs = processor(text=text, images=self.small_image)\n+\n+        # fmt: off\n+        tokenized_sentence = processor.tokenizer(text_str, add_special_tokens=False)\n+        expected_input_ids = [[self.image_start_token_id] + [self.image_token_id] * 9 + [self.image_end_token_id] + tokenized_sentence[\"input_ids\"]]\n+        self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n+        self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids[0])])\n+        encoder_feature_dims = 3 * processor.image_processor.encoder_patch_size * processor.image_processor.encoder_patch_size\n+        self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (1, processor.image_processor.max_num_patches, encoder_feature_dims))\n+        self.assertEqual(np.array(inputs[\"pixel_attention_mask\"]).shape, (1, processor.image_processor.max_num_patches))\n+        self.assertListEqual(inputs[\"spatial_shapes\"].tolist(), [[6, 6]])\n+        # fmt: on\n+\n+    def test_process_interleaved_images_prompts_no_image_splitting_multiple_images(self):\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\", do_image_splitting=False)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+\n+        image_str = \"<image>\"\n+        text_str_1 = \"In this image, we see\"\n+        text_str_2 = \"In this image, we see\"\n+\n+        text = [\n+            image_str + text_str_1,\n+            image_str + image_str + text_str_2,\n+        ]\n+        images = [[self.small_image], [self.small_image, self.small_image]]\n+\n+        inputs = processor(text=text, images=images, padding=True)\n+\n+        tokenized_sentence_1 = processor.tokenizer(text_str_1, add_special_tokens=False)\n+        tokenized_sentence_2 = processor.tokenizer(text_str_2, add_special_tokens=False)\n+        image_tokens = [self.image_start_token_id] + [self.image_token_id] * 9 + [self.image_end_token_id]\n+        expected_input_ids_1 = image_tokens + tokenized_sentence_1[\"input_ids\"]\n+        expected_input_ids_2 = 2 * image_tokens + tokenized_sentence_2[\"input_ids\"]\n+        # Pad the first input to match the second input\n+        pad_len = len(expected_input_ids_2) - len(expected_input_ids_1)\n+        padded_expected_input_ids_1 = [self.padding_token_id] * pad_len + expected_input_ids_1\n+\n+        self.assertEqual(inputs[\"input_ids\"], [padded_expected_input_ids_1, expected_input_ids_2])\n+        self.assertEqual(\n+            inputs[\"attention_mask\"],\n+            [[0] * pad_len + [1] * len(expected_input_ids_1), [1] * len(expected_input_ids_2)],\n+        )\n+        encoder_feature_dims = (\n+            3 * processor.image_processor.encoder_patch_size * processor.image_processor.encoder_patch_size\n+        )\n+        self.assertEqual(\n+            np.array(inputs[\"pixel_values\"]).shape,\n+            (3, processor.image_processor.max_num_patches, encoder_feature_dims),\n+        )\n+        self.assertEqual(\n+            np.array(inputs[\"pixel_attention_mask\"]).shape, (3, processor.image_processor.max_num_patches)\n+        )\n+        self.assertListEqual(inputs[\"spatial_shapes\"].tolist(), [[6, 6], [6, 6], [6, 6]])\n+\n+    def test_process_interleaved_images_prompts_image_splitting(self):\n+        processor = self.get_processor()\n+\n+        image_str = \"<image>\"\n+        text_str_1 = \"In this image, we see\"\n+        text_str_2 = \"bla, bla\"\n+\n+        text = [image_str + text_str_1, text_str_2 + image_str + image_str]\n+        images = [[self.small_image], [self.high_res_image, self.high_res_image]]\n+\n+        inputs = processor(\n+            text=text,\n+            images=images,\n+            padding=True,\n+            padding_side=\"left\",\n+            max_pixels_tolerance=2.0,\n+            use_thumbnail=True,\n+            do_image_splitting=True,\n+        )\n+\n+        tokenized_sentence_1 = processor.tokenizer(text_str_1, add_special_tokens=False)\n+        tokenized_sentence_2 = processor.tokenizer(text_str_2, add_special_tokens=False)\n+\n+        small_image_tokens = self.get_split_image_expected_tokens(processor, 3, 3, True, 9)\n+        large_image_tokens = self.get_split_image_expected_tokens(processor, 3, 3, True, 9)\n+        high_res_image_tokens = self.get_split_image_expected_tokens(processor, 3, 3, True, 9)\n+\n+        expected_input_ids_1 = small_image_tokens + tokenized_sentence_1[\"input_ids\"]\n+        expected_input_ids_2 = tokenized_sentence_2[\"input_ids\"] + large_image_tokens + high_res_image_tokens\n+        # Pad the first input to match the second input\n+        pad_len = len(expected_input_ids_2) - len(expected_input_ids_1)\n+        padded_expected_input_ids_1 = [self.padding_token_id] * pad_len + expected_input_ids_1\n+\n+        self.assertEqual(inputs[\"input_ids\"][0], padded_expected_input_ids_1)\n+        self.assertEqual(inputs[\"input_ids\"][1], expected_input_ids_2)\n+        self.assertEqual(\n+            inputs[\"attention_mask\"],\n+            [[0] * pad_len + [1] * len(expected_input_ids_1), [1] * len(expected_input_ids_2)],\n+        )\n+        self.assertEqual(np.array(inputs[\"pixel_values\"]).shape, (30, 49, 12))\n+        self.assertEqual(np.array(inputs[\"pixel_attention_mask\"]).shape, (30, 49))\n+        self.assertListEqual(inputs[\"spatial_shapes\"].tolist(), ([[7, 7]] * 9 + [[6, 6]]) * 3)\n+\n+    def test_add_special_tokens_processor_image_splitting(self):\n+        processor = self.get_processor()\n+\n+        image_str = \"<image>\"\n+        text_str = \"In this image, we see\"\n+        text = text_str + image_str\n+\n+        # fmt: off\n+        inputs = processor(text=text, images=self.high_res_image, add_special_tokens=False, do_image_splitting=True)\n+        tokenized_sentence = processor.tokenizer(text_str, add_special_tokens=False)\n+        split_high_res_image_tokens = self.get_split_image_expected_tokens(processor, 3, 3, True, 9)\n+        expected_input_ids = [tokenized_sentence[\"input_ids\"] + split_high_res_image_tokens]\n+        self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n+        # fmt: on\n+\n+    def test_add_special_tokens_processor_image_splitting_large_image(self):\n+        processor = self.get_processor()\n+\n+        image_str = \"<image>\"\n+        text_str = \"In this image, we see\"\n+        text = text_str + image_str\n+\n+        # fmt: off\n+        inputs = processor(text=text, images=self.large_image, add_special_tokens=False, max_pixels_tolerance=2.0, do_image_splitting=True)\n+        tokenized_sentence = processor.tokenizer(text_str, add_special_tokens=False)\n+        large_image_tokens = self.get_split_image_expected_tokens(processor, 2, 4, True, 8)\n+        expected_input_ids = [tokenized_sentence[\"input_ids\"] + large_image_tokens]\n+        self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n+        # fmt: on\n+\n+    def test_add_special_tokens_processor_image_no_splitting(self):\n+        processor = self.get_processor()\n+\n+        image_str = \"<image>\"\n+        text_str = \"In this image, we see\"\n+        text = image_str + text_str\n+\n+        # fmt: off\n+        inputs = processor(text=text, images=self.high_res_image, add_special_tokens=False, use_image_special_tokens=True, do_image_splitting=False)\n+        tokenized_sentence = processor.tokenizer(text_str, add_special_tokens=False)\n+        split_high_res_image_tokens = [self.image_start_token_id] + [self.image_token_id] * 9 + [self.image_end_token_id]\n+        expected_input_ids = [split_high_res_image_tokens + tokenized_sentence[\"input_ids\"]]\n+        self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n+        # fmt: on\n+\n+    def test_process_interleaved_images_prompts_image_error(self):\n+        processor = self.get_processor()\n+\n+        text = [\n+            \"This is a test sentence.\",\n+            \"In this other sentence we try some good things\",\n+        ]\n+        images = [[self.small_image], [self.large_image]]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+        images = [[self.small_image], []]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+\n+        text = [\n+            \"This is a test sentence.<image>\",\n+            \"In this other sentence we try some good things<image>\",\n+        ]\n+        images = [[self.small_image], [self.large_image, self.high_res_image]]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+        images = [[], [self.large_image]]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+        images = [self.small_image, self.large_image, self.high_res_image]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+        images = [self.small_image]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+\n+        text = [\n+            \"This is a test sentence.\",\n+            \"In this other sentence we try some good things<image>\",\n+        ]\n+        images = [[self.small_image], []]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+\n+        images = [[], [self.large_image]]\n+        processor(text=text, images=images, padding=True)\n+\n+        images = [self.small_image, self.large_image]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+\n+        images = [self.small_image]\n+        with self.assertRaises(ValueError):\n+            processor(text=text, images=images, padding=True)\n+\n+    def test_apply_chat_template(self):\n+        # Message contains content which a mix of lists with images and image urls and string\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What do these images show?\"},\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"image\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"The first image shows the statue of Liberty in New York. The second image picture depicts Idefix, the dog of Obelix in Asterix and Obelix.\",\n+                    }\n+                ],\n+            },\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"And who is that?\"}]},\n+        ]\n+        processor = self.get_processor()\n+        # Make short sequence length to test that the fake tokens are added correctly\n+        rendered = processor.apply_chat_template(messages, add_generation_prompt=True)\n+\n+        expected_rendered = (\n+            \"<|startoftext|><|im_start|>user\\nWhat do these images show?<image><image><|im_end|>\\n\"\n+            \"<|im_start|>assistant\\nThe first image shows the statue of Liberty in New York. The second image picture depicts Idefix, the dog of Obelix in Asterix and Obelix.<|im_end|>\\n\"\n+            \"<|im_start|>user\\nAnd who is that?<|im_end|>\\n\"\n+            \"<|im_start|>assistant\\n\"\n+        )\n+        self.assertEqual(rendered, expected_rendered)\n+\n+    def test_text_only_inference(self):\n+        \"\"\"Test that the processor works correctly with text-only input.\"\"\"\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+\n+        text = \"This is a simple text without images.\"\n+        inputs = processor(text=text)\n+\n+        tokenized_sentence = processor.tokenizer(text, add_special_tokens=False)\n+        expected_input_ids = [tokenized_sentence[\"input_ids\"]]\n+\n+        self.assertEqual(inputs[\"input_ids\"], expected_input_ids)\n+        self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids[0])])\n+        self.assertTrue(\"pixel_values\" not in inputs)\n+        self.assertTrue(\"pixel_attention_mask\" not in inputs)\n+\n+        # Test batch of texts without image tokens\n+        texts = [\"First text.\", \"Second piece of text.\"]\n+        batch_inputs = processor(text=texts, padding=True)\n+\n+        tokenized_1 = processor.tokenizer(texts[0], add_special_tokens=False)\n+        tokenized_2 = processor.tokenizer(texts[1], add_special_tokens=False)\n+\n+        expected_1 = tokenized_1[\"input_ids\"]\n+        expected_2 = tokenized_2[\"input_ids\"]\n+\n+        # Pad the shorter sequence\n+        pad_len = len(expected_2) - len(expected_1)\n+        if pad_len > 0:\n+            padded_expected_1 = [self.padding_token_id] * pad_len + expected_1\n+            expected_attention_1 = [0] * pad_len + [1] * len(expected_1)\n+            self.assertEqual(batch_inputs[\"input_ids\"], [padded_expected_1, expected_2])\n+            self.assertEqual(batch_inputs[\"attention_mask\"], [expected_attention_1, [1] * len(expected_2)])\n+        else:\n+            pad_len = -pad_len\n+            padded_expected_2 = [self.padding_token_id] * pad_len + expected_2\n+            expected_attention_2 = [0] * pad_len + [1] * len(expected_2)\n+            self.assertEqual(batch_inputs[\"input_ids\"], [expected_1, padded_expected_2])\n+            self.assertEqual(batch_inputs[\"attention_mask\"], [[1] * len(expected_1), expected_attention_2])\n+\n+    def test_missing_images_error(self):\n+        \"\"\"Test that appropriate error is raised when images are referenced but not provided.\"\"\"\n+        processor = self.get_processor()\n+\n+        # Test single text with image token but no image\n+        text = \"Let me show you this image: <image> What do you think?\"\n+        with self.assertRaises(ValueError) as context:\n+            processor(text=text)\n+        self.assertTrue(\"We detected 1 tokens in the text but no images were passed\" in str(context.exception))\n+\n+        # Test batch with image tokens but no images\n+        texts = [\n+            \"First text with <image> token.\",\n+            \"Second text <image> with token.\",\n+        ]\n+        with self.assertRaises(ValueError) as context:\n+            processor(text=texts)\n+        self.assertTrue(\"We detected 2 tokens in the text but no images were passed\" in str(context.exception))\n+\n+        # Test with None as Images\n+        with self.assertRaises(ValueError) as context:\n+            processor(text=text, images=None)\n+        self.assertTrue(\"We detected 1 tokens in the text but no images were passed\" in str(context.exception))\n+\n+        with self.assertRaises(ValueError) as context:\n+            processor(text=texts, images=None)\n+        self.assertTrue(\"We detected 2 tokens in the text but no images were passed\" in str(context.exception))"
        },
        {
            "sha": "e0094bafa6956720cb64b1886f6c921e7abdde3b",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c532575795e4ccbf3c912a457528a9a60c0b94de/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=c532575795e4ccbf3c912a457528a9a60c0b94de",
            "patch": "@@ -875,7 +875,8 @@ def test_overlapping_text_image_kwargs_handling(self):\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n \n         processor_components = self.prepare_components()\n-        processor = self.processor_class(**processor_components)\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(modalities=\"image\")"
        }
    ],
    "stats": {
        "total": 2951,
        "additions": 2950,
        "deletions": 1
    }
}