{
    "author": "ydshieh",
    "message": "delete some tokenizer tests using pickle (#41514)\n\n* hate pickle\n\n* hate pickle\n\n* hate pickle\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "abf5b57a684e665f03514535a53a668ddcc72303",
    "files": [
        {
            "sha": "66cc3f86afb5ffa332ef74027f51531e415e75ba",
            "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 63,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -14,7 +14,6 @@\n \n \n import os\n-import pickle\n import unittest\n \n from transformers import AutoTokenizer\n@@ -103,26 +102,6 @@ def test_full_tokenizer(self):\n         self.assertListEqual(tokens, [\"こんにちは\", \"、\", \"世界\", \"。\", \"こん\", \"##ばんは\", \"、\", \"世界\", \"。\"])\n         self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n \n-    def test_pickle_mecab_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"mecab\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"こんにちは、世界。\\nこんばんは、世界。\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"こんにちは\", \"、\", \"世界\", \"。\", \"こん\", \"##ばんは\", \"、\", \"世界\", \"。\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     def test_mecab_full_tokenizer_with_mecab_kwargs(self):\n         tokenizer = self.tokenizer_class(\n             self.vocab_file, word_tokenizer_type=\"mecab\", mecab_kwargs={\"mecab_dic\": \"ipadic\"}\n@@ -198,27 +177,6 @@ def test_mecab_tokenizer_no_normalize(self):\n             [\"ｱｯﾌﾟﾙストア\", \"で\", \"iPhone\", \"８\", \"が\", \"発売\", \"さ\", \"れ\", \"た\", \"　\", \"。\"],\n         )\n \n-    @require_sudachi_projection\n-    def test_pickle_sudachi_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"sudachi\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"こんにちは、世界。\\nこんばんは、世界。\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"こんにちは\", \"、\", \"世界\", \"。\", \"こん\", \"##ばんは\", \"、\", \"世界\", \"。\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_sudachi_projection\n     def test_sudachi_tokenizer_core(self):\n         tokenizer = SudachiTokenizer(sudachi_dict_type=\"core\")\n@@ -293,27 +251,6 @@ def test_sudachi_tokenizer_trim_whitespace(self):\n             [\"アップル\", \"ストア\", \"で\", \"iPhone\", \"8\", \"が\", \"発売\", \"さ\", \"れ\", \"た\", \"。\"],\n         )\n \n-    @require_jumanpp\n-    def test_pickle_jumanpp_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"jumanpp\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"こんにちは、世界。\\nこんばんは、世界。\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"こんにちは\", \"、\", \"世界\", \"。\", \"こん\", \"##ばんは\", \"、\", \"世界\", \"。\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_jumanpp\n     def test_jumanpp_tokenizer(self):\n         tokenizer = JumanppTokenizer()"
        },
        {
            "sha": "c0561165c8dc44bb61240189bff291e80b086533",
            "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -293,17 +292,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = CodeLlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
        },
        {
            "sha": "913f7546e84a415213f95c6597d66a97ca4ac7bd",
            "filename": "tests/models/gemma/test_tokenization_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -140,10 +140,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
        },
        {
            "sha": "58eb1f4e86e883625c9c591adc8590cc3f334c2f",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -291,17 +290,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = LlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
        },
        {
            "sha": "5e1cdac9d65e33a3072577eb67d0714ae73ed07e",
            "filename": "tests/models/moshi/test_tokenization_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -13,9 +13,6 @@\n # limitations under the License.\n \n import inspect\n-import pickle\n-import shutil\n-import tempfile\n import unittest\n \n from transformers import (\n@@ -171,18 +168,6 @@ def test_special_tokens_initialization(self):\n \n                 self.assertTrue(special_token_id in r_output)\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = PreTrainedTokenizerFast(\n-                tokenizer_object=MoshiConverter(vocab_file=f.name).converted(),\n-                bos_token=\"<s>\",\n-                unk_token=\"<unk>\",\n-                eos_token=\"</s>\",\n-            )\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_training_new_tokenizer(self):\n         # This feature only exists for fast tokenizers\n         if not self.test_rust_tokenizer:"
        },
        {
            "sha": "6dc433128f38689582e95ec7d1cebfee7d65adfd",
            "filename": "tests/models/pop2piano/test_tokenization_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fpop2piano%2Ftest_tokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fpop2piano%2Ftest_tokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_tokenization_pop2piano.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -15,8 +15,6 @@\n Please note that Pop2PianoTokenizer is too far from our usual tokenizers and thus cannot use the TokenizerTesterMixin class.\n \"\"\"\n \n-import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -224,23 +222,6 @@ def test_save_and_load_tokenizer(self):\n \n         shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        tmpdirname = tempfile.mkdtemp()\n-\n-        notes = self.get_input_notes()\n-        subwords = self.tokenizer(notes)[\"token_ids\"]\n-\n-        filename = os.path.join(tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(self.tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        subwords_loaded = tokenizer_new(notes)[\"token_ids\"]\n-\n-        self.assertListEqual(subwords, subwords_loaded)\n-\n     def test_padding_side_in_kwargs(self):\n         tokenizer_p = Pop2PianoTokenizer.from_pretrained(\"sweetcocoa/pop2piano\", padding_side=\"left\")\n         self.assertEqual(tokenizer_p.padding_side, \"left\")"
        },
        {
            "sha": "d395924da35dad7fee40a5dbb4052063d8d7197c",
            "filename": "tests/models/seamless_m4t/test_tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -426,10 +426,6 @@ def test_training_new_tokenizer(self):\n \n         self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)\n \n-    @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
        },
        {
            "sha": "af8eb8c4ba17292d64d73d6f6526db14b9e127e2",
            "filename": "tests/models/siglip/test_tokenization_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -207,10 +207,6 @@ def test_eos_in_input(self):\n     def test_subword_regularization_tokenizer(self):\n         pass\n \n-    @unittest.skip(reason=\"SiglipTokenizer strips the punctuation\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.test_special_tokens_initialization with T5->Siglip\n     def test_special_tokens_initialization(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:"
        },
        {
            "sha": "7398820cb5a58ecaa1c53fc37a2895c9fae56c22",
            "filename": "tests/models/speecht5/test_tokenization_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fspeecht5%2Ftest_tokenization_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fspeecht5%2Ftest_tokenization_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_tokenization_speecht5.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -143,10 +143,6 @@ def test_add_tokens_tokenizer(self):\n                 self.assertEqual(tokens[0], tokenizer.eos_token_id)\n                 self.assertEqual(tokens[-3], tokenizer.pad_token_id)\n \n-    @unittest.skip\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip\n     def test_subword_regularization_tokenizer(self):\n         pass"
        },
        {
            "sha": "746d96e142d39003e1910c7fbaa42669a3d81710",
            "filename": "tests/models/xglm/test_tokenization_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -12,9 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n-import shutil\n-import tempfile\n import unittest\n from functools import cached_property\n \n@@ -141,13 +138,6 @@ def test_full_tokenizer(self):\n     def big_tokenizer(self):\n         return XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XGLMTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
        },
        {
            "sha": "7c4a13ee3bb4016d11411c55452336e666e33a3b",
            "filename": "tests/models/xlm_roberta/test_tokenization_xlm_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -215,13 +214,6 @@ def test_save_pretrained(self):\n     def big_tokenizer(self):\n         return XLMRobertaTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XLMRobertaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
        },
        {
            "sha": "583ebc6b0dcaf8774e713033f75a994d7d46ba30",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 51,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -18,7 +18,6 @@\n import itertools\n import json\n import os\n-import pickle\n import re\n import shutil\n import tempfile\n@@ -520,28 +519,6 @@ def test_subword_regularization_tokenizer(self) -> None:\n             },\n         )\n \n-    def test_pickle_subword_regularization_tokenizer(self) -> None:\n-        if not self.test_sentencepiece:\n-            self.skipTest(reason=\"test_sentencepiece is set to False\")\n-\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        # Subword regularization is only available for the slow tokenizer.\n-        sp_model_kwargs = {\"enable_sampling\": True, \"alpha\": 0.1, \"nbest_size\": -1}\n-        tokenizer = self.get_tokenizer(sp_model_kwargs=sp_model_kwargs)\n-        tokenizer_bin = pickle.dumps(tokenizer)\n-        del tokenizer\n-        tokenizer_new = pickle.loads(tokenizer_bin)\n-\n-        run_test_in_subprocess(\n-            test_case=self,\n-            target_func=_test_subword_regularization_tokenizer,\n-            inputs={\n-                \"tokenizer\": tokenizer_new,\n-                \"sp_model_kwargs\": sp_model_kwargs,\n-                \"test_sentencepiece_ignore_case\": self.test_sentencepiece_ignore_case,\n-            },\n-        )\n-\n     def test_save_sentencepiece_tokenizer(self) -> None:\n         if not self.test_sentencepiece or not self.test_slow_tokenizer:\n             self.skipTest(reason=\"test_sentencepiece or test_slow_tokenizer is set to False\")\n@@ -827,34 +804,6 @@ def test_save_and_load_tokenizer(self):\n \n                 shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        tokenizers = self.get_tokenizers()\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                self.assertIsNotNone(tokenizer)\n-\n-                text = \"Munich and Berlin are nice cities\"\n-                subwords = tokenizer.tokenize(text)\n-\n-                filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-                with open(filename, \"wb\") as handle:\n-                    pickle.dump(tokenizer, handle)\n-\n-                with open(filename, \"rb\") as handle:\n-                    tokenizer_new = pickle.load(handle)\n-\n-                subwords_loaded = tokenizer_new.tokenize(text)\n-\n-                self.assertListEqual(subwords, subwords_loaded)\n-\n-    @require_tokenizers\n-    def test_pickle_added_tokens(self):\n-        tok1 = AddedToken(\"<s>\", rstrip=True, lstrip=True, normalized=False, single_word=True)\n-        tok2 = pickle.loads(pickle.dumps(tok1))\n-\n-        self.assertEqual(tok1.__getstate__(), tok2.__getstate__())\n-\n     def test_added_tokens_do_lower_case(self):\n         tokenizers = self.get_tokenizers(do_lower_case=True)\n         for tokenizer in tokenizers:"
        },
        {
            "sha": "24aac3719812a0b39e6a2428bc0caa42f92b25b2",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 65,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abf5b57a684e665f03514535a53a668ddcc72303/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=abf5b57a684e665f03514535a53a668ddcc72303",
            "patch": "@@ -16,11 +16,8 @@\n \"\"\"\n \n import os\n-import pickle\n import tempfile\n import unittest\n-from collections.abc import Callable\n-from typing import Optional\n \n import numpy as np\n \n@@ -66,28 +63,6 @@ def check_tokenizer_from_pretrained(self, tokenizer_class):\n                 special_tok_id = tokenizer.convert_tokens_to_ids(special_tok)\n                 self.assertIsInstance(special_tok_id, int)\n \n-    def assert_dump_and_restore(self, be_original: BatchEncoding, equal_op: Optional[Callable] = None):\n-        batch_encoding_str = pickle.dumps(be_original)\n-        self.assertIsNotNone(batch_encoding_str)\n-\n-        be_restored = pickle.loads(batch_encoding_str)\n-\n-        # Ensure is_fast is correctly restored\n-        self.assertEqual(be_restored.is_fast, be_original.is_fast)\n-\n-        # Ensure encodings are potentially correctly restored\n-        if be_original.is_fast:\n-            self.assertIsNotNone(be_restored.encodings)\n-        else:\n-            self.assertIsNone(be_restored.encodings)\n-\n-        # Ensure the keys are the same\n-        for original_v, restored_v in zip(be_original.values(), be_restored.values()):\n-            if equal_op:\n-                self.assertTrue(equal_op(restored_v, original_v))\n-            else:\n-                self.assertEqual(restored_v, original_v)\n-\n     @slow\n     def test_pretrained_tokenizers(self):\n         self.check_tokenizer_from_pretrained(GPT2Tokenizer)\n@@ -96,46 +71,6 @@ def test_tensor_type_from_str(self):\n         self.assertEqual(TensorType(\"pt\"), TensorType.PYTORCH)\n         self.assertEqual(TensorType(\"np\"), TensorType.NUMPY)\n \n-    @require_tokenizers\n-    def test_batch_encoding_pickle(self):\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        # Python no tensor\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_p(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_r(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-    @require_torch\n-    @require_tokenizers\n-    def test_batch_encoding_pickle_pt(self):\n-        import torch\n-\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n     @require_tokenizers\n     def test_batch_encoding_is_fast(self):\n         tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
        }
    ],
    "stats": {
        "total": 271,
        "additions": 0,
        "deletions": 271
    }
}