{
    "author": "gante",
    "message": "[Utils] torch version checks optionally accept dev versions (#36847)",
    "sha": "bc1c90a755505d576220ed2a161ca42dbf3cafaf",
    "files": [
        {
            "sha": "8fd8e963967a40929058ed51a1f17e8622e3ad6c",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc1c90a755505d576220ed2a161ca42dbf3cafaf/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc1c90a755505d576220ed2a161ca42dbf3cafaf/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=bc1c90a755505d576220ed2a161ca42dbf3cafaf",
            "patch": "@@ -611,21 +611,29 @@ class OffloadedCache(DynamicCache):\n     \"\"\"\n \n     def __init__(self) -> None:\n-        if not (torch.cuda.is_available() or (is_torch_greater_or_equal(\"2.7\") and torch.xpu.is_available())):\n+        if not (\n+            torch.cuda.is_available()\n+            or (is_torch_greater_or_equal(\"2.7\", accept_dev=True) and torch.xpu.is_available())\n+        ):\n             raise RuntimeError(\n-                \"OffloadedCache can only be used with a GPU\" + (\" or XPU\" if is_torch_greater_or_equal(\"2.7\") else \"\")\n+                \"OffloadedCache can only be used with a GPU\"\n+                + (\" or XPU\" if is_torch_greater_or_equal(\"2.7\", accept_dev=True) else \"\")\n             )\n \n         super().__init__()\n         self.original_device = []\n         self.prefetch_stream = None\n-        self.prefetch_stream = torch.Stream() if is_torch_greater_or_equal(\"2.7\") else torch.cuda.Stream()\n+        self.prefetch_stream = (\n+            torch.Stream() if is_torch_greater_or_equal(\"2.7\", accept_dev=True) else torch.cuda.Stream()\n+        )\n         self.beam_idx = None  # used to delay beam search operations\n \n     def prefetch_layer(self, layer_idx: int):\n         \"Starts prefetching the next layer cache\"\n         if layer_idx < len(self):\n-            with self.prefetch_stream if is_torch_greater_or_equal(\"2.7\") else torch.cuda.stream(self.prefetch_stream):\n+            with self.prefetch_stream if is_torch_greater_or_equal(\"2.7\", accept_dev=True) else torch.cuda.stream(\n+                self.prefetch_stream\n+            ):\n                 # Prefetch next layer tensors to GPU\n                 device = self.original_device[layer_idx]\n                 self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device, non_blocking=True)\n@@ -643,7 +651,7 @@ def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n         \"Gets the cache for this layer to the device. Prefetches the next and evicts the previous layer.\"\n         if layer_idx < len(self):\n             # Evict the previous layer if necessary\n-            if is_torch_greater_or_equal(\"2.7\"):\n+            if is_torch_greater_or_equal(\"2.7\", accept_dev=True):\n                 torch.accelerator.current_stream().synchronize()\n             else:\n                 torch.cuda.current_stream().synchronize()"
        },
        {
            "sha": "4082fba798e5f17d71b377a3e5f6a346dd48ef3c",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc1c90a755505d576220ed2a161ca42dbf3cafaf/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc1c90a755505d576220ed2a161ca42dbf3cafaf/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=bc1c90a755505d576220ed2a161ca42dbf3cafaf",
            "patch": "@@ -18,7 +18,6 @@\n from typing import Callable\n \n import torch\n-from packaging import version\n from safetensors.torch import storage_ptr, storage_size\n from torch import nn\n \n@@ -29,18 +28,16 @@\n \n logger = logging.get_logger(__name__)\n \n-parsed_torch_version_base = version.parse(version.parse(torch.__version__).base_version)\n-\n-is_torch_greater_or_equal_than_2_6 = parsed_torch_version_base >= version.parse(\"2.6\")\n-is_torch_greater_or_equal_than_2_4 = parsed_torch_version_base >= version.parse(\"2.4\")\n-is_torch_greater_or_equal_than_2_3 = parsed_torch_version_base >= version.parse(\"2.3\")\n-is_torch_greater_or_equal_than_2_2 = parsed_torch_version_base >= version.parse(\"2.2\")\n-is_torch_greater_or_equal_than_2_1 = parsed_torch_version_base >= version.parse(\"2.1\")\n+is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n+is_torch_greater_or_equal_than_2_4 = is_torch_greater_or_equal(\"2.4\", accept_dev=True)\n+is_torch_greater_or_equal_than_2_3 = is_torch_greater_or_equal(\"2.3\", accept_dev=True)\n+is_torch_greater_or_equal_than_2_2 = is_torch_greater_or_equal(\"2.2\", accept_dev=True)\n+is_torch_greater_or_equal_than_2_1 = is_torch_greater_or_equal(\"2.1\", accept_dev=True)\n \n # For backwards compatibility (e.g. some remote codes on Hub using those variables).\n-is_torch_greater_or_equal_than_2_0 = parsed_torch_version_base >= version.parse(\"2.0\")\n-is_torch_greater_or_equal_than_1_13 = parsed_torch_version_base >= version.parse(\"1.13\")\n-is_torch_greater_or_equal_than_1_12 = parsed_torch_version_base >= version.parse(\"1.12\")\n+is_torch_greater_or_equal_than_2_0 = is_torch_greater_or_equal(\"2.0\", accept_dev=True)\n+is_torch_greater_or_equal_than_1_13 = is_torch_greater_or_equal(\"1.13\", accept_dev=True)\n+is_torch_greater_or_equal_than_1_12 = is_torch_greater_or_equal(\"1.12\", accept_dev=True)\n \n # Cache this result has it's a C FFI call which can be pretty time-consuming\n _torch_distributed_available = torch.distributed.is_available()"
        },
        {
            "sha": "3897080516f960aa5d28c1bff3cfef823fd80004",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc1c90a755505d576220ed2a161ca42dbf3cafaf/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc1c90a755505d576220ed2a161ca42dbf3cafaf/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=bc1c90a755505d576220ed2a161ca42dbf3cafaf",
            "patch": "@@ -1070,13 +1070,21 @@ def is_flash_attn_greater_or_equal(library_version: str):\n \n \n @lru_cache()\n-def is_torch_greater_or_equal(library_version: str):\n+def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n+    \"\"\"\n+    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n+    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n+    2.7.0).\n+    \"\"\"\n     if not _is_package_available(\"torch\"):\n         return False\n \n-    return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n-        library_version\n-    )\n+    if accept_dev:\n+        return version.parse(version.parse(importlib.metadata.version(\"torch\")).base_version) >= version.parse(\n+            library_version\n+        )\n+    else:\n+        return version.parse(importlib.metadata.version(\"torch\")) >= version.parse(library_version)\n \n \n def is_torchdistx_available():"
        },
        {
            "sha": "816632ea53b71110cc883fdb1e49f5ec2972a7f4",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc1c90a755505d576220ed2a161ca42dbf3cafaf/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc1c90a755505d576220ed2a161ca42dbf3cafaf/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=bc1c90a755505d576220ed2a161ca42dbf3cafaf",
            "patch": "@@ -605,7 +605,7 @@ def test_offloaded_cache_equivalent_to_dynamic_cache(self):\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n         device = model.device\n \n-        if not is_torch_greater_or_equal(\"2.7\") and device.type == \"xpu\":\n+        if not is_torch_greater_or_equal(\"2.7\", accept_dev=True) and device.type == \"xpu\":\n             self.skipTest(reason=\"This test requires torch >= 2.7 to run on xpu.\")\n \n         input_text = \"Fun fact:\"\n@@ -633,7 +633,7 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n         device = model.device\n \n-        if not is_torch_greater_or_equal(\"2.7\") and device.type == \"xpu\":\n+        if not is_torch_greater_or_equal(\"2.7\", accept_dev=True) and device.type == \"xpu\":\n             self.skipTest(reason=\"This test requires torch >= 2.7 to run on xpu.\")\n \n         input_text = \"Fun fact:\""
        }
    ],
    "stats": {
        "total": 57,
        "additions": 35,
        "deletions": 22
    }
}