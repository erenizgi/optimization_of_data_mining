{
    "author": "Rocketknight1",
    "message": "Remove research projects (#36645)\n\n* Remove research projects\n\n* Add new README to explain where the projects went\n\n* Trigger tests\n\n* Cleanup all references to research_projects",
    "sha": "1e4286fd5962438d15af0257fa5feb9f2b2e58be",
    "files": [
        {
            "sha": "d12d7838906ea46629481b26e7853bde7770988c",
            "filename": "docs/source/ar/bertology.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Far%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Far%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fbertology.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -15,4 +15,4 @@\n - Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„ÙƒÙ„ Ø±Ø£Ø³ ÙÙŠ BERT/GPT/GPT-2ØŒ\n - Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù‚ÙŠÙ… ÙˆÙ…Ø´ØªÙ‚Ø§Øª  Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø±Ø£Ø³ Ù„Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø±Ø£Ø³ ÙˆØ­Ø°ÙÙ‡ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ https://arxiv.org/abs/1905.10650.\n \n-ÙˆÙ„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŒ Ø£Ø¶ÙÙ†Ø§ Ù…Ø«Ø§Ù„Ù‹Ø§ Ø¨Ø±Ù…Ø¬ÙŠÙ‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§: [bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py) Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª  ÙˆØªÙ‚Ù„ÙŠØµ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¹Ù„Ù‰ GLUE.\n\\ No newline at end of file\n+ÙˆÙ„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŒ Ø£Ø¶ÙÙ†Ø§ Ù…Ø«Ø§Ù„Ù‹Ø§ Ø¨Ø±Ù…Ø¬ÙŠÙ‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§: [bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py) Ø£Ø«Ù†Ø§Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª  ÙˆØªÙ‚Ù„ÙŠØµ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¹Ù„Ù‰ GLUE.\n\\ No newline at end of file"
        },
        {
            "sha": "c7aea4eb9611b0a6a7c83baf539d46cf12295caf",
            "filename": "docs/source/ar/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Far%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Far%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -2,7 +2,7 @@\n \n Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø¯ÙØ§ØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª [notebooks](./notebooks) Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ ğŸ¤— TransformersØŒ Ù‡Ù†Ø§Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ù†ØµÙˆØµ Ø¨Ø±Ù…Ø¬ÙŠØ© ØªÙˆØ¶ÙŠØ­ÙŠØ© ØªÙØ¸Ù‡Ø± ÙƒÙŠÙÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ù‡Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch) Ø£Ùˆ [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) Ø£Ùˆ [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n \n-ÙƒÙ…Ø§ Ø³ØªØ¬Ø¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡Ø§ ÙÙŠ [Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¨Ø­Ø§Ø«](https://github.com/huggingface/transformers/tree/main/examples/research_projects) Ùˆ [Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©](https://github.com/huggingface/transformers/tree/main/examples/legacy) ÙˆØ§Ù„ØªÙŠ Ø³Ø§Ù‡Ù… Ø¨Ù‡Ø§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ. Ù‡Ø°Ù‡ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø´ÙƒÙ„ Ù†Ø´Ø· ÙˆÙ‚Ø¯ ØªØªØ·Ù„Ø¨ Ø¥ØµØ¯Ø§Ø±Ù‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§ Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ø£Ù† ÙŠÙƒÙˆÙ† ØºÙŠØ± Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø£Ø­Ø¯Ø« Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø©.\n+ÙƒÙ…Ø§ Ø³ØªØ¬Ø¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªÙŠ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡Ø§ ÙÙŠ [Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¨Ø­Ø§Ø«](https://github.com/huggingface/transformers-research-projects/) Ùˆ [Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©](https://github.com/huggingface/transformers/tree/main/examples/legacy) ÙˆØ§Ù„ØªÙŠ Ø³Ø§Ù‡Ù… Ø¨Ù‡Ø§ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ. Ù‡Ø°Ù‡ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø´ÙƒÙ„ Ù†Ø´Ø· ÙˆÙ‚Ø¯ ØªØªØ·Ù„Ø¨ Ø¥ØµØ¯Ø§Ø±Ù‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§ Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers ÙˆØ§Ù„Ø°ÙŠ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ø£Ù† ÙŠÙƒÙˆÙ† ØºÙŠØ± Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø£Ø­Ø¯Ø« Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø©.\n \n Ù„Ø§ ÙŠÙØªÙˆÙ‚Ø¹ Ø£Ù† ØªØ¹Ù…Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø´ÙƒÙ„Ø©ØŒ ÙˆÙ‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙƒÙŠÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù…Ø¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªÙŠ ØªØ­Ø§ÙˆÙ„ Ø­Ù„Ù‡Ø§. ÙˆÙ„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø°Ù„ÙƒØŒ ØªØ¹Ø±Ø¶ Ù…Ø¹Ø¸Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙƒÙŠÙÙŠØ© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ØŒ Ù…Ù…Ø§ ÙŠØªÙŠØ­ Ù„Ùƒ ØªØ­Ø±ÙŠØ±Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ø­Ø§Ù„ØªÙƒ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….\n "
        },
        {
            "sha": "8aaaa5952c07fe55e373091df8b71f4b88974de0",
            "filename": "docs/source/de/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fde%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fde%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -88,7 +88,7 @@ Die Bibliothek enthÃ¤lt derzeit JAX-, PyTorch- und TensorFlow-Implementierungen,\n 1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou.\n 1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n 1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n 1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n 1. **[DPT](master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun."
        },
        {
            "sha": "4b62c73276e0cb75b9956e9055075d3c644e00f0",
            "filename": "docs/source/de/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fde%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fde%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n Neben den ğŸ¤— Transformers [notebooks](./notebooks) gibt es auch Beispielskripte, die zeigen, wie man ein Modell fÃ¼r eine Aufgabe mit [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) oder [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax) trainiert.\n \n-Sie werden auch Skripte finden, die wir in unseren [Forschungsprojekten](https://github.com/huggingface/transformers/tree/main/examples/research_projects) und [Legacy-Beispielen](https://github.com/huggingface/transformers/tree/main/examples/legacy) verwendet haben und die grÃ¶ÃŸtenteils von der Community stammen. Diese Skripte werden nicht aktiv gepflegt und erfordern eine bestimmte Version von ğŸ¤— Transformers, die hÃ¶chstwahrscheinlich nicht mit der neuesten Version der Bibliothek kompatibel ist.\n+Sie werden auch Skripte finden, die wir in unseren [Forschungsprojekten](https://github.com/huggingface/transformers-research-projects/) und [Legacy-Beispielen](https://github.com/huggingface/transformers/tree/main/examples/legacy) verwendet haben und die grÃ¶ÃŸtenteils von der Community stammen. Diese Skripte werden nicht aktiv gepflegt und erfordern eine bestimmte Version von ğŸ¤— Transformers, die hÃ¶chstwahrscheinlich nicht mit der neuesten Version der Bibliothek kompatibel ist.\n \n Es wird nicht erwartet, dass die Beispielskripte bei jedem Problem sofort funktionieren. MÃ¶glicherweise mÃ¼ssen Sie das Skript an das Problem anpassen, das Sie zu lÃ¶sen versuchen. Um Ihnen dabei zu helfen, legen die meisten Skripte vollstÃ¤ndig offen, wie die Daten vorverarbeitet werden, so dass Sie sie nach Bedarf fÃ¼r Ihren Anwendungsfall bearbeiten kÃ¶nnen.\n "
        },
        {
            "sha": "3f949d9443a6ffdf3e5853eac17abd26b28c9b59",
            "filename": "docs/source/en/model_doc/distilbert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdistilbert.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -49,7 +49,7 @@ demonstrate its capabilities for on-device computations in a proof-of-concept ex\n study.*\n \n This model was contributed by [victorsanh](https://huggingface.co/victorsanh). This model jax version was\n-contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation).\n+contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/huggingface/transformers-research-projects/tree/main/distillation).\n \n ## Usage tips\n "
        },
        {
            "sha": "5ab998dc3cd88b35b0292a8820a052db1cdcdd71",
            "filename": "docs/source/en/model_doc/layoutlmv3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -52,7 +52,7 @@ LayoutLMv3 is nearly identical to LayoutLMv2, so we've also included LayoutLMv2\n </Tip>\n \n - Demo notebooks for LayoutLMv3 can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/LayoutLMv3).\n-- Demo scripts can be found [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3).\n+- Demo scripts can be found [here](https://github.com/huggingface/transformers-research-projects/tree/main/layoutlmv3).\n \n <PipelineTag pipeline=\"text-classification\"/>\n \n@@ -61,7 +61,7 @@ LayoutLMv3 is nearly identical to LayoutLMv2, so we've also included LayoutLMv2\n \n <PipelineTag pipeline=\"token-classification\"/>\n \n-- [`LayoutLMv3ForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3) and [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb).\n+- [`LayoutLMv3ForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers-research-projects/tree/main/layoutlmv3) and [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb).\n - A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb) for how to perform inference with [`LayoutLMv2ForTokenClassification`] and a [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb) for how to perform inference when no labels are available with [`LayoutLMv2ForTokenClassification`].\n - A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb) for how to finetune [`LayoutLMv2ForTokenClassification`] with the ğŸ¤— Trainer.\n - [Token classification task guide](../tasks/token_classification)"
        },
        {
            "sha": "bdb61e66d98404027a79acbc934e93788bfc9350",
            "filename": "docs/source/en/model_doc/pegasus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -96,7 +96,7 @@ All the [checkpoints](https://huggingface.co/models?search=pegasus) are fine-tun\n \n ## Resources\n \n-- [Script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh) to fine-tune pegasus\n+- [Script](https://github.com/huggingface/transformers-research-projects/tree/main/seq2seq-distillation/finetune_pegasus_xsum.sh) to fine-tune pegasus\n   on the XSUM dataset. Data download instructions at [examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md).\n - [Causal language modeling task guide](../tasks/language_modeling)\n - [Translation task guide](../tasks/translation)"
        },
        {
            "sha": "4c1a485b116138479358dc62361f9f8534167b96",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -54,7 +54,7 @@ This model was contributed by [shangz](https://huggingface.co/shangz).\n - QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example *google-bert/bert-base-uncased*), and\n   perform Quantization Aware Training/Post Training Quantization.\n - A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for\n-  SQUAD task can be found at [transformers/examples/research_projects/quantization-qdqbert/](examples/research_projects/quantization-qdqbert/).\n+  SQUAD task can be found at https://github.com/huggingface/transformers-research-projects/tree/main/quantization-qdqbert.\n \n ### Set default quantizers\n "
        },
        {
            "sha": "265d482c1902e4d38258f98e5d43c7fcf72c63bf",
            "filename": "docs/source/en/model_doc/visual_bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -64,7 +64,7 @@ appropriately for the textual and visual parts.\n The [`BertTokenizer`] is used to encode the text. A custom detector/image processor must be used\n to get the visual embeddings. The following example notebooks show how to use VisualBERT with Detectron-like models:\n \n-- [VisualBERT VQA demo notebook](https://github.com/huggingface/transformers/tree/main/examples/research_projects/visual_bert) : This notebook\n+- [VisualBERT VQA demo notebook](https://github.com/huggingface/transformers-research-projects/tree/main/visual_bert) : This notebook\n   contains an example on VisualBERT VQA.\n \n - [Generate Embeddings for VisualBERT (Colab Notebook)](https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing) : This notebook contains"
        },
        {
            "sha": "37e00c9974c35458eaa5e79585919898c94aa0bb",
            "filename": "docs/source/en/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fen%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # Training scripts\n \n-Transformers provides many example training scripts for deep learning frameworks (PyTorch, TensorFlow, Flax) and tasks in [transformers/examples](https://github.com/huggingface/transformers/tree/main/examples). There are additional scripts in [transformers/research projects](https://github.com/huggingface/transformers/tree/main/examples/research_projects) and [transformers/legacy](https://github.com/huggingface/transformers/tree/main/examples/legacy), but these aren't actively maintained and requires a specific version of Transformers.\n+Transformers provides many example training scripts for deep learning frameworks (PyTorch, TensorFlow, Flax) and tasks in [transformers/examples](https://github.com/huggingface/transformers/tree/main/examples). There are additional scripts in [transformers/research projects](https://github.com/huggingface/transformers-research-projects/) and [transformers/legacy](https://github.com/huggingface/transformers/tree/main/examples/legacy), but these aren't actively maintained and requires a specific version of Transformers.\n \n Example scripts are only examples and you may need to adapt the script to your use-case. To help you with this, most scripts are very transparent in how data is preprocessed, allowing you to edit it as necessary.\n "
        },
        {
            "sha": "c62e5aaf9732de919289da284fdc3c5de81bb35b",
            "filename": "docs/source/es/bertology.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fes%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fes%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fbertology.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -37,5 +37,5 @@ ayudar a acceder a las representaciones internas, principalmente adaptado de la\n - adquiriendo los valores de salida y gradientes de las heads para poder computar la mÃ©trica de importancia de las heads y realizar la poda de heads como se explica\n   en https://arxiv.org/abs/1905.10650.\n \n-Para ayudarte a entender y usar estas features, hemos aÃ±adido un script especÃ­fico de ejemplo: [bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py) mientras extraes informaciÃ³n y cortas un modelo pre-entrenado en\n+Para ayudarte a entender y usar estas features, hemos aÃ±adido un script especÃ­fico de ejemplo: [bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py) mientras extraes informaciÃ³n y cortas un modelo pre-entrenado en\n GLUE."
        },
        {
            "sha": "3c10e71ebf91166a7034343c6662b86d472b8541",
            "filename": "docs/source/es/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fes%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fes%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -80,7 +80,7 @@ La biblioteca actualmente contiene implementaciones de JAX, PyTorch y TensorFlow\n 1. **[DeiT](model_doc/deit)** (de Facebook) publicado con el paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) por Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou.\n 1. **[DETR](model_doc/detr)** (de Facebook) publicado con el paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) por Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (de Microsoft Research) publicado con el paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) por Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (de HuggingFace), publicado junto con el paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) por Victor Sanh, Lysandre Debut y Thomas Wolf. Se ha aplicado el mismo mÃ©todo para comprimir GPT2 en [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa en [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), BERT multilingÃ¼e en [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) y una versiÃ³n alemana de DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (de HuggingFace), publicado junto con el paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) por Victor Sanh, Lysandre Debut y Thomas Wolf. Se ha aplicado el mismo mÃ©todo para comprimir GPT2 en [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa en [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), BERT multilingÃ¼e en [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) y una versiÃ³n alemana de DistilBERT.\n 1. **[DPR](model_doc/dpr)** (de Facebook) publicado con el paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) por Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, y Wen-tau Yih.\n 1. **[DPT](master/model_doc/dpt)** (de Intel Labs) publicado con el paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) por RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n 1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  by Mingxing Tan and Quoc V. Le."
        },
        {
            "sha": "a389b2d2fe418a096bed58369f10092f22fa6023",
            "filename": "docs/source/es/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fes%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fes%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n Junto con los [notebooks](./notebooks) de ğŸ¤— Transformers, tambiÃ©n hay scripts con ejemplos que muestran cÃ³mo entrenar un modelo para una tarea en [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), o [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n \n-TambiÃ©n encontrarÃ¡s scripts que hemos usado en nuestros [proyectos de investigaciÃ³n](https://github.com/huggingface/transformers/tree/main/examples/research_projects) y [ejemplos pasados](https://github.com/huggingface/transformers/tree/main/examples/legacy) que en su mayorÃ­a son aportados por la comunidad. Estos scripts no se mantienen activamente y requieren una versiÃ³n especÃ­fica de ğŸ¤— Transformers que probablemente sea incompatible con la Ãºltima versiÃ³n de la biblioteca.\n+TambiÃ©n encontrarÃ¡s scripts que hemos usado en nuestros [proyectos de investigaciÃ³n](https://github.com/huggingface/transformers-research-projects/) y [ejemplos pasados](https://github.com/huggingface/transformers/tree/main/examples/legacy) que en su mayorÃ­a son aportados por la comunidad. Estos scripts no se mantienen activamente y requieren una versiÃ³n especÃ­fica de ğŸ¤— Transformers que probablemente sea incompatible con la Ãºltima versiÃ³n de la biblioteca.\n \n No se espera que los scripts de ejemplo funcionen de inmediato en todos los problemas, y es posible que debas adaptar el script al problema que estÃ¡s tratando de resolver. Para ayudarte con esto, la mayorÃ­a de los scripts exponen completamente cÃ³mo se preprocesan los datos, lo que te permite editarlos segÃºn sea necesario para tu caso de uso.\n "
        },
        {
            "sha": "963afe48ce44822ce45c1f5bce5767f9e88807cb",
            "filename": "docs/source/fr/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Ffr%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Ffr%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -98,7 +98,7 @@ La documentation est organisÃ©e en 5 parties:\n 1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n 1. **[DiNAT](model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n 1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n 1. **[Donut](model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n 1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih."
        },
        {
            "sha": "a68d71035f01f64abca01de0e275d431691653fb",
            "filename": "docs/source/fr/run_scripts_fr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Ffr%2Frun_scripts_fr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Ffr%2Frun_scripts_fr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Frun_scripts_fr.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n En plus des [notebooks](./notebooks) de ğŸ¤— Transformers, il existe Ã©galement des exemples de scripts dÃ©montrant comment entraÃ®ner un modÃ¨le pour une tÃ¢che avec [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) ou [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n \n \n-Vous trouverez Ã©galement des scripts que nous avons utilisÃ© dans nos [projets de recherche](https://github.com/huggingface/transformers/tree/main/examples/research_projects) et des [exemples \"legacy\"](https://github.com/huggingface/transformers/tree/main/examples/legacy) qui sont des contributions de la communautÃ©. Ces scripts ne sont pas activement maintenus et nÃ©cessitent une version spÃ©cifique de ğŸ¤— Transformers qui sera probablement incompatible avec la derniÃ¨re version de la librairie.\n+Vous trouverez Ã©galement des scripts que nous avons utilisÃ© dans nos [projets de recherche](https://github.com/huggingface/transformers-research-projects/) et des [exemples \"legacy\"](https://github.com/huggingface/transformers/tree/main/examples/legacy) qui sont des contributions de la communautÃ©. Ces scripts ne sont pas activement maintenus et nÃ©cessitent une version spÃ©cifique de ğŸ¤— Transformers qui sera probablement incompatible avec la derniÃ¨re version de la librairie.\n \n Les exemples de scripts ne sont pas censÃ©s fonctionner immÃ©diatement pour chaque problÃ¨me, et il se peut que vous ayez besoin d'adapter le script au problÃ¨me que vous essayez de rÃ©soudre. Pour vous aider dans cette tÃ¢che, la plupart des scripts exposent entiÃ¨rement la maniÃ¨re dont les donnÃ©es sont prÃ©traitÃ©es, vous permettant de les modifier selon vos besoins.\n "
        },
        {
            "sha": "bbab23eed60fa9b8f083213c8c9c85d475c839e3",
            "filename": "docs/source/it/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fit%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fit%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -86,7 +86,7 @@ La libreria attualmente contiene implementazioni in JAX, PyTorch e TensorFlow, p\n 1. **[DeiT](model_doc/deit)** (da Facebook) rilasciato con il paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) da Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou.\n 1. **[DETR](model_doc/detr)** (da Facebook) rilasciato con il paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) da Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (da Microsoft Research) rilasciato con il paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) da Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (da HuggingFace), rilasciato assieme al paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) da Victor Sanh, Lysandre Debut e Thomas Wolf. La stessa tecnica Ã¨ stata applicata per comprimere GPT2 in [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa in [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT in [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (da HuggingFace), rilasciato assieme al paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) da Victor Sanh, Lysandre Debut e Thomas Wolf. La stessa tecnica Ã¨ stata applicata per comprimere GPT2 in [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa in [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT in [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n 1. **[DPR](model_doc/dpr)** (da Facebook) rilasciato con il paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) da Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, e Wen-tau Yih.\n 1. **[DPT](master/model_doc/dpt)** (da Intel Labs) rilasciato con il paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) da RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n 1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  by Mingxing Tan and Quoc V. Le."
        },
        {
            "sha": "b7d13f7019fbd26cee107fb4d38957f102dd7976",
            "filename": "docs/source/it/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fit%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fit%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n Insieme ai [notebooks](./notebooks) ğŸ¤— Transformers, ci sono anche esempi di script che dimostrano come addestrare un modello per un task con [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), o [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n \n-Troverai anche script che abbiamo usato nei nostri [progetti di ricerca](https://github.com/huggingface/transformers/tree/main/examples/research_projects) e [precedenti esempi](https://github.com/huggingface/transformers/tree/main/examples/legacy) a cui contribuisce per lo piÃ¹ la comunitÃ . Questi script non sono attivamente mantenuti e richiedono una specifica versione di ğŸ¤— Transformers che sarÃ  molto probabilmente incompatibile con l'ultima versione della libreria.\n+Troverai anche script che abbiamo usato nei nostri [progetti di ricerca](https://github.com/huggingface/transformers-research-projects/) e [precedenti esempi](https://github.com/huggingface/transformers/tree/main/examples/legacy) a cui contribuisce per lo piÃ¹ la comunitÃ . Questi script non sono attivamente mantenuti e richiedono una specifica versione di ğŸ¤— Transformers che sarÃ  molto probabilmente incompatibile con l'ultima versione della libreria.\n \n Non Ã¨ dato per scontato che gli script di esempio funzionino senza apportare modifiche per ogni problema, bensÃ¬ potrebbe essere necessario adattare lo script al tuo caso specifico. Per aiutarti in ciÃ², la maggioranza degli script espone le modalitÃ  di pre-processamento dei dati, consentendoti di modificare lo script come preferisci.\n "
        },
        {
            "sha": "2525d5edef4f99827d444fd233609e1276f5a16d",
            "filename": "docs/source/ja/bertology.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fja%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fja%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fbertology.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -31,4 +31,4 @@ rendered properly in your Markdown viewer.\n - BERT/GPT/GPT-2ã®å„ãƒ˜ãƒƒãƒ‰ã®æ³¨æ„é‡ã¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚\n - ãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›å€¤ã¨å‹¾é…ã‚’å–å¾—ã—ã€ãƒ˜ãƒƒãƒ‰ã®é‡è¦æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€[è«–æ–‡ãƒªãƒ³ã‚¯](https://arxiv.org/abs/1905.10650)ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚\n \n-ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã‚’ç†è§£ã—ã€ä½¿ç”¨ã™ã‚‹ã®ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«ã€ç‰¹å®šã®ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€Œ[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)ã€ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€GLUEã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã€ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã™ã‚‹å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚\n+ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã‚’ç†è§£ã—ã€ä½¿ç”¨ã™ã‚‹ã®ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã«ã€ç‰¹å®šã®ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€Œ[bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py)ã€ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€GLUEã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã€ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã™ã‚‹å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚"
        },
        {
            "sha": "d606662ed8333ef417654304c8915731e81c8608",
            "filename": "docs/source/ja/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fja%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fja%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -95,7 +95,7 @@ rendered properly in your Markdown viewer.\n 1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (Facebook ã‹ã‚‰) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)\n 1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (Microsoft Research ã‹ã‚‰) Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)\n 1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (SHI Labs ã‹ã‚‰) Ali Hassani and Humphrey Shi ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)\n-1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (HuggingFace ã‹ã‚‰), Victor Sanh, Lysandre Debut and Thomas Wolf. åŒã˜æ‰‹æ³•ã§ GPT2, RoBERTa ã¨ Multilingual BERT ã®åœ§ç¸®ã‚’è¡Œã„ã¾ã—ãŸ.åœ§ç¸®ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ãã‚Œãã‚Œ [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation)ã€[DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation)ã€[DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) ã¨åä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ. å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n+1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (HuggingFace ã‹ã‚‰), Victor Sanh, Lysandre Debut and Thomas Wolf. åŒã˜æ‰‹æ³•ã§ GPT2, RoBERTa ã¨ Multilingual BERT ã®åœ§ç¸®ã‚’è¡Œã„ã¾ã—ãŸ.åœ§ç¸®ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ãã‚Œãã‚Œ [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation)ã€[DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation)ã€[DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) ã¨åä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸ. å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n 1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (Microsoft Research ã‹ã‚‰) Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378)\n 1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (NAVER ã‹ã‚‰), Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664)\n 1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (Facebook ã‹ã‚‰) Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)"
        },
        {
            "sha": "69437819e36b2d5bfc1681ef5003c5ebd667f35b",
            "filename": "docs/source/ja/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fja%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fja%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ğŸ¤— Transformersã®[notebooks](./notebooks/README)ã¨ä¸€ç·’ã«ã€[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ã€[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)ã€ã¾ãŸã¯[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã™ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚ã‚ã‚Šã¾ã™ã€‚\n \n-ã¾ãŸã€ç§ãŸã¡ã®[ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/research_projects)ã‚„[ãƒ¬ã‚¬ã‚·ãƒ¼ã®ä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ã§ä½¿ç”¨ã—ãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ç¾åœ¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã•ã‚Œã¦ãŠã‚‰ãšã€ãŠãã‚‰ãæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨äº’æ›æ€§ãŒãªã„ç‰¹å®šã®ğŸ¤— Transformersã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¿…è¦ã§ã™ã€‚\n+ã¾ãŸã€ç§ãŸã¡ã®[ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ](https://github.com/huggingface/transformers-research-projects/)ã‚„[ãƒ¬ã‚¬ã‚·ãƒ¼ã®ä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ã§ä½¿ç”¨ã—ãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚è¦‹ã¤ã‹ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ç¾åœ¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã•ã‚Œã¦ãŠã‚‰ãšã€ãŠãã‚‰ãæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨äº’æ›æ€§ãŒãªã„ç‰¹å®šã®ğŸ¤— Transformersã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¿…è¦ã§ã™ã€‚\n \n ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã™ã¹ã¦ã®å•é¡Œã§ãã®ã¾ã¾å‹•ä½œã™ã‚‹ã“ã¨ã¯æœŸå¾…ã•ã‚Œã¦ãŠã‚‰ãšã€è§£æ±ºã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹å•é¡Œã«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é©å¿œã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã®ç‚¹ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€ã»ã¨ã‚“ã©ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ‡ãƒ¼ã‚¿ãŒã©ã®ã‚ˆã†ã«å‰å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’å®Œå…¨ã«å…¬é–‹ã—ã€å¿…è¦ã«å¿œã˜ã¦ç·¨é›†ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚\n "
        },
        {
            "sha": "1f69a0381707dc5ecc862068593dc261953a36a0",
            "filename": "docs/source/ko/bertology.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fko%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fko%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fbertology.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -38,4 +38,4 @@ BERTì™€ ê°™ì€ ëŒ€ê·œëª¨ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë‚´ë¶€ ë™ì‘ì„ ì¡°ì‚¬í•˜ëŠ” ì—°êµ¬\n - BERT/GPT/GPT-2ì˜ ê° í—¤ë“œì˜ ëª¨ë“  ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì— ì ‘ê·¼í•˜ê¸°,\n - í—¤ë“œì˜ ì¶œë ¥ ê°’ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê²€ìƒ‰í•˜ì—¬ í—¤ë“œ ì¤‘ìš”ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  https://arxiv.org/abs/1905.10650ì—ì„œ ì„¤ëª…ëœ ëŒ€ë¡œ í—¤ë“œë¥¼ ì œê±°í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n \n-ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì„ ì´í•´í•˜ê³  ì§ì ‘ ì‚¬ìš©í•´ë³¼ ìˆ˜ ìˆë„ë¡ [bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py) ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” GLUEì— ëŒ€í•´ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ëª¨ë¸ì„ ê°€ì§€ì¹˜ê¸°(prune)í•´ë´…ë‹ˆë‹¤.\n+ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì„ ì´í•´í•˜ê³  ì§ì ‘ ì‚¬ìš©í•´ë³¼ ìˆ˜ ìˆë„ë¡ [bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py) ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ì´ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” GLUEì— ëŒ€í•´ ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ëª¨ë¸ì„ ê°€ì§€ì¹˜ê¸°(prune)í•´ë´…ë‹ˆë‹¤."
        },
        {
            "sha": "bd95cbc0ab09d12c08808fb8c5ff75bf1e1ee9b9",
            "filename": "docs/source/ko/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fko%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fko%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -88,7 +88,7 @@ rendered properly in your Markdown viewer.\n 1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou.\n 1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n 1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n 1. **[Donut](model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n 1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih."
        },
        {
            "sha": "70520f1a97f87c1513ab81f6c0861fb74e29dbcd",
            "filename": "docs/source/ko/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fko%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fko%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ğŸ¤— Transformers ë…¸íŠ¸ë¶ê³¼ í•¨ê»˜ [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), ë˜ëŠ” [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)ë¥¼ ì‚¬ìš©í•´ íŠ¹ì • íƒœìŠ¤í¬ì— ëŒ€í•œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ë„ ìˆìŠµë‹ˆë‹¤.\n \n-ë˜í•œ [ì—°êµ¬ í”„ë¡œì íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/research_projects) ë° [ë ˆê±°ì‹œ ì˜ˆì œ](https://github.com/huggingface/transformers/tree/main/examples/legacy)ì—ì„œ ëŒ€ë¶€ë¶„ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œê³µí•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n+ë˜í•œ [ì—°êµ¬ í”„ë¡œì íŠ¸](https://github.com/huggingface/transformers-research-projects/) ë° [ë ˆê±°ì‹œ ì˜ˆì œ](https://github.com/huggingface/transformers/tree/main/examples/legacy)ì—ì„œ ëŒ€ë¶€ë¶„ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œê³µí•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n ì´ëŸ¬í•œ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì ê·¹ì ìœ¼ë¡œ ìœ ì§€ ê´€ë¦¬ë˜ì§€ ì•Šìœ¼ë©° ìµœì‹  ë²„ì „ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ íŠ¹ì • ë²„ì „ì˜ ğŸ¤— Transformersë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.\n \n ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ê°€ ëª¨ë“  ë¬¸ì œì—ì„œ ë°”ë¡œ ì‘ë™í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì— ë§ê²Œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë³€ê²½í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
        },
        {
            "sha": "e0adb8a8a8e53b588eb032085e35f2c87781dcc9",
            "filename": "docs/source/ms/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fms%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fms%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fms%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -104,7 +104,7 @@ Dokumentasi disusun kepada lima bahagian:\n 1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n 1. **[DiNAT](model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n 1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n 1. **[Donut](model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n 1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih."
        },
        {
            "sha": "365933bd658de5347d3fe1c641e4bc094d5964a2",
            "filename": "docs/source/pt/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fpt%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fpt%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Findex.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -93,7 +93,7 @@ Atualmente a biblioteca contÃ©m implementaÃ§Ãµes do PyTorch, TensorFlow e JAX, p\n 1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou.\n 1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n 1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n 1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n 1. **[DPT](master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by RenÃ© Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n 1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  by Mingxing Tan and Quoc V. Le."
        },
        {
            "sha": "ad19a8fdea09b060bc6096ae7b28dfd96f7cc77b",
            "filename": "docs/source/pt/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fpt%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fpt%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n Junto com os ğŸ¤— Transformers [notebooks](./notebooks), tambÃ©m hÃ¡ scripts de exemplo demonstrando como treinar um modelo para uma tarefa com [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch), [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow) ou [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n \n-VocÃª tambÃ©m encontrarÃ¡ scripts que usamos em nossos [projetos de pesquisa](https://github.com/huggingface/transformers/tree/main/examples/research_projects) e [exemplos legados](https://github.com/huggingface/transformers/tree/main/examples/legacy) que sÃ£o principalmente contribuiÃ§Ãµes da comunidade. Esses scripts nÃ£o sÃ£o mantidos ativamente e exigem uma versÃ£o especÃ­fica de ğŸ¤— Transformers que provavelmente serÃ¡ incompatÃ­vel com a versÃ£o mais recente da biblioteca.\n+VocÃª tambÃ©m encontrarÃ¡ scripts que usamos em nossos [projetos de pesquisa](https://github.com/huggingface/transformers-research-projects/) e [exemplos legados](https://github.com/huggingface/transformers/tree/main/examples/legacy) que sÃ£o principalmente contribuiÃ§Ãµes da comunidade. Esses scripts nÃ£o sÃ£o mantidos ativamente e exigem uma versÃ£o especÃ­fica de ğŸ¤— Transformers que provavelmente serÃ¡ incompatÃ­vel com a versÃ£o mais recente da biblioteca.\n \n NÃ£o se espera que os scripts de exemplo funcionem imediatamente em todos os problemas, vocÃª pode precisar adaptar o script ao problema que estÃ¡ tentando resolver. Para ajudÃ¡-lo com isso, a maioria dos scripts expÃµe totalmente como os dados sÃ£o prÃ©-processados, permitindo que vocÃª os edite conforme necessÃ¡rio para seu caso de uso.\n "
        },
        {
            "sha": "e7df7593a2bd134a7aa2712b491ab65baffd04dc",
            "filename": "docs/source/zh/bertology.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fzh%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fzh%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fbertology.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -30,4 +30,4 @@ http://www.apache.org/licenses/LICENSE-2.0\n - è®¿é—®BERT/GPT/GPT-2æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„æ‰€æœ‰æ³¨æ„åŠ›æƒé‡ï¼Œ\n - æ£€ç´¢æ³¨æ„åŠ›å¤´çš„è¾“å‡ºå€¼å’Œæ¢¯åº¦ï¼Œä»¥ä¾¿è®¡ç®—å¤´çš„é‡è¦æ€§å¾—åˆ†å¹¶å¯¹å¤´è¿›è¡Œå‰ªæï¼Œè¯¦æƒ…å¯è§è®ºæ–‡ï¼šhttps://arxiv.org/abs/1905.10650ã€‚\n \n-ä¸ºäº†å¸®åŠ©æ‚¨ç†è§£å’Œä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹è„šæœ¬ï¼š[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)ï¼Œè¯¥è„šæœ¬å¯ä»¥å¯¹ä¸€ä¸ªåœ¨ GLUE æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œä¿¡æ¯æå–ä¸å‰ªæã€‚\n\\ No newline at end of file\n+ä¸ºäº†å¸®åŠ©æ‚¨ç†è§£å’Œä½¿ç”¨è¿™äº›åŠŸèƒ½ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªå…·ä½“çš„ç¤ºä¾‹è„šæœ¬ï¼š[bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py)ï¼Œè¯¥è„šæœ¬å¯ä»¥å¯¹ä¸€ä¸ªåœ¨ GLUE æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œä¿¡æ¯æå–ä¸å‰ªæã€‚\n\\ No newline at end of file"
        },
        {
            "sha": "8c21266afce04a051bd98c2e572441493de096f6",
            "filename": "docs/source/zh/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fzh%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/docs%2Fsource%2Fzh%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Frun_scripts.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n é™¤äº† ğŸ¤— Transformers [notebooks](./notebooks)ï¼Œè¿˜æœ‰ç¤ºä¾‹è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)ã€[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)æˆ–[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)è®­ç»ƒæ¨¡å‹ä»¥è§£å†³ç‰¹å®šä»»åŠ¡ã€‚\n \n-æ‚¨è¿˜å¯ä»¥åœ¨è¿™äº›ç¤ºä¾‹ä¸­æ‰¾åˆ°æˆ‘ä»¬åœ¨[ç ”ç©¶é¡¹ç›®](https://github.com/huggingface/transformers/tree/main/examples/research_projects)å’Œ[é—ç•™ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ä¸­ä½¿ç”¨è¿‡çš„è„šæœ¬ï¼Œè¿™äº›è„šæœ¬ä¸»è¦æ˜¯ç”±ç¤¾åŒºè´¡çŒ®çš„ã€‚è¿™äº›è„šæœ¬å·²ä¸å†è¢«ç§¯æç»´æŠ¤ï¼Œéœ€è¦ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„ğŸ¤— Transformersï¼Œ å¯èƒ½ä¸åº“çš„æœ€æ–°ç‰ˆæœ¬ä¸å…¼å®¹ã€‚\n+æ‚¨è¿˜å¯ä»¥åœ¨è¿™äº›ç¤ºä¾‹ä¸­æ‰¾åˆ°æˆ‘ä»¬åœ¨[ç ”ç©¶é¡¹ç›®](https://github.com/huggingface/transformers-research-projects/)å’Œ[é—ç•™ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples/legacy)ä¸­ä½¿ç”¨è¿‡çš„è„šæœ¬ï¼Œè¿™äº›è„šæœ¬ä¸»è¦æ˜¯ç”±ç¤¾åŒºè´¡çŒ®çš„ã€‚è¿™äº›è„šæœ¬å·²ä¸å†è¢«ç§¯æç»´æŠ¤ï¼Œéœ€è¦ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„ğŸ¤— Transformersï¼Œ å¯èƒ½ä¸åº“çš„æœ€æ–°ç‰ˆæœ¬ä¸å…¼å®¹ã€‚\n \n ç¤ºä¾‹è„šæœ¬å¯èƒ½æ— æ³•åœ¨åˆå§‹é…ç½®ä¸‹ç›´æ¥è§£å†³æ¯ä¸ªé—®é¢˜ï¼Œæ‚¨å¯èƒ½éœ€è¦æ ¹æ®è¦è§£å†³çš„é—®é¢˜è°ƒæ•´è„šæœ¬ã€‚ä¸ºäº†å¸®åŠ©æ‚¨ï¼Œå¤§å¤šæ•°è„šæœ¬éƒ½å®Œå…¨æš´éœ²äº†æ•°æ®é¢„å¤„ç†çš„æ–¹å¼ï¼Œå…è®¸æ‚¨æ ¹æ®éœ€è¦å¯¹å…¶è¿›è¡Œç¼–è¾‘ã€‚\n "
        },
        {
            "sha": "86c1cdbb503750c23771e6dc10417c68281d1336",
            "filename": "examples/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2FREADME.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -17,7 +17,7 @@ limitations under the License.\n \n We host a wide range of example scripts for multiple learning frameworks. Simply choose your favorite: [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch) or [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n \n-We also have some [research projects](https://github.com/huggingface/transformers/tree/main/examples/research_projects), as well as some [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy). Note that unlike the main examples these are not actively maintained, and may require specific older versions of dependencies in order to run.\n+We also have some [research projects](https://github.com/huggingface/transformers-research-projects/), as well as some [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy). Note that unlike the main examples these are not actively maintained, and may require specific older versions of dependencies in order to run.\n \n While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data, allowing you to tweak and edit them as required.\n "
        },
        {
            "sha": "fb826129a8ef08fac99fc26fe8d27b2f2dee5e68",
            "filename": "examples/legacy/seq2seq/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2FREADME.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -17,7 +17,7 @@ limitations under the License.\n # Sequence-to-Sequence Training and Evaluation\n \n This directory contains examples for finetuning and evaluating transformers on summarization and translation tasks.\n-For deprecated `bertabs` instructions, see [`bertabs/README.md`](https://github.com/huggingface/transformers/blob/main/examples/research_projects/bertabs/README.md).\n+For deprecated `bertabs` instructions, see https://github.com/huggingface/transformers-research-projects/blob/main/bertabs/README.md.\n \n ### Supported Architectures\n "
        },
        {
            "sha": "700d1a2b561313c32c781a7fcac42e3928d83a1b",
            "filename": "examples/pytorch/language-modeling/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -177,7 +177,7 @@ sure all your batches have the same length.\n \n ### Whole word masking\n \n-This part was moved to `examples/research_projects/mlm_wwm`.\n+This part was moved to https://github.com/huggingface/transformers-research-projects/tree/main/mlm_wwm.\n \n ### XLNet and permutation language modeling\n "
        },
        {
            "sha": "0d332564de84ae810fc58b00dfe2def801231d88",
            "filename": "examples/pytorch/summarization/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fpytorch%2Fsummarization%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fpytorch%2Fsummarization%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2FREADME.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n This directory contains examples for finetuning and evaluating transformers on summarization  tasks.\n Please tag @patil-suraj with any issues/unexpected behaviors, or send a PR!\n-For deprecated `bertabs` instructions, see [`bertabs/README.md`](https://github.com/huggingface/transformers/blob/main/examples/research_projects/bertabs/README.md).\n+For deprecated `bertabs` instructions, see https://github.com/huggingface/transformers-research-projects/blob/main/bertabs/README.md.\n For the old `finetune_trainer.py` and related utils, see [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/blob/main/examples/legacy/seq2seq).\n \n ### Supported Architectures"
        },
        {
            "sha": "8285355fb0b5ceaf10cb23c6cd4e7d5fe93c620e",
            "filename": "examples/pytorch/translation/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fpytorch%2Ftranslation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fpytorch%2Ftranslation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2FREADME.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -18,7 +18,7 @@ limitations under the License.\n \n This directory contains examples for finetuning and evaluating transformers on translation tasks.\n Please tag @patil-suraj with any issues/unexpected behaviors, or send a PR!\n-For deprecated `bertabs` instructions, see [`bertabs/README.md`](https://github.com/huggingface/transformers/blob/main/examples/research_projects/bertabs/README.md).\n+For deprecated `bertabs` instructions, see https://github.com/huggingface/transformers-research-projects/blob/main/bertabs/README.md.\n For the old `finetune_trainer.py` and related utils, see [`examples/legacy/seq2seq`](https://github.com/huggingface/transformers/blob/main/examples/legacy/seq2seq).\n \n ### Supported Architectures"
        },
        {
            "sha": "e8a0ecd8c4e9e51ac90f09fab0fe68678f37fa2f",
            "filename": "examples/research_projects/README.md",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fresearch_projects%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1e4286fd5962438d15af0257fa5feb9f2b2e58be/examples%2Fresearch_projects%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2FREADME.md?ref=1e4286fd5962438d15af0257fa5feb9f2b2e58be",
            "patch": "@@ -1,5 +1,5 @@\n <!---\n-Copyright 2020 The HuggingFace Team. All rights reserved.\n+Copyright 2025 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n@@ -16,13 +16,5 @@ limitations under the License.\n \n # Research projects\n \n-This folder contains various research projects using ğŸ¤— Transformers. They are not maintained and require a specific\n-version of ğŸ¤— Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.\n-\n-To use any of them, just run the command\n-```bash\n-pip install -r requirements.txt\n-```\n-inside the folder of your choice.\n-\n-If you need help with any of those, contact the author(s), indicated at the top of the `README` of each folder.\n+This directory previously contained various research projects using ğŸ¤— Transformers. They have been moved\n+to a separate repo, so you can now find them at https://github.com/huggingface/transformers-research-projects/"
        },
        {
            "sha": "3e331a05f4534067ca371ab44832ef4a86dc67f4",
            "filename": "examples/research_projects/adversarial/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fadversarial%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,38 +0,0 @@\n-## Adversarial evaluation of model performances\n-\n-Here is an example on evaluating a model using adversarial evaluation of natural language inference with the Heuristic Analysis for NLI Systems (HANS) dataset [McCoy et al., 2019](https://arxiv.org/abs/1902.01007). The example was gracefully provided by [Nafise Sadat Moosavi](https://github.com/ns-moosavi).\n-\n-The HANS dataset can be downloaded from [this location](https://github.com/tommccoy1/hans).\n-\n-This is an example of using test_hans.py:\n-\n-```bash\n-export HANS_DIR=path-to-hans\n-export MODEL_TYPE=type-of-the-model-e.g.-bert-roberta-xlnet-etc\n-export MODEL_PATH=path-to-the-model-directory-that-is-trained-on-NLI-e.g.-by-using-run_glue.py\n-\n-python run_hans.py \\\n-        --task_name hans \\\n-        --model_type $MODEL_TYPE \\\n-        --do_eval \\\n-        --data_dir $HANS_DIR \\\n-        --model_name_or_path $MODEL_PATH \\\n-        --max_seq_length 128 \\\n-        --output_dir $MODEL_PATH \\\n-```\n-\n-This will create the hans_predictions.txt file in MODEL_PATH, which can then be evaluated using hans/evaluate_heur_output.py from the HANS dataset.\n-\n-The results of the BERT-base model that is trained on MNLI using batch size 8 and the random seed 42 on the HANS dataset is as follows:\n-\n-```bash\n-Heuristic entailed results:\n-lexical_overlap: 0.9702\n-subsequence: 0.9942\n-constituent: 0.9962\n-\n-Heuristic non-entailed results:\n-lexical_overlap: 0.199\n-subsequence: 0.0396\n-constituent: 0.118\n-```"
        },
        {
            "sha": "76c6528f6f0a79679142b2ed4ab182dc8ecffb65",
            "filename": "examples/research_projects/adversarial/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fadversarial%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1 +0,0 @@\n-transformers == 4.48.0"
        },
        {
            "sha": "23625dfa7ee43c8d34afd57c77a4642c14e4a184",
            "filename": "examples/research_projects/adversarial/run_hans.py",
            "status": "removed",
            "additions": 0,
            "deletions": 242,
            "changes": 242,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2Frun_hans.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2Frun_hans.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fadversarial%2Frun_hans.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,242 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Finetuning the library models for sequence classification on HANS.\"\"\"\n-\n-import logging\n-import os\n-from dataclasses import dataclass, field\n-from typing import Dict, List, Optional\n-\n-import numpy as np\n-import torch\n-from utils_hans import HansDataset, InputFeatures, hans_processors, hans_tasks_num_labels\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoModelForSequenceClassification,\n-    AutoTokenizer,\n-    HfArgumentParser,\n-    Trainer,\n-    TrainingArguments,\n-    default_data_collator,\n-    set_seed,\n-)\n-from transformers.trainer_utils import is_main_process\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    task_name: str = field(\n-        metadata={\"help\": \"The name of the task to train selected in the list: \" + \", \".join(hans_processors.keys())}\n-    )\n-    data_dir: str = field(\n-        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the task.\"}\n-    )\n-    max_seq_length: int = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-\n-\n-def hans_data_collator(features: List[InputFeatures]) -> Dict[str, torch.Tensor]:\n-    \"\"\"\n-    Data collator that removes the \"pairID\" key if present.\n-    \"\"\"\n-    batch = default_data_collator(features)\n-    _ = batch.pop(\"pairID\", None)\n-    return batch\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use\"\n-            \" --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_rank,\n-        training_args.device,\n-        training_args.n_gpu,\n-        bool(training_args.local_rank != -1),\n-        training_args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Set seed\n-    set_seed(training_args.seed)\n-\n-    try:\n-        num_labels = hans_tasks_num_labels[data_args.task_name]\n-    except KeyError:\n-        raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n-\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=data_args.task_name,\n-        cache_dir=model_args.cache_dir,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-    )\n-    model = AutoModelForSequenceClassification.from_pretrained(\n-        model_args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    # Get datasets\n-    train_dataset = (\n-        HansDataset(\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            task=data_args.task_name,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-        )\n-        if training_args.do_train\n-        else None\n-    )\n-    eval_dataset = (\n-        HansDataset(\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            task=data_args.task_name,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-            evaluate=True,\n-        )\n-        if training_args.do_eval\n-        else None\n-    )\n-\n-    # Initialize our Trainer\n-    trainer = Trainer(\n-        model=model,\n-        args=training_args,\n-        train_dataset=train_dataset,\n-        eval_dataset=eval_dataset,\n-        data_collator=hans_data_collator,\n-    )\n-\n-    # Training\n-    if training_args.do_train:\n-        trainer.train(\n-            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n-        )\n-        trainer.save_model()\n-        # For convenience, we also re-save the tokenizer to the same directory,\n-        # so that you can share your model easily on huggingface.co/models =)\n-        if trainer.is_world_master():\n-            tokenizer.save_pretrained(training_args.output_dir)\n-\n-    # Evaluation\n-    if training_args.do_eval:\n-        logger.info(\"*** Evaluate ***\")\n-\n-        output = trainer.predict(eval_dataset)\n-        preds = output.predictions\n-        preds = np.argmax(preds, axis=1)\n-\n-        pair_ids = [ex.pairID for ex in eval_dataset]\n-        output_eval_file = os.path.join(training_args.output_dir, \"hans_predictions.txt\")\n-        label_list = eval_dataset.get_labels()\n-        if trainer.is_world_master():\n-            with open(output_eval_file, \"w\") as writer:\n-                writer.write(\"pairID,gold_label\\n\")\n-                for pid, pred in zip(pair_ids, preds):\n-                    writer.write(\"ex\" + str(pid) + \",\" + label_list[int(pred)] + \"\\n\")\n-\n-        trainer._log(output.metrics)\n-\n-\n-def _mp_fn(index):\n-    # For xla_spawn (TPUs)\n-    main()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "f051e60f84fefdb54ee991a1668d14528da28ac0",
            "filename": "examples/research_projects/adversarial/utils_hans.py",
            "status": "removed",
            "additions": 0,
            "deletions": 339,
            "changes": 339,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2Futils_hans.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fadversarial%2Futils_hans.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fadversarial%2Futils_hans.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,339 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-import os\n-from dataclasses import dataclass\n-from typing import List, Optional, Union\n-\n-import tqdm\n-from filelock import FileLock\n-\n-from transformers import (\n-    BartTokenizer,\n-    BartTokenizerFast,\n-    DataProcessor,\n-    PreTrainedTokenizer,\n-    RobertaTokenizer,\n-    RobertaTokenizerFast,\n-    XLMRobertaTokenizer,\n-    is_tf_available,\n-    is_torch_available,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass(frozen=True)\n-class InputExample:\n-    \"\"\"\n-    A single training/test example for simple sequence classification.\n-\n-    Args:\n-        guid: Unique id for the example.\n-        text_a: string. The untokenized text of the first sequence. For single\n-            sequence tasks, only this sequence must be specified.\n-        text_b: (Optional) string. The untokenized text of the second sequence.\n-            Only must be specified for sequence pair tasks.\n-        label: (Optional) string. The label of the example. This should be\n-            specified for train and dev examples, but not for test examples.\n-        pairID: (Optional) string. Unique identifier for the pair of sentences.\n-    \"\"\"\n-\n-    guid: str\n-    text_a: str\n-    text_b: Optional[str] = None\n-    label: Optional[str] = None\n-    pairID: Optional[str] = None\n-\n-\n-@dataclass(frozen=True)\n-class InputFeatures:\n-    \"\"\"\n-    A single set of features of data.\n-    Property names are the same names as the corresponding inputs to a model.\n-\n-    Args:\n-        input_ids: Indices of input sequence tokens in the vocabulary.\n-        attention_mask: Mask to avoid performing attention on padding token indices.\n-            Mask values selected in ``[0, 1]``:\n-            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n-        token_type_ids: (Optional) Segment token indices to indicate first and second\n-            portions of the inputs. Only some models use them.\n-        label: (Optional) Label corresponding to the input. Int for classification problems,\n-            float for regression problems.\n-        pairID: (Optional) Unique identifier for the pair of sentences.\n-    \"\"\"\n-\n-    input_ids: List[int]\n-    attention_mask: Optional[List[int]] = None\n-    token_type_ids: Optional[List[int]] = None\n-    label: Optional[Union[int, float]] = None\n-    pairID: Optional[int] = None\n-\n-\n-if is_torch_available():\n-    import torch\n-    from torch.utils.data import Dataset\n-\n-    class HansDataset(Dataset):\n-        \"\"\"\n-        This will be superseded by a framework-agnostic approach\n-        soon.\n-        \"\"\"\n-\n-        features: List[InputFeatures]\n-\n-        def __init__(\n-            self,\n-            data_dir: str,\n-            tokenizer: PreTrainedTokenizer,\n-            task: str,\n-            max_seq_length: Optional[int] = None,\n-            overwrite_cache=False,\n-            evaluate: bool = False,\n-        ):\n-            processor = hans_processors[task]()\n-\n-            cached_features_file = os.path.join(\n-                data_dir,\n-                \"cached_{}_{}_{}_{}\".format(\n-                    \"dev\" if evaluate else \"train\",\n-                    tokenizer.__class__.__name__,\n-                    str(max_seq_length),\n-                    task,\n-                ),\n-            )\n-            label_list = processor.get_labels()\n-            if tokenizer.__class__ in (\n-                RobertaTokenizer,\n-                RobertaTokenizerFast,\n-                XLMRobertaTokenizer,\n-                BartTokenizer,\n-                BartTokenizerFast,\n-            ):\n-                # HACK(label indices are swapped in RoBERTa pretrained model)\n-                label_list[1], label_list[2] = label_list[2], label_list[1]\n-            self.label_list = label_list\n-\n-            # Make sure only the first process in distributed training processes the dataset,\n-            # and the others will use the cache.\n-            lock_path = cached_features_file + \".lock\"\n-            with FileLock(lock_path):\n-                if os.path.exists(cached_features_file) and not overwrite_cache:\n-                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n-                    self.features = torch.load(cached_features_file)\n-                else:\n-                    logger.info(f\"Creating features from dataset file at {data_dir}\")\n-\n-                    examples = (\n-                        processor.get_dev_examples(data_dir) if evaluate else processor.get_train_examples(data_dir)\n-                    )\n-\n-                    logger.info(\"Training examples: %s\", len(examples))\n-                    self.features = hans_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n-                    logger.info(\"Saving features into cached file %s\", cached_features_file)\n-                    torch.save(self.features, cached_features_file)\n-\n-        def __len__(self):\n-            return len(self.features)\n-\n-        def __getitem__(self, i) -> InputFeatures:\n-            return self.features[i]\n-\n-        def get_labels(self):\n-            return self.label_list\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    class TFHansDataset:\n-        \"\"\"\n-        This will be superseded by a framework-agnostic approach\n-        soon.\n-        \"\"\"\n-\n-        features: List[InputFeatures]\n-\n-        def __init__(\n-            self,\n-            data_dir: str,\n-            tokenizer: PreTrainedTokenizer,\n-            task: str,\n-            max_seq_length: Optional[int] = 128,\n-            overwrite_cache=False,\n-            evaluate: bool = False,\n-        ):\n-            processor = hans_processors[task]()\n-            label_list = processor.get_labels()\n-            if tokenizer.__class__ in (\n-                RobertaTokenizer,\n-                RobertaTokenizerFast,\n-                XLMRobertaTokenizer,\n-                BartTokenizer,\n-                BartTokenizerFast,\n-            ):\n-                # HACK(label indices are swapped in RoBERTa pretrained model)\n-                label_list[1], label_list[2] = label_list[2], label_list[1]\n-            self.label_list = label_list\n-\n-            examples = processor.get_dev_examples(data_dir) if evaluate else processor.get_train_examples(data_dir)\n-            self.features = hans_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n-\n-            def gen():\n-                for ex_index, ex in tqdm.tqdm(enumerate(self.features), desc=\"convert examples to features\"):\n-                    if ex_index % 10000 == 0:\n-                        logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n-\n-                    yield (\n-                        {\n-                            \"example_id\": 0,\n-                            \"input_ids\": ex.input_ids,\n-                            \"attention_mask\": ex.attention_mask,\n-                            \"token_type_ids\": ex.token_type_ids,\n-                        },\n-                        ex.label,\n-                    )\n-\n-            self.dataset = tf.data.Dataset.from_generator(\n-                gen,\n-                (\n-                    {\n-                        \"example_id\": tf.int32,\n-                        \"input_ids\": tf.int32,\n-                        \"attention_mask\": tf.int32,\n-                        \"token_type_ids\": tf.int32,\n-                    },\n-                    tf.int64,\n-                ),\n-                (\n-                    {\n-                        \"example_id\": tf.TensorShape([]),\n-                        \"input_ids\": tf.TensorShape([None, None]),\n-                        \"attention_mask\": tf.TensorShape([None, None]),\n-                        \"token_type_ids\": tf.TensorShape([None, None]),\n-                    },\n-                    tf.TensorShape([]),\n-                ),\n-            )\n-\n-        def get_dataset(self):\n-            return self.dataset\n-\n-        def __len__(self):\n-            return len(self.features)\n-\n-        def __getitem__(self, i) -> InputFeatures:\n-            return self.features[i]\n-\n-        def get_labels(self):\n-            return self.label_list\n-\n-\n-class HansProcessor(DataProcessor):\n-    \"\"\"Processor for the HANS data set.\"\"\"\n-\n-    def get_train_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"heuristics_train_set.txt\")), \"train\")\n-\n-    def get_dev_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"heuristics_evaluation_set.txt\")), \"dev\")\n-\n-    def get_labels(self):\n-        \"\"\"See base class.\n-        Note that we follow the standard three labels for MNLI\n-        (see :class:`~transformers.data.processors.utils.MnliProcessor`)\n-        but the HANS evaluation groups `contradiction` and `neutral` into `non-entailment` (label 0) while\n-        `entailment` is label 1.\"\"\"\n-        return [\"contradiction\", \"entailment\", \"neutral\"]\n-\n-    def _create_examples(self, lines, set_type):\n-        \"\"\"Creates examples for the training and dev sets.\"\"\"\n-        examples = []\n-        for i, line in enumerate(lines):\n-            if i == 0:\n-                continue\n-            guid = \"%s-%s\" % (set_type, line[0])\n-            text_a = line[5]\n-            text_b = line[6]\n-            pairID = line[7][2:] if line[7].startswith(\"ex\") else line[7]\n-            label = line[0]\n-            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label, pairID=pairID))\n-        return examples\n-\n-\n-def hans_convert_examples_to_features(\n-    examples: List[InputExample],\n-    label_list: List[str],\n-    max_length: int,\n-    tokenizer: PreTrainedTokenizer,\n-):\n-    \"\"\"\n-    Loads a data file into a list of ``InputFeatures``\n-\n-    Args:\n-        examples: List of ``InputExamples`` containing the examples.\n-        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method.\n-        max_length: Maximum example length.\n-        tokenizer: Instance of a tokenizer that will tokenize the examples.\n-\n-    Returns:\n-        A list of task-specific ``InputFeatures`` which can be fed to the model.\n-\n-    \"\"\"\n-\n-    label_map = {label: i for i, label in enumerate(label_list)}\n-\n-    features = []\n-    for ex_index, example in tqdm.tqdm(enumerate(examples), desc=\"convert examples to features\"):\n-        if ex_index % 10000 == 0:\n-            logger.info(\"Writing example %d\" % (ex_index))\n-\n-        inputs = tokenizer(\n-            example.text_a,\n-            example.text_b,\n-            add_special_tokens=True,\n-            max_length=max_length,\n-            padding=\"max_length\",\n-            truncation=True,\n-            return_overflowing_tokens=True,\n-        )\n-\n-        label = label_map[example.label] if example.label in label_map else 0\n-\n-        pairID = int(example.pairID)\n-\n-        features.append(InputFeatures(**inputs, label=label, pairID=pairID))\n-\n-    for i, example in enumerate(examples[:5]):\n-        logger.info(\"*** Example ***\")\n-        logger.info(f\"guid: {example}\")\n-        logger.info(f\"features: {features[i]}\")\n-\n-    return features\n-\n-\n-hans_tasks_num_labels = {\n-    \"hans\": 3,\n-}\n-\n-hans_processors = {\n-    \"hans\": HansProcessor,\n-}"
        },
        {
            "sha": "b405e8a94887504d6debbea6051ff4e976740f25",
            "filename": "examples/research_projects/bert-loses-patience/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,89 +0,0 @@\n-# Patience-based Early Exit\n-\n-Patience-based Early Exit (PABEE) is a plug-and-play inference method for pretrained language models.\n-We have already implemented it on BERT and ALBERT. Basically, you can make your LM faster and more robust with PABEE. It can even improve the performance of ALBERT on GLUE. The only sacrifice is that the batch size can only be 1.\n-Learn more in the paper [\"BERT Loses Patience: Fast and Robust Inference with Early Exit\"](https://arxiv.org/abs/2006.04152) and the official [GitHub repo](https://github.com/JetRunner/PABEE).\n-\n-![PABEE](https://github.com/JetRunner/PABEE/raw/master/bert-loses-patience.png)\n-\n-## Training\n-\n-You can fine-tune a pretrained language model (you can choose from BERT and ALBERT) and train the internal classifiers by:\n-```bash\n-export GLUE_DIR=/path/to/glue_data\n-export TASK_NAME=MRPC\n-\n-python ./run_glue_with_pabee.py \\\n-  --model_type albert \\\n-  --model_name_or_path google-bert/bert-base-uncased/albert/albert-base-v2 \\\n-  --task_name $TASK_NAME \\\n-  --do_train \\\n-  --do_eval \\\n-  --do_lower_case \\\n-  --data_dir \"$GLUE_DIR/$TASK_NAME\" \\\n-  --max_seq_length 128 \\\n-  --per_gpu_train_batch_size 32 \\\n-  --per_gpu_eval_batch_size 32 \\\n-  --learning_rate 2e-5 \\\n-  --save_steps 50 \\\n-  --logging_steps 50 \\\n-  --num_train_epochs 5 \\\n-  --output_dir /path/to/save/ \\\n-  --evaluate_during_training\n-```\n-\n-## Inference\n-\n-You can inference with different patience settings by:\n-```bash\n-export GLUE_DIR=/path/to/glue_data\n-export TASK_NAME=MRPC\n-\n-python ./run_glue_with_pabee.py \\\n-  --model_type albert \\\n-  --model_name_or_path /path/to/save/ \\\n-  --task_name $TASK_NAME \\\n-  --do_eval \\\n-  --do_lower_case \\\n-  --data_dir \"$GLUE_DIR/$TASK_NAME\" \\\n-  --max_seq_length 128 \\\n-  --per_gpu_eval_batch_size 1 \\\n-  --learning_rate 2e-5 \\\n-  --logging_steps 50 \\\n-  --num_train_epochs 15 \\\n-  --output_dir /path/to/save/ \\\n-  --eval_all_checkpoints \\\n-  --patience 3,4,5,6,7,8\n-```\n-where `patience` can be a list of patience settings, separated by a comma. It will help determine which patience works best.\n-\n-When evaluating on a regression task (STS-B), you may add `--regression_threshold 0.1` to define the regression threshold.\n-\n-## Results\n-On the GLUE dev set:\n-\n-| Model        | \\#Param | Speed  | CoLA  | MNLI  | MRPC  | QNLI  | QQP   | RTE   | SST\\-2 | STS\\-B |\n-|--------------|---------|--------|-------|-------|-------|-------|-------|-------|--------|--------|\n-| ALBERT\\-base | 12M     |        | 58\\.9 | 84\\.6 | 89\\.5 | 91\\.7 | 89\\.6 | 78\\.6 | 92\\.8  | 89\\.5  |\n-| \\+PABEE      | 12M     | 1\\.57x | 61\\.2 | 85\\.1 | 90\\.0 | 91\\.8 | 89\\.6 | 80\\.1 | 93\\.0  | 90\\.1  |\n-\n-| Model         | \\#Param | Speed\\-up | MNLI  | SST\\-2 | STS\\-B |\n-|---------------|---------|-----------|-------|--------|--------|\n-| BERT\\-base    | 108M    |           | 84\\.5 | 92\\.1  | 88\\.9  |\n-| \\+PABEE       | 108M    | 1\\.62x    | 83\\.6 | 92\\.0  | 88\\.7  |\n-| ALBERT\\-large | 18M     |           | 86\\.4 | 94\\.9  | 90\\.4  |\n-| \\+PABEE       | 18M     | 2\\.42x    | 86\\.8 | 95\\.2  | 90\\.6  |\n-\n-\n-## Citation\n-If you find this resource useful, please consider citing the following paper:\n-```bibtex\n-@misc{zhou2020bert,\n-    title={BERT Loses Patience: Fast and Robust Inference with Early Exit},\n-    author={Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},\n-    year={2020},\n-    eprint={2006.04152},\n-    archivePrefix={arXiv},\n-    primaryClass={cs.CL}\n-}\n-```"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/research_projects/bert-loses-patience/pabee/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "5b30155a736a964da6f7ac4181498d304b05bba4",
            "filename": "examples/research_projects/bert-loses-patience/pabee/modeling_pabee_albert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 320,
            "changes": 320,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2Fmodeling_pabee_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2Fmodeling_pabee_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2Fmodeling_pabee_albert.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,320 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 Google AI, Google Brain, the HuggingFace Inc. team and Microsoft Corporation.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch ALBERT model with Patience-based Early Exit.\"\"\"\n-\n-import logging\n-\n-import torch\n-from torch import nn\n-from torch.nn import CrossEntropyLoss, MSELoss\n-\n-from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n-from transformers.models.albert.modeling_albert import (\n-    ALBERT_INPUTS_DOCSTRING,\n-    ALBERT_START_DOCSTRING,\n-    AlbertModel,\n-    AlbertPreTrainedModel,\n-    AlbertTransformer,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class AlbertTransformerWithPabee(AlbertTransformer):\n-    def adaptive_forward(self, hidden_states, current_layer, attention_mask=None, head_mask=None):\n-        if current_layer == 0:\n-            hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n-        else:\n-            hidden_states = hidden_states[0]\n-\n-        layers_per_group = int(self.config.num_hidden_layers / self.config.num_hidden_groups)\n-\n-        # Index of the hidden group\n-        group_idx = int(current_layer / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n-\n-        layer_group_output = self.albert_layer_groups[group_idx](\n-            hidden_states,\n-            attention_mask,\n-            head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n-        )\n-        hidden_states = layer_group_output[0]\n-\n-        return (hidden_states,)\n-\n-\n-@add_start_docstrings(\n-    \"The bare ALBERT Model transformer with PABEE outputting raw hidden-states without any specific head on top.\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class AlbertModelWithPabee(AlbertModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.encoder = AlbertTransformerWithPabee(config)\n-\n-        self.init_weights()\n-        self.patience = 0\n-        self.inference_instances_num = 0\n-        self.inference_layers_num = 0\n-\n-        self.regression_threshold = 0\n-\n-    def set_regression_threshold(self, threshold):\n-        self.regression_threshold = threshold\n-\n-    def set_patience(self, patience):\n-        self.patience = patience\n-\n-    def reset_stats(self):\n-        self.inference_instances_num = 0\n-        self.inference_layers_num = 0\n-\n-    def log_stats(self):\n-        avg_inf_layers = self.inference_layers_num / self.inference_instances_num\n-        message = (\n-            f\"*** Patience = {self.patience} Avg. Inference Layers = {avg_inf_layers:.2f} Speed Up =\"\n-            f\" {1 - avg_inf_layers / self.config.num_hidden_layers:.2f} ***\"\n-        )\n-        print(message)\n-\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        output_dropout=None,\n-        output_layers=None,\n-        regression=False,\n-    ):\n-        r\"\"\"\n-        Return:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.AlbertConfig`) and inputs:\n-            last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n-                Sequence of hidden-states at the output of the last layer of the model.\n-            pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n-                Last layer hidden-state of the first token of the sequence (classification token)\n-                further processed by a Linear layer and a Tanh activation function. The Linear\n-                layer weights are trained from the next sentence prediction (classification)\n-                objective during pre-training.\n-\n-                This output is usually *not* a good summary\n-                of the semantic content of the input, you're often better with averaging or pooling\n-                the sequence of hidden-states for the whole input sequence.\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-        \"\"\"\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        if token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n-        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n-        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n-        embedding_output = self.embeddings(\n-            input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n-        )\n-        encoder_outputs = embedding_output\n-\n-        if self.training:\n-            res = []\n-            for i in range(self.config.num_hidden_layers):\n-                encoder_outputs = self.encoder.adaptive_forward(\n-                    encoder_outputs,\n-                    current_layer=i,\n-                    attention_mask=extended_attention_mask,\n-                    head_mask=head_mask,\n-                )\n-\n-                pooled_output = self.pooler_activation(self.pooler(encoder_outputs[0][:, 0]))\n-                logits = output_layers[i](output_dropout(pooled_output))\n-                res.append(logits)\n-        elif self.patience == 0:  # Use all layers for inference\n-            encoder_outputs = self.encoder(encoder_outputs, extended_attention_mask, head_mask=head_mask)\n-            pooled_output = self.pooler_activation(self.pooler(encoder_outputs[0][:, 0]))\n-            res = [output_layers[self.config.num_hidden_layers - 1](pooled_output)]\n-        else:\n-            patient_counter = 0\n-            patient_result = None\n-            calculated_layer_num = 0\n-            for i in range(self.config.num_hidden_layers):\n-                calculated_layer_num += 1\n-                encoder_outputs = self.encoder.adaptive_forward(\n-                    encoder_outputs,\n-                    current_layer=i,\n-                    attention_mask=extended_attention_mask,\n-                    head_mask=head_mask,\n-                )\n-\n-                pooled_output = self.pooler_activation(self.pooler(encoder_outputs[0][:, 0]))\n-                logits = output_layers[i](pooled_output)\n-                if regression:\n-                    labels = logits.detach()\n-                    if patient_result is not None:\n-                        patient_labels = patient_result.detach()\n-                    if (patient_result is not None) and torch.abs(patient_result - labels) < self.regression_threshold:\n-                        patient_counter += 1\n-                    else:\n-                        patient_counter = 0\n-                else:\n-                    labels = logits.detach().argmax(dim=1)\n-                    if patient_result is not None:\n-                        patient_labels = patient_result.detach().argmax(dim=1)\n-                    if (patient_result is not None) and torch.all(labels.eq(patient_labels)):\n-                        patient_counter += 1\n-                    else:\n-                        patient_counter = 0\n-\n-                patient_result = logits\n-                if patient_counter == self.patience:\n-                    break\n-            res = [patient_result]\n-            self.inference_layers_num += calculated_layer_num\n-            self.inference_instances_num += 1\n-\n-        return res\n-\n-\n-@add_start_docstrings(\n-    \"\"\"Albert Model transformer with PABEE and a sequence classification/regression head on top (a linear layer on top of\n-    the pooled output) e.g. for GLUE tasks. \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n-class AlbertForSequenceClassificationWithPabee(AlbertPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-\n-        self.albert = AlbertModelWithPabee(config)\n-        self.dropout = nn.Dropout(config.classifier_dropout_prob)\n-        self.classifiers = nn.ModuleList(\n-            [nn.Linear(config.hidden_size, self.config.num_labels) for _ in range(config.num_hidden_layers)]\n-        )\n-\n-        self.init_weights()\n-\n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        labels=None,\n-    ):\n-        r\"\"\"\n-            labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n-                Labels for computing the sequence classification/regression loss.\n-                Indices should be in ``[0, ..., config.num_labels - 1]``.\n-                If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n-                If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n-\n-        Returns:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.AlbertConfig`) and inputs:\n-            loss (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n-                Classification (or regression if config.num_labels==1) loss.\n-            logits ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n-                Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-\n-            Examples::\n-\n-                from transformers import AlbertTokenizer\n-                from pabee import AlbertForSequenceClassificationWithPabee\n-                from torch import nn\n-                import torch\n-\n-                tokenizer = AlbertTokenizer.from_pretrained('albert/albert-base-v2')\n-                model = AlbertForSequenceClassificationWithPabee.from_pretrained('albert/albert-base-v2')\n-                input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n-                labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n-                outputs = model(input_ids, labels=labels)\n-                loss, logits = outputs[:2]\n-\n-        \"\"\"\n-\n-        logits = self.albert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_dropout=self.dropout,\n-            output_layers=self.classifiers,\n-            regression=self.num_labels == 1,\n-        )\n-\n-        outputs = (logits[-1],)\n-\n-        if labels is not None:\n-            total_loss = None\n-            total_weights = 0\n-            for ix, logits_item in enumerate(logits):\n-                if self.num_labels == 1:\n-                    #  We are doing regression\n-                    loss_fct = MSELoss()\n-                    loss = loss_fct(logits_item.view(-1), labels.view(-1))\n-                else:\n-                    loss_fct = CrossEntropyLoss()\n-                    loss = loss_fct(logits_item.view(-1, self.num_labels), labels.view(-1))\n-                if total_loss is None:\n-                    total_loss = loss\n-                else:\n-                    total_loss += loss * (ix + 1)\n-                total_weights += ix + 1\n-            outputs = (total_loss / total_weights,) + outputs\n-\n-        return outputs"
        },
        {
            "sha": "c1ce924a57a2972ba2e7766ce2827010a5699fd0",
            "filename": "examples/research_projects/bert-loses-patience/pabee/modeling_pabee_bert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 345,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2Fmodeling_pabee_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2Fmodeling_pabee_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2Fpabee%2Fmodeling_pabee_bert.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,345 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The Google AI Language Team Authors, The HuggingFace Inc. team and Microsoft Corporation.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch BERT model with Patience-based Early Exit.\"\"\"\n-\n-import logging\n-\n-import torch\n-from torch import nn\n-from torch.nn import CrossEntropyLoss, MSELoss\n-\n-from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n-from transformers.models.bert.modeling_bert import (\n-    BERT_INPUTS_DOCSTRING,\n-    BERT_START_DOCSTRING,\n-    BertEncoder,\n-    BertModel,\n-    BertPreTrainedModel,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class BertEncoderWithPabee(BertEncoder):\n-    def adaptive_forward(self, hidden_states, current_layer, attention_mask=None, head_mask=None):\n-        layer_outputs = self.layer[current_layer](hidden_states, attention_mask, head_mask[current_layer])\n-\n-        hidden_states = layer_outputs[0]\n-\n-        return hidden_states\n-\n-\n-@add_start_docstrings(\n-    \"The bare Bert Model transformer with PABEE outputting raw hidden-states without any specific head on top.\",\n-    BERT_START_DOCSTRING,\n-)\n-class BertModelWithPabee(BertModel):\n-    \"\"\"\n-\n-    The model can behave as an encoder (with only self-attention) as well\n-    as a decoder, in which case a layer of cross-attention is added between\n-    the self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\n-    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n-\n-    To behave as a decoder the model needs to be initialized with the\n-    :obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n-    :obj:`encoder_hidden_states` is expected as an input to the forward pass.\n-\n-    .. _`Attention is all you need`:\n-        https://arxiv.org/abs/1706.03762\n-\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.encoder = BertEncoderWithPabee(config)\n-\n-        self.init_weights()\n-        self.patience = 0\n-        self.inference_instances_num = 0\n-        self.inference_layers_num = 0\n-\n-        self.regression_threshold = 0\n-\n-    def set_regression_threshold(self, threshold):\n-        self.regression_threshold = threshold\n-\n-    def set_patience(self, patience):\n-        self.patience = patience\n-\n-    def reset_stats(self):\n-        self.inference_instances_num = 0\n-        self.inference_layers_num = 0\n-\n-    def log_stats(self):\n-        avg_inf_layers = self.inference_layers_num / self.inference_instances_num\n-        message = (\n-            f\"*** Patience = {self.patience} Avg. Inference Layers = {avg_inf_layers:.2f} Speed Up =\"\n-            f\" {1 - avg_inf_layers / self.config.num_hidden_layers:.2f} ***\"\n-        )\n-        print(message)\n-\n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n-        output_dropout=None,\n-        output_layers=None,\n-        regression=False,\n-    ):\n-        r\"\"\"\n-        Return:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n-            last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n-                Sequence of hidden-states at the output of the last layer of the model.\n-            pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n-                Last layer hidden-state of the first token of the sequence (classification token)\n-                further processed by a Linear layer and a Tanh activation function. The Linear\n-                layer weights are trained from the next sentence prediction (classification)\n-                objective during pre-training.\n-\n-                This output is usually *not* a good summary\n-                of the semantic content of the input, you're often better with averaging or pooling\n-                the sequence of hidden-states for the whole input sequence.\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-        \"\"\"\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        if token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n-\n-        # If a 2D ou 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n-        embedding_output = self.embeddings(\n-            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n-        )\n-        encoder_outputs = embedding_output\n-\n-        if self.training:\n-            res = []\n-            for i in range(self.config.num_hidden_layers):\n-                encoder_outputs = self.encoder.adaptive_forward(\n-                    encoder_outputs, current_layer=i, attention_mask=extended_attention_mask, head_mask=head_mask\n-                )\n-\n-                pooled_output = self.pooler(encoder_outputs)\n-                logits = output_layers[i](output_dropout(pooled_output))\n-                res.append(logits)\n-        elif self.patience == 0:  # Use all layers for inference\n-            encoder_outputs = self.encoder(\n-                embedding_output,\n-                attention_mask=extended_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                encoder_attention_mask=encoder_extended_attention_mask,\n-            )\n-            pooled_output = self.pooler(encoder_outputs[0])\n-            res = [output_layers[self.config.num_hidden_layers - 1](pooled_output)]\n-        else:\n-            patient_counter = 0\n-            patient_result = None\n-            calculated_layer_num = 0\n-            for i in range(self.config.num_hidden_layers):\n-                calculated_layer_num += 1\n-                encoder_outputs = self.encoder.adaptive_forward(\n-                    encoder_outputs, current_layer=i, attention_mask=extended_attention_mask, head_mask=head_mask\n-                )\n-\n-                pooled_output = self.pooler(encoder_outputs)\n-                logits = output_layers[i](pooled_output)\n-                if regression:\n-                    labels = logits.detach()\n-                    if patient_result is not None:\n-                        patient_labels = patient_result.detach()\n-                    if (patient_result is not None) and torch.abs(patient_result - labels) < self.regression_threshold:\n-                        patient_counter += 1\n-                    else:\n-                        patient_counter = 0\n-                else:\n-                    labels = logits.detach().argmax(dim=1)\n-                    if patient_result is not None:\n-                        patient_labels = patient_result.detach().argmax(dim=1)\n-                    if (patient_result is not None) and torch.all(labels.eq(patient_labels)):\n-                        patient_counter += 1\n-                    else:\n-                        patient_counter = 0\n-\n-                patient_result = logits\n-                if patient_counter == self.patience:\n-                    break\n-            res = [patient_result]\n-            self.inference_layers_num += calculated_layer_num\n-            self.inference_instances_num += 1\n-\n-        return res\n-\n-\n-@add_start_docstrings(\n-    \"\"\"Bert Model transformer with PABEE and a sequence classification/regression head on top (a linear layer on top of\n-    the pooled output) e.g. for GLUE tasks. \"\"\",\n-    BERT_START_DOCSTRING,\n-)\n-class BertForSequenceClassificationWithPabee(BertPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-\n-        self.bert = BertModelWithPabee(config)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.classifiers = nn.ModuleList(\n-            [nn.Linear(config.hidden_size, self.config.num_labels) for _ in range(config.num_hidden_layers)]\n-        )\n-\n-        self.init_weights()\n-\n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        labels=None,\n-    ):\n-        r\"\"\"\n-            labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n-                Labels for computing the sequence classification/regression loss.\n-                Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n-                If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n-                If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-\n-        Returns:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n-            loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n-                Classification (or regression if config.num_labels==1) loss.\n-            logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n-                Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-\n-        Examples::\n-\n-            from transformers import BertTokenizer, BertForSequenceClassification\n-            from pabee import BertForSequenceClassificationWithPabee\n-            from torch import nn\n-            import torch\n-\n-            tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n-            model = BertForSequenceClassificationWithPabee.from_pretrained('google-bert/bert-base-uncased')\n-\n-            input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n-            labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n-            outputs = model(input_ids, labels=labels)\n-\n-            loss, logits = outputs[:2]\n-\n-        \"\"\"\n-\n-        logits = self.bert(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            head_mask=head_mask,\n-            inputs_embeds=inputs_embeds,\n-            output_dropout=self.dropout,\n-            output_layers=self.classifiers,\n-            regression=self.num_labels == 1,\n-        )\n-\n-        outputs = (logits[-1],)\n-\n-        if labels is not None:\n-            total_loss = None\n-            total_weights = 0\n-            for ix, logits_item in enumerate(logits):\n-                if self.num_labels == 1:\n-                    #  We are doing regression\n-                    loss_fct = MSELoss()\n-                    loss = loss_fct(logits_item.view(-1), labels.view(-1))\n-                else:\n-                    loss_fct = CrossEntropyLoss()\n-                    loss = loss_fct(logits_item.view(-1, self.num_labels), labels.view(-1))\n-                if total_loss is None:\n-                    total_loss = loss\n-                else:\n-                    total_loss += loss * (ix + 1)\n-                total_weights += ix + 1\n-            outputs = (total_loss / total_weights,) + outputs\n-\n-        return outputs"
        },
        {
            "sha": "af3b01e0645d7996ce1e1d04e7723f9c7b02d2b5",
            "filename": "examples/research_projects/bert-loses-patience/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1 +0,0 @@\n-transformers == 4.38.0\n\\ No newline at end of file"
        },
        {
            "sha": "d1ee5ddde3c6cab9a4fedebf7262711ec0fda44b",
            "filename": "examples/research_projects/bert-loses-patience/run_glue_with_pabee.py",
            "status": "removed",
            "additions": 0,
            "deletions": 751,
            "changes": 751,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Frun_glue_with_pabee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Frun_glue_with_pabee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2Frun_glue_with_pabee.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,751 +0,0 @@\n-# coding=utf-8\n-# Copyright 2020 The Google AI Language Team Authors, The HuggingFace Inc. team and Microsoft Corporation.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Training and inference using the library models for sequence classification on GLUE (Bert, Albert) with PABEE.\"\"\"\n-\n-import argparse\n-import glob\n-import json\n-import logging\n-import os\n-import random\n-\n-import numpy as np\n-import torch\n-from pabee.modeling_pabee_albert import AlbertForSequenceClassificationWithPabee\n-from pabee.modeling_pabee_bert import BertForSequenceClassificationWithPabee\n-from torch import nn\n-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm, trange\n-\n-import transformers\n-from transformers import (\n-    WEIGHTS_NAME,\n-    AdamW,\n-    AlbertConfig,\n-    AlbertTokenizer,\n-    BertConfig,\n-    BertTokenizer,\n-    get_linear_schedule_with_warmup,\n-)\n-from transformers import glue_compute_metrics as compute_metrics\n-from transformers import glue_convert_examples_to_features as convert_examples_to_features\n-from transformers import glue_output_modes as output_modes\n-from transformers import glue_processors as processors\n-from transformers.trainer_utils import is_main_process\n-\n-\n-try:\n-    from torch.utils.tensorboard import SummaryWriter\n-except ImportError:\n-    from tensorboardX import SummaryWriter\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-MODEL_CLASSES = {\n-    \"bert\": (BertConfig, BertForSequenceClassificationWithPabee, BertTokenizer),\n-    \"albert\": (AlbertConfig, AlbertForSequenceClassificationWithPabee, AlbertTokenizer),\n-}\n-\n-\n-def set_seed(args):\n-    random.seed(args.seed)\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    if args.n_gpu > 0:\n-        torch.cuda.manual_seed_all(args.seed)\n-\n-\n-def train(args, train_dataset, model, tokenizer):\n-    \"\"\"Train the model\"\"\"\n-    if args.local_rank in [-1, 0]:\n-        tb_writer = SummaryWriter()\n-\n-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n-\n-    if args.max_steps > 0:\n-        t_total = args.max_steps\n-        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n-    else:\n-        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n-\n-    # Prepare optimizer and schedule (linear warmup and decay)\n-    no_decay = [\"bias\", \"LayerNorm.weight\"]\n-    optimizer_grouped_parameters = [\n-        {\n-            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n-            \"weight_decay\": args.weight_decay,\n-        },\n-        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n-    ]\n-\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n-    scheduler = get_linear_schedule_with_warmup(\n-        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n-    )\n-\n-    # Check if saved optimizer or scheduler states exist\n-    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n-        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n-    ):\n-        # Load in optimizer and scheduler states\n-        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n-        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n-\n-    if args.fp16:\n-        try:\n-            from apex import amp\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n-\n-    # multi-gpu training (should be after apex fp16 initialization)\n-    if args.n_gpu > 1:\n-        model = nn.DataParallel(model)\n-\n-    # Distributed training (should be after apex fp16 initialization)\n-    if args.local_rank != -1:\n-        model = nn.parallel.DistributedDataParallel(\n-            model,\n-            device_ids=[args.local_rank],\n-            output_device=args.local_rank,\n-            find_unused_parameters=True,\n-        )\n-\n-    # Train!\n-    logger.info(\"***** Running training *****\")\n-    logger.info(\"  Num examples = %d\", len(train_dataset))\n-    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n-    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n-    logger.info(\n-        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n-        args.train_batch_size\n-        * args.gradient_accumulation_steps\n-        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n-    )\n-    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n-    logger.info(\"  Total optimization steps = %d\", t_total)\n-\n-    global_step = 0\n-    epochs_trained = 0\n-    steps_trained_in_current_epoch = 0\n-    # Check if continuing training from a checkpoint\n-    if os.path.exists(args.model_name_or_path):\n-        # set global_step to global_step of last saved checkpoint from model path\n-        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n-        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n-        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n-\n-        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n-        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n-        logger.info(\"  Continuing training from global step %d\", global_step)\n-        logger.info(\n-            \"  Will skip the first %d steps in the first epoch\",\n-            steps_trained_in_current_epoch,\n-        )\n-\n-    tr_loss, logging_loss = 0.0, 0.0\n-    model.zero_grad()\n-    train_iterator = trange(\n-        epochs_trained,\n-        int(args.num_train_epochs),\n-        desc=\"Epoch\",\n-        disable=args.local_rank not in [-1, 0],\n-    )\n-    set_seed(args)  # Added here for reproducibility\n-    for _ in train_iterator:\n-        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n-        for step, batch in enumerate(epoch_iterator):\n-            # Skip past any already trained steps if resuming training\n-            if steps_trained_in_current_epoch > 0:\n-                steps_trained_in_current_epoch -= 1\n-                continue\n-\n-            model.train()\n-            batch = tuple(t.to(args.device) for t in batch)\n-            inputs = {\n-                \"input_ids\": batch[0],\n-                \"attention_mask\": batch[1],\n-                \"labels\": batch[3],\n-            }\n-            inputs[\"token_type_ids\"] = batch[2]\n-            outputs = model(**inputs)\n-            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n-\n-            if args.n_gpu > 1:\n-                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n-            if args.gradient_accumulation_steps > 1:\n-                loss = loss / args.gradient_accumulation_steps\n-\n-            if args.fp16:\n-                with amp.scale_loss(loss, optimizer) as scaled_loss:\n-                    scaled_loss.backward()\n-            else:\n-                loss.backward()\n-\n-            tr_loss += loss.item()\n-            if (step + 1) % args.gradient_accumulation_steps == 0:\n-                if args.fp16:\n-                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n-                else:\n-                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n-\n-                optimizer.step()\n-                scheduler.step()  # Update learning rate schedule\n-                model.zero_grad()\n-                global_step += 1\n-\n-                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n-                    logs = {}\n-                    if (\n-                        args.local_rank == -1 and args.evaluate_during_training\n-                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n-                        results = evaluate(args, model, tokenizer)\n-                        for key, value in results.items():\n-                            eval_key = \"eval_{}\".format(key)\n-                            logs[eval_key] = value\n-\n-                    loss_scalar = (tr_loss - logging_loss) / args.logging_steps\n-                    learning_rate_scalar = scheduler.get_lr()[0]\n-                    logs[\"learning_rate\"] = learning_rate_scalar\n-                    logs[\"loss\"] = loss_scalar\n-                    logging_loss = tr_loss\n-\n-                    for key, value in logs.items():\n-                        tb_writer.add_scalar(key, value, global_step)\n-                    print(json.dumps({**logs, **{\"step\": global_step}}))\n-\n-                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n-                    # Save model checkpoint\n-                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n-                    model_to_save = (\n-                        model.module if hasattr(model, \"module\") else model\n-                    )  # Take care of distributed/parallel training\n-                    model_to_save.save_pretrained(output_dir)\n-                    tokenizer.save_pretrained(output_dir)\n-\n-                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n-                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n-\n-                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n-                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n-                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n-\n-            if args.max_steps > 0 and global_step > args.max_steps:\n-                epoch_iterator.close()\n-                break\n-        if args.max_steps > 0 and global_step > args.max_steps:\n-            train_iterator.close()\n-            break\n-\n-    if args.local_rank in [-1, 0]:\n-        tb_writer.close()\n-\n-    return global_step, tr_loss / global_step\n-\n-\n-def evaluate(args, model, tokenizer, prefix=\"\", patience=0):\n-    if args.model_type == \"albert\":\n-        model.albert.set_regression_threshold(args.regression_threshold)\n-        model.albert.set_patience(patience)\n-        model.albert.reset_stats()\n-    elif args.model_type == \"bert\":\n-        model.bert.set_regression_threshold(args.regression_threshold)\n-        model.bert.set_patience(patience)\n-        model.bert.reset_stats()\n-    else:\n-        raise NotImplementedError()\n-\n-    # Loop to handle MNLI double evaluation (matched, mis-matched)\n-    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n-    eval_outputs_dirs = (args.output_dir, args.output_dir + \"-MM\") if args.task_name == \"mnli\" else (args.output_dir,)\n-\n-    results = {}\n-    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n-        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n-\n-        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n-            os.makedirs(eval_output_dir)\n-\n-        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n-        # Note that DistributedSampler samples randomly\n-        eval_sampler = SequentialSampler(eval_dataset)\n-        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n-\n-        # multi-gpu eval\n-        if args.n_gpu > 1 and not isinstance(model, nn.DataParallel):\n-            model = nn.DataParallel(model)\n-\n-        # Eval!\n-        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n-        logger.info(\"  Num examples = %d\", len(eval_dataset))\n-        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n-        eval_loss = 0.0\n-        nb_eval_steps = 0\n-        preds = None\n-        out_label_ids = None\n-        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n-            model.eval()\n-            batch = tuple(t.to(args.device) for t in batch)\n-\n-            with torch.no_grad():\n-                inputs = {\n-                    \"input_ids\": batch[0],\n-                    \"attention_mask\": batch[1],\n-                    \"labels\": batch[3],\n-                }\n-                inputs[\"token_type_ids\"] = batch[2]\n-                outputs = model(**inputs)\n-                tmp_eval_loss, logits = outputs[:2]\n-\n-                eval_loss += tmp_eval_loss.mean().item()\n-            nb_eval_steps += 1\n-            if preds is None:\n-                preds = logits.detach().cpu().numpy()\n-                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n-            else:\n-                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n-                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n-\n-        eval_loss = eval_loss / nb_eval_steps\n-        if args.output_mode == \"classification\":\n-            preds = np.argmax(preds, axis=1)\n-        elif args.output_mode == \"regression\":\n-            preds = np.squeeze(preds)\n-        result = compute_metrics(eval_task, preds, out_label_ids)\n-        results.update(result)\n-\n-        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n-        with open(output_eval_file, \"w\") as writer:\n-            logger.info(\"***** Eval results {} *****\".format(prefix))\n-            for key in sorted(result.keys()):\n-                logger.info(\"  %s = %s\", key, str(result[key]))\n-                print(\"  %s = %s\" % (key, str(result[key])))\n-                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n-\n-    if args.eval_all_checkpoints and patience != 0:\n-        if args.model_type == \"albert\":\n-            model.albert.log_stats()\n-        elif args.model_type == \"bert\":\n-            model.bert.log_stats()\n-        else:\n-            raise NotImplementedError()\n-\n-    return results\n-\n-\n-def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n-    if args.local_rank not in [-1, 0] and not evaluate:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-\n-    processor = processors[task]()\n-    output_mode = output_modes[task]\n-    # Load data features from cache or dataset file\n-    cached_features_file = os.path.join(\n-        args.data_dir,\n-        \"cached_{}_{}_{}_{}\".format(\n-            \"dev\" if evaluate else \"train\",\n-            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n-            str(args.max_seq_length),\n-            str(task),\n-        ),\n-    )\n-    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file)\n-    else:\n-        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n-        label_list = processor.get_labels()\n-        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\", \"xlmroberta\"]:\n-            # HACK(label indices are swapped in RoBERTa pretrained model)\n-            label_list[1], label_list[2] = label_list[2], label_list[1]\n-        examples = (\n-            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n-        )\n-        features = convert_examples_to_features(\n-            examples,\n-            tokenizer,\n-            label_list=label_list,\n-            max_length=args.max_seq_length,\n-            output_mode=output_mode,\n-        )\n-        if args.local_rank in [-1, 0]:\n-            logger.info(\"Saving features into cached file %s\", cached_features_file)\n-            torch.save(features, cached_features_file)\n-\n-    if args.local_rank == 0 and not evaluate:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-\n-    # Convert to Tensors and build dataset\n-    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n-    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n-    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n-    if output_mode == \"classification\":\n-        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n-    elif output_mode == \"regression\":\n-        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n-\n-    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n-    return dataset\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-\n-    # Required parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n-    )\n-    parser.add_argument(\n-        \"--model_type\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pre-trained model or shortcut name.\",\n-    )\n-    parser.add_argument(\n-        \"--task_name\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model predictions and checkpoints will be written.\",\n-    )\n-    parser.add_argument(\n-        \"--patience\",\n-        default=\"0\",\n-        type=str,\n-        required=False,\n-    )\n-    parser.add_argument(\n-        \"--regression_threshold\",\n-        default=0,\n-        type=float,\n-        required=False,\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--config_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained config name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\n-        \"--cache_dir\",\n-        default=\"\",\n-        type=str,\n-        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n-    )\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=128,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after tokenization. Sequences longer \"\n-            \"than this will be truncated, sequences shorter will be padded.\"\n-        ),\n-    )\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n-    parser.add_argument(\n-        \"--evaluate_during_training\",\n-        action=\"store_true\",\n-        help=\"Run evaluation during training at each logging step.\",\n-    )\n-    parser.add_argument(\n-        \"--do_lower_case\",\n-        action=\"store_true\",\n-        help=\"Set this flag if you are using an uncased model.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--per_gpu_train_batch_size\",\n-        default=8,\n-        type=int,\n-        help=\"Batch size per GPU/CPU for training.\",\n-    )\n-    parser.add_argument(\n-        \"--per_gpu_eval_batch_size\",\n-        default=1,\n-        type=int,\n-        help=\"Batch size per GPU/CPU for evaluation.\",\n-    )\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\n-        \"--learning_rate\",\n-        default=5e-5,\n-        type=float,\n-        help=\"The initial learning rate for Adam.\",\n-    )\n-    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n-    parser.add_argument(\n-        \"--num_train_epochs\",\n-        default=3.0,\n-        type=float,\n-        help=\"Total number of training epochs to perform.\",\n-    )\n-    parser.add_argument(\n-        \"--max_steps\",\n-        default=-1,\n-        type=int,\n-        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n-    )\n-    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-\n-    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n-    parser.add_argument(\n-        \"--save_steps\",\n-        type=int,\n-        default=500,\n-        help=\"Save checkpoint every X updates steps.\",\n-    )\n-    parser.add_argument(\n-        \"--eval_all_checkpoints\",\n-        action=\"store_true\",\n-        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n-    )\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_output_dir\",\n-        action=\"store_true\",\n-        help=\"Overwrite the content of the output directory\",\n-    )\n-    parser.add_argument(\n-        \"--overwrite_cache\",\n-        action=\"store_true\",\n-        help=\"Overwrite the cached training and evaluation sets\",\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O1\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--local_rank\",\n-        type=int,\n-        default=-1,\n-        help=\"For distributed training: local_rank\",\n-    )\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n-    args = parser.parse_args()\n-\n-    if (\n-        os.path.exists(args.output_dir)\n-        and os.listdir(args.output_dir)\n-        and args.do_train\n-        and not args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n-                args.output_dir\n-            )\n-        )\n-\n-    # Setup distant debugging if needed\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup CUDA, GPU & distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = torch.cuda.device_count()\n-    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n-        torch.cuda.set_device(args.local_rank)\n-        device = torch.device(\"cuda\", args.local_rank)\n-        torch.distributed.init_process_group(backend=\"nccl\")\n-        args.n_gpu = 1\n-    args.device = device\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        args.local_rank,\n-        device,\n-        args.n_gpu,\n-        bool(args.local_rank != -1),\n-        args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    # Set seed\n-    set_seed(args)\n-\n-    # Prepare GLUE task\n-    args.task_name = args.task_name.lower()\n-    if args.task_name not in processors:\n-        raise ValueError(\"Task not found: %s\" % (args.task_name))\n-    processor = processors[args.task_name]()\n-    args.output_mode = output_modes[args.task_name]\n-    label_list = processor.get_labels()\n-    num_labels = len(label_list)\n-\n-    if args.patience != \"0\" and args.per_gpu_eval_batch_size != 1:\n-        raise ValueError(\"The eval batch size must be 1 with PABEE inference on.\")\n-\n-    # Load pretrained model and tokenizer\n-    if args.local_rank not in [-1, 0]:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n-\n-    args.model_type = args.model_type.lower()\n-    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n-    config = config_class.from_pretrained(\n-        args.config_name if args.config_name else args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=args.task_name,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    tokenizer = tokenizer_class.from_pretrained(\n-        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n-        do_lower_case=args.do_lower_case,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    model = model_class.from_pretrained(\n-        args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n-        config=config,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-\n-    if args.local_rank == 0:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n-\n-    model.to(args.device)\n-\n-    print(\"Total Model Parameters:\", sum(param.numel() for param in model.parameters()))\n-    output_layers_param_num = sum(param.numel() for param in model.classifiers.parameters())\n-    print(\"Output Layers Parameters:\", output_layers_param_num)\n-    single_output_layer_param_num = sum(param.numel() for param in model.classifiers[0].parameters())\n-    print(\n-        \"Added Output Layers Parameters:\",\n-        output_layers_param_num - single_output_layer_param_num,\n-    )\n-\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Training\n-    if args.do_train:\n-        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n-        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n-\n-    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n-    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n-        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n-        # They can then be reloaded using `from_pretrained()`\n-        model_to_save = (\n-            model.module if hasattr(model, \"module\") else model\n-        )  # Take care of distributed/parallel training\n-        model_to_save.save_pretrained(args.output_dir)\n-        tokenizer.save_pretrained(args.output_dir)\n-\n-        # Good practice: save your training arguments together with the trained model\n-        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n-\n-        # Load a trained model and vocabulary that you have fine-tuned\n-        model = model_class.from_pretrained(args.output_dir)\n-        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n-        model.to(args.device)\n-\n-    # Evaluation\n-    results = {}\n-    if args.do_eval and args.local_rank in [-1, 0]:\n-        patience_list = [int(x) for x in args.patience.split(\",\")]\n-        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n-        checkpoints = [args.output_dir]\n-        if args.eval_all_checkpoints:\n-            checkpoints = [\n-                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n-            ]\n-\n-        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n-\n-        for checkpoint in checkpoints:\n-            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n-            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n-\n-            model = model_class.from_pretrained(checkpoint)\n-            model.to(args.device)\n-\n-            print(f\"Evaluation for checkpoint {prefix}\")\n-            for patience in patience_list:\n-                result = evaluate(args, model, tokenizer, prefix=prefix, patience=patience)\n-                result = {k + \"_{}\".format(global_step): v for k, v in result.items()}\n-                results.update(result)\n-    return results\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "5516924f0f2fb7408d88470cc890132ce231f095",
            "filename": "examples/research_projects/bert-loses-patience/test_run_glue_with_pabee.py",
            "status": "removed",
            "additions": 0,
            "deletions": 51,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Ftest_run_glue_with_pabee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbert-loses-patience%2Ftest_run_glue_with_pabee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbert-loses-patience%2Ftest_run_glue_with_pabee.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,51 +0,0 @@\n-import argparse\n-import logging\n-import sys\n-from unittest.mock import patch\n-\n-import run_glue_with_pabee\n-\n-from transformers.testing_utils import TestCasePlus\n-\n-\n-logging.basicConfig(level=logging.DEBUG)\n-\n-logger = logging.getLogger()\n-\n-\n-def get_setup_file():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"-f\")\n-    args = parser.parse_args()\n-    return args.f\n-\n-\n-class PabeeTests(TestCasePlus):\n-    def test_run_glue(self):\n-        stream_handler = logging.StreamHandler(sys.stdout)\n-        logger.addHandler(stream_handler)\n-\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_glue_with_pabee.py\n-            --model_type albert\n-            --model_name_or_path albert/albert-base-v2\n-            --data_dir ./tests/fixtures/tests_samples/MRPC/\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --task_name mrpc\n-            --do_train\n-            --do_eval\n-            --per_gpu_train_batch_size=2\n-            --per_gpu_eval_batch_size=1\n-            --learning_rate=2e-5\n-            --max_steps=50\n-            --warmup_steps=2\n-            --seed=42\n-            --max_seq_length=128\n-            \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            result = run_glue_with_pabee.main()\n-            for value in result.values():\n-                self.assertGreaterEqual(value, 0.75)"
        },
        {
            "sha": "7109c0fb72be1bf438efb767d918770fffffc350",
            "filename": "examples/research_projects/bertabs/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,61 +0,0 @@\n-# Text Summarization with Pretrained Encoders\n-\n-This folder contains part of the code necessary to reproduce the results on abstractive summarization from the article [Text Summarization with Pretrained Encoders](https://arxiv.org/pdf/1908.08345.pdf) by [Yang Liu](https://nlp-yang.github.io/) and [Mirella Lapata](https://homepages.inf.ed.ac.uk/mlap/). It can also be used to summarize any document.\n-\n-The original code can be found on the Yang Liu's [github repository](https://github.com/nlpyang/PreSumm).\n-\n-The model is loaded with the pre-trained weights for the abstractive summarization model trained on the CNN/Daily Mail dataset with an extractive and then abstractive tasks.\n-\n-## Setup\n-\n-```bash\n-git clone https://github.com/huggingface/transformers && cd transformers\n-pip install .\n-pip install nltk py-rouge\n-cd examples/seq2seq/bertabs\n-```\n-\n-## Reproduce the authors'  ROUGE score\n-\n-To be able to reproduce the authors' results on the CNN/Daily Mail dataset you first need to download both CNN and Daily Mail datasets [from Kyunghyun Cho's website](https://cs.nyu.edu/~kcho/DMQA/) (the links next to \"Stories\") in the same folder. Then uncompress the archives by running:\n-\n-```bash\n-tar -xvf cnn_stories.tgz && tar -xvf dailymail_stories.tgz\n-```\n-\n-And move all the stories to the same folder. We will refer as `$DATA_PATH` the path to where you uncompressed both archive. Then run the following in the same folder as `run_summarization.py`:\n-\n-```bash\n-python run_summarization.py \\\n-    --documents_dir $DATA_PATH \\\n-    --summaries_output_dir $SUMMARIES_PATH \\ # optional\n-    --no_cuda false \\\n-    --batch_size 4 \\\n-    --min_length 50 \\\n-    --max_length 200 \\\n-    --beam_size 5 \\\n-    --alpha 0.95 \\\n-    --block_trigram true \\\n-    --compute_rouge true\n-```\n-\n-The scripts executes on GPU if one is available and if `no_cuda` is not set to `true`. Inference on multiple GPUs is not supported yet. The ROUGE scores will be displayed in the console at the end of evaluation and written in a `rouge_scores.txt` file. The script takes 30 hours to compute with a single Tesla V100 GPU and a batch size of 10 (300,000 texts to summarize).\n-\n-## Summarize any text\n-\n-Put the documents that you would like to summarize in a folder (the path to which is referred to as `$DATA_PATH` below) and run the following in the same folder as `run_summarization.py`:\n-\n-```bash\n-python run_summarization.py \\\n-    --documents_dir $DATA_PATH \\\n-    --summaries_output_dir $SUMMARIES_PATH \\ # optional\n-    --no_cuda false \\\n-    --batch_size 4 \\\n-    --min_length 50 \\\n-    --max_length 200 \\\n-    --beam_size 5 \\\n-    --alpha 0.95 \\\n-    --block_trigram true \\\n-```\n-\n-You may want to play around with `min_length`, `max_length` and `alpha` to suit your use case. If you want to compute ROUGE on another dataset you will need to tweak the stories/summaries import in `utils_summarization.py` and tell it where to fetch the reference summaries."
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/research_projects/bertabs/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "3e7222d490fc1035369c7408c9c2af243af1cf5f",
            "filename": "examples/research_projects/bertabs/configuration_bertabs.py",
            "status": "removed",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Fconfiguration_bertabs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Fconfiguration_bertabs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Fconfiguration_bertabs.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,98 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019 The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"BertAbs configuration\"\"\"\n-\n-import logging\n-\n-from transformers import PretrainedConfig\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-BERTABS_FINETUNED_CONFIG_MAP = {\n-    \"bertabs-finetuned-cnndm\": \"https://huggingface.co/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/resolve/main/config.json\",\n-}\n-\n-\n-class BertAbsConfig(PretrainedConfig):\n-    r\"\"\"Class to store the configuration of the BertAbs model.\n-\n-    Arguments:\n-        vocab_size: int\n-            Number of tokens in the vocabulary.\n-        max_pos: int\n-            The maximum sequence length that this model will be used with.\n-        enc_layer: int\n-            The number of hidden layers in the Transformer encoder.\n-        enc_hidden_size: int\n-            The size of the encoder's layers.\n-        enc_heads: int\n-            The number of attention heads for each attention layer in the encoder.\n-        enc_ff_size: int\n-            The size of the encoder's feed-forward layers.\n-        enc_dropout: int\n-            The dropout probability for all fully connected layers in the\n-            embeddings, layers, pooler and also the attention probabilities in\n-            the encoder.\n-        dec_layer: int\n-            The number of hidden layers in the decoder.\n-        dec_hidden_size: int\n-            The size of the decoder's layers.\n-        dec_heads: int\n-            The number of attention heads for each attention layer in the decoder.\n-        dec_ff_size: int\n-            The size of the decoder's feed-forward layers.\n-        dec_dropout: int\n-            The dropout probability for all fully connected layers in the\n-            embeddings, layers, pooler and also the attention probabilities in\n-            the decoder.\n-    \"\"\"\n-\n-    model_type = \"bertabs\"\n-\n-    def __init__(\n-        self,\n-        vocab_size=30522,\n-        max_pos=512,\n-        enc_layers=6,\n-        enc_hidden_size=512,\n-        enc_heads=8,\n-        enc_ff_size=512,\n-        enc_dropout=0.2,\n-        dec_layers=6,\n-        dec_hidden_size=768,\n-        dec_heads=8,\n-        dec_ff_size=2048,\n-        dec_dropout=0.2,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        self.vocab_size = vocab_size\n-        self.max_pos = max_pos\n-\n-        self.enc_layers = enc_layers\n-        self.enc_hidden_size = enc_hidden_size\n-        self.enc_heads = enc_heads\n-        self.enc_ff_size = enc_ff_size\n-        self.enc_dropout = enc_dropout\n-\n-        self.dec_layers = dec_layers\n-        self.dec_hidden_size = dec_hidden_size\n-        self.dec_heads = dec_heads\n-        self.dec_ff_size = dec_ff_size\n-        self.dec_dropout = dec_dropout"
        },
        {
            "sha": "f6222d35d40bfbc3ea0454b4bee03b6c63ea0c34",
            "filename": "examples/research_projects/bertabs/convert_bertabs_original_pytorch_checkpoint.py",
            "status": "removed",
            "additions": 0,
            "deletions": 185,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Fconvert_bertabs_original_pytorch_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Fconvert_bertabs_original_pytorch_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Fconvert_bertabs_original_pytorch_checkpoint.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,185 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Convert BertExtAbs's checkpoints.\n-\n-The script looks like it is doing something trivial but it is not. The \"weights\"\n-proposed by the authors are actually the entire model pickled. We need to load\n-the model within the original codebase to be able to only save its `state_dict`.\n-\"\"\"\n-\n-import argparse\n-import logging\n-from collections import namedtuple\n-\n-import torch\n-from model_bertabs import BertAbsSummarizer\n-from models.model_builder import AbsSummarizer  # The authors' implementation\n-\n-from transformers import BertTokenizer\n-\n-\n-logging.basicConfig(level=logging.INFO)\n-logger = logging.getLogger(__name__)\n-\n-\n-SAMPLE_TEXT = \"Hello world! cÃ©cÃ© herlolip\"\n-\n-\n-BertAbsConfig = namedtuple(\n-    \"BertAbsConfig\",\n-    [\n-        \"temp_dir\",\n-        \"large\",\n-        \"use_bert_emb\",\n-        \"finetune_bert\",\n-        \"encoder\",\n-        \"share_emb\",\n-        \"max_pos\",\n-        \"enc_layers\",\n-        \"enc_hidden_size\",\n-        \"enc_heads\",\n-        \"enc_ff_size\",\n-        \"enc_dropout\",\n-        \"dec_layers\",\n-        \"dec_hidden_size\",\n-        \"dec_heads\",\n-        \"dec_ff_size\",\n-        \"dec_dropout\",\n-    ],\n-)\n-\n-\n-def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n-    \"\"\"Copy/paste and tweak the pre-trained weights provided by the creators\n-    of BertAbs for the internal architecture.\n-    \"\"\"\n-\n-    # Instantiate the authors' model with the pre-trained weights\n-    config = BertAbsConfig(\n-        temp_dir=\".\",\n-        finetune_bert=False,\n-        large=False,\n-        share_emb=True,\n-        use_bert_emb=False,\n-        encoder=\"bert\",\n-        max_pos=512,\n-        enc_layers=6,\n-        enc_hidden_size=512,\n-        enc_heads=8,\n-        enc_ff_size=512,\n-        enc_dropout=0.2,\n-        dec_layers=6,\n-        dec_hidden_size=768,\n-        dec_heads=8,\n-        dec_ff_size=2048,\n-        dec_dropout=0.2,\n-    )\n-    checkpoints = torch.load(path_to_checkpoints, lambda storage, loc: storage)\n-    original = AbsSummarizer(config, torch.device(\"cpu\"), checkpoints)\n-    original.eval()\n-\n-    new_model = BertAbsSummarizer(config, torch.device(\"cpu\"))\n-    new_model.eval()\n-\n-    # -------------------\n-    # Convert the weights\n-    # -------------------\n-\n-    logging.info(\"convert the model\")\n-    new_model.bert.load_state_dict(original.bert.state_dict())\n-    new_model.decoder.load_state_dict(original.decoder.state_dict())\n-    new_model.generator.load_state_dict(original.generator.state_dict())\n-\n-    # ----------------------------------\n-    # Make sure the outpus are identical\n-    # ----------------------------------\n-\n-    logging.info(\"Make sure that the models' outputs are identical\")\n-    tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-    # prepare the model inputs\n-    encoder_input_ids = tokenizer.encode(\"This is sample Ã©Ã alj'-.\")\n-    encoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(encoder_input_ids)))\n-    encoder_input_ids = torch.tensor(encoder_input_ids).unsqueeze(0)\n-    decoder_input_ids = tokenizer.encode(\"This is sample 3 Ã©Ã alj'-.\")\n-    decoder_input_ids.extend([tokenizer.pad_token_id] * (512 - len(decoder_input_ids)))\n-    decoder_input_ids = torch.tensor(decoder_input_ids).unsqueeze(0)\n-\n-    # failsafe to make sure the weights reset does not affect the\n-    # loaded weights.\n-    assert torch.max(torch.abs(original.generator[0].weight - new_model.generator[0].weight)) == 0\n-\n-    # forward pass\n-    src = encoder_input_ids\n-    tgt = decoder_input_ids\n-    segs = token_type_ids = None\n-    clss = None\n-    mask_src = encoder_attention_mask = None\n-    mask_tgt = decoder_attention_mask = None\n-    mask_cls = None\n-\n-    # The original model does not apply the generator layer immediatly but rather in\n-    # the beam search (where it combines softmax + linear layer). Since we already\n-    # apply the softmax in our generation process we only apply the linear layer here.\n-    # We make sure that the outputs of the full stack are identical\n-    output_original_model = original(src, tgt, segs, clss, mask_src, mask_tgt, mask_cls)[0]\n-    output_original_generator = original.generator(output_original_model)\n-\n-    output_converted_model = new_model(\n-        encoder_input_ids, decoder_input_ids, token_type_ids, encoder_attention_mask, decoder_attention_mask\n-    )[0]\n-    output_converted_generator = new_model.generator(output_converted_model)\n-\n-    maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n-    print(\"Maximum absolute difference between weights: {:.2f}\".format(maximum_absolute_difference))\n-    maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n-    print(\"Maximum absolute difference between weights: {:.2f}\".format(maximum_absolute_difference))\n-\n-    are_identical = torch.allclose(output_converted_model, output_original_model, atol=1e-3)\n-    if are_identical:\n-        logging.info(\"all weights are equal up to 1e-3\")\n-    else:\n-        raise ValueError(\"the weights are different. The new model is likely different from the original one.\")\n-\n-    # The model has been saved with torch.save(model) and this is bound to the exact\n-    # directory structure. We save the state_dict instead.\n-    logging.info(\"saving the model's state dictionary\")\n-    torch.save(\n-        new_model.state_dict(), \"./bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin\"\n-    )\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\n-        \"--bertabs_checkpoint_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path the official PyTorch dump.\",\n-    )\n-    parser.add_argument(\n-        \"--pytorch_dump_folder_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to the output PyTorch model.\",\n-    )\n-    args = parser.parse_args()\n-\n-    convert_bertabs_checkpoints(\n-        args.bertabs_checkpoint_path,\n-        args.pytorch_dump_folder_path,\n-    )"
        },
        {
            "sha": "d65a0ca59d1036f8147f0a6506e75237f9bcd800",
            "filename": "examples/research_projects/bertabs/modeling_bertabs.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1054,
            "changes": 1054,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Fmodeling_bertabs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Fmodeling_bertabs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Fmodeling_bertabs.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,1054 +0,0 @@\n-# MIT License\n-\n-# Copyright (c) 2019 Yang Liu and the HuggingFace team\n-\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the \"Software\"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-\n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-\n-# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-import copy\n-import math\n-\n-import numpy as np\n-import torch\n-from configuration_bertabs import BertAbsConfig\n-from torch import nn\n-from torch.nn.init import xavier_uniform_\n-\n-from transformers import BertConfig, BertModel, PreTrainedModel\n-\n-\n-MAX_SIZE = 5000\n-\n-\n-class BertAbsPreTrainedModel(PreTrainedModel):\n-    config_class = BertAbsConfig\n-    load_tf_weights = False\n-    base_model_prefix = \"bert\"\n-\n-\n-class BertAbs(BertAbsPreTrainedModel):\n-    def __init__(self, args, checkpoint=None, bert_extractive_checkpoint=None):\n-        super().__init__(args)\n-        self.args = args\n-        self.bert = Bert()\n-\n-        # If pre-trained weights are passed for Bert, load these.\n-        load_bert_pretrained_extractive = True if bert_extractive_checkpoint else False\n-        if load_bert_pretrained_extractive:\n-            self.bert.model.load_state_dict(\n-                {n[11:]: p for n, p in bert_extractive_checkpoint.items() if n.startswith(\"bert.model\")},\n-                strict=True,\n-            )\n-\n-        self.vocab_size = self.bert.model.config.vocab_size\n-\n-        if args.max_pos > 512:\n-            my_pos_embeddings = nn.Embedding(args.max_pos, self.bert.model.config.hidden_size)\n-            my_pos_embeddings.weight.data[:512] = self.bert.model.embeddings.position_embeddings.weight.data\n-            my_pos_embeddings.weight.data[512:] = self.bert.model.embeddings.position_embeddings.weight.data[-1][\n-                None, :\n-            ].repeat(args.max_pos - 512, 1)\n-            self.bert.model.embeddings.position_embeddings = my_pos_embeddings\n-        tgt_embeddings = nn.Embedding(self.vocab_size, self.bert.model.config.hidden_size, padding_idx=0)\n-\n-        tgt_embeddings.weight = copy.deepcopy(self.bert.model.embeddings.word_embeddings.weight)\n-\n-        self.decoder = TransformerDecoder(\n-            self.args.dec_layers,\n-            self.args.dec_hidden_size,\n-            heads=self.args.dec_heads,\n-            d_ff=self.args.dec_ff_size,\n-            dropout=self.args.dec_dropout,\n-            embeddings=tgt_embeddings,\n-            vocab_size=self.vocab_size,\n-        )\n-\n-        gen_func = nn.LogSoftmax(dim=-1)\n-        self.generator = nn.Sequential(nn.Linear(args.dec_hidden_size, args.vocab_size), gen_func)\n-        self.generator[0].weight = self.decoder.embeddings.weight\n-\n-        load_from_checkpoints = False if checkpoint is None else True\n-        if load_from_checkpoints:\n-            self.load_state_dict(checkpoint)\n-\n-    def init_weights(self):\n-        for module in self.decoder.modules():\n-            if isinstance(module, (nn.Linear, nn.Embedding)):\n-                module.weight.data.normal_(mean=0.0, std=0.02)\n-            elif isinstance(module, nn.LayerNorm):\n-                module.bias.data.zero_()\n-                module.weight.data.fill_(1.0)\n-            if isinstance(module, nn.Linear) and module.bias is not None:\n-                module.bias.data.zero_()\n-        for p in self.generator.parameters():\n-            if p.dim() > 1:\n-                xavier_uniform_(p)\n-            else:\n-                p.data.zero_()\n-\n-    def forward(\n-        self,\n-        encoder_input_ids,\n-        decoder_input_ids,\n-        token_type_ids,\n-        encoder_attention_mask,\n-        decoder_attention_mask,\n-    ):\n-        encoder_output = self.bert(\n-            input_ids=encoder_input_ids,\n-            token_type_ids=token_type_ids,\n-            attention_mask=encoder_attention_mask,\n-        )\n-        encoder_hidden_states = encoder_output[0]\n-        dec_state = self.decoder.init_decoder_state(encoder_input_ids, encoder_hidden_states)\n-        decoder_outputs, _ = self.decoder(decoder_input_ids[:, :-1], encoder_hidden_states, dec_state)\n-        return decoder_outputs\n-\n-\n-class Bert(nn.Module):\n-    \"\"\"This class is not really necessary and should probably disappear.\"\"\"\n-\n-    def __init__(self):\n-        super().__init__()\n-        config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\")\n-        self.model = BertModel(config)\n-\n-    def forward(self, input_ids, attention_mask=None, token_type_ids=None, **kwargs):\n-        self.eval()\n-        with torch.no_grad():\n-            encoder_outputs, _ = self.model(\n-                input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, **kwargs\n-            )\n-        return encoder_outputs\n-\n-\n-class TransformerDecoder(nn.Module):\n-    \"\"\"\n-    The Transformer decoder from \"Attention is All You Need\".\n-\n-    Args:\n-       num_layers (int): number of encoder layers.\n-       d_model (int): size of the model\n-       heads (int): number of heads\n-       d_ff (int): size of the inner FF layer\n-       dropout (float): dropout parameters\n-       embeddings (:obj:`onmt.modules.Embeddings`):\n-          embeddings to use, should have positional encodings\n-       attn_type (str): if using a separate copy attention\n-    \"\"\"\n-\n-    def __init__(self, num_layers, d_model, heads, d_ff, dropout, embeddings, vocab_size):\n-        super().__init__()\n-\n-        # Basic attributes.\n-        self.decoder_type = \"transformer\"\n-        self.num_layers = num_layers\n-        self.embeddings = embeddings\n-        self.pos_emb = PositionalEncoding(dropout, self.embeddings.embedding_dim)\n-\n-        # Build TransformerDecoder.\n-        self.transformer_layers = nn.ModuleList(\n-            [TransformerDecoderLayer(d_model, heads, d_ff, dropout) for _ in range(num_layers)]\n-        )\n-\n-        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n-\n-    # forward(input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask)\n-    # def forward(self, input_ids, state, attention_mask=None, memory_lengths=None,\n-    # step=None, cache=None, encoder_attention_mask=None, encoder_hidden_states=None, memory_masks=None):\n-    def forward(\n-        self,\n-        input_ids,\n-        encoder_hidden_states=None,\n-        state=None,\n-        attention_mask=None,\n-        memory_lengths=None,\n-        step=None,\n-        cache=None,\n-        encoder_attention_mask=None,\n-    ):\n-        \"\"\"\n-        See :obj:`onmt.modules.RNNDecoderBase.forward()`\n-        memory_bank = encoder_hidden_states\n-        \"\"\"\n-        # Name conversion\n-        tgt = input_ids\n-        memory_bank = encoder_hidden_states\n-        memory_mask = encoder_attention_mask\n-\n-        # src_words = state.src\n-        src_words = state.src\n-        src_batch, src_len = src_words.size()\n-\n-        padding_idx = self.embeddings.padding_idx\n-\n-        # Decoder padding mask\n-        tgt_words = tgt\n-        tgt_batch, tgt_len = tgt_words.size()\n-        tgt_pad_mask = tgt_words.data.eq(padding_idx).unsqueeze(1).expand(tgt_batch, tgt_len, tgt_len)\n-\n-        # Encoder padding mask\n-        if memory_mask is not None:\n-            src_len = memory_mask.size(-1)\n-            src_pad_mask = memory_mask.expand(src_batch, tgt_len, src_len)\n-        else:\n-            src_pad_mask = src_words.data.eq(padding_idx).unsqueeze(1).expand(src_batch, tgt_len, src_len)\n-\n-        # Pass through the embeddings\n-        emb = self.embeddings(input_ids)\n-        output = self.pos_emb(emb, step)\n-        assert emb.dim() == 3  # len x batch x embedding_dim\n-\n-        if state.cache is None:\n-            saved_inputs = []\n-\n-        for i in range(self.num_layers):\n-            prev_layer_input = None\n-            if state.cache is None:\n-                if state.previous_input is not None:\n-                    prev_layer_input = state.previous_layer_inputs[i]\n-\n-            output, all_input = self.transformer_layers[i](\n-                output,\n-                memory_bank,\n-                src_pad_mask,\n-                tgt_pad_mask,\n-                previous_input=prev_layer_input,\n-                layer_cache=state.cache[\"layer_{}\".format(i)] if state.cache is not None else None,\n-                step=step,\n-            )\n-            if state.cache is None:\n-                saved_inputs.append(all_input)\n-\n-        if state.cache is None:\n-            saved_inputs = torch.stack(saved_inputs)\n-\n-        output = self.layer_norm(output)\n-\n-        if state.cache is None:\n-            state = state.update_state(tgt, saved_inputs)\n-\n-        # Decoders in transformers return a tuple. Beam search will fail\n-        # if we don't follow this convention.\n-        return output, state  # , state\n-\n-    def init_decoder_state(self, src, memory_bank, with_cache=False):\n-        \"\"\"Init decoder state\"\"\"\n-        state = TransformerDecoderState(src)\n-        if with_cache:\n-            state._init_cache(memory_bank, self.num_layers)\n-        return state\n-\n-\n-class PositionalEncoding(nn.Module):\n-    def __init__(self, dropout, dim, max_len=5000):\n-        pe = torch.zeros(max_len, dim)\n-        position = torch.arange(0, max_len).unsqueeze(1)\n-        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) * -(math.log(10000.0) / dim)))\n-        pe[:, 0::2] = torch.sin(position.float() * div_term)\n-        pe[:, 1::2] = torch.cos(position.float() * div_term)\n-        pe = pe.unsqueeze(0)\n-        super().__init__()\n-        self.register_buffer(\"pe\", pe)\n-        self.dropout = nn.Dropout(p=dropout)\n-        self.dim = dim\n-\n-    def forward(self, emb, step=None):\n-        emb = emb * math.sqrt(self.dim)\n-        if step:\n-            emb = emb + self.pe[:, step][:, None, :]\n-\n-        else:\n-            emb = emb + self.pe[:, : emb.size(1)]\n-        emb = self.dropout(emb)\n-        return emb\n-\n-    def get_emb(self, emb):\n-        return self.pe[:, : emb.size(1)]\n-\n-\n-class TransformerDecoderLayer(nn.Module):\n-    \"\"\"\n-    Args:\n-      d_model (int): the dimension of keys/values/queries in\n-                       MultiHeadedAttention, also the input size of\n-                       the first-layer of the PositionwiseFeedForward.\n-      heads (int): the number of heads for MultiHeadedAttention.\n-      d_ff (int): the second-layer of the PositionwiseFeedForward.\n-      dropout (float): dropout probability(0-1.0).\n-      self_attn_type (string): type of self-attention scaled-dot, average\n-    \"\"\"\n-\n-    def __init__(self, d_model, heads, d_ff, dropout):\n-        super().__init__()\n-\n-        self.self_attn = MultiHeadedAttention(heads, d_model, dropout=dropout)\n-\n-        self.context_attn = MultiHeadedAttention(heads, d_model, dropout=dropout)\n-        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n-        self.layer_norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n-        self.layer_norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n-        self.drop = nn.Dropout(dropout)\n-        mask = self._get_attn_subsequent_mask(MAX_SIZE)\n-        # Register self.mask as a saved_state in TransformerDecoderLayer, so\n-        # it gets TransformerDecoderLayer's cuda behavior automatically.\n-        self.register_buffer(\"mask\", mask)\n-\n-    def forward(\n-        self,\n-        inputs,\n-        memory_bank,\n-        src_pad_mask,\n-        tgt_pad_mask,\n-        previous_input=None,\n-        layer_cache=None,\n-        step=None,\n-    ):\n-        \"\"\"\n-        Args:\n-            inputs (`FloatTensor`): `[batch_size x 1 x model_dim]`\n-            memory_bank (`FloatTensor`): `[batch_size x src_len x model_dim]`\n-            src_pad_mask (`LongTensor`): `[batch_size x 1 x src_len]`\n-            tgt_pad_mask (`LongTensor`): `[batch_size x 1 x 1]`\n-\n-        Returns:\n-            (`FloatTensor`, `FloatTensor`, `FloatTensor`):\n-\n-            * output `[batch_size x 1 x model_dim]`\n-            * attn `[batch_size x 1 x src_len]`\n-            * all_input `[batch_size x current_step x model_dim]`\n-\n-        \"\"\"\n-        dec_mask = torch.gt(tgt_pad_mask + self.mask[:, : tgt_pad_mask.size(1), : tgt_pad_mask.size(1)], 0)\n-        input_norm = self.layer_norm_1(inputs)\n-        all_input = input_norm\n-        if previous_input is not None:\n-            all_input = torch.cat((previous_input, input_norm), dim=1)\n-            dec_mask = None\n-\n-        query = self.self_attn(\n-            all_input,\n-            all_input,\n-            input_norm,\n-            mask=dec_mask,\n-            layer_cache=layer_cache,\n-            type=\"self\",\n-        )\n-\n-        query = self.drop(query) + inputs\n-\n-        query_norm = self.layer_norm_2(query)\n-        mid = self.context_attn(\n-            memory_bank,\n-            memory_bank,\n-            query_norm,\n-            mask=src_pad_mask,\n-            layer_cache=layer_cache,\n-            type=\"context\",\n-        )\n-        output = self.feed_forward(self.drop(mid) + query)\n-\n-        return output, all_input\n-        # return output\n-\n-    def _get_attn_subsequent_mask(self, size):\n-        \"\"\"\n-        Get an attention mask to avoid using the subsequent info.\n-\n-        Args:\n-            size: int\n-\n-        Returns:\n-            (`LongTensor`):\n-\n-            * subsequent_mask `[1 x size x size]`\n-        \"\"\"\n-        attn_shape = (1, size, size)\n-        subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\"uint8\")\n-        subsequent_mask = torch.from_numpy(subsequent_mask)\n-        return subsequent_mask\n-\n-\n-class MultiHeadedAttention(nn.Module):\n-    \"\"\"\n-    Multi-Head Attention module from\n-    \"Attention is All You Need\"\n-    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n-\n-    Similar to standard `dot` attention but uses\n-    multiple attention distributions simultaneously\n-    to select relevant items.\n-\n-    .. mermaid::\n-\n-       graph BT\n-          A[key]\n-          B[value]\n-          C[query]\n-          O[output]\n-          subgraph Attn\n-            D[Attn 1]\n-            E[Attn 2]\n-            F[Attn N]\n-          end\n-          A --> D\n-          C --> D\n-          A --> E\n-          C --> E\n-          A --> F\n-          C --> F\n-          D --> O\n-          E --> O\n-          F --> O\n-          B --> O\n-\n-    Also includes several additional tricks.\n-\n-    Args:\n-       head_count (int): number of parallel heads\n-       model_dim (int): the dimension of keys/values/queries,\n-           must be divisible by head_count\n-       dropout (float): dropout parameter\n-    \"\"\"\n-\n-    def __init__(self, head_count, model_dim, dropout=0.1, use_final_linear=True):\n-        assert model_dim % head_count == 0\n-        self.dim_per_head = model_dim // head_count\n-        self.model_dim = model_dim\n-\n-        super().__init__()\n-        self.head_count = head_count\n-\n-        self.linear_keys = nn.Linear(model_dim, head_count * self.dim_per_head)\n-        self.linear_values = nn.Linear(model_dim, head_count * self.dim_per_head)\n-        self.linear_query = nn.Linear(model_dim, head_count * self.dim_per_head)\n-        self.softmax = nn.Softmax(dim=-1)\n-        self.dropout = nn.Dropout(dropout)\n-        self.use_final_linear = use_final_linear\n-        if self.use_final_linear:\n-            self.final_linear = nn.Linear(model_dim, model_dim)\n-\n-    def forward(\n-        self,\n-        key,\n-        value,\n-        query,\n-        mask=None,\n-        layer_cache=None,\n-        type=None,\n-        predefined_graph_1=None,\n-    ):\n-        \"\"\"\n-        Compute the context vector and the attention vectors.\n-\n-        Args:\n-           key (`FloatTensor`): set of `key_len`\n-                key vectors `[batch, key_len, dim]`\n-           value (`FloatTensor`): set of `key_len`\n-                value vectors `[batch, key_len, dim]`\n-           query (`FloatTensor`): set of `query_len`\n-                 query vectors  `[batch, query_len, dim]`\n-           mask: binary mask indicating which keys have\n-                 non-zero attention `[batch, query_len, key_len]`\n-        Returns:\n-           (`FloatTensor`, `FloatTensor`) :\n-\n-           * output context vectors `[batch, query_len, dim]`\n-           * one of the attention vectors `[batch, query_len, key_len]`\n-        \"\"\"\n-        batch_size = key.size(0)\n-        dim_per_head = self.dim_per_head\n-        head_count = self.head_count\n-\n-        def shape(x):\n-            \"\"\"projection\"\"\"\n-            return x.view(batch_size, -1, head_count, dim_per_head).transpose(1, 2)\n-\n-        def unshape(x):\n-            \"\"\"compute context\"\"\"\n-            return x.transpose(1, 2).contiguous().view(batch_size, -1, head_count * dim_per_head)\n-\n-        # 1) Project key, value, and query.\n-        if layer_cache is not None:\n-            if type == \"self\":\n-                query, key, value = (\n-                    self.linear_query(query),\n-                    self.linear_keys(query),\n-                    self.linear_values(query),\n-                )\n-\n-                key = shape(key)\n-                value = shape(value)\n-\n-                if layer_cache is not None:\n-                    device = key.device\n-                    if layer_cache[\"self_keys\"] is not None:\n-                        key = torch.cat((layer_cache[\"self_keys\"].to(device), key), dim=2)\n-                    if layer_cache[\"self_values\"] is not None:\n-                        value = torch.cat((layer_cache[\"self_values\"].to(device), value), dim=2)\n-                    layer_cache[\"self_keys\"] = key\n-                    layer_cache[\"self_values\"] = value\n-            elif type == \"context\":\n-                query = self.linear_query(query)\n-                if layer_cache is not None:\n-                    if layer_cache[\"memory_keys\"] is None:\n-                        key, value = self.linear_keys(key), self.linear_values(value)\n-                        key = shape(key)\n-                        value = shape(value)\n-                    else:\n-                        key, value = (\n-                            layer_cache[\"memory_keys\"],\n-                            layer_cache[\"memory_values\"],\n-                        )\n-                    layer_cache[\"memory_keys\"] = key\n-                    layer_cache[\"memory_values\"] = value\n-                else:\n-                    key, value = self.linear_keys(key), self.linear_values(value)\n-                    key = shape(key)\n-                    value = shape(value)\n-        else:\n-            key = self.linear_keys(key)\n-            value = self.linear_values(value)\n-            query = self.linear_query(query)\n-            key = shape(key)\n-            value = shape(value)\n-\n-        query = shape(query)\n-\n-        # 2) Calculate and scale scores.\n-        query = query / math.sqrt(dim_per_head)\n-        scores = torch.matmul(query, key.transpose(2, 3))\n-\n-        if mask is not None:\n-            mask = mask.unsqueeze(1).expand_as(scores)\n-            scores = scores.masked_fill(mask, -1e18)\n-\n-        # 3) Apply attention dropout and compute context vectors.\n-\n-        attn = self.softmax(scores)\n-\n-        if predefined_graph_1 is not None:\n-            attn_masked = attn[:, -1] * predefined_graph_1\n-            attn_masked = attn_masked / (torch.sum(attn_masked, 2).unsqueeze(2) + 1e-9)\n-\n-            attn = torch.cat([attn[:, :-1], attn_masked.unsqueeze(1)], 1)\n-\n-        drop_attn = self.dropout(attn)\n-        if self.use_final_linear:\n-            context = unshape(torch.matmul(drop_attn, value))\n-            output = self.final_linear(context)\n-            return output\n-        else:\n-            context = torch.matmul(drop_attn, value)\n-            return context\n-\n-\n-class DecoderState:\n-    \"\"\"Interface for grouping together the current state of a recurrent\n-    decoder. In the simplest case just represents the hidden state of\n-    the model.  But can also be used for implementing various forms of\n-    input_feeding and non-recurrent models.\n-\n-    Modules need to implement this to utilize beam search decoding.\n-    \"\"\"\n-\n-    def detach(self):\n-        \"\"\"Need to document this\"\"\"\n-        self.hidden = tuple([_.detach() for _ in self.hidden])\n-        self.input_feed = self.input_feed.detach()\n-\n-    def beam_update(self, idx, positions, beam_size):\n-        \"\"\"Need to document this\"\"\"\n-        for e in self._all:\n-            sizes = e.size()\n-            br = sizes[1]\n-            if len(sizes) == 3:\n-                sent_states = e.view(sizes[0], beam_size, br // beam_size, sizes[2])[:, :, idx]\n-            else:\n-                sent_states = e.view(sizes[0], beam_size, br // beam_size, sizes[2], sizes[3])[:, :, idx]\n-\n-            sent_states.data.copy_(sent_states.data.index_select(1, positions))\n-\n-    def map_batch_fn(self, fn):\n-        raise NotImplementedError()\n-\n-\n-class TransformerDecoderState(DecoderState):\n-    \"\"\"Transformer Decoder state base class\"\"\"\n-\n-    def __init__(self, src):\n-        \"\"\"\n-        Args:\n-            src (FloatTensor): a sequence of source words tensors\n-                    with optional feature tensors, of size (len x batch).\n-        \"\"\"\n-        self.src = src\n-        self.previous_input = None\n-        self.previous_layer_inputs = None\n-        self.cache = None\n-\n-    @property\n-    def _all(self):\n-        \"\"\"\n-        Contains attributes that need to be updated in self.beam_update().\n-        \"\"\"\n-        if self.previous_input is not None and self.previous_layer_inputs is not None:\n-            return (self.previous_input, self.previous_layer_inputs, self.src)\n-        else:\n-            return (self.src,)\n-\n-    def detach(self):\n-        if self.previous_input is not None:\n-            self.previous_input = self.previous_input.detach()\n-        if self.previous_layer_inputs is not None:\n-            self.previous_layer_inputs = self.previous_layer_inputs.detach()\n-        self.src = self.src.detach()\n-\n-    def update_state(self, new_input, previous_layer_inputs):\n-        state = TransformerDecoderState(self.src)\n-        state.previous_input = new_input\n-        state.previous_layer_inputs = previous_layer_inputs\n-        return state\n-\n-    def _init_cache(self, memory_bank, num_layers):\n-        self.cache = {}\n-\n-        for l in range(num_layers):\n-            layer_cache = {\"memory_keys\": None, \"memory_values\": None}\n-            layer_cache[\"self_keys\"] = None\n-            layer_cache[\"self_values\"] = None\n-            self.cache[\"layer_{}\".format(l)] = layer_cache\n-\n-    def repeat_beam_size_times(self, beam_size):\n-        \"\"\"Repeat beam_size times along batch dimension.\"\"\"\n-        self.src = self.src.data.repeat(1, beam_size, 1)\n-\n-    def map_batch_fn(self, fn):\n-        def _recursive_map(struct, batch_dim=0):\n-            for k, v in struct.items():\n-                if v is not None:\n-                    if isinstance(v, dict):\n-                        _recursive_map(v)\n-                    else:\n-                        struct[k] = fn(v, batch_dim)\n-\n-        self.src = fn(self.src, 0)\n-        if self.cache is not None:\n-            _recursive_map(self.cache)\n-\n-\n-def gelu(x):\n-    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n-\n-\n-class PositionwiseFeedForward(nn.Module):\n-    \"\"\"A two-layer Feed-Forward-Network with residual layer norm.\n-\n-    Args:\n-        d_model (int): the size of input for the first-layer of the FFN.\n-        d_ff (int): the hidden layer size of the second-layer\n-            of the FNN.\n-        dropout (float): dropout probability in :math:`[0, 1)`.\n-    \"\"\"\n-\n-    def __init__(self, d_model, d_ff, dropout=0.1):\n-        super().__init__()\n-        self.w_1 = nn.Linear(d_model, d_ff)\n-        self.w_2 = nn.Linear(d_ff, d_model)\n-        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n-        self.actv = gelu\n-        self.dropout_1 = nn.Dropout(dropout)\n-        self.dropout_2 = nn.Dropout(dropout)\n-\n-    def forward(self, x):\n-        inter = self.dropout_1(self.actv(self.w_1(self.layer_norm(x))))\n-        output = self.dropout_2(self.w_2(inter))\n-        return output + x\n-\n-\n-#\n-# TRANSLATOR\n-# The following code is used to generate summaries using the\n-# pre-trained weights and beam search.\n-#\n-\n-\n-def build_predictor(args, tokenizer, symbols, model, logger=None):\n-    # we should be able to refactor the global scorer a lot\n-    scorer = GNMTGlobalScorer(args.alpha, length_penalty=\"wu\")\n-    translator = Translator(args, model, tokenizer, symbols, global_scorer=scorer, logger=logger)\n-    return translator\n-\n-\n-class GNMTGlobalScorer:\n-    \"\"\"\n-    NMT re-ranking score from\n-    \"Google's Neural Machine Translation System\" :cite:`wu2016google`\n-\n-    Args:\n-       alpha (float): length parameter\n-       beta (float):  coverage parameter\n-    \"\"\"\n-\n-    def __init__(self, alpha, length_penalty):\n-        self.alpha = alpha\n-        penalty_builder = PenaltyBuilder(length_penalty)\n-        self.length_penalty = penalty_builder.length_penalty()\n-\n-    def score(self, beam, logprobs):\n-        \"\"\"\n-        Rescores a prediction based on penalty functions\n-        \"\"\"\n-        normalized_probs = self.length_penalty(beam, logprobs, self.alpha)\n-        return normalized_probs\n-\n-\n-class PenaltyBuilder:\n-    \"\"\"\n-    Returns the Length and Coverage Penalty function for Beam Search.\n-\n-    Args:\n-        length_pen (str): option name of length pen\n-        cov_pen (str): option name of cov pen\n-    \"\"\"\n-\n-    def __init__(self, length_pen):\n-        self.length_pen = length_pen\n-\n-    def length_penalty(self):\n-        if self.length_pen == \"wu\":\n-            return self.length_wu\n-        elif self.length_pen == \"avg\":\n-            return self.length_average\n-        else:\n-            return self.length_none\n-\n-    \"\"\"\n-    Below are all the different penalty terms implemented so far\n-    \"\"\"\n-\n-    def length_wu(self, beam, logprobs, alpha=0.0):\n-        \"\"\"\n-        NMT length re-ranking score from\n-        \"Google's Neural Machine Translation System\" :cite:`wu2016google`.\n-        \"\"\"\n-\n-        modifier = ((5 + len(beam.next_ys)) ** alpha) / ((5 + 1) ** alpha)\n-        return logprobs / modifier\n-\n-    def length_average(self, beam, logprobs, alpha=0.0):\n-        \"\"\"\n-        Returns the average probability of tokens in a sequence.\n-        \"\"\"\n-        return logprobs / len(beam.next_ys)\n-\n-    def length_none(self, beam, logprobs, alpha=0.0, beta=0.0):\n-        \"\"\"\n-        Returns unmodified scores.\n-        \"\"\"\n-        return logprobs\n-\n-\n-class Translator:\n-    \"\"\"\n-    Uses a model to translate a batch of sentences.\n-\n-    Args:\n-       model (:obj:`onmt.modules.NMTModel`):\n-          NMT model to use for translation\n-       fields (dict of Fields): data fields\n-       beam_size (int): size of beam to use\n-       n_best (int): number of translations produced\n-       max_length (int): maximum length output to produce\n-       global_scores (:obj:`GlobalScorer`):\n-         object to rescore final translations\n-       copy_attn (bool): use copy attention during translation\n-       beam_trace (bool): trace beam search for debugging\n-       logger(logging.Logger): logger.\n-    \"\"\"\n-\n-    def __init__(self, args, model, vocab, symbols, global_scorer=None, logger=None):\n-        self.logger = logger\n-\n-        self.args = args\n-        self.model = model\n-        self.generator = self.model.generator\n-        self.vocab = vocab\n-        self.symbols = symbols\n-        self.start_token = symbols[\"BOS\"]\n-        self.end_token = symbols[\"EOS\"]\n-\n-        self.global_scorer = global_scorer\n-        self.beam_size = args.beam_size\n-        self.min_length = args.min_length\n-        self.max_length = args.max_length\n-\n-    def translate(self, batch, step, attn_debug=False):\n-        \"\"\"Generates summaries from one batch of data.\"\"\"\n-        self.model.eval()\n-        with torch.no_grad():\n-            batch_data = self.translate_batch(batch)\n-            translations = self.from_batch(batch_data)\n-        return translations\n-\n-    def translate_batch(self, batch, fast=False):\n-        \"\"\"\n-        Translate a batch of sentences.\n-\n-        Mostly a wrapper around :obj:`Beam`.\n-\n-        Args:\n-           batch (:obj:`Batch`): a batch from a dataset object\n-           fast (bool): enables fast beam search (may not support all features)\n-        \"\"\"\n-        with torch.no_grad():\n-            return self._fast_translate_batch(batch, self.max_length, min_length=self.min_length)\n-\n-    # Where the beam search lives\n-    # I have no idea why it is being called from the method above\n-    def _fast_translate_batch(self, batch, max_length, min_length=0):\n-        \"\"\"Beam Search using the encoder inputs contained in `batch`.\"\"\"\n-\n-        # The batch object is funny\n-        # Instead of just looking at the size of the arguments we encapsulate\n-        # a size argument.\n-        # Where is it defined?\n-        beam_size = self.beam_size\n-        batch_size = batch.batch_size\n-        src = batch.src\n-        segs = batch.segs\n-        mask_src = batch.mask_src\n-\n-        src_features = self.model.bert(src, segs, mask_src)\n-        dec_states = self.model.decoder.init_decoder_state(src, src_features, with_cache=True)\n-        device = src_features.device\n-\n-        # Tile states and memory beam_size times.\n-        dec_states.map_batch_fn(lambda state, dim: tile(state, beam_size, dim=dim))\n-        src_features = tile(src_features, beam_size, dim=0)\n-        batch_offset = torch.arange(batch_size, dtype=torch.long, device=device)\n-        beam_offset = torch.arange(0, batch_size * beam_size, step=beam_size, dtype=torch.long, device=device)\n-        alive_seq = torch.full([batch_size * beam_size, 1], self.start_token, dtype=torch.long, device=device)\n-\n-        # Give full probability to the first beam on the first step.\n-        topk_log_probs = torch.tensor([0.0] + [float(\"-inf\")] * (beam_size - 1), device=device).repeat(batch_size)\n-\n-        # Structure that holds finished hypotheses.\n-        hypotheses = [[] for _ in range(batch_size)]  # noqa: F812\n-\n-        results = {}\n-        results[\"predictions\"] = [[] for _ in range(batch_size)]  # noqa: F812\n-        results[\"scores\"] = [[] for _ in range(batch_size)]  # noqa: F812\n-        results[\"gold_score\"] = [0] * batch_size\n-        results[\"batch\"] = batch\n-\n-        for step in range(max_length):\n-            decoder_input = alive_seq[:, -1].view(1, -1)\n-\n-            # Decoder forward.\n-            decoder_input = decoder_input.transpose(0, 1)\n-\n-            dec_out, dec_states = self.model.decoder(decoder_input, src_features, dec_states, step=step)\n-\n-            # Generator forward.\n-            log_probs = self.generator(dec_out.transpose(0, 1).squeeze(0))\n-            vocab_size = log_probs.size(-1)\n-\n-            if step < min_length:\n-                log_probs[:, self.end_token] = -1e20\n-\n-            # Multiply probs by the beam probability.\n-            log_probs += topk_log_probs.view(-1).unsqueeze(1)\n-\n-            alpha = self.global_scorer.alpha\n-            length_penalty = ((5.0 + (step + 1)) / 6.0) ** alpha\n-\n-            # Flatten probs into a list of possibilities.\n-            curr_scores = log_probs / length_penalty\n-\n-            if self.args.block_trigram:\n-                cur_len = alive_seq.size(1)\n-                if cur_len > 3:\n-                    for i in range(alive_seq.size(0)):\n-                        fail = False\n-                        words = [int(w) for w in alive_seq[i]]\n-                        words = [self.vocab.ids_to_tokens[w] for w in words]\n-                        words = \" \".join(words).replace(\" ##\", \"\").split()\n-                        if len(words) <= 3:\n-                            continue\n-                        trigrams = [(words[i - 1], words[i], words[i + 1]) for i in range(1, len(words) - 1)]\n-                        trigram = tuple(trigrams[-1])\n-                        if trigram in trigrams[:-1]:\n-                            fail = True\n-                        if fail:\n-                            curr_scores[i] = -10e20\n-\n-            curr_scores = curr_scores.reshape(-1, beam_size * vocab_size)\n-            topk_scores, topk_ids = curr_scores.topk(beam_size, dim=-1)\n-\n-            # Recover log probs.\n-            topk_log_probs = topk_scores * length_penalty\n-\n-            # Resolve beam origin and true word ids.\n-            topk_beam_index = topk_ids.div(vocab_size)\n-            topk_ids = topk_ids.fmod(vocab_size)\n-\n-            # Map beam_index to batch_index in the flat representation.\n-            batch_index = topk_beam_index + beam_offset[: topk_beam_index.size(0)].unsqueeze(1)\n-            select_indices = batch_index.view(-1)\n-\n-            # Append last prediction.\n-            alive_seq = torch.cat([alive_seq.index_select(0, select_indices), topk_ids.view(-1, 1)], -1)\n-\n-            is_finished = topk_ids.eq(self.end_token)\n-            if step + 1 == max_length:\n-                is_finished.fill_(1)\n-            # End condition is top beam is finished.\n-            end_condition = is_finished[:, 0].eq(1)\n-            # Save finished hypotheses.\n-            if is_finished.any():\n-                predictions = alive_seq.view(-1, beam_size, alive_seq.size(-1))\n-                for i in range(is_finished.size(0)):\n-                    b = batch_offset[i]\n-                    if end_condition[i]:\n-                        is_finished[i].fill_(1)\n-                    finished_hyp = is_finished[i].nonzero().view(-1)\n-                    # Store finished hypotheses for this batch.\n-                    for j in finished_hyp:\n-                        hypotheses[b].append((topk_scores[i, j], predictions[i, j, 1:]))\n-                    # If the batch reached the end, save the n_best hypotheses.\n-                    if end_condition[i]:\n-                        best_hyp = sorted(hypotheses[b], key=lambda x: x[0], reverse=True)\n-                        score, pred = best_hyp[0]\n-\n-                        results[\"scores\"][b].append(score)\n-                        results[\"predictions\"][b].append(pred)\n-                non_finished = end_condition.eq(0).nonzero().view(-1)\n-                # If all sentences are translated, no need to go further.\n-                if len(non_finished) == 0:\n-                    break\n-                # Remove finished batches for the next step.\n-                topk_log_probs = topk_log_probs.index_select(0, non_finished)\n-                batch_index = batch_index.index_select(0, non_finished)\n-                batch_offset = batch_offset.index_select(0, non_finished)\n-                alive_seq = predictions.index_select(0, non_finished).view(-1, alive_seq.size(-1))\n-            # Reorder states.\n-            select_indices = batch_index.view(-1)\n-            src_features = src_features.index_select(0, select_indices)\n-            dec_states.map_batch_fn(lambda state, dim: state.index_select(dim, select_indices))\n-\n-        return results\n-\n-    def from_batch(self, translation_batch):\n-        batch = translation_batch[\"batch\"]\n-        assert len(translation_batch[\"gold_score\"]) == len(translation_batch[\"predictions\"])\n-        batch_size = batch.batch_size\n-\n-        preds, _, _, tgt_str, src = (\n-            translation_batch[\"predictions\"],\n-            translation_batch[\"scores\"],\n-            translation_batch[\"gold_score\"],\n-            batch.tgt_str,\n-            batch.src,\n-        )\n-\n-        translations = []\n-        for b in range(batch_size):\n-            pred_sents = self.vocab.convert_ids_to_tokens([int(n) for n in preds[b][0]])\n-            pred_sents = \" \".join(pred_sents).replace(\" ##\", \"\")\n-            gold_sent = \" \".join(tgt_str[b].split())\n-            raw_src = [self.vocab.ids_to_tokens[int(t)] for t in src[b]][:500]\n-            raw_src = \" \".join(raw_src)\n-            translation = (pred_sents, gold_sent, raw_src)\n-            translations.append(translation)\n-\n-        return translations\n-\n-\n-def tile(x, count, dim=0):\n-    \"\"\"\n-    Tiles x on dimension dim count times.\n-    \"\"\"\n-    perm = list(range(len(x.size())))\n-    if dim != 0:\n-        perm[0], perm[dim] = perm[dim], perm[0]\n-        x = x.permute(perm).contiguous()\n-    out_size = list(x.size())\n-    out_size[0] *= count\n-    batch = x.size(0)\n-    x = x.view(batch, -1).transpose(0, 1).repeat(count, 1).transpose(0, 1).contiguous().view(*out_size)\n-    if dim != 0:\n-        x = x.permute(perm).contiguous()\n-    return x\n-\n-\n-#\n-# Optimizer for training. We keep this here in case we want to add\n-# a finetuning script.\n-#\n-\n-\n-class BertSumOptimizer:\n-    \"\"\"Specific optimizer for BertSum.\n-\n-    As described in [1], the authors fine-tune BertSum for abstractive\n-    summarization using two Adam Optimizers with different warm-up steps and\n-    learning rate. They also use a custom learning rate scheduler.\n-\n-    [1] Liu, Yang, and Mirella Lapata. \"Text summarization with pretrained encoders.\"\n-        arXiv preprint arXiv:1908.08345 (2019).\n-    \"\"\"\n-\n-    def __init__(self, model, lr, warmup_steps, beta_1=0.99, beta_2=0.999, eps=1e-8):\n-        self.encoder = model.encoder\n-        self.decoder = model.decoder\n-        self.lr = lr\n-        self.warmup_steps = warmup_steps\n-\n-        self.optimizers = {\n-            \"encoder\": torch.optim.Adam(\n-                model.encoder.parameters(),\n-                lr=lr[\"encoder\"],\n-                betas=(beta_1, beta_2),\n-                eps=eps,\n-            ),\n-            \"decoder\": torch.optim.Adam(\n-                model.decoder.parameters(),\n-                lr=lr[\"decoder\"],\n-                betas=(beta_1, beta_2),\n-                eps=eps,\n-            ),\n-        }\n-\n-        self._step = 0\n-        self.current_learning_rates = {}\n-\n-    def _update_rate(self, stack):\n-        return self.lr[stack] * min(self._step ** (-0.5), self._step * self.warmup_steps[stack] ** (-1.5))\n-\n-    def zero_grad(self):\n-        self.optimizer_decoder.zero_grad()\n-        self.optimizer_encoder.zero_grad()\n-\n-    def step(self):\n-        self._step += 1\n-        for stack, optimizer in self.optimizers.items():\n-            new_rate = self._update_rate(stack)\n-            for param_group in optimizer.param_groups:\n-                param_group[\"lr\"] = new_rate\n-            optimizer.step()\n-            self.current_learning_rates[stack] = new_rate"
        },
        {
            "sha": "bc2a3d6a163005ad3d4f2817a7ca3985c87a9f1b",
            "filename": "examples/research_projects/bertabs/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,5 +0,0 @@\n-transformers == 4.38.0\n-\n-# For ROUGE\n-nltk\n-py-rouge"
        },
        {
            "sha": "bc13de558999542de32ce09dd615d70dc88b0b18",
            "filename": "examples/research_projects/bertabs/run_summarization.py",
            "status": "removed",
            "additions": 0,
            "deletions": 347,
            "changes": 347,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Frun_summarization.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,347 +0,0 @@\n-#! /usr/bin/python3\n-import argparse\n-import logging\n-import os\n-import sys\n-from collections import namedtuple\n-\n-import torch\n-from modeling_bertabs import BertAbs, build_predictor\n-from torch.utils.data import DataLoader, SequentialSampler\n-from tqdm import tqdm\n-\n-from transformers import BertTokenizer\n-\n-from .utils_summarization import (\n-    CNNDMDataset,\n-    build_mask,\n-    compute_token_type_ids,\n-    encode_for_summarization,\n-    truncate_or_pad,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n-\n-\n-Batch = namedtuple(\"Batch\", [\"document_names\", \"batch_size\", \"src\", \"segs\", \"mask_src\", \"tgt_str\"])\n-\n-\n-def evaluate(args):\n-    tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", do_lower_case=True)\n-    model = BertAbs.from_pretrained(\"remi/bertabs-finetuned-extractive-abstractive-summarization\")\n-    model.to(args.device)\n-    model.eval()\n-\n-    symbols = {\n-        \"BOS\": tokenizer.vocab[\"[unused0]\"],\n-        \"EOS\": tokenizer.vocab[\"[unused1]\"],\n-        \"PAD\": tokenizer.vocab[\"[PAD]\"],\n-    }\n-\n-    if args.compute_rouge:\n-        reference_summaries = []\n-        generated_summaries = []\n-\n-        import nltk\n-        import rouge\n-\n-        nltk.download(\"punkt\")\n-        rouge_evaluator = rouge.Rouge(\n-            metrics=[\"rouge-n\", \"rouge-l\"],\n-            max_n=2,\n-            limit_length=True,\n-            length_limit=args.beam_size,\n-            length_limit_type=\"words\",\n-            apply_avg=True,\n-            apply_best=False,\n-            alpha=0.5,  # Default F1_score\n-            weight_factor=1.2,\n-            stemming=True,\n-        )\n-\n-    # these (unused) arguments are defined to keep the compatibility\n-    # with the legacy code and will be deleted in a next iteration.\n-    args.result_path = \"\"\n-    args.temp_dir = \"\"\n-\n-    data_iterator = build_data_iterator(args, tokenizer)\n-    predictor = build_predictor(args, tokenizer, symbols, model)\n-\n-    logger.info(\"***** Running evaluation *****\")\n-    logger.info(\"  Number examples = %d\", len(data_iterator.dataset))\n-    logger.info(\"  Batch size = %d\", args.batch_size)\n-    logger.info(\"\")\n-    logger.info(\"***** Beam Search parameters *****\")\n-    logger.info(\"  Beam size = %d\", args.beam_size)\n-    logger.info(\"  Minimum length = %d\", args.min_length)\n-    logger.info(\"  Maximum length = %d\", args.max_length)\n-    logger.info(\"  Alpha (length penalty) = %.2f\", args.alpha)\n-    logger.info(\"  Trigrams %s be blocked\", (\"will\" if args.block_trigram else \"will NOT\"))\n-\n-    for batch in tqdm(data_iterator):\n-        batch_data = predictor.translate_batch(batch)\n-        translations = predictor.from_batch(batch_data)\n-        summaries = [format_summary(t) for t in translations]\n-        save_summaries(summaries, args.summaries_output_dir, batch.document_names)\n-\n-        if args.compute_rouge:\n-            reference_summaries += batch.tgt_str\n-            generated_summaries += summaries\n-\n-    if args.compute_rouge:\n-        scores = rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n-        str_scores = format_rouge_scores(scores)\n-        save_rouge_scores(str_scores)\n-        print(str_scores)\n-\n-\n-def save_summaries(summaries, path, original_document_name):\n-    \"\"\"Write the summaries in fies that are prefixed by the original\n-    files' name with the `_summary` appended.\n-\n-    Attributes:\n-        original_document_names: List[string]\n-            Name of the document that was summarized.\n-        path: string\n-            Path were the summaries will be written\n-        summaries: List[string]\n-            The summaries that we produced.\n-    \"\"\"\n-    for summary, document_name in zip(summaries, original_document_name):\n-        # Prepare the summary file's name\n-        if \".\" in document_name:\n-            bare_document_name = \".\".join(document_name.split(\".\")[:-1])\n-            extension = document_name.split(\".\")[-1]\n-            name = bare_document_name + \"_summary.\" + extension\n-        else:\n-            name = document_name + \"_summary\"\n-\n-        file_path = os.path.join(path, name)\n-        with open(file_path, \"w\") as output:\n-            output.write(summary)\n-\n-\n-def format_summary(translation):\n-    \"\"\"Transforms the output of the `from_batch` function\n-    into nicely formatted summaries.\n-    \"\"\"\n-    raw_summary, _, _ = translation\n-    summary = (\n-        raw_summary.replace(\"[unused0]\", \"\")\n-        .replace(\"[unused3]\", \"\")\n-        .replace(\"[PAD]\", \"\")\n-        .replace(\"[unused1]\", \"\")\n-        .replace(r\" +\", \" \")\n-        .replace(\" [unused2] \", \". \")\n-        .replace(\"[unused2]\", \"\")\n-        .strip()\n-    )\n-\n-    return summary\n-\n-\n-def format_rouge_scores(scores):\n-    return \"\"\"\\n\n-****** ROUGE SCORES ******\n-\n-** ROUGE 1\n-F1        >> {:.3f}\n-Precision >> {:.3f}\n-Recall    >> {:.3f}\n-\n-** ROUGE 2\n-F1        >> {:.3f}\n-Precision >> {:.3f}\n-Recall    >> {:.3f}\n-\n-** ROUGE L\n-F1        >> {:.3f}\n-Precision >> {:.3f}\n-Recall    >> {:.3f}\"\"\".format(\n-        scores[\"rouge-1\"][\"f\"],\n-        scores[\"rouge-1\"][\"p\"],\n-        scores[\"rouge-1\"][\"r\"],\n-        scores[\"rouge-2\"][\"f\"],\n-        scores[\"rouge-2\"][\"p\"],\n-        scores[\"rouge-2\"][\"r\"],\n-        scores[\"rouge-l\"][\"f\"],\n-        scores[\"rouge-l\"][\"p\"],\n-        scores[\"rouge-l\"][\"r\"],\n-    )\n-\n-\n-def save_rouge_scores(str_scores):\n-    with open(\"rouge_scores.txt\", \"w\") as output:\n-        output.write(str_scores)\n-\n-\n-#\n-# LOAD the dataset\n-#\n-\n-\n-def build_data_iterator(args, tokenizer):\n-    dataset = load_and_cache_examples(args, tokenizer)\n-    sampler = SequentialSampler(dataset)\n-\n-    def collate_fn(data):\n-        return collate(data, tokenizer, block_size=512, device=args.device)\n-\n-    iterator = DataLoader(\n-        dataset,\n-        sampler=sampler,\n-        batch_size=args.batch_size,\n-        collate_fn=collate_fn,\n-    )\n-\n-    return iterator\n-\n-\n-def load_and_cache_examples(args, tokenizer):\n-    dataset = CNNDMDataset(args.documents_dir)\n-    return dataset\n-\n-\n-def collate(data, tokenizer, block_size, device):\n-    \"\"\"Collate formats the data passed to the data loader.\n-\n-    In particular we tokenize the data batch after batch to avoid keeping them\n-    all in memory. We output the data as a namedtuple to fit the original BertAbs's\n-    API.\n-    \"\"\"\n-    data = [x for x in data if not len(x[1]) == 0]  # remove empty_files\n-    names = [name for name, _, _ in data]\n-    summaries = [\" \".join(summary_list) for _, _, summary_list in data]\n-\n-    encoded_text = [encode_for_summarization(story, summary, tokenizer) for _, story, summary in data]\n-    encoded_stories = torch.tensor(\n-        [truncate_or_pad(story, block_size, tokenizer.pad_token_id) for story, _ in encoded_text]\n-    )\n-    encoder_token_type_ids = compute_token_type_ids(encoded_stories, tokenizer.cls_token_id)\n-    encoder_mask = build_mask(encoded_stories, tokenizer.pad_token_id)\n-\n-    batch = Batch(\n-        document_names=names,\n-        batch_size=len(encoded_stories),\n-        src=encoded_stories.to(device),\n-        segs=encoder_token_type_ids.to(device),\n-        mask_src=encoder_mask.to(device),\n-        tgt_str=summaries,\n-    )\n-\n-    return batch\n-\n-\n-def decode_summary(summary_tokens, tokenizer):\n-    \"\"\"Decode the summary and return it in a format\n-    suitable for evaluation.\n-    \"\"\"\n-    summary_tokens = summary_tokens.to(\"cpu\").numpy()\n-    summary = tokenizer.decode(summary_tokens)\n-    sentences = summary.split(\".\")\n-    sentences = [s + \".\" for s in sentences]\n-    return sentences\n-\n-\n-def main():\n-    \"\"\"The main function defines the interface with the users.\"\"\"\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\n-        \"--documents_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The folder where the documents to summarize are located.\",\n-    )\n-    parser.add_argument(\n-        \"--summaries_output_dir\",\n-        default=None,\n-        type=str,\n-        required=False,\n-        help=\"The folder in which the summaries should be written. Defaults to the folder where the documents are\",\n-    )\n-    parser.add_argument(\n-        \"--compute_rouge\",\n-        default=False,\n-        type=bool,\n-        required=False,\n-        help=\"Compute the ROUGE metrics during evaluation. Only available for the CNN/DailyMail dataset.\",\n-    )\n-    # EVALUATION options\n-    parser.add_argument(\n-        \"--no_cuda\",\n-        default=False,\n-        type=bool,\n-        help=\"Whether to force the execution on CPU.\",\n-    )\n-    parser.add_argument(\n-        \"--batch_size\",\n-        default=4,\n-        type=int,\n-        help=\"Batch size per GPU/CPU for training.\",\n-    )\n-    # BEAM SEARCH arguments\n-    parser.add_argument(\n-        \"--min_length\",\n-        default=50,\n-        type=int,\n-        help=\"Minimum number of tokens for the summaries.\",\n-    )\n-    parser.add_argument(\n-        \"--max_length\",\n-        default=200,\n-        type=int,\n-        help=\"Maixmum number of tokens for the summaries.\",\n-    )\n-    parser.add_argument(\n-        \"--beam_size\",\n-        default=5,\n-        type=int,\n-        help=\"The number of beams to start with for each example.\",\n-    )\n-    parser.add_argument(\n-        \"--alpha\",\n-        default=0.95,\n-        type=float,\n-        help=\"The value of alpha for the length penalty in the beam search.\",\n-    )\n-    parser.add_argument(\n-        \"--block_trigram\",\n-        default=True,\n-        type=bool,\n-        help=\"Whether to block the existence of repeating trigrams in the text generated by beam search.\",\n-    )\n-    args = parser.parse_args()\n-\n-    # Select device (distributed not available)\n-    args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-\n-    # Check the existence of directories\n-    if not args.summaries_output_dir:\n-        args.summaries_output_dir = args.documents_dir\n-\n-    if not documents_dir_is_valid(args.documents_dir):\n-        raise FileNotFoundError(\n-            \"We could not find the directory you specified for the documents to summarize, or it was empty. Please\"\n-            \" specify a valid path.\"\n-        )\n-    os.makedirs(args.summaries_output_dir, exist_ok=True)\n-\n-    evaluate(args)\n-\n-\n-def documents_dir_is_valid(path):\n-    if not os.path.exists(path):\n-        return False\n-\n-    file_list = os.listdir(path)\n-    if len(file_list) == 0:\n-        return False\n-\n-    return True\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "18120c9063edaf95a4896d11e84a22d1b51882dd",
            "filename": "examples/research_projects/bertabs/test_utils_summarization.py",
            "status": "removed",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Ftest_utils_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Ftest_utils_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Ftest_utils_summarization.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,98 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import unittest\n-\n-import numpy as np\n-import torch\n-\n-from .utils_summarization import build_mask, compute_token_type_ids, process_story, truncate_or_pad\n-\n-\n-class SummarizationDataProcessingTest(unittest.TestCase):\n-    def setUp(self):\n-        self.block_size = 10\n-\n-    def test_fit_to_block_sequence_too_small(self):\n-        \"\"\"Pad the sequence with 0 if the sequence is smaller than the block size.\"\"\"\n-        sequence = [1, 2, 3, 4]\n-        expected_output = [1, 2, 3, 4, 0, 0, 0, 0, 0, 0]\n-        self.assertEqual(truncate_or_pad(sequence, self.block_size, 0), expected_output)\n-\n-    def test_fit_to_block_sequence_fit_exactly(self):\n-        \"\"\"Do nothing if the sequence is the right size.\"\"\"\n-        sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n-        expected_output = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n-        self.assertEqual(truncate_or_pad(sequence, self.block_size, 0), expected_output)\n-\n-    def test_fit_to_block_sequence_too_big(self):\n-        \"\"\"Truncate the sequence if it is too long.\"\"\"\n-        sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n-        expected_output = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n-        self.assertEqual(truncate_or_pad(sequence, self.block_size, 0), expected_output)\n-\n-    def test_process_story_no_highlights(self):\n-        \"\"\"Processing a story with no highlights returns an empty list for the summary.\"\"\"\n-        raw_story = \"\"\"It was the year of Our Lord one thousand seven hundred and\n-        seventy-five.\\n\\nSpiritual revelations were conceded to England at that\n-        favoured period, as at this.\"\"\"\n-        _, summary_lines = process_story(raw_story)\n-        self.assertEqual(summary_lines, [])\n-\n-    def test_process_empty_story(self):\n-        \"\"\"An empty story returns an empty collection of lines.\"\"\"\n-        raw_story = \"\"\n-        story_lines, summary_lines = process_story(raw_story)\n-        self.assertEqual(story_lines, [])\n-        self.assertEqual(summary_lines, [])\n-\n-    def test_process_story_with_missing_period(self):\n-        raw_story = (\n-            \"It was the year of Our Lord one thousand seven hundred and \"\n-            \"seventy-five\\n\\nSpiritual revelations were conceded to England \"\n-            \"at that favoured period, as at this.\\n@highlight\\n\\nIt was the best of times\"\n-        )\n-        story_lines, summary_lines = process_story(raw_story)\n-\n-        expected_story_lines = [\n-            \"It was the year of Our Lord one thousand seven hundred and seventy-five.\",\n-            \"Spiritual revelations were conceded to England at that favoured period, as at this.\",\n-        ]\n-        self.assertEqual(expected_story_lines, story_lines)\n-\n-        expected_summary_lines = [\"It was the best of times.\"]\n-        self.assertEqual(expected_summary_lines, summary_lines)\n-\n-    def test_build_mask_no_padding(self):\n-        sequence = torch.tensor([1, 2, 3, 4])\n-        expected = torch.tensor([1, 1, 1, 1])\n-        np.testing.assert_array_equal(build_mask(sequence, 0).numpy(), expected.numpy())\n-\n-    def test_build_mask(self):\n-        sequence = torch.tensor([1, 2, 3, 4, 23, 23, 23])\n-        expected = torch.tensor([1, 1, 1, 1, 0, 0, 0])\n-        np.testing.assert_array_equal(build_mask(sequence, 23).numpy(), expected.numpy())\n-\n-    def test_build_mask_with_padding_equal_to_one(self):\n-        sequence = torch.tensor([8, 2, 3, 4, 1, 1, 1])\n-        expected = torch.tensor([1, 1, 1, 1, 0, 0, 0])\n-        np.testing.assert_array_equal(build_mask(sequence, 1).numpy(), expected.numpy())\n-\n-    def test_compute_token_type_ids(self):\n-        separator = 101\n-        batch = torch.tensor([[1, 2, 3, 4, 5, 6], [1, 2, 3, 101, 5, 6], [1, 101, 3, 4, 101, 6]])\n-        expected = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0], [1, 0, 0, 0, 1, 1]])\n-\n-        result = compute_token_type_ids(batch, separator)\n-        np.testing.assert_array_equal(result, expected)"
        },
        {
            "sha": "716365336bb5393dd496e8541a773b9a1951ca01",
            "filename": "examples/research_projects/bertabs/utils_summarization.py",
            "status": "removed",
            "additions": 0,
            "deletions": 167,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Futils_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertabs%2Futils_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Futils_summarization.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,167 +0,0 @@\n-import os\n-from collections import deque\n-\n-import torch\n-from torch.utils.data import Dataset\n-\n-\n-# ------------\n-# Data loading\n-# ------------\n-\n-\n-class CNNDMDataset(Dataset):\n-    \"\"\"Abstracts the dataset used to train seq2seq models.\n-\n-    The class will process the documents that are located in the specified\n-    folder. The preprocessing will work on any document that is reasonably\n-    formatted. On the CNN/DailyMail dataset it will extract both the story\n-    and the summary.\n-\n-    CNN/Daily News:\n-\n-    The CNN/Daily News raw datasets are downloaded from [1]. The stories are\n-    stored in different files; the summary appears at the end of the story as\n-    sentences that are prefixed by the special `@highlight` line. To process\n-    the data, untar both datasets in the same folder, and pass the path to this\n-    folder as the \"data_dir argument. The formatting code was inspired by [2].\n-\n-    [1] https://cs.nyu.edu/~kcho/\n-    [2] https://github.com/abisee/cnn-dailymail/\n-    \"\"\"\n-\n-    def __init__(self, path=\"\", prefix=\"train\"):\n-        \"\"\"We initialize the class by listing all the documents to summarize.\n-        Files are not read in memory due to the size of some datasets (like CNN/DailyMail).\n-        \"\"\"\n-        assert os.path.isdir(path)\n-\n-        self.documents = []\n-        story_filenames_list = os.listdir(path)\n-        for story_filename in story_filenames_list:\n-            if \"summary\" in story_filename:\n-                continue\n-            path_to_story = os.path.join(path, story_filename)\n-            if not os.path.isfile(path_to_story):\n-                continue\n-            self.documents.append(path_to_story)\n-\n-    def __len__(self):\n-        \"\"\"Returns the number of documents.\"\"\"\n-        return len(self.documents)\n-\n-    def __getitem__(self, idx):\n-        document_path = self.documents[idx]\n-        document_name = document_path.split(\"/\")[-1]\n-        with open(document_path, encoding=\"utf-8\") as source:\n-            raw_story = source.read()\n-            story_lines, summary_lines = process_story(raw_story)\n-        return document_name, story_lines, summary_lines\n-\n-\n-def process_story(raw_story):\n-    \"\"\"Extract the story and summary from a story file.\n-\n-    Arguments:\n-        raw_story (str): content of the story file as an utf-8 encoded string.\n-\n-    Raises:\n-        IndexError: If the story is empty or contains no highlights.\n-    \"\"\"\n-    nonempty_lines = list(filter(lambda x: len(x) != 0, [line.strip() for line in raw_story.split(\"\\n\")]))\n-\n-    # for some unknown reason some lines miss a period, add it\n-    nonempty_lines = [_add_missing_period(line) for line in nonempty_lines]\n-\n-    # gather article lines\n-    story_lines = []\n-    lines = deque(nonempty_lines)\n-    while True:\n-        try:\n-            element = lines.popleft()\n-            if element.startswith(\"@highlight\"):\n-                break\n-            story_lines.append(element)\n-        except IndexError:\n-            # if \"@highlight\" is absent from the file we pop\n-            # all elements until there is None, raising an exception.\n-            return story_lines, []\n-\n-    # gather summary lines\n-    summary_lines = list(filter(lambda t: not t.startswith(\"@highlight\"), lines))\n-\n-    return story_lines, summary_lines\n-\n-\n-def _add_missing_period(line):\n-    END_TOKENS = [\".\", \"!\", \"?\", \"...\", \"'\", \"`\", '\"', \"\\u2019\", \"\\u2019\", \")\"]\n-    if line.startswith(\"@highlight\"):\n-        return line\n-    if line[-1] in END_TOKENS:\n-        return line\n-    return line + \".\"\n-\n-\n-# --------------------------\n-# Encoding and preprocessing\n-# --------------------------\n-\n-\n-def truncate_or_pad(sequence, block_size, pad_token_id):\n-    \"\"\"Adapt the source and target sequences' lengths to the block size.\n-    If the sequence is shorter we append padding token to the right of the sequence.\n-    \"\"\"\n-    if len(sequence) > block_size:\n-        return sequence[:block_size]\n-    else:\n-        sequence.extend([pad_token_id] * (block_size - len(sequence)))\n-        return sequence\n-\n-\n-def build_mask(sequence, pad_token_id):\n-    \"\"\"Builds the mask. The attention mechanism will only attend to positions\n-    with value 1.\"\"\"\n-    mask = torch.ones_like(sequence)\n-    idx_pad_tokens = sequence == pad_token_id\n-    mask[idx_pad_tokens] = 0\n-    return mask\n-\n-\n-def encode_for_summarization(story_lines, summary_lines, tokenizer):\n-    \"\"\"Encode the story and summary lines, and join them\n-    as specified in [1] by using `[SEP] [CLS]` tokens to separate\n-    sentences.\n-    \"\"\"\n-    story_lines_token_ids = [tokenizer.encode(line) for line in story_lines]\n-    story_token_ids = [token for sentence in story_lines_token_ids for token in sentence]\n-    summary_lines_token_ids = [tokenizer.encode(line) for line in summary_lines]\n-    summary_token_ids = [token for sentence in summary_lines_token_ids for token in sentence]\n-\n-    return story_token_ids, summary_token_ids\n-\n-\n-def compute_token_type_ids(batch, separator_token_id):\n-    \"\"\"Segment embeddings as described in [1]\n-\n-    The values {0,1} were found in the repository [2].\n-\n-    Attributes:\n-        batch: torch.Tensor, size [batch_size, block_size]\n-            Batch of input.\n-        separator_token_id: int\n-            The value of the token that separates the segments.\n-\n-    [1] Liu, Yang, and Mirella Lapata. \"Text summarization with pretrained encoders.\"\n-        arXiv preprint arXiv:1908.08345 (2019).\n-    [2] https://github.com/nlpyang/PreSumm (/src/prepro/data_builder.py, commit fac1217)\n-    \"\"\"\n-    batch_embeddings = []\n-    for sequence in batch:\n-        sentence_num = -1\n-        embeddings = []\n-        for s in sequence:\n-            if s == separator_token_id:\n-                sentence_num += 1\n-            embeddings.append(sentence_num % 2)\n-        batch_embeddings.append(embeddings)\n-    return torch.tensor(batch_embeddings)"
        },
        {
            "sha": "99636a7fce1b8ebdd049db2bdeb2248c02791fce",
            "filename": "examples/research_projects/bertology/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertology%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertology%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertology%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1 +0,0 @@\n-transformers == 4.38.0"
        },
        {
            "sha": "35d096f164991553f39e1e70e2074e6aaa179f72",
            "filename": "examples/research_projects/bertology/run_bertology.py",
            "status": "removed",
            "additions": 0,
            "deletions": 453,
            "changes": 453,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertology%2Frun_bertology.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertology%2Frun_bertology.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertology%2Frun_bertology.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,453 +0,0 @@\n-#!/usr/bin/env python3\n-# Copyright 2018 CMU and The HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Bertology: this script shows how you can explore the internals of the models in the library to:\n-- compute the entropy of the head attentions\n-- compute the importance of each head\n-- prune (remove) the low importance head.\n-Some parts of this script are adapted from the code of Michel et al. (http://arxiv.org/abs/1905.10650)\n-which is available at https://github.com/pmichel31415/are-16-heads-really-better-than-1\n-\"\"\"\n-\n-import argparse\n-import logging\n-import os\n-from datetime import datetime\n-\n-import numpy as np\n-import torch\n-from torch import nn\n-from torch.utils.data import DataLoader, SequentialSampler, Subset\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoModelForSequenceClassification,\n-    AutoTokenizer,\n-    GlueDataset,\n-    default_data_collator,\n-    glue_compute_metrics,\n-    glue_output_modes,\n-    glue_processors,\n-    set_seed,\n-)\n-from transformers.trainer_utils import is_main_process\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def entropy(p):\n-    \"\"\"Compute the entropy of a probability distribution\"\"\"\n-    plogp = p * torch.log(p)\n-    plogp[p == 0] = 0\n-    return -plogp.sum(dim=-1)\n-\n-\n-def print_2d_tensor(tensor):\n-    \"\"\"Print a 2D tensor\"\"\"\n-    logger.info(\"lv, h >\\t\" + \"\\t\".join(f\"{x + 1}\" for x in range(len(tensor))))\n-    for row in range(len(tensor)):\n-        if tensor.dtype != torch.long:\n-            logger.info(f\"layer {row + 1}:\\t\" + \"\\t\".join(f\"{x:.5f}\" for x in tensor[row].cpu().data))\n-        else:\n-            logger.info(f\"layer {row + 1}:\\t\" + \"\\t\".join(f\"{x:d}\" for x in tensor[row].cpu().data))\n-\n-\n-def compute_heads_importance(\n-    args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False\n-):\n-    \"\"\"This method shows how to compute:\n-    - head attention entropy\n-    - head importance scores according to http://arxiv.org/abs/1905.10650\n-    \"\"\"\n-    # Prepare our tensors\n-    n_layers, n_heads = model.config.num_hidden_layers, model.config.num_attention_heads\n-    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n-    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n-\n-    if head_mask is None:\n-        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n-\n-    head_mask.requires_grad_(requires_grad=True)\n-    # If actually pruned attention multi-head, set head mask to None to avoid shape mismatch\n-    if actually_pruned:\n-        head_mask = None\n-\n-    preds = None\n-    labels = None\n-    tot_tokens = 0.0\n-\n-    for step, inputs in enumerate(tqdm(eval_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])):\n-        for k, v in inputs.items():\n-            inputs[k] = v.to(args.device)\n-\n-        # Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below)\n-        outputs = model(**inputs, head_mask=head_mask)\n-        loss, logits, all_attentions = (\n-            outputs[0],\n-            outputs[1],\n-            outputs[-1],\n-        )  # Loss and logits are the first, attention the last\n-        loss.backward()  # Backpropagate to populate the gradients in the head mask\n-\n-        if compute_entropy:\n-            for layer, attn in enumerate(all_attentions):\n-                masked_entropy = entropy(attn.detach()) * inputs[\"attention_mask\"].float().unsqueeze(1)\n-                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()\n-\n-        if compute_importance:\n-            head_importance += head_mask.grad.abs().detach()\n-\n-        # Also store our logits/labels if we want to compute metrics afterwards\n-        if preds is None:\n-            preds = logits.detach().cpu().numpy()\n-            labels = inputs[\"labels\"].detach().cpu().numpy()\n-        else:\n-            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n-            labels = np.append(labels, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n-\n-        tot_tokens += inputs[\"attention_mask\"].float().detach().sum().data\n-\n-    # Normalize\n-    attn_entropy /= tot_tokens\n-    head_importance /= tot_tokens\n-    # Layerwise importance normalization\n-    if not args.dont_normalize_importance_by_layer:\n-        exponent = 2\n-        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n-        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n-\n-    if not args.dont_normalize_global_importance:\n-        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n-\n-    # Print/save matrices\n-    np.save(os.path.join(args.output_dir, \"attn_entropy.npy\"), attn_entropy.detach().cpu().numpy())\n-    np.save(os.path.join(args.output_dir, \"head_importance.npy\"), head_importance.detach().cpu().numpy())\n-\n-    logger.info(\"Attention entropies\")\n-    print_2d_tensor(attn_entropy)\n-    logger.info(\"Head importance scores\")\n-    print_2d_tensor(head_importance)\n-    logger.info(\"Head ranked by importance scores\")\n-    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n-    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(\n-        head_importance.numel(), device=args.device\n-    )\n-    head_ranks = head_ranks.view_as(head_importance)\n-    print_2d_tensor(head_ranks)\n-\n-    return attn_entropy, head_importance, preds, labels\n-\n-\n-def mask_heads(args, model, eval_dataloader):\n-    \"\"\"This method shows how to mask head (set some heads to zero), to test the effect on the network,\n-    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n-    \"\"\"\n-    _, head_importance, preds, labels = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n-    preds = np.argmax(preds, axis=1) if args.output_mode == \"classification\" else np.squeeze(preds)\n-    original_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n-    logger.info(\"Pruning: original score: %f, threshold: %f\", original_score, original_score * args.masking_threshold)\n-\n-    new_head_mask = torch.ones_like(head_importance)\n-    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n-\n-    current_score = original_score\n-    while current_score >= original_score * args.masking_threshold:\n-        head_mask = new_head_mask.clone()  # save current head mask\n-        # heads from least important to most - keep only not-masked heads\n-        head_importance[head_mask == 0.0] = float(\"Inf\")\n-        current_heads_to_mask = head_importance.view(-1).sort()[1]\n-\n-        if len(current_heads_to_mask) <= num_to_mask:\n-            break\n-\n-        # mask heads\n-        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n-        logger.info(\"Heads to mask: %s\", str(current_heads_to_mask.tolist()))\n-        new_head_mask = new_head_mask.view(-1)\n-        new_head_mask[current_heads_to_mask] = 0.0\n-        new_head_mask = new_head_mask.view_as(head_mask)\n-        new_head_mask = new_head_mask.clone().detach()\n-        print_2d_tensor(new_head_mask)\n-\n-        # Compute metric and head importance again\n-        _, head_importance, preds, labels = compute_heads_importance(\n-            args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask\n-        )\n-        preds = np.argmax(preds, axis=1) if args.output_mode == \"classification\" else np.squeeze(preds)\n-        current_score = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n-        logger.info(\n-            \"Masking: current score: %f, remaining heads %d (%.1f percents)\",\n-            current_score,\n-            new_head_mask.sum(),\n-            new_head_mask.sum() / new_head_mask.numel() * 100,\n-        )\n-\n-    logger.info(\"Final head mask\")\n-    print_2d_tensor(head_mask)\n-    np.save(os.path.join(args.output_dir, \"head_mask.npy\"), head_mask.detach().cpu().numpy())\n-\n-    return head_mask\n-\n-\n-def prune_heads(args, model, eval_dataloader, head_mask):\n-    \"\"\"This method shows how to prune head (remove heads weights) based on\n-    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n-    \"\"\"\n-    # Try pruning and test time speedup\n-    # Pruning is like masking but we actually remove the masked weights\n-    before_time = datetime.now()\n-    _, _, preds, labels = compute_heads_importance(\n-        args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask\n-    )\n-    preds = np.argmax(preds, axis=1) if args.output_mode == \"classification\" else np.squeeze(preds)\n-    score_masking = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n-    original_time = datetime.now() - before_time\n-\n-    original_num_params = sum(p.numel() for p in model.parameters())\n-    heads_to_prune = {\n-        layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))\n-    }\n-\n-    assert sum(len(h) for h in heads_to_prune.values()) == (1 - head_mask.long()).sum().item()\n-    model.prune_heads(heads_to_prune)\n-    pruned_num_params = sum(p.numel() for p in model.parameters())\n-\n-    before_time = datetime.now()\n-    _, _, preds, labels = compute_heads_importance(\n-        args,\n-        model,\n-        eval_dataloader,\n-        compute_entropy=False,\n-        compute_importance=False,\n-        head_mask=None,\n-        actually_pruned=True,\n-    )\n-    preds = np.argmax(preds, axis=1) if args.output_mode == \"classification\" else np.squeeze(preds)\n-    score_pruning = glue_compute_metrics(args.task_name, preds, labels)[args.metric_name]\n-    new_time = datetime.now() - before_time\n-\n-    logger.info(\n-        \"Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)\",\n-        original_num_params,\n-        pruned_num_params,\n-        pruned_num_params / original_num_params * 100,\n-    )\n-    logger.info(\"Pruning: score with masking: %f score with pruning: %f\", score_masking, score_pruning)\n-    logger.info(\"Pruning: speed ratio (new timing / original timing): %f percents\", original_time / new_time * 100)\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-    )\n-    parser.add_argument(\n-        \"--task_name\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The name of the task to train selected in the list: \" + \", \".join(glue_processors.keys()),\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model predictions and checkpoints will be written.\",\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--config_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained config name or path if not the same as model_name_or_path\",\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name_or_path\",\n-    )\n-    parser.add_argument(\n-        \"--cache_dir\",\n-        default=None,\n-        type=str,\n-        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n-    )\n-    parser.add_argument(\n-        \"--data_subset\", type=int, default=-1, help=\"If > 0: limit the data to a subset of data_subset instances.\"\n-    )\n-    parser.add_argument(\n-        \"--overwrite_output_dir\", action=\"store_true\", help=\"Whether to overwrite data in output directory\"\n-    )\n-    parser.add_argument(\n-        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-    )\n-\n-    parser.add_argument(\n-        \"--dont_normalize_importance_by_layer\", action=\"store_true\", help=\"Don't normalize importance score by layers\"\n-    )\n-    parser.add_argument(\n-        \"--dont_normalize_global_importance\",\n-        action=\"store_true\",\n-        help=\"Don't normalize all importance scores between 0 and 1\",\n-    )\n-\n-    parser.add_argument(\n-        \"--try_masking\", action=\"store_true\", help=\"Whether to try to mask head until a threshold of accuracy.\"\n-    )\n-    parser.add_argument(\n-        \"--masking_threshold\",\n-        default=0.9,\n-        type=float,\n-        help=\"masking threshold in term of metrics (stop masking when metric < threshold * original metric value).\",\n-    )\n-    parser.add_argument(\n-        \"--masking_amount\", default=0.1, type=float, help=\"Amount to heads to masking at each masking step.\"\n-    )\n-    parser.add_argument(\"--metric_name\", default=\"acc\", type=str, help=\"Metric to use for head masking.\")\n-\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=128,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after WordPiece tokenization. \\n\"\n-            \"Sequences longer than this will be truncated, sequences shorter padded.\"\n-        ),\n-    )\n-    parser.add_argument(\"--batch_size\", default=1, type=int, help=\"Batch size.\")\n-\n-    parser.add_argument(\"--seed\", type=int, default=42)\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    args = parser.parse_args()\n-\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup devices and distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n-    else:\n-        torch.cuda.set_device(args.local_rank)\n-        args.device = torch.device(\"cuda\", args.local_rank)\n-        args.n_gpu = 1\n-        torch.distributed.init_process_group(backend=\"nccl\")  # Initializes the distributed backend\n-\n-    # Setup logging\n-    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n-    logger.info(\"device: {} n_gpu: {}, distributed: {}\".format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-\n-    # Set seeds\n-    set_seed(args.seed)\n-\n-    # Prepare GLUE task\n-    args.task_name = args.task_name.lower()\n-    if args.task_name not in glue_processors:\n-        raise ValueError(\"Task not found: %s\" % (args.task_name))\n-    processor = glue_processors[args.task_name]()\n-    args.output_mode = glue_output_modes[args.task_name]\n-    label_list = processor.get_labels()\n-    num_labels = len(label_list)\n-\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        args.config_name if args.config_name else args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=args.task_name,\n-        output_attentions=True,\n-        cache_dir=args.cache_dir,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n-        cache_dir=args.cache_dir,\n-    )\n-    model = AutoModelForSequenceClassification.from_pretrained(\n-        args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n-        config=config,\n-        cache_dir=args.cache_dir,\n-    )\n-\n-    # Distributed and parallel training\n-    model.to(args.device)\n-    if args.local_rank != -1:\n-        model = nn.parallel.DistributedDataParallel(\n-            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n-        )\n-    elif args.n_gpu > 1:\n-        model = nn.DataParallel(model)\n-\n-    # Print/save training arguments\n-    os.makedirs(args.output_dir, exist_ok=True)\n-    torch.save(args, os.path.join(args.output_dir, \"run_args.bin\"))\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Prepare dataset for the GLUE task\n-    eval_dataset = GlueDataset(args, tokenizer=tokenizer, mode=\"dev\")\n-    if args.data_subset > 0:\n-        eval_dataset = Subset(eval_dataset, list(range(min(args.data_subset, len(eval_dataset)))))\n-    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n-    eval_dataloader = DataLoader(\n-        eval_dataset, sampler=eval_sampler, batch_size=args.batch_size, collate_fn=default_data_collator\n-    )\n-\n-    # Compute head entropy and importance score\n-    compute_heads_importance(args, model, eval_dataloader)\n-\n-    # Try head masking (set heads to zero until the score goes under a threshole)\n-    # and head pruning (remove masked heads and see the effect on the network)\n-    if args.try_masking and args.masking_threshold > 0.0 and args.masking_threshold < 1.0:\n-        head_mask = mask_heads(args, model, eval_dataloader)\n-        prune_heads(args, model, eval_dataloader, head_mask)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "d227634c2bf787fc7feba327fa2b7ea2312931d7",
            "filename": "examples/research_projects/bertology/run_prune_gpt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 391,
            "changes": 391,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertology%2Frun_prune_gpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fbertology%2Frun_prune_gpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertology%2Frun_prune_gpt.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,391 +0,0 @@\n-#!/usr/bin/env python3\n-\"\"\"This script is adapted from the Bertology pruning code (https://github.com/huggingface/transformers/blob/783d7d2629e97c5f0c5f9ef01b8c66410275c204/examples/research_projects/bertology/run_bertology.py)\n-to prune GPT-like models. The author is @altsoph.\n-\"\"\"\n-\n-import argparse\n-import logging\n-import os\n-from datetime import datetime\n-\n-import numpy as np\n-import torch\n-from torch import nn\n-from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n-from tqdm import tqdm\n-\n-from transformers import GPT2LMHeadModel\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def save_model(model, dirpath):\n-    # save results\n-    if os.path.exists(dirpath):\n-        if os.path.exists(os.path.join(dirpath, \"config.json\")) and os.path.isfile(\n-            os.path.join(dirpath, \"config.json\")\n-        ):\n-            os.remove(os.path.join(dirpath, \"config.json\"))\n-        if os.path.exists(os.path.join(dirpath, \"pytorch_model.bin\")) and os.path.isfile(\n-            os.path.join(dirpath, \"pytorch_model.bin\")\n-        ):\n-            os.remove(os.path.join(dirpath, \"pytorch_model.bin\"))\n-    else:\n-        os.makedirs(dirpath)\n-    model.save_pretrained(dirpath)\n-\n-\n-def entropy(p, unlogit=False):\n-    \"\"\"Compute the entropy of a probability distribution\"\"\"\n-    exponent = 2\n-    if unlogit:\n-        p = torch.pow(p, exponent)\n-    plogp = p * torch.log(p)\n-    plogp[p == 0] = 0\n-    return -plogp.sum(dim=-1)\n-\n-\n-def print_2d_tensor(tensor):\n-    \"\"\"Print a 2D tensor\"\"\"\n-    logger.info(\"lv, h >\\t\" + \"\\t\".join(f\"{x + 1}\" for x in range(len(tensor))))\n-    for row in range(len(tensor)):\n-        if tensor.dtype != torch.long:\n-            logger.info(f\"layer {row + 1}:\\t\" + \"\\t\".join(f\"{x:.5f}\" for x in tensor[row].cpu().data))\n-        else:\n-            logger.info(f\"layer {row + 1}:\\t\" + \"\\t\".join(f\"{x:d}\" for x in tensor[row].cpu().data))\n-\n-\n-def compute_heads_importance(\n-    args, model, eval_dataloader, compute_entropy=True, compute_importance=True, head_mask=None, actually_pruned=False\n-):\n-    \"\"\"This method shows how to compute:\n-    - head attention entropy\n-    - head importance scores according to http://arxiv.org/abs/1905.10650\n-    \"\"\"\n-    # Prepare our tensors\n-    n_layers, n_heads = model.config.num_hidden_layers, model.config.num_attention_heads\n-    head_importance = torch.zeros(n_layers, n_heads).to(args.device)\n-    attn_entropy = torch.zeros(n_layers, n_heads).to(args.device)\n-\n-    if head_mask is None:\n-        head_mask = torch.ones(n_layers, n_heads).to(args.device)\n-\n-    head_mask.requires_grad_(requires_grad=True)\n-    # If actually pruned attention multi-head, set head mask to None to avoid shape mismatch\n-    if actually_pruned:\n-        head_mask = None\n-\n-    tot_tokens = 0.0\n-    total_loss = 0.0\n-    for step, inputs in enumerate(tqdm(eval_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])):\n-        inputs = tuple(t.to(args.device) for t in inputs)\n-        (input_ids,) = inputs\n-\n-        # Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below)\n-        outputs = model(input_ids, labels=input_ids, head_mask=head_mask)\n-        #  (loss), lm_logits, presents, (all hidden_states), (attentions)\n-        loss, _, all_attentions = (\n-            outputs[0],\n-            outputs[1],\n-            outputs[-1],\n-        )  # Loss and logits are the first, attention the last\n-        loss.backward()  # Backpropagate to populate the gradients in the head mask\n-        total_loss += loss.detach().cpu().numpy()\n-        if compute_entropy:\n-            for layer, attn in enumerate(all_attentions):\n-                masked_entropy = entropy(attn.detach(), True)\n-                attn_entropy[layer] += masked_entropy.sum(-1).sum(0).sum(0).detach()\n-\n-        if compute_importance:\n-            head_importance += head_mask.grad.abs().detach()\n-        tot_tokens += torch.ones_like(input_ids).float().detach().sum().data\n-\n-    # Normalize\n-    attn_entropy /= tot_tokens\n-    head_importance /= tot_tokens\n-    # Layerwise importance normalization\n-    if not args.dont_normalize_importance_by_layer:\n-        exponent = 2\n-        norm_by_layer = torch.pow(torch.pow(head_importance, exponent).sum(-1), 1 / exponent)\n-        head_importance /= norm_by_layer.unsqueeze(-1) + 1e-20\n-\n-    if not args.dont_normalize_global_importance:\n-        head_importance = (head_importance - head_importance.min()) / (head_importance.max() - head_importance.min())\n-\n-    # Print matrices\n-    if compute_entropy:\n-        logger.info(\"Attention entropies\")\n-        print_2d_tensor(attn_entropy)\n-    if compute_importance:\n-        logger.info(\"Head importance scores\")\n-        print_2d_tensor(head_importance)\n-    logger.info(\"Head ranked by importance scores\")\n-    head_ranks = torch.zeros(head_importance.numel(), dtype=torch.long, device=args.device)\n-    head_ranks[head_importance.view(-1).sort(descending=True)[1]] = torch.arange(\n-        head_importance.numel(), device=args.device\n-    )\n-    head_ranks = head_ranks.view_as(head_importance)\n-    print_2d_tensor(head_ranks)\n-    return attn_entropy, head_importance, total_loss\n-\n-\n-def mask_heads(args, model, eval_dataloader):\n-    \"\"\"This method shows how to mask head (set some heads to zero), to test the effect on the network,\n-    based on the head importance scores, as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n-    \"\"\"\n-    _, head_importance, loss = compute_heads_importance(args, model, eval_dataloader, compute_entropy=False)\n-    original_score = 1 / loss  # instead of downsteam score use the LM loss\n-    logger.info(\"Pruning: original score: %f, threshold: %f\", original_score, original_score * args.masking_threshold)\n-\n-    new_head_mask = torch.ones_like(head_importance)\n-    num_to_mask = max(1, int(new_head_mask.numel() * args.masking_amount))\n-\n-    current_score = original_score\n-    while current_score >= original_score * args.masking_threshold:\n-        head_mask = new_head_mask.clone().detach()  # save current head mask\n-        # heads from least important to most - keep only not-masked heads\n-        head_importance[head_mask == 0.0] = float(\"Inf\")\n-        current_heads_to_mask = head_importance.view(-1).sort()[1]\n-\n-        if len(current_heads_to_mask) <= num_to_mask:\n-            print(\"BREAK BY num_to_mask\")\n-            break\n-\n-        # mask heads\n-        current_heads_to_mask = current_heads_to_mask[:num_to_mask]\n-        logger.info(\"Heads to mask: %s\", str(current_heads_to_mask.tolist()))\n-        new_head_mask = new_head_mask.view(-1)\n-        new_head_mask[current_heads_to_mask] = 0.0\n-        new_head_mask = new_head_mask.view_as(head_mask)\n-        new_head_mask = new_head_mask.clone().detach()\n-        print_2d_tensor(new_head_mask)\n-\n-        # Compute metric and head importance again\n-        _, head_importance, loss = compute_heads_importance(\n-            args, model, eval_dataloader, compute_entropy=False, head_mask=new_head_mask\n-        )\n-        current_score = 1 / loss\n-        logger.info(\n-            \"Masking: current score: %f, remaining heads %d (%.1f percents)\",\n-            current_score,\n-            new_head_mask.sum(),\n-            new_head_mask.sum() / new_head_mask.numel() * 100,\n-        )\n-\n-    logger.info(\"Final head mask\")\n-    print_2d_tensor(head_mask)\n-    np.save(os.path.join(args.output_dir, \"head_mask.npy\"), head_mask.detach().cpu().numpy())\n-\n-    return head_mask\n-\n-\n-def prune_heads(args, model, eval_dataloader, head_mask):\n-    \"\"\"This method shows how to prune head (remove heads weights) based on\n-    the head importance scores as described in Michel et al. (http://arxiv.org/abs/1905.10650)\n-    \"\"\"\n-    # Try pruning and test time speedup\n-    # Pruning is like masking but we actually remove the masked weights\n-    before_time = datetime.now()\n-    _, _, loss = compute_heads_importance(\n-        args, model, eval_dataloader, compute_entropy=False, compute_importance=False, head_mask=head_mask\n-    )\n-    score_masking = 1 / loss\n-    original_time = datetime.now() - before_time\n-\n-    original_num_params = sum(p.numel() for p in model.parameters())\n-    heads_to_prune = {\n-        layer: (1 - head_mask[layer].long()).nonzero().squeeze().tolist() for layer in range(len(head_mask))\n-    }\n-\n-    for k, v in heads_to_prune.items():\n-        if isinstance(v, int):\n-            heads_to_prune[k] = [\n-                v,\n-            ]\n-\n-    assert sum(len(h) for h in heads_to_prune.values()) == (1 - head_mask.long()).sum().item()\n-    model.prune_heads(heads_to_prune)\n-    pruned_num_params = sum(p.numel() for p in model.parameters())\n-\n-    before_time = datetime.now()\n-    _, _, loss = compute_heads_importance(\n-        args,\n-        model,\n-        eval_dataloader,\n-        compute_entropy=False,\n-        compute_importance=False,\n-        head_mask=None,\n-        actually_pruned=True,\n-    )\n-\n-    score_pruning = 1 / loss\n-    new_time = datetime.now() - before_time\n-\n-    logger.info(\n-        \"Pruning: original num of params: %.2e, after pruning %.2e (%.1f percents)\",\n-        original_num_params,\n-        pruned_num_params,\n-        pruned_num_params / original_num_params * 100,\n-    )\n-    logger.info(\"Pruning: score with masking: %f score with pruning: %f\", score_masking, score_pruning)\n-    logger.info(\"Pruning: speed ratio (original timing / new timing): %f percents\", original_time / new_time * 100)\n-    save_model(model, args.output_dir)\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    # Required parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model predictions and checkpoints will be written.\",\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--config_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained config name or path if not the same as model_name_or_path\",\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name_or_path\",\n-    )\n-    parser.add_argument(\n-        \"--cache_dir\",\n-        default=None,\n-        type=str,\n-        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n-    )\n-    parser.add_argument(\n-        \"--data_subset\", type=int, default=-1, help=\"If > 0: limit the data to a subset of data_subset instances.\"\n-    )\n-    parser.add_argument(\n-        \"--overwrite_output_dir\", action=\"store_true\", help=\"Whether to overwrite data in output directory\"\n-    )\n-    parser.add_argument(\n-        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-    )\n-\n-    parser.add_argument(\n-        \"--dont_normalize_importance_by_layer\", action=\"store_true\", help=\"Don't normalize importance score by layers\"\n-    )\n-    parser.add_argument(\n-        \"--dont_normalize_global_importance\",\n-        action=\"store_true\",\n-        help=\"Don't normalize all importance scores between 0 and 1\",\n-    )\n-\n-    parser.add_argument(\n-        \"--try_masking\", action=\"store_true\", help=\"Whether to try to mask head until a threshold of accuracy.\"\n-    )\n-    parser.add_argument(\n-        \"--masking_threshold\",\n-        default=0.9,\n-        type=float,\n-        help=\"masking threshold in term of metrics (stop masking when metric < threshold * original metric value).\",\n-    )\n-    parser.add_argument(\n-        \"--masking_amount\", default=0.1, type=float, help=\"Amount to heads to masking at each masking step.\"\n-    )\n-    parser.add_argument(\"--metric_name\", default=\"acc\", type=str, help=\"Metric to use for head masking.\")\n-\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=128,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after WordPiece tokenization. \\n\"\n-            \"Sequences longer than this will be truncated, sequences shorter padded.\"\n-        ),\n-    )\n-    parser.add_argument(\"--batch_size\", default=1, type=int, help=\"Batch size.\")\n-\n-    parser.add_argument(\"--seed\", type=int, default=42)\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    args = parser.parse_args()\n-\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup devices and distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n-    else:\n-        torch.cuda.set_device(args.local_rank)\n-        args.device = torch.device(\"cuda\", args.local_rank)\n-        args.n_gpu = 1\n-        torch.distributed.init_process_group(backend=\"nccl\")  # Initializes the distributed backend\n-\n-    # Setup logging\n-    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n-    logger.info(\"device: {} n_gpu: {}, distributed: {}\".format(args.device, args.n_gpu, bool(args.local_rank != -1)))\n-\n-    model = GPT2LMHeadModel.from_pretrained(args.model_name_or_path)\n-\n-    # Distributed and parallel training\n-    model.to(args.device)\n-    if args.local_rank != -1:\n-        model = nn.parallel.DistributedDataParallel(\n-            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n-        )\n-    elif args.n_gpu > 1:\n-        model = nn.DataParallel(model)\n-\n-    # Print/save training arguments\n-    os.makedirs(args.output_dir, exist_ok=True)\n-    torch.save(args, os.path.join(args.output_dir, \"run_args.bin\"))\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Prepare dataset\n-    numpy_data = np.concatenate(\n-        [\n-            np.loadtxt(args.data_dir, dtype=np.int64),\n-        ]\n-    )\n-    train_tensor_dataset = (torch.from_numpy(numpy_data),)\n-    train_data = TensorDataset(*train_tensor_dataset)\n-    train_sampler = RandomSampler(train_data)\n-    eval_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.batch_size)\n-\n-    # Compute head entropy and importance score\n-    compute_heads_importance(args, model, eval_dataloader)\n-\n-    # Try head masking (set heads to zero until the score goes under a threshole)\n-    # and head pruning (remove masked heads and see the effect on the network)\n-    if args.try_masking and args.masking_threshold > 0.0 and args.masking_threshold < 1.0:\n-        head_mask = mask_heads(args, model, eval_dataloader)\n-        prune_heads(args, model, eval_dataloader, head_mask)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "f0af3d144f781af190e3bad0f6067b8232023d34",
            "filename": "examples/research_projects/codeparrot/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 316,
            "changes": 316,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,316 +0,0 @@\n-# CodeParrot ğŸ¦œ\n-<p align=\"center\">\n-    <img src=\"https://huggingface.co/datasets/lvwerra/repo-images/raw/main/code-highlighting-streamlit.png\" alt=\"drawing\" width=\"350\"/>\n-</p>\n-\n-## What is this about?\n-This is an open-source effort to train and evaluate code generation models. CodeParrot ğŸ¦œ is a GPT-2 model trained from scratch on Python code. The highlights of this project are:\n-- initialize and train a GPT-2 language model from scratch for code generation\n-- train a custom tokenizer adapted for Python code\n-- clean and deduplicate a large (>100GB) dataset with `datasets`\n-- train with `accelerate` on multiple GPUs using data parallelism and mixed precision\n-- continuously push checkpoints to the hub with `huggingface_hub`\n-- stream the dataset with `datasets` during training to avoid disk bottlenecks\n-- apply the `code_eval` metric in `datasets` to evaluate on [OpenAI's _HumanEval_ benchmark](https://huggingface.co/datasets/openai_humaneval)\n-- showcase examples for downstream tasks with code models in [examples](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot/examples) folder:\n-    - Algorithmic complexity prediction\n-    - Code generation from english text\n-    - Code explanation\n-    \n-## Installation\n-To install the dependencies simply run the following command:\n-```bash\n-pip install -r requirements.txt\n-```\n-\n-To reproduce the results you can follow the scripts in the following sections. Note that we don't always show all possible arguments to the scripts. To get the full list of arguments with descriptions you can run the following command on any script:\n-\n-```bash\n-python scripts/some_script.py --help\n-```\n-\n-Before you run any of the scripts make sure you are logged in and can push to the hub:\n-\n-```bash\n-huggingface-cli login\n-```\n-\n-Additionally, sure you have git-lfs installed. You can find instructions for how to install it [here](https://git-lfs.github.com/).\n-\n-## Dataset\n-The source of the dataset is the GitHub dump available on Google's [BigQuery](https://cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code). The database was queried for all Python files with less than 1MB in size resulting in a 180GB dataset with over 20M files. The dataset is available on the Hugging Face Hub [here](https://huggingface.co/datasets/transformersbook/codeparrot).\n-\n-### Preprocessing\n-The raw dataset contains many duplicates. We deduplicated and filtered the dataset using the heuristics proposed in OpenAI's Codex [paper](https://arxiv.org/abs/2107.03374) and some new ones:\n-\n-- exact deduplication using each file's hash after having removed whistespaces.\n-- near deduplication using MinHash and Jaccard similarity. MinHash with a Jaccard threshold (default=0.85) is first used to create duplicate clusters. Then these clusters are then reduced to unique files based on the exact Jaccard similarity. See `deduplicate_dataset` in `minhash_deduplication.py` for a detailed description.\n-- filtering files with max line length > 1000\n-- filtering files with mean line length > 100\n-- fraction of alphanumeric characters < 0.25\n-- containing the word \"auto-generated\" or similar in the first 5 lines\n-- filtering with a probability of 0.7 of files with a mention of \"test file\" or \"configuration file\" or similar in the first 5 lines\n-- filtering with a probability of 0.7 of files with high occurrence of the keywords \"test \" or \"config\" \n-- filtering with a probability of 0.7  of files without a mention of the keywords `def` , `for`, `while`  and `class`\n-- filtering files that use the assignment operator `=` less than 5 times \n-- filtering files with ratio between number of characters and number of tokens after tokenization < 1.5 (the average ratio is 3.6)\n-\n-The script to process the full dataset can be found in `scripts/preprocessing.py`. Executing the script on 16 vCPUs takes roughly 3h and removes 70% of the original dataset. The cleaned [train](https://huggingface.co/datasets/codeparrot/codeparrot-clean-train-v2) and [validation](https://huggingface.co/datasets/codeparrot/codeparrot-clean-valid-v2) splits are also available on the Hub if you want to skip this step or use the data for another project.\n-\n-To execute the preprocessing run the following command:\n-```bash\n-python scripts/preprocessing.py \\\n---dataset_name transformersbook/codeparrot \\\n---output_dir codeparrot-clean\n-```\n-During preprocessing the dataset is downloaded and stored locally as well as caches of the computations. Make sure you have more than 500GB free disk space to execute it.\n-\n-### Pretokenization\n-The tokenization of the data might be slow during the training especially for small models. We provide code to pretokenize the data beforehand in `scripts/pretokenizing.py`, but this step is optional. The dataset is downloaded and stored locally and the tokenized data is pushed to the hub. The tokenized clean [train](https://huggingface.co/datasets/codeparrot/tokenized-codeparrot-train) and [validation](https://huggingface.co/datasets/codeparrot/tokenized-codeparrot-valid) datasets are available if you want to use them directly.\n-\n-To execute the pretokenization, for the clean train data for instance, run the following command:\n-```bash\n-python scripts/pretokenizing.py \\\n---dataset_name codeparrot/codeparrot-clean-train \\\n---tokenized_data_repo tokenized-codeparrot-train\n-```\n-\n-## Tokenizer\n-Before training a new model for code we create a new tokenizer that is efficient at code tokenization. To train the tokenizer you can run the following command: \n-```bash\n-python scripts/bpe_training.py \\\n-    --base_tokenizer openai-community/gpt2 \\\n-    --dataset_name codeparrot/codeparrot-clean-train\n-```\n-\n-_Note:_ We originally trained the tokenizer on the unprocessed train split of the dataset `transformersbook/codeparrot-train`.\n-\n-## Training\n-The models are randomly initialized and trained from scratch. To initialize a new model you can run:\n-\n-```bash\n-python scripts/initialize_model.py \\\n---config_name openai-community/gpt2-large \\\n---tokenizer_name codeparrot/codeparrot \\\n---model_name codeparrot \\\n---push_to_hub True\n-```\n-This will initialize a new model with the architecture and configuration of `openai-community/gpt2-large` and use the tokenizer to appropriately size the input embeddings. Finally, the initilaized model is pushed the hub.\n-\n-We can either pass the name of a text dataset or a pretokenized dataset which speeds up training a bit.\n-Now that the tokenizer and model are also ready we can start training the model. The main training script is built with `accelerate` to scale across a wide range of platforms and infrastructure scales. We train two models with [110M](https://huggingface.co/codeparrot/codeparrot-small/) and [1.5B](https://huggingface.co/codeparrot/codeparrot/) parameters for 25-30B tokens on a 16xA100 (40GB) machine which takes 1 day and 1 week, respectively.\n-\n-First you need to configure `accelerate` and login to Weights & Biases:\n-\n-```bash\n-accelerate config\n-wandb login\n-```\n-\n-Note that during the `accelerate` configuration we enabled FP16. Then to train the large model you can run\n-\n-```bash\n-accelerate launch scripts/codeparrot_training.py\n-```\n-\n-If you want to train the small model you need to make some modifications:\n-\n-```bash\n-accelerate launch scripts/codeparrot_training.py \\\n---model_ckpt codeparrot/codeparrot-small \\\n---train_batch_size 12 \\\n---valid_batch_size 12 \\\n---learning_rate 5e-4 \\\n---num_warmup_steps 2000 \\\n---gradient_accumulation 1 \\\n---gradient_checkpointing False \\\n---max_train_steps 150000 \\\n---save_checkpoint_steps 15000\n-```\n-\n-Recall that you can see the full set of possible options with descriptions (for all scripts) by running:\n-\n-```bash\n-python scripts/codeparrot_training.py --help\n-```\n-\n-Instead of streaming the dataset from the hub you can also stream it from disk. This can be helpful for long training runs where the connection can be interrupted sometimes. To stream locally you simply need to clone the datasets and replace the dataset name with their path. In this example we store the data in a folder called `data`: \n-\n-```bash\n-git lfs install\n-mkdir data\n-git -C \"./data\" clone https://huggingface.co/datasets/codeparrot/codeparrot-clean-train\n-git -C \"./data\" clone https://huggingface.co/datasets/codeparrot/codeparrot-clean-valid\n-```\n-\n-And then pass the paths to the datasets when we run the training script:\n-\n-```bash\n-accelerate launch scripts/codeparrot_training.py \\\n---model_ckpt codeparrot/codeparrot-small \\\n---dataset_name_train ./data/codeparrot-clean-train \\\n---dataset_name_valid ./data/codeparrot-clean-valid \\\n---train_batch_size 12 \\\n---valid_batch_size 12 \\\n---learning_rate 5e-4 \\\n---num_warmup_steps 2000 \\\n---gradient_accumulation 1 \\\n---gradient_checkpointing False \\\n---max_train_steps 150000 \\\n---save_checkpoint_steps 15000\n-```\n-\n-## Evaluation\n-For evaluating the language modeling loss on the validation set or any other dataset you can use the following command:\n-```bash\n-python scripts/validation_loss.py \\\n---model_ckpt codeparrot/codeparrot \\\n---dataset_name codeparrot/codeparrot-clean-valid\n-```\n-In addition we evaluate the model on OpenAI's _HumanEval_ benchmark. You can run the evaluation with the following command:\n-\n-```bash\n-accelerate launch  scripts/human_eval.py --model_ckpt codeparrot/codeparrot \\\n---do_sample True \\\n---temperature 0.2 \\\n---top_p 0.95 \\\n---n_samples=200 \\\n---HF_ALLOW_CODE_EVAL=\"0\"\n-```\n-\n-The results as well as reference values are shown in the following table:\n-\n-| Model | pass@1 | pass@10 | pass@100|\n-|-------|--------|---------|---------|\n-|CodeParrot ğŸ¦œ (110M) | 3.80% | 6.57% | 12.78% |\n-|CodeParrot ğŸ¦œ (1.5B) | 3.99% | 8.69% | 17.88% |\n-|||||\n-|Codex (25M)| 3.21% | 7.1% |\t12.89%|\n-|Codex (85M)| 8.22%\t| 12.81% | 22.40% |\n-|Codex (300M)| 13.17%| 20.37% | 36.27% |\n-|Codex (12B)| 28.81%| 46.81% | 72.31% |\n-|||||\n-|GPT-neo (125M)| 0.75% | 1.88% | 2.97% |\n-|GPT-neo (1.5B)| 4.79% | 7.47% | 16.30% |\n-|GPT-neo (2.7B)| 6.41% | 11.27% | 21.37% |\n-|GPT-J (6B)| 11.62% | 15.74% | 27.74% |\n-\n-The numbers were obtained by sampling with `T = [0.2, 0.6, 0.8]` and picking the best value for each metric. Both CodeParrot ğŸ¦œ models are still underfitted and longer training would likely improve the performance.\n-\n-## Demo\n-Give the model a shot yourself! There are three demos to interact with CodeParrot ğŸ¦œ:\n-- [Code generation](https://huggingface.co/spaces/codeparrot/codeparrot-generation)\n-- [Code highlighting](https://huggingface.co/spaces/codeparrot/codeparrot-highlighting)\n-- [Comparison to other code models](https://huggingface.co/spaces/codeparrot/loubnabnl/code-generation-models)\n-\n-## Training with Megatron\n-[Megatron](https://github.com/NVIDIA/Megatron-LM) is a framework developed by NVIDIA for training large transformer models. While the CodeParrot code is easy to follow and modify to your needs the Megatron framework lets you train models faster. Below we explain how to use it.\n-\n-### Setup\n-You can pull an NVIDIA PyTorch Container that comes with all the required installations from [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch). See [documentation](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html) for more details:\n-\n-With the following Docker command you can run the container (`xx.xx` denotes your Docker version), and clone [Megatron repository](https://github.com/NVIDIA/Megatron-LM) into it:\n-```bash\n-docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:xx.xx-py3\n-git clone https://github.com/NVIDIA/Megatron-LM\n-```\n-\n-You also need to add the vocabulary file and merges table of the tokenizer that you trained on code into the container. You can also find these files in [vocab.json](https://huggingface.co/codeparrot/codeparrot/raw/main/vocab.json) and [merges.txt](https://huggingface.co/codeparrot/codeparrot/raw/main/merges.txt).\n-```bash\n-sudo docker cp vocab.json CONTAINER_ID:/workspace/Megatron-LM\n-sudo docker cp merges.txt CONTAINER_ID:/workspace/Megatron-LM\n-```\n-\n-### Data preprocessing\n-The training data requires preprocessing. First, you need to convert it into a loose json format, with one json containing a text sample per line. In python this can be done this way:\n-```python\n-from datasets import load_dataset\n-\n-train_data = load_dataset('codeparrot/codeparrot-clean-train', split='train')\n-train_data.to_json(\"codeparrot_data.json\", lines=True)  \n-```\n-\n-The data is then tokenized, shuffled and processed into a binary format for training using the following command:\n-```bash\n-pip install nltk\n-cd Megatron-LM\n-python tools/preprocess_data.py \\\n-       --input codeparrot_data.json \\\n-       --output-prefix codeparrot \\\n-       --vocab vocab.json \\\n-       --dataset-impl mmap \\\n-       --tokenizer-type GPT2BPETokenizer \\\n-       --merge-file merges.txt \\\n-       --json-keys content \\\n-       --workers 32 \\\n-       --chunk-size 25 \\\n-       --append-eod\n-```\n-This outputs two files `codeparrot_content_document.idx` and `codeparrot_content_document.bin` which are used in the training.\n-\n-### Training\n-You can configure the model architecture and training parameters as shown below, or put it in a bash script that you will run. This runs on 8 GPUs the 110M parameter CodeParrot pretraining, with the same settings as before. Note that the data is partitioned by default into a 969:30:1 ratio for training/validation/test sets.\n-```bash\n-GPUS_PER_NODE=8\n-MASTER_ADDR=localhost\n-MASTER_PORT=6001\n-NNODES=1\n-NODE_RANK=0\n-WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES))\n-DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n-CHECKPOINT_PATH=/workspace/Megatron-LM/experiments/codeparrot-small\n-VOCAB_FILE=vocab.json\n-MERGE_FILE=merges.txt\n-DATA_PATH=codeparrot_content_document\n-GPT_ARGS=\"--num-layers 12\n---hidden-size 768\n---num-attention-heads 12\n---seq-length 1024\n---max-position-embeddings 1024\n---micro-batch-size 12\n---global-batch-size 192\n---lr 0.0005\n---train-iters 150000\n---lr-decay-iters 150000\n---lr-decay-style cosine\n---lr-warmup-iters 2000\n---weight-decay .1\n---adam-beta2 .999\n---fp16\n---log-interval 10\n---save-interval 2000\n---eval-interval 200\n---eval-iters 10\n-\"\n-TENSORBOARD_ARGS=\"--tensorboard-dir experiments/tensorboard\"\n-python3 -m torch.distributed.launch $DISTRIBUTED_ARGS \\\n-        pretrain_gpt.py \\\n-        --tensor-model-parallel-size 1 \\\n-        --pipeline-model-parallel-size 1 \\\n-        $GPT_ARGS \\\n-        --vocab-file $VOCAB_FILE \\\n-        --merge-file $MERGE_FILE \\\n-        --save $CHECKPOINT_PATH \\\n-        --load $CHECKPOINT_PATH \\\n-        --data-path $DATA_PATH \\\n-        $TENSORBOARD_ARGS\n-```\n-The training takes almost 12 hours in this setting.\n-\n-### Convert model to `transformers`\n-After training we want to use the model in `transformers` e.g. to evaluate it on HumanEval. You can convert it to `transformers` following [this](https://huggingface.co/nvidia/megatron-gpt2-345m) tutorial. For instance, after the training is finished you can copy the weights of the last iteration 150k and convert the `model_optim_rng.pt` file to a `pytorch_model.bin` file that is supported by `transformers`.\n-\n-```bash\n-mkdir -p nvidia/megatron-codeparrot-small\n-sudo docker cp CONTAINER_ID:/workspace/Megatron-LM/experiments/codeparrot-small/iter_0150000/mp_rank_00/model_optim_rng.pt nvidia/megatron-codeparrot-small\n-git clone https://github.com/huggingface/transformers.git\n-git clone https://github.com/NVIDIA/Megatron-LM.git\n-export PYTHONPATH=Megatron-LM\n-python transformers/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py nvidia/megatron-codeparrot-small/model_optim_rng.pt\n-```\n-Be careful, you will need to replace the generated vocabulary file and merges table after the conversion, with the original ones if you plan to load the tokenizer from there.\n-\n-## Further Resources\n-A detailed description of the project can be found in the chapter \"Training Transformers from Scratch\" in the upcoming O'Reilly book [Natural Language Processing with Transformers](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/).\n-\n-This example was provided by [Leandro von Werra](www.github.com/lvwerra)."
        },
        {
            "sha": "c1980262d8275b9e0a9abe3d5a00d3c955ddb73d",
            "filename": "examples/research_projects/codeparrot/examples/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,58 +0,0 @@\n-# Examples\n-In this folder we showcase some examples to use code models for downstream tasks.\n-\n-## Complexity prediction\n-In this task we want to predict the complexity of Java programs in [CodeComplex](https://huggingface.co/datasets/codeparrot/codecomplex) dataset. Using Hugging Face `trainer`, we finetuned [multilingual CodeParrot](https://huggingface.co/codeparrot/codeparrot-small-multi) and [UniXcoder](https://huggingface.co/microsoft/unixcoder-base-nine) on it, and we used the latter to build this Java complexity prediction [space](https://huggingface.co/spaces/codeparrot/code-complexity-predictor) on Hugging Face hub.\n-\n-To fine-tune a model on this dataset you can use the following commands:\n-\n-```python\n-python train_complexity_predictor.py \\\n-    --model_ckpt microsoft/unixcoder-base-nine \\\n-    --num_epochs 60 \\\n-    --num_warmup_steps 10 \\\n-    --batch_size 8 \\\n-    --learning_rate 5e-4 \n-```\n-\n-## Code generation: text to python\n-In this task we want to train a model to generate code from english text. We finetuned Codeparrot-small on [github-jupyter-text-to-code](https://huggingface.co/datasets/codeparrot/github-jupyter-text-to-code), a dataset where the samples are a succession of docstrings and their Python code, originally extracted from Jupyter notebooks parsed in this [dataset](https://huggingface.co/datasets/codeparrot/github-jupyter-parsed).\n-\n-To fine-tune a model on this dataset we use the same [script](https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/codeparrot_training.py) as the pretraining of codeparrot:\n-\n-```python\n-accelerate launch scripts/codeparrot_training.py \\\n-    --model_ckpt codeparrot/codeparrot-small \\\n-    --dataset_name_train codeparrot/github-jupyter-text-to-code \\\n-    --dataset_name_valid codeparrot/github-jupyter-text-to-code \\\n-    --train_batch_size 12 \\\n-    --valid_batch_size 12 \\\n-    --learning_rate 5e-4 \\\n-    --num_warmup_steps 100 \\\n-    --gradient_accumulation 1 \\\n-    --gradient_checkpointing False \\\n-    --max_train_steps 3000 \\\n-    --save_checkpoint_steps 200 \\\n-    --save_dir jupyter-text-to-python\n-```\n-\n-## Code explanation: python to text\n-In this task we want to train a model to explain python code. We finetuned Codeparrot-small on [github-jupyter-code-to-text](https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text), a dataset where the samples are a succession of Python code and its explanation as a docstring, we just inverted the order of text and code pairs in github-jupyter-code-to-text dataset and added the delimiters \"Explanation:\" and \"End of explanation\" inside the doctrings.\n-\n-To fine-tune a model on this dataset we use the same [script](https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/codeparrot_training.py) as the pretraining of codeparrot:\n-\n-```python\n-accelerate launch scripts/codeparrot_training.py \\\n-    --model_ckpt codeparrot/codeparrot-small \\\n-    --dataset_name_train codeparrot/github-jupyter-code-to-text \\\n-    --dataset_name_valid codeparrot/github-jupyter-code-to-text \\\n-    --train_batch_size 12 \\\n-    --valid_batch_size 12 \\\n-    --learning_rate 5e-4 \\\n-    --num_warmup_steps 100 \\\n-    --gradient_accumulation 1 \\\n-    --gradient_checkpointing False \\\n-    --max_train_steps 3000 \\\n-    --save_checkpoint_steps 200 \\\n-    --save_dir jupyter-python-to-text\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "c5e21ab98192e25c6d0efb4e610737943805c522",
            "filename": "examples/research_projects/codeparrot/examples/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,5 +0,0 @@\n-datasets==2.3.2\n-transformers==4.48.0\n-wandb==0.13.1\n-evaluate==0.2.2\n-scikit-learn==1.5.0\n\\ No newline at end of file"
        },
        {
            "sha": "de06b988db634c0112e15dd2baf0d689d3ad2256",
            "filename": "examples/research_projects/codeparrot/examples/train_complexity_predictor.py",
            "status": "removed",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2Ftrain_complexity_predictor.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2Ftrain_complexity_predictor.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fexamples%2Ftrain_complexity_predictor.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,132 +0,0 @@\n-import argparse\n-from copy import deepcopy\n-\n-import numpy as np\n-from datasets import ClassLabel, DatasetDict, load_dataset\n-from evaluate import load\n-\n-from transformers import (\n-    AutoModelForSequenceClassification,\n-    AutoTokenizer,\n-    DataCollatorWithPadding,\n-    Trainer,\n-    TrainerCallback,\n-    TrainingArguments,\n-    set_seed,\n-)\n-\n-\n-def get_args():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--model_ckpt\", type=str, default=\"microsoft/unixcoder-base-nine\")\n-    parser.add_argument(\"--num_epochs\", type=int, default=5)\n-    parser.add_argument(\"--batch_size\", type=int, default=6)\n-    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n-    parser.add_argument(\"--freeze\", type=bool, default=True)\n-    parser.add_argument(\"--learning_rate\", type=float, default=5e-4)\n-    parser.add_argument(\"--seed\", type=int, default=0)\n-    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n-    parser.add_argument(\"--num_warmup_steps\", type=int, default=10)\n-    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n-    parser.add_argument(\"--output_dir\", type=str, default=\"./results\")\n-    return parser.parse_args()\n-\n-\n-metric = load(\"accuracy\")\n-\n-\n-def compute_metrics(eval_pred):\n-    predictions, labels = eval_pred\n-    predictions = np.argmax(predictions, axis=1)\n-    return metric.compute(predictions=predictions, references=labels)\n-\n-\n-class CustomCallback(TrainerCallback):\n-    def __init__(self, trainer) -> None:\n-        super().__init__()\n-        self._trainer = trainer\n-\n-    def on_epoch_end(self, args, state, control, **kwargs):\n-        if control.should_evaluate:\n-            control_copy = deepcopy(control)\n-            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n-            return control_copy\n-\n-\n-def main():\n-    args = get_args()\n-    set_seed(args.seed)\n-\n-    dataset = load_dataset(\"codeparrot/codecomplex\", split=\"train\")\n-    train_test = dataset.train_test_split(test_size=0.2)\n-    test_validation = train_test[\"test\"].train_test_split(test_size=0.5)\n-    train_test_validation = DatasetDict(\n-        {\n-            \"train\": train_test[\"train\"],\n-            \"test\": test_validation[\"train\"],\n-            \"valid\": test_validation[\"test\"],\n-        }\n-    )\n-\n-    print(\"Loading tokenizer and model\")\n-    tokenizer = AutoTokenizer.from_pretrained(args.model_ckpt)\n-    tokenizer.pad_token = tokenizer.eos_token\n-    model = AutoModelForSequenceClassification.from_pretrained(args.model_ckpt, num_labels=7)\n-    model.config.pad_token_id = model.config.eos_token_id\n-\n-    if args.freeze:\n-        for param in model.roberta.parameters():\n-            param.requires_grad = False\n-\n-    labels = ClassLabel(num_classes=7, names=list(set(train_test_validation[\"train\"][\"complexity\"])))\n-\n-    def tokenize(example):\n-        inputs = tokenizer(example[\"src\"], truncation=True, max_length=1024)\n-        label = labels.str2int(example[\"complexity\"])\n-        return {\n-            \"input_ids\": inputs[\"input_ids\"],\n-            \"attention_mask\": inputs[\"attention_mask\"],\n-            \"label\": label,\n-        }\n-\n-    tokenized_datasets = train_test_validation.map(\n-        tokenize,\n-        batched=True,\n-        remove_columns=train_test_validation[\"train\"].column_names,\n-    )\n-    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n-\n-    training_args = TrainingArguments(\n-        output_dir=args.output_dir,\n-        learning_rate=args.learning_rate,\n-        lr_scheduler_type=args.lr_scheduler_type,\n-        eval_strategy=\"epoch\",\n-        save_strategy=\"epoch\",\n-        logging_strategy=\"epoch\",\n-        per_device_train_batch_size=args.batch_size,\n-        per_device_eval_batch_size=args.batch_size,\n-        num_train_epochs=args.num_epochs,\n-        gradient_accumulation_steps=args.gradient_accumulation_steps,\n-        weight_decay=0.01,\n-        metric_for_best_model=\"accuracy\",\n-        run_name=\"complexity-java\",\n-        report_to=\"wandb\",\n-    )\n-\n-    trainer = Trainer(\n-        model=model,\n-        args=training_args,\n-        train_dataset=tokenized_datasets[\"train\"],\n-        eval_dataset=tokenized_datasets[\"valid\"],\n-        tokenizer=tokenizer,\n-        data_collator=data_collator,\n-        compute_metrics=compute_metrics,\n-    )\n-\n-    print(\"Training...\")\n-    trainer.add_callback(CustomCallback(trainer))\n-    trainer.train()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "ee4fc0691b06a69a411a741c4004b2555be30f99",
            "filename": "examples/research_projects/codeparrot/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,9 +0,0 @@\n-transformers==4.38.0\n-datasets==1.16.0\n-wandb==0.12.0\n-tensorboard==2.6.0\n-torch==2.2.0\n-huggingface-hub==0.1.0\n-git+https://github.com/huggingface/accelerate.git@3c45b6f760ad8745be9ebc9bbb26f5b04dea4abe\n-datasketch==1.5.7\n-dpu_utils\n\\ No newline at end of file"
        },
        {
            "sha": "1540319b3daf65726400baf7681fb73afe41aaff",
            "filename": "examples/research_projects/codeparrot/scripts/arguments.py",
            "status": "removed",
            "additions": 0,
            "deletions": 220,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Farguments.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Farguments.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Farguments.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,220 +0,0 @@\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    \"\"\"\n-    Configuration for training model.\n-    \"\"\"\n-\n-    model_ckpt: Optional[str] = field(\n-        default=\"codeparrot/codeparrot\", metadata={\"help\": \"Model name or path of model to be trained.\"}\n-    )\n-    save_dir: Optional[str] = field(\n-        default=\"./\", metadata={\"help\": \"Save dir where model repo is cloned and models updates are saved to.\"}\n-    )\n-    dataset_name_train: Optional[str] = field(\n-        default=\"codeparrot/codeparrot-clean-train\", metadata={\"help\": \"Name or path of training dataset.\"}\n-    )\n-    dataset_name_valid: Optional[str] = field(\n-        default=\"codeparrot/codeparrot-clean-valid\", metadata={\"help\": \"Name or path of validation dataset.\"}\n-    )\n-    train_batch_size: Optional[int] = field(default=2, metadata={\"help\": \"Batch size for training.\"})\n-    valid_batch_size: Optional[int] = field(default=2, metadata={\"help\": \"Batch size for evaluation.\"})\n-    weight_decay: Optional[float] = field(default=0.1, metadata={\"help\": \"Value of weight decay.\"})\n-    shuffle_buffer: Optional[int] = field(\n-        default=10000, metadata={\"help\": \"Size of buffer used to shuffle streaming dataset.\"}\n-    )\n-    learning_rate: Optional[float] = field(default=2e-4, metadata={\"help\": \"Learning rate fo training.\"})\n-    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"Learning rate.\"})\n-    num_warmup_steps: Optional[int] = field(\n-        default=750, metadata={\"help\": \"Number of warmup steps in the learning rate schedule.\"}\n-    )\n-    gradient_accumulation_steps: Optional[int] = field(\n-        default=16, metadata={\"help\": \"Number of gradient accumulation steps.\"}\n-    )\n-    gradient_checkpointing: Optional[bool] = field(\n-        default=True, metadata={\"help\": \"Use gradient checkpointing to reduce memory footprint.\"}\n-    )\n-    max_train_steps: Optional[int] = field(default=50000, metadata={\"help\": \"Maximum number of training steps.\"})\n-    max_eval_steps: Optional[int] = field(\n-        default=-1, metadata={\"help\": \"Maximum number of evaluation steps. If -1 the full dataset is evaluated.\"}\n-    )\n-    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"Sequence lengths used for training.\"})\n-    seed: Optional[int] = field(default=1, metadata={\"help\": \"Training seed.\"})\n-    save_checkpoint_steps: Optional[int] = field(\n-        default=1024,\n-        metadata={\"help\": \"Interval to save checkpoints. Measured as number of forward passes not training steps.\"},\n-    )\n-    resume_from_checkpoint: Optional[str] = field(\n-        default=None, metadata={\"help\": \"States path if the training should continue from a checkpoint folder.\"}\n-    )\n-    tokenized: Optional[bool] = field(default=False, metadata={\"help\": \"If True the data is pretokenized.\"})\n-\n-\n-@dataclass\n-class EvaluationArguments:\n-    \"\"\"\n-    Configuration for evaluating model.\n-    \"\"\"\n-\n-    model_ckpt: Optional[str] = field(\n-        default=\"codeparrot/codeparrot\", metadata={\"help\": \"Model name or path of model to be evaluated.\"}\n-    )\n-    dataset_name: Optional[str] = field(\n-        default=\"codeparrot/codeparrot-clean-valid\", metadata={\"help\": \"Name or path of validation dataset.\"}\n-    )\n-    batch_size: Optional[int] = field(default=2, metadata={\"help\": \"Batch size used for evaluation.\"})\n-    max_eval_steps: Optional[int] = field(\n-        default=-1, metadata={\"help\": \"Maximum number of evaluation steps. If -1 the full dataset is evaluated.\"}\n-    )\n-    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"Length of sequences to be evaluated.\"})\n-    seed: Optional[int] = field(default=1, metadata={\"help\": \"Random seed used for evaluation.\"})\n-\n-\n-@dataclass\n-class HumanEvalArguments:\n-    \"\"\"\n-    Configuration for running evaluation on HumanEval dataset.\n-    \"\"\"\n-\n-    model_ckpt: Optional[str] = field(\n-        default=\"codeparrot/codeparrot\", metadata={\"help\": \"Model name or path of model to be evaluated.\"}\n-    )\n-    num_workers: Optional[int] = field(default=None, metadata={\"help\": \"Number of workers used for code evaluation.\"})\n-    num_tasks: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of human-eval tasks to run. If not included all tasks are evaluated.\"},\n-    )\n-    do_sample: Optional[bool] = field(\n-        default=True, metadata={\"help\": \"Sample from the language model's output distribution.\"}\n-    )\n-    temperature: Optional[float] = field(default=0.2, metadata={\"help\": \"Sampling temperature used for generation.\"})\n-    max_new_tokens: Optional[int] = field(default=256, metadata={\"help\": \"Maximum number of newly generated tokens.\"})\n-    top_k: Optional[int] = field(default=0, metadata={\"help\": \"Top-k parameter used for generation.\"})\n-    top_p: Optional[float] = field(default=0.95, metadata={\"help\": \"Top-p parameter used for nucleus sampling.\"})\n-    batch_size: Optional[int] = field(default=10, metadata={\"help\": \"Number of generations to run in parallel.\"})\n-    n_samples: Optional[int] = field(\n-        default=200, metadata={\"help\": \"Number of completions to generate for each sample.\"}\n-    )\n-    seed: Optional[int] = field(default=1, metadata={\"help\": \"Random seed used for evaluation.\"})\n-    output_file: Optional[str] = field(\n-        default=\"eval_results.json\", metadata={\"help\": \"Random seed used for evaluation.\"}\n-    )\n-    HF_ALLOW_CODE_EVAL: Optional[str] = field(\n-        default=\"0\", metadata={\"help\": \"Allow `code_eval` to execute Python code on machine\"}\n-    )\n-    device_int: Optional[int] = field(\n-        default=-1,\n-        metadata={\n-            \"help\": (\n-                \"Determine which device to run the `text-generation` Pipeline on. -1 is CPU and any zero or positive\"\n-                \" number corresponds to which GPU device id to run on.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class PreprocessingArguments:\n-    \"\"\"\n-    Configuration for preprocessing data.\n-    \"\"\"\n-\n-    num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"The number of CPU cores to use for parallel preprocessing. Default uses the maximum available.\"\n-        },\n-    )\n-    dataset_name: Optional[str] = field(\n-        default=\"transformersbook/codeparrot\", metadata={\"help\": \"Folder or name of dataset to process.\"}\n-    )\n-    output_dir: Optional[str] = field(\n-        default=\"codeparrot-clean\", metadata={\"help\": \"Folder to save processed dataset.\"}\n-    )\n-    samples_per_file: Optional[int] = field(\n-        default=100_000, metadata={\"help\": \"Number of files to save per JSON output file.\"}\n-    )\n-    text_column: Optional[str] = field(default=\"content\", metadata={\"help\": \"Column containing text data to process.\"})\n-    line_max: Optional[float] = field(\n-        default=1000, metadata={\"help\": \"Maximum line length in file, otherwise file is filtered.\"}\n-    )\n-    line_mean: Optional[float] = field(\n-        default=100, metadata={\"help\": \"Maximum mean line length in file, otherwise file is filtered.\"}\n-    )\n-    alpha_frac: Optional[float] = field(\n-        default=0.25, metadata={\"help\": \"Maximum fraction of non-alphanumeric characters, otherwise file is filtered.\"}\n-    )\n-    min_token_ratio: Optional[float] = field(\n-        default=1.5, metadata={\"help\": \"Minimum character token ratio for the file, otherwise file is filtered.\"}\n-    )\n-    filter_proba: Optional[float] = field(\n-        default=0.7, metadata={\"help\": \"Probability for filtering config, test and uncommon files.\"}\n-    )\n-    tokenizer: Optional[str] = field(\n-        default=\"codeparrot/codeparrot\",\n-        metadata={\"help\": \"Name or path to the tokenizer.\"},\n-    )\n-    near_deduplication: Optional[bool] = field(\n-        default=False, metadata={\"help\": \"If True, near-duplicate samples are removed.\"}\n-    )\n-    jaccard_threshold: Optional[float] = field(\n-        default=0.85, metadata={\"help\": \"Jaccard threshold for near-duplicate samples.\"}\n-    )\n-\n-\n-@dataclass\n-class TokenizerTrainingArguments:\n-    \"\"\"\n-    Configuration for tokenizer training.\n-    \"\"\"\n-\n-    base_tokenizer: Optional[str] = field(\n-        default=\"openai-community/gpt2\", metadata={\"help\": \"Base tokenizer to build new tokenizer from.\"}\n-    )\n-    dataset_name: Optional[str] = field(\n-        default=\"transformersbook/codeparrot-train\", metadata={\"help\": \"Dataset to train tokenizer on.\"}\n-    )\n-    text_column: Optional[str] = field(default=\"content\", metadata={\"help\": \"Column containing text data to process.\"})\n-    vocab_size: Optional[int] = field(default=200_000, metadata={\"help\": \"Number of examples to train tokenizer on.\"})\n-    n_examples: Optional[int] = field(\n-        default=32768, metadata={\"help\": \"Number of examples to train the tokenizer on.\"}\n-    )\n-    tokenizer_name: Optional[str] = field(default=\"codeparrot\", metadata={\"help\": \"Name of new tokenizer.\"})\n-    push_to_hub: Optional[bool] = field(default=True, metadata={\"help\": \"Push saved tokenizer to the hub.\"})\n-\n-\n-@dataclass\n-class PretokenizationArguments:\n-    \"\"\"\n-    Configuration for data pretokenization.\n-    \"\"\"\n-\n-    tokenizer_dir: Optional[str] = field(\n-        default=\"codeparrot/codeparrot\", metadata={\"help\": \"Name or path to the tokenizer.\"}\n-    )\n-    dataset_name: Optional[str] = field(\n-        default=\"codeparrot/codeparrot-clean-train\", metadata={\"help\": \"Name or path to the dataset to pretokenize.\"}\n-    )\n-    tokenized_data_repo: Optional[str] = field(\n-        default=\"tokenized-codeparrot-train\", metadata={\"help\": \"Repo name of the pretokenized data.\"}\n-    )\n-    num_workers: Optional[int] = field(default=None, metadata={\"help\": \"Number of workers used for code evaluation.\"})\n-\n-\n-@dataclass\n-class InitializationArguments:\n-    \"\"\"\n-    Configuration for initializing new model.\n-    \"\"\"\n-\n-    config_name: Optional[str] = field(\n-        default=\"openai-community/gpt2-large\", metadata={\"help\": \"Configuration to use for model initialization.\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=\"codeparrot/codeparrot\", metadata={\"help\": \"Tokenizer attached to model.\"}\n-    )\n-    model_name: Optional[str] = field(default=\"codeparrot\", metadata={\"help\": \"Name of the created model.\"})\n-    push_to_hub: Optional[bool] = field(default=True, metadata={\"help\": \"Push saved tokenizer to the hub.\"})"
        },
        {
            "sha": "1cbeb4b4ee3240d7f49f661bd1947fa8a252a5a6",
            "filename": "examples/research_projects/codeparrot/scripts/bpe_training.py",
            "status": "removed",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fbpe_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fbpe_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fbpe_training.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,32 +0,0 @@\n-from arguments import TokenizerTrainingArguments\n-from datasets import load_dataset\n-from tqdm import tqdm\n-\n-from transformers import AutoTokenizer, HfArgumentParser\n-from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n-\n-\n-# Iterator for Training\n-def batch_iterator(batch_size=10):\n-    for _ in tqdm(range(0, args.n_examples, batch_size)):\n-        yield [next(iter_dataset)[args.text_column] for _ in range(batch_size)]\n-\n-\n-# Configuration\n-parser = HfArgumentParser(TokenizerTrainingArguments)\n-args = parser.parse_args()\n-\n-# Base tokenizer\n-tokenizer = AutoTokenizer.from_pretrained(args.base_tokenizer)\n-base_vocab = list(bytes_to_unicode().values())\n-\n-# Load dataset\n-dataset = load_dataset(args.dataset_name, split=\"train\", streaming=True)\n-iter_dataset = iter(dataset)\n-\n-\n-# Training and saving\n-new_tokenizer = tokenizer.train_new_from_iterator(\n-    batch_iterator(), vocab_size=args.vocab_size, initial_alphabet=base_vocab\n-)\n-new_tokenizer.save_pretrained(args.tokenizer_name, push_to_hub=args.push_to_hub)"
        },
        {
            "sha": "549627d6ca7355506bcdfd6088b4dbec2dcf7ca8",
            "filename": "examples/research_projects/codeparrot/scripts/codeparrot_training.py",
            "status": "removed",
            "additions": 0,
            "deletions": 328,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fcodeparrot_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fcodeparrot_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fcodeparrot_training.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,328 +0,0 @@\n-import logging\n-import os\n-import time\n-from argparse import Namespace\n-from pathlib import Path\n-\n-import datasets\n-import torch\n-from accelerate import Accelerator, DistributedType\n-from accelerate.utils import ProjectConfiguration\n-from arguments import TrainingArguments\n-from datasets import load_dataset\n-from huggingface_hub import Repository\n-from torch.optim import AdamW\n-from torch.utils.data import IterableDataset\n-from torch.utils.data.dataloader import DataLoader\n-from torch.utils.data.datapipes.iter.combinatorics import ShufflerIterDataPipe\n-\n-import transformers\n-from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, get_scheduler, set_seed\n-\n-\n-class ConstantLengthDataset(IterableDataset):\n-    \"\"\"\n-    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n-        Args:\n-            tokenizer (Tokenizer): The processor used for processing the data.\n-            dataset (dataset.Dataset): Dataset with text files.\n-            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n-            seq_length (int): Length of token sequences to return.\n-            num_of_sequences (int): Number of token sequences to keep in buffer.\n-            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n-            tokenized (bool): If true we use a pretokenized dataset.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        tokenizer,\n-        dataset,\n-        infinite=False,\n-        seq_length=1024,\n-        num_of_sequences=1024,\n-        chars_per_token=3.6,\n-        tokenized=False,\n-    ):\n-        self.tokenizer = tokenizer\n-        self.concat_token_id = tokenizer.bos_token_id\n-        self.dataset = dataset\n-        self.seq_length = seq_length\n-        self.epoch = 0\n-        self.infinite = infinite\n-        self.current_size = 0\n-        self.tokenized = tokenized\n-\n-        if self.tokenized:\n-            self.max_buffer_size = seq_length * num_of_sequences\n-            self.content_field = \"input_ids\"\n-        else:\n-            self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n-            self.content_field = \"content\"\n-\n-    def __iter__(self):\n-        iterator = iter(self.dataset)\n-        more_examples = True\n-        while more_examples:\n-            buffer, buffer_len = [], 0\n-            while True:\n-                if buffer_len >= self.max_buffer_size:\n-                    break\n-                try:\n-                    buffer.append(next(iterator)[self.content_field])\n-                    buffer_len += len(buffer[-1])\n-                except StopIteration:\n-                    if self.infinite:\n-                        iterator = iter(self.dataset)\n-                        self.epoch += 1\n-                        logger.info(f\"Dataset epoch: {self.epoch}\")\n-                    else:\n-                        more_examples = False\n-                        break\n-            if self.tokenized:\n-                tokenized_inputs = buffer\n-            else:\n-                tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n-            all_token_ids = []\n-            for tokenized_input in tokenized_inputs:\n-                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n-            for i in range(0, len(all_token_ids), self.seq_length):\n-                input_ids = all_token_ids[i : i + self.seq_length]\n-                if len(input_ids) == self.seq_length:\n-                    self.current_size += 1\n-                    yield torch.tensor(input_ids)\n-\n-    def shuffle(self, buffer_size=1000):\n-        return ShufflerIterDataPipe(self, buffer_size=buffer_size)\n-\n-\n-def setup_logging(args):\n-    project_name = args.model_ckpt.split(\"/\")[-1]\n-    logger = logging.getLogger(__name__)\n-    log_dir = Path(args.save_dir) / \"log/\"\n-    log_dir.mkdir(exist_ok=True)\n-    filename = f\"debug_{accelerator.process_index}.log\"\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-        handlers=[logging.FileHandler(log_dir / filename), logging.StreamHandler()],\n-    )\n-    if accelerator.is_main_process:  # we only want to setup logging once\n-        accelerator.init_trackers(project_name, vars(args))\n-        run_name = accelerator.trackers[0].run.name\n-        logger.setLevel(logging.INFO)\n-        datasets.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        run_name = \"\"\n-        logger.setLevel(logging.ERROR)\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-    return logger, run_name\n-\n-\n-def create_dataloaders(args):\n-    ds_kwargs = {\"streaming\": True}\n-    train_data = load_dataset(args.dataset_name_train, split=\"train\", **ds_kwargs)\n-    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n-    valid_data = load_dataset(args.dataset_name_valid, split=\"train\", **ds_kwargs)\n-    train_dataset = ConstantLengthDataset(\n-        tokenizer, train_data, infinite=True, seq_length=args.seq_length, tokenized=args.tokenized\n-    )\n-    valid_dataset = ConstantLengthDataset(\n-        tokenizer, valid_data, infinite=False, seq_length=args.seq_length, tokenized=args.tokenized\n-    )\n-    train_dataset = train_dataset.shuffle(buffer_size=args.shuffle_buffer)\n-    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n-    eval_dataloader = DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n-    return train_dataloader, eval_dataloader\n-\n-\n-def get_grouped_params(model, args, no_decay=[\"bias\", \"ln_1.weight\", \"ln_2.weight\", \"ln_f.weight\"]):\n-    params_with_wd, params_without_wd = [], []\n-    for n, p in model.named_parameters():\n-        if any(nd in n for nd in no_decay):\n-            params_without_wd.append(p)\n-        else:\n-            params_with_wd.append(p)\n-    return [\n-        {\"params\": params_with_wd, \"weight_decay\": args.weight_decay},\n-        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n-    ]\n-\n-\n-def log_metrics(step, metrics):\n-    logger.info(f\"Step {step}: {metrics}\")\n-    if accelerator.is_main_process:\n-        accelerator.log(metrics, step)\n-\n-\n-def compute_tflops(elapsed_time, accelerator, args):\n-    # TFLOPs formula (from Equation 3 in Section 5.1 of https://arxiv.org/pdf/2104.04473.pdf).\n-    config_model = accelerator.unwrap_model(model).config\n-    checkpoint_factor = 4 if args.gradient_checkpointing else 3\n-    batch_size = args.train_batch_size * accelerator.state.num_processes * args.gradient_accumulation_steps\n-    factor = 24 * checkpoint_factor * batch_size * args.seq_length * config_model.n_layer * (config_model.n_embd**2)\n-    flops_per_iteration = factor * (\n-        1.0\n-        + (args.seq_length / (6.0 * config_model.n_embd))\n-        + (tokenizer.vocab_size / (16.0 * config_model.n_layer * config_model.n_embd))\n-    )\n-    tflops = flops_per_iteration / (elapsed_time * accelerator.state.num_processes * (10**12))\n-    return tflops\n-\n-\n-def evaluate(args):\n-    model.eval()\n-    losses = []\n-    for step, batch in enumerate(eval_dataloader):\n-        with torch.no_grad():\n-            outputs = model(batch, labels=batch)\n-        loss = outputs.loss.repeat(args.valid_batch_size)\n-        losses.append(accelerator.gather(loss))\n-        if args.max_eval_steps > 0 and step >= args.max_eval_steps:\n-            break\n-    losses = torch.cat(losses)\n-    loss = losses[: eval_dataloader.dataset.current_size].mean()\n-    try:\n-        perplexity = torch.exp(loss)\n-    except OverflowError:\n-        perplexity = float(\"inf\")\n-    return loss.item(), perplexity.item()\n-\n-\n-# Settings\n-parser = HfArgumentParser(TrainingArguments)\n-args = parser.parse_args()\n-\n-# Accelerator\n-config = ProjectConfiguration(project_dir=args.save_dir, logging_dir=\"log\")\n-accelerator = Accelerator(log_with=[\"wandb\", \"tensorboard\"], project_config=config)\n-acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n-\n-args = Namespace(**vars(args), **acc_state)\n-samples_per_step = accelerator.state.num_processes * args.train_batch_size\n-set_seed(args.seed)\n-\n-# Clone model repository\n-if accelerator.is_main_process:\n-    hf_repo = Repository(args.save_dir, clone_from=args.model_ckpt)\n-\n-# Logging\n-logger, run_name = setup_logging(args)\n-logger.info(accelerator.state)\n-\n-# Checkout new branch on repo\n-if accelerator.is_main_process:\n-    hf_repo.git_checkout(run_name, create_branch_ok=True)\n-\n-# Load model and tokenizer\n-model = AutoModelForCausalLM.from_pretrained(args.save_dir)\n-if args.gradient_checkpointing:\n-    model.gradient_checkpointing_enable()\n-tokenizer = AutoTokenizer.from_pretrained(args.save_dir)\n-\n-# Load dataset and dataloader\n-train_dataloader, eval_dataloader = create_dataloaders(args)\n-\n-# Prepare the optimizer and learning rate scheduler\n-optimizer = AdamW(get_grouped_params(model, args), lr=args.learning_rate)\n-lr_scheduler = get_scheduler(\n-    name=args.lr_scheduler_type,\n-    optimizer=optimizer,\n-    num_warmup_steps=args.num_warmup_steps,\n-    num_training_steps=args.max_train_steps,\n-)\n-accelerator.register_for_checkpointing(lr_scheduler)\n-\n-\n-def get_lr():\n-    return optimizer.param_groups[0][\"lr\"]\n-\n-\n-# Prepare everything with our `accelerator`.\n-model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n-    model, optimizer, train_dataloader, eval_dataloader\n-)\n-\n-# load in the weights and states from a previous save\n-if args.resume_from_checkpoint:\n-    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n-        accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n-        accelerator.load_state(args.resume_from_checkpoint)\n-        path = os.path.basename(args.resume_from_checkpoint)\n-    else:\n-        # Get the most recent checkpoint\n-        dirs = [f.name for f in os.scandir(args.save_dir) if f.is_dir() and \"step\" in str(f)]\n-        dirs.sort(key=os.path.getctime)\n-        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n-    # Extract the step of the checkpoint to continue from there\n-    training_difference = os.path.splitext(path)[0]\n-    resume_step = int(training_difference.replace(\"step_\", \"\"))\n-\n-# Train model\n-model.train()\n-completed_steps = 0\n-t_start = time.time()\n-loss_tracking = 0\n-for step, batch in enumerate(train_dataloader, start=1):\n-    if args.resume_from_checkpoint and step < resume_step:\n-        continue  # we need to skip steps until we reach the resumed step\n-    loss = model(batch, labels=batch, use_cache=False).loss\n-    avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()\n-    loss_tracking += avg_loss.item() / args.gradient_accumulation_steps\n-    log_metrics(step, {\"samples\": step * samples_per_step, \"loss_per_step/train\": loss.item()})\n-    loss = loss / args.gradient_accumulation_steps\n-    if step % args.gradient_accumulation_steps != 0:\n-        # Prevent backward from doing gradient all_reduce in every step\n-        if accelerator.distributed_type == DistributedType.MULTI_GPU:\n-            with model.no_sync():\n-                accelerator.backward(loss)\n-        else:\n-            accelerator.backward(loss)\n-    else:\n-        lr = get_lr()\n-        accelerator.backward(loss)\n-        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n-        optimizer.step()\n-        lr_scheduler.step()\n-        optimizer.zero_grad()\n-        elapsed_time = time.time() - t_start\n-        tflops = compute_tflops(elapsed_time, accelerator, args)\n-        log_metrics(\n-            step,\n-            {\n-                \"steps\": completed_steps,\n-                \"loss/train\": loss_tracking,\n-                \"lr\": lr,\n-                \"tflops\": tflops,\n-                \"time_per_iteration\": elapsed_time,\n-            },\n-        )\n-        t_start = time.time()\n-        loss_tracking = 0\n-        completed_steps += 1\n-    if step % args.save_checkpoint_steps == 0:\n-        logger.info(\"Evaluating and saving model checkpoint\")\n-        eval_loss, perplexity = evaluate(args)\n-        log_metrics(step, {\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n-        accelerator.wait_for_everyone()\n-        save_dir = os.path.join(args.save_dir, f\"step_{step}\")\n-        accelerator.save_state(save_dir)\n-        if accelerator.is_main_process:\n-            hf_repo.push_to_hub(commit_message=f\"step {step}\")\n-        model.train()\n-    if completed_steps >= args.max_train_steps:\n-        break\n-\n-# Evaluate and save the last checkpoint\n-logger.info(\"Evaluating and saving model after training\")\n-eval_loss, perplexity = evaluate(args)\n-log_metrics(step, {\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n-accelerator.wait_for_everyone()\n-unwrapped_model = accelerator.unwrap_model(model)\n-unwrapped_model.save_pretrained(args.save_dir, save_function=accelerator.save)\n-save_dir = os.path.join(args.save_dir, f\"step_{step}\")\n-accelerator.save_state(save_dir)\n-if accelerator.is_main_process:\n-    hf_repo.push_to_hub(commit_message=\"final model\")"
        },
        {
            "sha": "ef217a597e338550ae9e419e1abd01a6cf76a633",
            "filename": "examples/research_projects/codeparrot/scripts/human_eval.py",
            "status": "removed",
            "additions": 0,
            "deletions": 228,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fhuman_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fhuman_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fhuman_eval.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,228 +0,0 @@\n-import json\n-import multiprocessing\n-import os\n-import re\n-from collections import defaultdict\n-\n-import torch\n-from accelerate import Accelerator\n-from accelerate.utils import set_seed\n-from arguments import HumanEvalArguments\n-from datasets import load_dataset, load_metric\n-from torch.utils.data import IterableDataset\n-from torch.utils.data.dataloader import DataLoader\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, StoppingCriteria, StoppingCriteriaList\n-\n-\n-EOF_STRINGS = [\"\\nclass\", \"\\ndef\", \"\\n#\", \"\\n@\", \"\\nprint\", \"\\nif\"]\n-\n-\n-class TokenizedDataset(IterableDataset):\n-    \"\"\"Tokenize and preprocess the dataset\n-    Multiple copies of the same prompt are sent sequentially.\n-    See compute_code for more details.\n-    \"\"\"\n-\n-    def __init__(self, tokenizer, dataset, n_tasks=None, n_copies=1):\n-        self.tokenizer = tokenizer\n-        self.dataset = dataset\n-        self.n_tasks = len(dataset) if n_tasks is None else n_tasks\n-        self.n_copies = n_copies\n-\n-    def __iter__(self):\n-        prompts = []\n-        for task in range(self.n_tasks):\n-            # without strip, the model generate commented codes ...\n-            prompts.append(self.tokenizer.eos_token + self.dataset[task][\"prompt\"].strip())\n-        outputs = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n-        for task in range(self.n_tasks):\n-            for _ in range(self.n_copies):\n-                yield {\n-                    \"ids\": outputs.input_ids[task],\n-                    \"task_id\": task,\n-                    \"input_len\": outputs.attention_mask[task].sum(),\n-                }\n-\n-\n-class EndOfFunctionCriteria(StoppingCriteria):\n-    \"\"\"Custom `StoppingCriteria` which checks if all generated functions in the batch are completed.\"\"\"\n-\n-    def __init__(self, start_length, eof_strings, tokenizer):\n-        self.start_length = start_length\n-        self.eof_strings = eof_strings\n-        self.tokenizer = tokenizer\n-\n-    def __call__(self, input_ids, scores, **kwargs):\n-        \"\"\"Returns true if all generated sequences contain any of the end-of-function strings.\"\"\"\n-        decoded_generations = self.tokenizer.batch_decode(input_ids[:, self.start_length :])\n-        done = []\n-        for decoded_generation in decoded_generations:\n-            done.append(any(stop_string in decoded_generation for stop_string in self.eof_strings))\n-        return all(done)\n-\n-\n-def remove_last_block(string):\n-    \"\"\"Remove the last block of the code containing EOF_STRINGS\"\"\"\n-    string_list = re.split(\"(%s)\" % \"|\".join(EOF_STRINGS), string)\n-    # last string should be \"\"\n-    return \"\".join(string_list[:-2])\n-\n-\n-def complete_code(accelerator, model, tokenizer, dataloader, n_tasks, batch_size=20, **gen_kwargs):\n-    \"\"\"Generate multiple codes for each task in the dataset. This function leverage accelerator to distribute\n-    the processing to multiple GPUs.\n-    dataloader, a wrapper around a TokenizeDataset objectm is supposed to send all the prompts from\n-    the evalution dataset to the modelm as the following:\n-    [p_0_0, p_0_1, ..., p_0_nc-1, p_1_0, ..., p_nt-1_nc-1]\n-    where nc is the number of copies of the prompt, and nt is the number of tasks.\n-    nc is such that num_sample = nc * batch_size\n-\n-    Parameters\n-    ----------\n-    accelerator: Accelerator\n-\n-    model: transformers.PreTrainedModel\n-        Code generation model. AutoTokenizer.from_pretrained(model_ckpt), ex model_ckpt = \"lvwerra/codeparrot\"\n-\n-    tokenizer: transformers.AutoTokenizer\n-        The tokenizer used to train model\n-\n-    dataloader: DataLoader\n-        The dataloader is a wrapper around a TokenizeDataset object. It is designed to be used with multiple GPUs.\n-\n-    n_tasks: int\n-        The number of tasks in the dataset. It is used to determine the length of the output.\n-        Should be aligned with the number of tasks in the TokenizeDataset.\n-\n-    batch_size: int\n-        num_return_sequences per copy of the prompt such that num_sample = batch_size * n_copies\n-\n-    gen_kwargs: dict\n-        Keyword arguments for the generation function of the model.\n-\n-    Returns\n-    -------\n-    code_gens: list of list of str, of length n_tasks\n-        List of generated codes for each task.\n-        Each element is a list of generated codes for each task, with length num_samples\n-    \"\"\"\n-    gen_token_dict = defaultdict(list)  # dict of list of generated tokens\n-    for step, batch in tqdm(enumerate(dataloader)):\n-        with torch.no_grad():\n-            gen_kwargs[\"stopping_criteria\"][0].start_length = batch[\"ids\"].shape[-1]\n-            generated_tokens = accelerator.unwrap_model(model).generate(\n-                input_ids=batch[\"ids\"][:, : batch[\"input_len\"]], num_return_sequences=batch_size, **gen_kwargs\n-            )\n-            # each task is generated batch_size times\n-            generated_tasks = batch[\"task_id\"].repeat(batch_size)\n-            generated_tokens = accelerator.pad_across_processes(\n-                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n-            )\n-\n-            generated_tokens, generated_tasks = accelerator.gather((generated_tokens, generated_tasks))\n-            generated_tokens = generated_tokens.cpu().numpy()\n-            generated_tasks = generated_tasks.cpu().numpy()\n-\n-            for task, generated_tokens in zip(generated_tasks, generated_tokens):\n-                gen_token_dict[task].append(generated_tokens)\n-\n-    code_gens = [[] for _ in range(n_tasks)]\n-    for task, generated_tokens in gen_token_dict.items():\n-        for s in generated_tokens:\n-            gen_code = tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n-            code_gens[task].append(remove_last_block(gen_code))\n-    return code_gens\n-\n-\n-def main():\n-    # Setup configuration\n-    parser = HfArgumentParser(HumanEvalArguments)\n-    args = parser.parse_args()\n-\n-    transformers.logging.set_verbosity_error()\n-    # enables code execution in code_eval metric\n-    os.environ[\"HF_ALLOW_CODE_EVAL\"] = args.HF_ALLOW_CODE_EVAL\n-    # make sure tokenizer plays nice with multiprocessing\n-    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n-\n-    if args.num_workers is None:\n-        args.num_workers = multiprocessing.cpu_count()\n-\n-    # Use dataset load to feed to accelerate\n-    accelerator = Accelerator()\n-    set_seed(args.seed, device_specific=True)\n-\n-    # Load model and tokenizer\n-    tokenizer = AutoTokenizer.from_pretrained(args.model_ckpt)\n-    tokenizer.pad_token = tokenizer.eos_token\n-    model = AutoModelForCausalLM.from_pretrained(args.model_ckpt)\n-\n-    # Generation settings\n-    gen_kwargs = {\n-        \"do_sample\": args.do_sample,\n-        \"temperature\": args.temperature,\n-        \"max_new_tokens\": args.max_new_tokens,\n-        \"top_p\": args.top_p,\n-        \"top_k\": args.top_k,\n-        \"stopping_criteria\": StoppingCriteriaList([EndOfFunctionCriteria(0, EOF_STRINGS, tokenizer)]),\n-    }\n-\n-    # Load evaluation dataset and metric\n-    human_eval = load_dataset(\"openai_humaneval\")\n-    code_eval_metric = load_metric(\"code_eval\")\n-\n-    n_tasks = args.num_tasks if args.num_tasks is not None else len(human_eval[\"test\"])\n-    n_copies = args.n_samples // args.batch_size\n-\n-    human_eval_tokenized = TokenizedDataset(tokenizer, human_eval[\"test\"], n_copies=n_copies, n_tasks=n_tasks)\n-    # do not confuse args.batch_size, which is actually the num_return_sequences\n-    human_eval_loader = DataLoader(human_eval_tokenized, batch_size=1)\n-\n-    # Run a quick test to see if code evaluation is enabled\n-    try:\n-        _ = code_eval_metric.compute(references=[\"\"], predictions=[[\"\"]])\n-    except ValueError as exception:\n-        print(\n-            'Code evaluation not enabled. Read the warning below carefully and then use `--HF_ALLOW_CODE_EVAL=\"1\"`'\n-            \" flag to enable code evaluation.\"\n-        )\n-        raise exception\n-\n-    model, human_eval_loader = accelerator.prepare(model, human_eval_loader)\n-\n-    generations = complete_code(\n-        accelerator,\n-        model,\n-        tokenizer,\n-        human_eval_loader,\n-        n_tasks=n_tasks,\n-        batch_size=args.batch_size,\n-        **gen_kwargs,\n-    )\n-\n-    if accelerator.is_main_process:\n-        references = []\n-\n-        for task in tqdm(range(n_tasks)):\n-            test_func = human_eval[\"test\"][task][\"test\"]\n-            entry_point = f\"check({human_eval['test'][task]['entry_point']})\"\n-            references.append(\"\\n\" + test_func + \"\\n\" + entry_point)\n-\n-        # Evaluate completions with \"code_eval\" metric\n-        pass_at_k, _ = code_eval_metric.compute(\n-            references=references, predictions=generations, num_workers=args.num_workers\n-        )\n-        print(f\"Results: {pass_at_k}\")\n-\n-        # Save results to json file\n-        with open(args.output_file, \"w\") as fp:\n-            json.dump(pass_at_k, fp)\n-\n-\n-# For some reason the folliwng seems to be necessary sometimes for code_eval to work nice with multiprocessing\n-# https://stackoverflow.com/questions/60804599/python-multiprocessing-keeps-spawning-the-whole-script\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "6bf028688f12627b23f5fb2236ad403d7c9e6442",
            "filename": "examples/research_projects/codeparrot/scripts/initialize_model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Finitialize_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Finitialize_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Finitialize_model.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,27 +0,0 @@\n-from arguments import InitializationArguments\n-\n-from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n-\n-\n-# Configuration\n-parser = HfArgumentParser(InitializationArguments)\n-args = parser.parse_args()\n-\n-# Load codeparrot tokenizer trained for Python code tokenization\n-tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n-\n-# Config: \"scale_attn_by_layer_idx\" and \"reorder_and_upcast_attn\" are Mistral stability tweaks\n-config_kwargs = {\n-    \"vocab_size\": len(tokenizer),\n-    \"scale_attn_by_inverse_layer_idx\": True,\n-    \"reorder_and_upcast_attn\": True,\n-}\n-\n-# Load model config (GPT-2 large in this case)\n-config = AutoConfig.from_pretrained(args.config_name, **config_kwargs)\n-\n-# Initialize new model with config\n-model = AutoModelForCausalLM.from_config(config)\n-\n-# Save model to the hub\n-model.save_pretrained(args.model_name, push_to_hub=args.push_to_hub)"
        },
        {
            "sha": "f1984711278a105f8cabf65218c4448ec6357670",
            "filename": "examples/research_projects/codeparrot/scripts/minhash_deduplication.py",
            "status": "removed",
            "additions": 0,
            "deletions": 268,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fminhash_deduplication.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fminhash_deduplication.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fminhash_deduplication.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,268 +0,0 @@\n-import json\n-import multiprocessing as mp\n-import re\n-from collections import defaultdict\n-from functools import partial\n-from typing import Dict, List, Optional, Set, Tuple, Type\n-\n-from datasets import Dataset\n-from datasketch import MinHash, MinHashLSH\n-from dpu_utils.utils.iterators import ThreadedIterator\n-from tqdm import tqdm\n-\n-\n-NON_ALPHA = re.compile(\"[^A-Za-z_0-9]\")\n-# parameters used in DuplicationIndex\n-MIN_NUM_TOKENS = 10\n-NUM_PERM = 256\n-\n-\n-def get_min_hash(tokens: List[str]) -> Optional[MinHash]:\n-    \"\"\"Compute the MinHash of a code snippet.\"\"\"\n-    if len(tokens) < MIN_NUM_TOKENS:\n-        return None\n-    min_hash = MinHash(num_perm=NUM_PERM)\n-    for token in set(tokens):\n-        min_hash.update(token.encode())\n-    return min_hash\n-\n-\n-def get_tokens(code: str) -> Set[str]:\n-    \"\"\"Tokenize a code snippet.\"\"\"\n-    return {t for t in NON_ALPHA.split(code) if len(t.strip()) > 0}\n-\n-\n-class DuplicationIndex:\n-    def __init__(\n-        self,\n-        *,\n-        duplication_jaccard_threshold: float = 0.85,\n-    ):\n-        self._duplication_jaccard_threshold = duplication_jaccard_threshold\n-        self._num_perm = NUM_PERM\n-        self._index = MinHashLSH(threshold=self._duplication_jaccard_threshold, num_perm=self._num_perm)\n-\n-        self._duplicate_clusters = defaultdict(set)\n-\n-    def add(self, code_key: Tuple, min_hash: MinHash) -> None:\n-        \"\"\"Add a key to _index (MinHashLSH)\n-        the min_hash is used to query closest matches based on the jaccard_threshold.\n-        The new key is either added to a existing cluster of one close match,\n-        or a new cluster is created. The clusters created in this way, depend on the order of add.\n-\n-        Args:\n-            code_key (Tuple of (index, repo_name, path)):\n-                Theoritically any hasbale key. Here we use a tuple to retrieve the information later.\n-            min_hash: MinHash of the code_key.\n-        \"\"\"\n-        close_duplicates = self._index.query(min_hash)\n-        if code_key in self._index.keys:\n-            print(f\"Duplicate key {code_key}\")\n-            return\n-\n-        self._index.insert(code_key, min_hash)\n-        if len(close_duplicates) > 0:\n-            for base_duplicate in close_duplicates:\n-                if base_duplicate in self._duplicate_clusters:\n-                    self._duplicate_clusters[base_duplicate].add(code_key)\n-                    break\n-            else:\n-                self._duplicate_clusters[close_duplicates[0]].add(code_key)\n-\n-    def get_duplicate_clusters(self) -> List[List[Dict]]:\n-        \"\"\"Export the duplicate clusters.\n-        For each cluster, the first element is the base element of the cluster.\n-        The base element has an estimation jaccard similarity higher than the threshold with all the other elements.\n-\n-        Returns:\n-            duplicate_clusters (List[List[Dict]]):\n-                List of duplicate clusters.\n-        \"\"\"\n-        duplicate_clusters = []\n-        for base, duplicates in self._duplicate_clusters.items():\n-            cluster = [base] + list(duplicates)\n-            # reformat the cluster to be a list of dict\n-            cluster = [{\"base_index\": el[0], \"repo_name\": el[1], \"path\": el[2]} for el in cluster]\n-            duplicate_clusters.append(cluster)\n-        return duplicate_clusters\n-\n-    def save(self, filepath) -> None:\n-        duplicate_clusters = self.get_duplicate_clusters()\n-        with open(filepath, \"w\") as f:\n-            json.dump(duplicate_clusters, f)\n-\n-\n-def _compute_min_hash(element):\n-    index, data = element\n-    min_hash = get_min_hash([t for t in NON_ALPHA.split(data[\"content\"]) if len(t.strip()) > 0])\n-    if min_hash is not None:\n-        return (index, data[\"repo_name\"], data[\"path\"]), min_hash\n-\n-\n-def minhash_iter(dataset_iterator: Type[Dataset]):\n-    with mp.Pool() as pool:\n-        for data in pool.imap_unordered(\n-            _compute_min_hash,\n-            ThreadedIterator(dataset_iterator, max_queue_size=10000),\n-            chunksize=100,\n-        ):\n-            if data is not None:\n-                yield data\n-\n-\n-def make_duplicate_clusters(dataset_iterator: Type[Dataset], jaccard_threshold: float):\n-    \"\"\"Find duplicate clusters in the dataset in two steps:\n-    1. Compute MinHash for each code snippet. MinHash is a tool for fast jaccard similarity estimation.\n-    This step is computed using an asynchronous multiprocessing pool, minhash_iter\n-    2. Find duplicate clusters. The computed MinHash is added sequentially to the DuplicationIndex.\n-    This step cannot be parallelized. So using asynchronous thread in the previous step helps to speed up the process.\n-    \"\"\"\n-    di = DuplicationIndex(duplication_jaccard_threshold=jaccard_threshold)\n-\n-    for filename, min_hash in tqdm(ThreadedIterator(minhash_iter(enumerate(dataset_iterator)), max_queue_size=100)):\n-        di.add(filename, min_hash)\n-\n-    # Returns a List[Cluster] where Cluster is List[str] with the filenames.\n-    return di.get_duplicate_clusters()\n-\n-\n-def jaccard_similarity(code1: str, code2: str) -> float:\n-    \"\"\"Compute the Jaccard similarity of two code snippets.\"\"\"\n-    tokens1 = get_tokens(code1)\n-    tokens2 = get_tokens(code2)\n-    return len(tokens1 & tokens2) / len(tokens1 | tokens2)\n-\n-\n-_shared_dataset = None\n-\n-\n-def _find_cluster_extremes_shared(cluster, jaccard_threshold):\n-    \"\"\"Find a reduced cluster such that each code in the origin cluster is similar to at least one code in the reduced cluster.\n-    Two codes are similar if their Jaccard similarity is above the threshold.\n-\n-    Args:\n-        cluster (List[dict]):\n-           cluster is a list of dict, each dict contains the following keys:\n-                - base_index\n-                - repo_name\n-                - path\n-            This is a typical output of DuplicationIndex.get_duplicate_clusters()\n-        jaccard_threshold (float):\n-            threshold for Jaccard similarity.\n-            Two codes are similar if their Jaccard similarity is above the threshold.\n-\n-    Returns:\n-        extremes (List[dict]):\n-            A reduced representation of the cluster. The field copies is added to each dict.\n-            The copies field indicates the number of similar codes in the cluster for a extreme.\n-    \"\"\"\n-    extremes = []\n-    for element1 in cluster:\n-        code1 = _shared_dataset[element1[\"base_index\"]][\"content\"]\n-        for element2 in extremes:\n-            code2 = _shared_dataset[element2[\"base_index\"]][\"content\"]\n-            if jaccard_similarity(code1, code2) >= jaccard_threshold:\n-                element2[\"copies\"] += 1\n-                break\n-        else:\n-            element1[\"copies\"] = 1\n-            extremes.append(element1)\n-    return extremes\n-\n-\n-def find_extremes(cluster_list, dataset, jaccard_threshold):\n-    \"\"\"Call the _find_cluster_extremes_shared function in a parallel fashion.\n-\n-    Args:\n-        cluster_list (List[List[Dict]]):\n-            each cluster is a list of dicts with the key base_index,\n-            referring to the index of the base code in the dataset.\n-        dataset (Type[Dataset]):\n-            dataset is used to access the content of the code snippets,\n-            using the base_index from the cluster_list.\n-            dataset is shared between all the processes using a glabal variable (any other way to share the dataset?),\n-            otherwise the multi processing is not speeded up.\n-        jaccard_threshold (float):\n-            the threshold for the jaccard similarity. The default value is 0.85\n-\n-    Returns:\n-        extremes_list (List[Dict]):\n-            Each cluster is reduced to extremes.\n-            See _find_cluster_extremes_shared for the definition of extremes.\n-    \"\"\"\n-    global _shared_dataset\n-    _shared_dataset = dataset\n-    extremes_list = []\n-    f = partial(_find_cluster_extremes_shared, jaccard_threshold=jaccard_threshold)\n-    with mp.Pool() as pool:\n-        for extremes in tqdm(\n-            pool.imap_unordered(\n-                f,\n-                cluster_list,\n-            ),\n-            total=len(cluster_list),\n-        ):\n-            extremes_list.append(extremes)\n-    return extremes_list\n-\n-\n-def deduplicate_dataset(\n-    dataset: Type[Dataset], jaccard_threshold: float = 0.85\n-) -> Tuple[Type[Dataset], List[List[Dict]]]:\n-    \"\"\"Deduplicate the dataset using minhash and jaccard similarity.\n-    This function first generate duplicate clusters, then each cluster\n-    is reduced to the extremes that are similar to the other elements in the cluster.\n-    Codes are called similar if their Jaccard similarity is greater than jaccard_threshold (0.85 default).\n-\n-    Args:\n-        dataset (Type[Dataset]):\n-            The dataset to deduplicate.\n-        jaccard_threshold (float, default=0.85):\n-            jaccard threshold to determine if two codes are similar\n-\n-    Returns:\n-        ds_dedup (Type[Dataset]):\n-            The deduplicated dataset.\n-        duplicate_clusters (List[List[Dict]]):\n-            The list of duplicate clusters.\n-            Each cluster is a list of dicts with the following keys:\n-            - base_index : int\n-                The index of the code in the original dataset.\n-            - repo_name : str\n-            - path : str\n-            - copies : int\n-                The number of copies of the code in the cluster. (find_cluster_extremes)\n-            - is_extreme : bool\n-                Whether the code is an extreme in the cluster.\n-            All the codes in the cluster are removed from the dataset except the extremes.\n-\n-    Example:\n-        >>> from datasets import load_dataset\n-        >>> from minhash_deduplication import deduplicate_dataset\n-        >>> ds = load_dataset(\"lvwerra/codeparrot-clean\", split=\"train\")\n-        >>> ds_dedup, duplicate_clusters = deduplicate_dataset(ds, jaccard_threshold=0.85)\n-    \"\"\"\n-    duplicate_clusters = make_duplicate_clusters(dataset, jaccard_threshold)\n-    duplicate_indices = {x[\"base_index\"] for cluster in duplicate_clusters for x in cluster}\n-    extreme_dict = {}\n-    extremes_clusters = find_extremes(duplicate_clusters, dataset, jaccard_threshold)\n-    for extremes in extremes_clusters:\n-        for element in extremes:\n-            extreme_dict[element[\"base_index\"]] = element\n-    remove_indices = duplicate_indices - set(extreme_dict.keys())\n-    ds_filter = dataset.filter(lambda x, idx: idx not in remove_indices, with_indices=True)\n-\n-    # update duplicate_clusters\n-    for cluster in duplicate_clusters:\n-        for element in cluster:\n-            element[\"is_extreme\"] = element[\"base_index\"] in extreme_dict\n-            if element[\"is_extreme\"]:\n-                element[\"copies\"] = extreme_dict[element[\"base_index\"]][\"copies\"]\n-\n-    print(f\"Original dataset size: {len(dataset)}\")\n-    print(f\"Number of duplicate clusters: {len(duplicate_clusters)}\")\n-    print(f\"Files in duplicate cluster: {len(duplicate_indices)}\")\n-    print(f\"Unique files in duplicate cluster: {len(extreme_dict)}\")\n-    print(f\"Filtered dataset size: {len(ds_filter)}\")\n-\n-    return ds_filter, duplicate_clusters"
        },
        {
            "sha": "3e932c8ef61990ede6712fa52cd5d92e5eff518b",
            "filename": "examples/research_projects/codeparrot/scripts/preprocessing.py",
            "status": "removed",
            "additions": 0,
            "deletions": 215,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpreprocessing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpreprocessing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpreprocessing.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,215 +0,0 @@\n-import gzip\n-import json\n-import multiprocessing\n-import os\n-import re\n-import shutil\n-import time\n-from pathlib import Path\n-\n-import numpy as np\n-from arguments import PreprocessingArguments\n-from datasets import load_dataset\n-from huggingface_hub.utils import insecure_hashlib\n-from minhash_deduplication import deduplicate_dataset\n-\n-from transformers import AutoTokenizer, HfArgumentParser\n-\n-\n-PATTERN = re.compile(r\"\\s+\")\n-\n-\n-def get_hash(example):\n-    \"\"\"Get hash of content field.\"\"\"\n-    return {\"hash\": insecure_hashlib.md5(re.sub(PATTERN, \"\", example[\"content\"]).encode(\"utf-8\")).hexdigest()}\n-\n-\n-def line_stats(example):\n-    \"\"\"Calculates mean and max line length of file.\"\"\"\n-    line_lengths = [len(line) for line in example[\"content\"].splitlines()]\n-    return {\"line_mean\": np.mean(line_lengths), \"line_max\": max(line_lengths)}\n-\n-\n-def alpha_stats(example):\n-    \"\"\"Calculates mean and max line length of file.\"\"\"\n-    alpha_frac = np.mean([c.isalnum() for c in example[\"content\"]])\n-    return {\"alpha_frac\": alpha_frac}\n-\n-\n-def check_uniques(example, uniques):\n-    \"\"\"Check if current hash is still in set of unique hashes and remove if true.\"\"\"\n-    if example[\"hash\"] in uniques:\n-        uniques.remove(example[\"hash\"])\n-        return True\n-    else:\n-        return False\n-\n-\n-def is_autogenerated(example, scan_width=5):\n-    \"\"\"Check if file is autogenerated by looking for keywords in the first few lines of the file.\"\"\"\n-    keywords = [\"auto-generated\", \"autogenerated\", \"automatically generated\"]\n-    lines = example[\"content\"].splitlines()\n-    for _, line in zip(range(scan_width), lines):\n-        for keyword in keywords:\n-            if keyword in line.lower():\n-                return {\"autogenerated\": True}\n-    else:\n-        return {\"autogenerated\": False}\n-\n-\n-def is_config_or_test(example, scan_width=5, coeff=0.05):\n-    \"\"\"Check if file is a configuration file or a unit test by :\n-    1- looking for keywords in the first few lines of the file.\n-    2- counting number of occurrence of the words 'config' and 'test' with respect to number of lines.\n-    \"\"\"\n-\n-    keywords = [\"unit tests\", \"test file\", \"configuration file\"]\n-    lines = example[\"content\"].splitlines()\n-    count_config = 0\n-    count_test = 0\n-    # first test\n-    for _, line in zip(range(scan_width), lines):\n-        for keyword in keywords:\n-            if keyword in line.lower():\n-                return {\"config_or_test\": True}\n-    # second test\n-    nlines = example[\"content\"].count(\"\\n\")\n-    threshold = int(coeff * nlines)\n-    for line in lines:\n-        count_config += line.lower().count(\"config\")\n-        count_test += line.lower().count(\"test\")\n-        if count_config > threshold or count_test > threshold:\n-            return {\"config_or_test\": True}\n-    return {\"config_or_test\": False}\n-\n-\n-def has_no_keywords(example):\n-    \"\"\"Check if a python file has none of the keywords for: function, class, for loop, while loop.\"\"\"\n-    keywords = [\"def \", \"class \", \"for \", \"while \"]\n-    lines = example[\"content\"].splitlines()\n-    for line in lines:\n-        for keyword in keywords:\n-            if keyword in line.lower():\n-                return {\"has_no_keywords\": False}\n-    return {\"has_no_keywords\": True}\n-\n-\n-def has_few_assignments(example, minimum=4):\n-    \"\"\"Check if file uses symbol '=' less than `minimum` times.\"\"\"\n-    lines = example[\"content\"].splitlines()\n-    counter = 0\n-    for line in lines:\n-        counter += line.lower().count(\"=\")\n-        if counter > minimum:\n-            return {\"has_few_assignments\": False}\n-    return {\"has_few_assignments\": True}\n-\n-\n-def char_token_ratio(example):\n-    \"\"\"Compute character/token ratio of the file with tokenizer.\"\"\"\n-    input_ids = tokenizer(example[\"content\"], truncation=False)[\"input_ids\"]\n-    ratio = len(example[\"content\"]) / len(input_ids)\n-    return {\"ratio\": ratio}\n-\n-\n-def preprocess(example):\n-    \"\"\"Chain all preprocessing steps into one function to not fill cache.\"\"\"\n-    results = {}\n-    results.update(get_hash(example))\n-    results.update(line_stats(example))\n-    results.update(alpha_stats(example))\n-    results.update(char_token_ratio(example))\n-    results.update(is_autogenerated(example))\n-    results.update(is_config_or_test(example))\n-    results.update(has_no_keywords(example))\n-    results.update(has_few_assignments(example))\n-    return results\n-\n-\n-def filter(example, uniques, args):\n-    \"\"\"Filter dataset with heuristics. Config, test and has_no_keywords files are removed with a given probability.\"\"\"\n-    if not check_uniques(example, uniques):\n-        return False\n-    elif example[\"autogenerated\"]:\n-        return False\n-    elif example[\"line_max\"] > args.line_max:\n-        return False\n-    elif example[\"line_mean\"] > args.line_mean:\n-        return False\n-    elif example[\"alpha_frac\"] < args.alpha_frac:\n-        return False\n-    elif example[\"ratio\"] < args.min_token_ratio:\n-        return False\n-    elif example[\"config_or_test\"] and np.random.rand() <= args.filter_proba:\n-        return False\n-    elif example[\"has_no_keywords\"] and np.random.rand() <= args.filter_proba:\n-        return False\n-    elif example[\"has_few_assignments\"]:\n-        return False\n-    else:\n-        return True\n-\n-\n-def compress_file(file_path):\n-    \"\"\"Compress a file with g-zip.\"\"\"\n-    with open(file_path, \"rb\") as f_in:\n-        with gzip.open(str(file_path) + \".gz\", \"wb\", compresslevel=6) as f_out:\n-            shutil.copyfileobj(f_in, f_out)\n-    os.unlink(file_path)\n-\n-\n-# Settings\n-parser = HfArgumentParser(PreprocessingArguments)\n-args = parser.parse_args()\n-if args.num_workers is None:\n-    args.num_workers = multiprocessing.cpu_count()\n-tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_dir)\n-\n-# Load dataset\n-t_start = time.time()\n-ds = load_dataset(args.dataset_name, split=\"train\")\n-print(f\"Time to load dataset: {time.time()-t_start:.2f}\")\n-\n-# Run preprocessing\n-t_start = time.time()\n-ds = ds.map(preprocess, num_proc=args.num_workers)\n-print(f\"Time to preprocess dataset: {time.time()-t_start:.2f}\")\n-\n-# Deduplicate hashes\n-uniques = set(ds.unique(\"hash\"))\n-frac = len(uniques) / len(ds)\n-print(f\"Fraction of duplicates: {1-frac:.2%}\")\n-\n-# Deduplicate data and apply heuristics\n-t_start = time.time()\n-ds_filter = ds.filter(filter, fn_kwargs={\"uniques\": uniques, \"args\": args})\n-print(f\"Time to filter dataset: {time.time()-t_start:.2f}\")\n-print(f\"Size of filtered dataset: {len(ds_filter)}\")\n-\n-# Deduplicate with minhash and jaccard similarity\n-if args.near_deduplication:\n-    t_start = time.time()\n-    ds_filter, duplicate_clusters = deduplicate_dataset(ds_filter, args.jaccard_threshold)\n-    print(f\"Time to deduplicate dataset: {time.time()-t_start:.2f}\")\n-    print(f\"Size of deduplicate dataset: {len(ds_filter)}\")\n-\n-# Save data in batches of samples_per_file\n-output_dir = Path(args.output_dir)\n-output_dir.mkdir(exist_ok=True)\n-\n-# save duplicate_clusters in the output_dir as artifacts\n-# not sure it is the right place the save it\n-if args.near_deduplication:\n-    with open(output_dir / \"duplicate_clusters.json\", \"w\") as f:\n-        json.dump(duplicate_clusters, f)\n-\n-data_dir = output_dir / \"data\"\n-data_dir.mkdir(exist_ok=True)\n-\n-t_start = time.time()\n-for file_number, index in enumerate(range(0, len(ds_filter), args.samples_per_file)):\n-    file_path = str(data_dir / f\"file-{file_number+1:012}.json\")\n-    end_index = min(len(ds_filter), index + args.samples_per_file)\n-    ds_filter.select(list(range(index, end_index))).to_json(file_path)\n-    compress_file(file_path)\n-print(f\"Time to save dataset: {time.time()-t_start:.2f}\")"
        },
        {
            "sha": "7cac8f511918d1accc4e855ed6283f211ef6fbc4",
            "filename": "examples/research_projects/codeparrot/scripts/pretokenizing.py",
            "status": "removed",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpretokenizing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpretokenizing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpretokenizing.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,49 +0,0 @@\n-import multiprocessing\n-import time\n-\n-from arguments import PretokenizationArguments\n-from datasets import load_dataset\n-\n-from transformers import AutoTokenizer, HfArgumentParser\n-\n-\n-def tokenize(example):\n-    output = {}\n-    output[\"input_ids\"] = tokenizer(example[\"content\"], truncation=False)[\"input_ids\"]\n-    output[\"ratio_char_token\"] = len(example[\"content\"]) / len(output[\"input_ids\"])\n-    return output\n-\n-\n-parser = HfArgumentParser(PretokenizationArguments)\n-args = parser.parse_args()\n-if args.num_workers is None:\n-    args.num_workers = multiprocessing.cpu_count()\n-tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_dir)\n-\n-t_start = time.time()\n-ds = load_dataset(args.dataset_name, split=\"train\")\n-print(f\"Dataset loaded in {time.time()-t_start:.2f}s\")\n-\n-t_start = time.time()\n-ds = ds.map(\n-    tokenize,\n-    num_proc=args.num_workers,\n-    remove_columns=[\n-        \"repo_name\",\n-        \"path\",\n-        \"copies\",\n-        \"size\",\n-        \"content\",\n-        \"license\",\n-        \"hash\",\n-        \"line_mean\",\n-        \"line_max\",\n-        \"alpha_frac\",\n-        \"autogenerated\",\n-    ],\n-)\n-print(f\"Dataset tokenized in {time.time()-t_start:.2f}s\")\n-\n-t_start = time.time()\n-ds.push_to_hub(args.tokenized_data_repo)\n-print(f\"Data pushed to the hub in {time.time()-t_start:.2f}s\")"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/research_projects/codeparrot/scripts/tests/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Ftests%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Ftests%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Ftests%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "aaf53de137f4905565738ce9eafca15cc755a2fb",
            "filename": "examples/research_projects/codeparrot/scripts/tests/test_deduplicate.py",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Ftests%2Ftest_deduplicate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Ftests%2Ftest_deduplicate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Ftests%2Ftest_deduplicate.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,29 +0,0 @@\n-from unittest import TestCase\n-\n-from datasets import Dataset\n-from minhash_deduplication import deduplicate_dataset, make_duplicate_clusters\n-\n-\n-def get_dataset():\n-    data_dict = {\n-        \"repo_name\": [\"test_repo1\", \"test_repo2\", \"test_repo3\"],\n-        \"path\": [\"test_1.py\", \"test_2.py\", \"unit_test.py\"],\n-        \"content\": [\"a \" * 20, \"a \" * 30, \"b \" * 7],\n-    }\n-    dataset = Dataset.from_dict(data_dict)\n-    return dataset\n-\n-\n-class MakeDuplicateClustersTest(TestCase):\n-    def test_make_duplicate_clusters(self):\n-        ds = get_dataset()\n-        duplicate_clusters = make_duplicate_clusters(ds, 0.85)\n-        self.assertEqual(len(duplicate_clusters[0]), 2)\n-\n-    def test_deduplicate_dataset(self):\n-        ds = get_dataset()\n-        ds_filter, duplicate_clusters = deduplicate_dataset(ds)\n-        self.assertEqual(len(ds_filter), 2)\n-        print(duplicate_clusters)\n-        self.assertEqual(duplicate_clusters[0][0][\"copies\"], 2)\n-        self.assertEqual(duplicate_clusters[0][0][\"is_extreme\"], True)"
        },
        {
            "sha": "929c2df427e227d70eb69ad9394d300d64e85bc5",
            "filename": "examples/research_projects/codeparrot/scripts/validation_loss.py",
            "status": "removed",
            "additions": 0,
            "deletions": 99,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fvalidation_loss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fvalidation_loss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fvalidation_loss.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,99 +0,0 @@\n-import logging\n-\n-import torch\n-from accelerate import Accelerator\n-from arguments import EvaluationArguments\n-from datasets import load_dataset\n-from torch.utils.data import IterableDataset\n-from torch.utils.data.dataloader import DataLoader\n-\n-from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, set_seed\n-\n-\n-class ConstantLengthDataset(IterableDataset):\n-    def __init__(self, tokenizer, dataset, seq_length=1024, num_of_sequences=1024, chars_per_token=3.6):\n-        self.tokenizer = tokenizer\n-        self.concat_token_id = tokenizer.bos_token_id\n-        self.dataset = dataset\n-        self.seq_length = seq_length\n-        self.input_characters = seq_length * chars_per_token * num_of_sequences\n-\n-    def __iter__(self):\n-        iterator = iter(self.dataset)\n-        more_examples = True\n-        while more_examples:\n-            buffer, buffer_len = [], 0\n-            while True:\n-                if buffer_len >= self.input_characters:\n-                    break\n-                try:\n-                    buffer.append(next(iterator)[\"content\"])\n-                    buffer_len += len(buffer[-1])\n-                except StopIteration:\n-                    more_examples = False\n-                    break\n-            tokenized_inputs = tokenizer(buffer, truncation=False)[\"input_ids\"]\n-            all_token_ids = []\n-            for tokenized_input in tokenized_inputs:\n-                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n-            for i in range(0, len(all_token_ids), self.seq_length):\n-                input_ids = all_token_ids[i : i + self.seq_length]\n-                if len(input_ids) == self.seq_length:\n-                    yield torch.tensor(input_ids)\n-\n-\n-def create_dataloader(args):\n-    ds_kwargs = {\"streaming\": True}\n-    valid_data = load_dataset(args.dataset_name, split=\"train\", **ds_kwargs)\n-    valid_dataset = ConstantLengthDataset(tokenizer, valid_data, seq_length=args.seq_length)\n-    eval_dataloader = DataLoader(valid_dataset, batch_size=args.batch_size)\n-    return eval_dataloader\n-\n-\n-def evaluate(args):\n-    model.eval()\n-    losses = []\n-    for step, batch in enumerate(eval_dataloader):\n-        with torch.no_grad():\n-            outputs = model(batch, labels=batch)\n-        loss = outputs.loss.repeat(args.batch_size)\n-        losses.append(accelerator.gather(loss))\n-\n-        if args.max_eval_steps > 0 and step >= args.max_eval_steps:\n-            break\n-    loss = torch.mean(torch.cat(losses))\n-    try:\n-        perplexity = torch.exp(loss)\n-    except OverflowError:\n-        perplexity = float(\"inf\")\n-    return loss.item(), perplexity.item()\n-\n-\n-# Setup Accelerator\n-accelerator = Accelerator()\n-\n-# Parse configuration\n-parser = HfArgumentParser(EvaluationArguments)\n-args = parser.parse_args()\n-set_seed(args.seed)\n-\n-# Logging\n-logger = logging.getLogger(__name__)\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n-)\n-\n-# Load model and tokenizer\n-model = AutoModelForCausalLM.from_pretrained(args.model_ckpt)\n-tokenizer = AutoTokenizer.from_pretrained(args.model_ckpt)\n-\n-# Load dataset and dataloader\n-eval_dataloader = create_dataloader(args)\n-\n-# Prepare everything with our `accelerator`.\n-model, eval_dataloader = accelerator.prepare(model, eval_dataloader)\n-\n-# Evaluate and save the last checkpoint\n-logger.info(\"Evaluating and saving model after training\")\n-eval_loss, perplexity = evaluate(args)\n-logger.info(f\"loss/eval: {eval_loss}, perplexity: {perplexity}\")"
        },
        {
            "sha": "14632a74d80805757e4307ace37a0252bfc4dc2e",
            "filename": "examples/research_projects/decision_transformer/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 240,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdecision_transformer%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdecision_transformer%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdecision_transformer%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,240 +0,0 @@\n-absl-py==1.0.0\n-aiohttp==3.10.11\n-aiosignal==1.2.0\n-alembic==1.7.7\n-appdirs==1.4.4\n-APScheduler==3.9.1\n-arrow==1.2.2\n-asttokens==2.0.5\n-astunparse==1.6.3\n-async-timeout==4.0.2\n-attrs==21.4.0\n-audioread==2.1.9\n-autopage==0.5.0\n-backcall==0.2.0\n-backoff==1.11.1\n-backports.zoneinfo==0.2.1\n-binaryornot==0.4.4\n-black==24.3.0\n-boto3==1.16.34\n-botocore==1.19.63\n-Brotli==1.0.9\n-cachetools==5.0.0\n-certifi==2024.7.4\n-cffi==1.15.0\n-chardet==4.0.0\n-charset-normalizer==2.0.12\n-chex==0.1.1\n-click==8.0.4\n-cliff==3.10.1\n-clldutils==3.11.1\n-cloudpickle==2.0.0\n-cmaes==0.8.2\n-cmd2==2.4.0\n-codecarbon==1.2.0\n-colorlog==6.6.0\n-cookiecutter==2.1.1\n-cryptography==44.0.1\n-csvw==2.0.0\n-cycler==0.11.0\n-Cython==0.29.28\n-dash==2.15.0\n-dash-bootstrap-components==1.0.3\n-dash-core-components==2.0.0\n-dash-html-components==2.0.0\n-dash-table==5.0.0\n-datasets==2.0.0\n-decorator==5.1.1\n-Deprecated==1.2.13\n-dill==0.3.4\n-dlinfo==1.2.1\n-dm-tree==0.1.6\n-docker==4.4.4\n-execnet==1.9.0\n-executing==0.8.3\n-faiss-cpu==1.7.2\n-fasteners==0.17.3\n-filelock==3.6.0\n-fire==0.4.0\n-flake8==4.0.1\n-Flask==2.3.2\n-Flask-Compress==1.11\n-flatbuffers==2.0\n-flax==0.4.0\n-fonttools==4.43.0\n-frozenlist==1.3.0\n-fsspec==2022.2.0\n-fugashi==1.1.2\n-gast==0.5.3\n-gitdb==4.0.9\n-GitPython==3.1.41\n-glfw==2.5.1\n-google-auth==2.6.2\n-google-auth-oauthlib==0.4.6\n-google-pasta==0.2.0\n-greenlet==1.1.2\n-grpcio==1.53.2\n-gym==0.23.1\n-gym-notices==0.0.6\n-h5py==3.6.0\n-huggingface-hub==0.4.0\n-hypothesis==6.39.4\n-idna==3.7\n-imageio==2.16.1\n-importlib-metadata==4.11.3\n-importlib-resources==5.4.0\n-iniconfig==1.1.1\n-ipadic==1.0.0\n-ipython==8.10.0\n-isodate==0.6.1\n-isort==5.10.1\n-itsdangerous==2.1.1\n-jax==0.3.4\n-jaxlib==0.3.2\n-jedi==0.18.1\n-Jinja2==3.1.6\n-jinja2-time==0.2.0\n-jmespath==0.10.0\n-joblib==1.2.0\n-jsonschema==4.4.0\n-keras==2.13.1\n-Keras-Preprocessing==1.1.2\n-kiwisolver==1.4.0\n-kubernetes==12.0.1\n-libclang==13.0.0\n-librosa==0.9.1\n-llvmlite==0.38.0\n-Mako==1.2.2\n-Markdown==3.3.6\n-MarkupSafe==1.1.1\n-matplotlib==3.5.1\n-matplotlib-inline==0.1.3\n-mccabe==0.6.1\n-msgpack==1.0.3\n-mujoco-py==2.1.2.14\n-multidict==6.0.2\n-multiprocess==0.70.12.2\n-mypy-extensions==0.4.3\n-nltk==3.9\n-numba==0.55.1\n-numpy==1.22.3\n-oauthlib==3.2.2\n-onnx>=1.15.0\n-onnxconverter-common==1.9.0\n-opt-einsum==3.3.0\n-optax==0.1.1\n-optuna==2.10.0\n-packaging==21.3\n-pandas==1.4.1\n-parameterized==0.8.1\n-parso==0.8.3\n-pathspec==0.9.0\n-pbr==5.8.1\n-pexpect==4.8.0\n-phonemizer==3.0.1\n-pickleshare==0.7.5\n-Pillow==10.3.0\n-Pint==0.16.1\n-plac==1.3.4\n-platformdirs==2.5.1\n-plotly==5.6.0\n-pluggy==1.0.0\n-pooch==1.6.0\n-portalocker==2.0.0\n-poyo==0.5.0\n-prettytable==3.2.0\n-prompt-toolkit==3.0.28\n-protobuf==3.19.5\n-psutil==5.9.0\n-ptyprocess==0.7.0\n-pure-eval==0.2.2\n-py==1.11.0\n-py-cpuinfo==8.0.0\n-pyarrow==15.0.0\n-pyasn1==0.4.8\n-pyasn1-modules==0.2.8\n-pycodestyle==2.8.0\n-pycparser==2.21\n-pyctcdecode==0.3.0\n-pyflakes==2.4.0\n-Pygments==2.15.0\n-pygtrie==2.4.2\n-pynvml==11.4.1\n-pyOpenSSL==22.0.0\n-pyparsing==3.0.7\n-pyperclip==1.8.2\n-pypng==0.0.21\n-pyrsistent==0.18.1\n-pytest==7.1.1\n-pytest-forked==1.4.0\n-pytest-timeout==2.1.0\n-pytest-xdist==2.5.0\n-python-dateutil==2.8.2\n-python-slugify==6.1.1\n-pytz==2022.1\n-pytz-deprecation-shim==0.1.0.post0\n-PyYAML==6.0\n-ray>2.6.3\n-redis==4.5.4\n-regex==2022.3.15\n-requests==2.32.0\n-requests-oauthlib==1.3.1\n-resampy==0.2.2\n-responses==0.18.0\n-rfc3986==1.5.0\n-rouge-score==0.0.4\n-rsa==4.8\n-s3transfer==0.3.7\n-sacrebleu==1.5.1\n-sacremoses==0.0.49\n-scikit-learn==1.5.0\n-scipy==1.8.0\n-segments==2.2.0\n-sentencepiece==0.1.96\n-sigopt==8.2.0\n-six==1.16.0\n-smmap==5.0.0\n-sortedcontainers==2.4.0\n-SoundFile==0.10.3.post1\n-SQLAlchemy==1.4.32\n-stack-data==0.2.0\n-stevedore==3.5.0\n-tabulate==0.8.9\n-tenacity==8.0.1\n-tensorboard==2.8.0\n-tensorboard-data-server==0.6.1\n-tensorboard-plugin-wit==1.8.1\n-tensorboardX==2.5\n-tensorflow==2.12.1\n-tensorflow-io-gcs-filesystem==0.24.0\n-termcolor==1.1.0\n-text-unidecode==1.3\n-tf-estimator-nightly==2.8.0.dev2021122109\n-tf2onnx==1.9.3\n-threadpoolctl==3.1.0\n-timeout-decorator==0.5.0\n-timm==0.5.4\n-tokenizers==0.11.6\n-tomli==2.0.1\n-toolz==0.11.2\n-torch==2.2.0\n-torchaudio==0.11.0\n-torchvision==0.12.0\n-tqdm==4.66.3\n-traitlets==5.1.1\n--e git+git@github.com:edbeeching/transformers.git@77b90113ca0a0e4058b046796c874bdc98f1da61#egg=transformers\n-typing-extensions==4.1.1\n-tzdata==2022.1\n-tzlocal==4.1\n-unidic==1.1.0\n-unidic-lite==1.0.8\n-uritemplate==4.1.1\n-urllib3==1.26.19\n-wasabi==0.9.0\n-wcwidth==0.2.5\n-websocket-client==1.3.1\n-Werkzeug==3.0.6\n-wrapt==1.14.0\n-xxhash==3.0.0\n-yarl==1.7.2\n-zipp==3.19.1\n\\ No newline at end of file"
        },
        {
            "sha": "d6c3e28331259d01fa0f82c29f0dca593c908856",
            "filename": "examples/research_projects/decision_transformer/run_decision_transformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 173,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdecision_transformer%2Frun_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdecision_transformer%2Frun_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdecision_transformer%2Frun_decision_transformer.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,173 +0,0 @@\n-import gym\n-import numpy as np\n-import torch\n-from mujoco_py import GlfwContext\n-\n-from transformers import DecisionTransformerModel\n-\n-\n-GlfwContext(offscreen=True)  # Create a window to init GLFW.\n-\n-\n-def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n-    # we don't care about the past rewards in this model\n-\n-    states = states.reshape(1, -1, model.config.state_dim)\n-    actions = actions.reshape(1, -1, model.config.act_dim)\n-    returns_to_go = returns_to_go.reshape(1, -1, 1)\n-    timesteps = timesteps.reshape(1, -1)\n-\n-    if model.config.max_length is not None:\n-        states = states[:, -model.config.max_length :]\n-        actions = actions[:, -model.config.max_length :]\n-        returns_to_go = returns_to_go[:, -model.config.max_length :]\n-        timesteps = timesteps[:, -model.config.max_length :]\n-\n-        # pad all tokens to sequence length\n-        attention_mask = torch.cat(\n-            [torch.zeros(model.config.max_length - states.shape[1]), torch.ones(states.shape[1])]\n-        )\n-        attention_mask = attention_mask.to(dtype=torch.long, device=states.device).reshape(1, -1)\n-        states = torch.cat(\n-            [\n-                torch.zeros(\n-                    (states.shape[0], model.config.max_length - states.shape[1], model.config.state_dim),\n-                    device=states.device,\n-                ),\n-                states,\n-            ],\n-            dim=1,\n-        ).to(dtype=torch.float32)\n-        actions = torch.cat(\n-            [\n-                torch.zeros(\n-                    (actions.shape[0], model.config.max_length - actions.shape[1], model.config.act_dim),\n-                    device=actions.device,\n-                ),\n-                actions,\n-            ],\n-            dim=1,\n-        ).to(dtype=torch.float32)\n-        returns_to_go = torch.cat(\n-            [\n-                torch.zeros(\n-                    (returns_to_go.shape[0], model.config.max_length - returns_to_go.shape[1], 1),\n-                    device=returns_to_go.device,\n-                ),\n-                returns_to_go,\n-            ],\n-            dim=1,\n-        ).to(dtype=torch.float32)\n-        timesteps = torch.cat(\n-            [\n-                torch.zeros(\n-                    (timesteps.shape[0], model.config.max_length - timesteps.shape[1]), device=timesteps.device\n-                ),\n-                timesteps,\n-            ],\n-            dim=1,\n-        ).to(dtype=torch.long)\n-    else:\n-        attention_mask = None\n-\n-    _, action_preds, _ = model(\n-        states=states,\n-        actions=actions,\n-        rewards=rewards,\n-        returns_to_go=returns_to_go,\n-        timesteps=timesteps,\n-        attention_mask=attention_mask,\n-        return_dict=False,\n-    )\n-\n-    return action_preds[0, -1]\n-\n-\n-# build the environment\n-\n-env = gym.make(\"Hopper-v3\")\n-state_dim = env.observation_space.shape[0]\n-act_dim = env.action_space.shape[0]\n-max_ep_len = 1000\n-device = \"cuda\"\n-scale = 1000.0  # normalization for rewards/returns\n-TARGET_RETURN = 3600 / scale  # evaluation conditioning targets, 3600 is reasonable from the paper LINK\n-state_mean = np.array(\n-    [\n-        1.311279,\n-        -0.08469521,\n-        -0.5382719,\n-        -0.07201576,\n-        0.04932366,\n-        2.1066856,\n-        -0.15017354,\n-        0.00878345,\n-        -0.2848186,\n-        -0.18540096,\n-        -0.28461286,\n-    ]\n-)\n-state_std = np.array(\n-    [\n-        0.17790751,\n-        0.05444621,\n-        0.21297139,\n-        0.14530419,\n-        0.6124444,\n-        0.85174465,\n-        1.4515252,\n-        0.6751696,\n-        1.536239,\n-        1.6160746,\n-        5.6072536,\n-    ]\n-)\n-state_mean = torch.from_numpy(state_mean).to(device=device)\n-state_std = torch.from_numpy(state_std).to(device=device)\n-\n-# Create the decision transformer model\n-model = DecisionTransformerModel.from_pretrained(\"edbeeching/decision-transformer-gym-hopper-medium\")\n-model = model.to(device)\n-model.eval()\n-\n-for ep in range(10):\n-    episode_return, episode_length = 0, 0\n-    state = env.reset()\n-    target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n-    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n-    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n-    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n-\n-    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n-    for t in range(max_ep_len):\n-        env.render()\n-        # add padding\n-        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n-        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n-\n-        action = get_action(\n-            model,\n-            (states.to(dtype=torch.float32) - state_mean) / state_std,\n-            actions.to(dtype=torch.float32),\n-            rewards.to(dtype=torch.float32),\n-            target_return.to(dtype=torch.float32),\n-            timesteps.to(dtype=torch.long),\n-        )\n-        actions[-1] = action\n-        action = action.detach().cpu().numpy()\n-\n-        state, reward, done, _ = env.step(action)\n-\n-        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n-        states = torch.cat([states, cur_state], dim=0)\n-        rewards[-1] = reward\n-\n-        pred_return = target_return[0, -1] - (reward / scale)\n-        target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n-        timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n-\n-        episode_return += reward\n-        episode_length += 1\n-\n-        if done:\n-            break"
        },
        {
            "sha": "08a087dc03ebafec1e257c8e0b4982645d5ce01e",
            "filename": "examples/research_projects/deebert/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 54,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,54 +0,0 @@\n-# DeeBERT: Early Exiting for *BERT\n-\n-This is the code base for the paper [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://www.aclweb.org/anthology/2020.acl-main.204/), modified from its [original code base](https://github.com/castorini/deebert).\n-\n-The original code base also has information for downloading sample models that we have trained in advance.\n-\n-## Usage\n-\n-There are three scripts in the folder which can be run directly.\n-\n-In each script, there are several things to modify before running:\n-\n-* `PATH_TO_DATA`: path to the GLUE dataset.\n-* `--output_dir`: path for saving fine-tuned models. Default: `./saved_models`.\n-* `--plot_data_dir`: path for saving evaluation results. Default: `./results`. Results are printed to stdout and also saved to `npy` files in this directory to facilitate plotting figures and further analyses.\n-* `MODEL_TYPE`: bert or roberta\n-* `MODEL_SIZE`: base or large\n-* `DATASET`: SST-2, MRPC, RTE, QNLI, QQP, or MNLI\n-\n-#### train_deebert.sh\n-\n-This is for fine-tuning DeeBERT models.\n-\n-#### eval_deebert.sh\n-\n-This is for evaluating each exit layer for fine-tuned DeeBERT models.\n-\n-#### entropy_eval.sh\n-\n-This is for evaluating fine-tuned DeeBERT models, given a number of different early exit entropy thresholds.\n-\n-\n-\n-## Citation\n-\n-Please cite our paper if you find the resource useful:\n-```bibtex\n-@inproceedings{xin-etal-2020-deebert,\n-    title = \"{D}ee{BERT}: Dynamic Early Exiting for Accelerating {BERT} Inference\",\n-    author = \"Xin, Ji  and\n-      Tang, Raphael  and\n-      Lee, Jaejun  and\n-      Yu, Yaoliang  and\n-      Lin, Jimmy\",\n-    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n-    month = jul,\n-    year = \"2020\",\n-    address = \"Online\",\n-    publisher = \"Association for Computational Linguistics\",\n-    url = \"https://www.aclweb.org/anthology/2020.acl-main.204\",\n-    pages = \"2246--2251\",\n-}\n-```\n-"
        },
        {
            "sha": "884c286a56a598bf5f1e79debe258821b673ac46",
            "filename": "examples/research_projects/deebert/entropy_eval.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fentropy_eval.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fentropy_eval.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Fentropy_eval.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,33 +0,0 @@\n-#!/bin/bash\n-export CUDA_VISIBLE_DEVICES=0\n-\n-PATH_TO_DATA=/h/xinji/projects/GLUE\n-\n-MODEL_TYPE=bert  # bert or roberta\n-MODEL_SIZE=base  # base or large\n-DATASET=MRPC  # SST-2, MRPC, RTE, QNLI, QQP, or MNLI\n-\n-MODEL_NAME=${MODEL_TYPE}-${MODEL_SIZE}\n-if [ $MODEL_TYPE = 'bert' ]\n-then\n-  MODEL_NAME=${MODEL_NAME}-uncased\n-fi\n-\n-ENTROPIES=\"0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\"\n-\n-for ENTROPY in $ENTROPIES; do\n-  python -u run_glue_deebert.py \\\n-    --model_type $MODEL_TYPE \\\n-    --model_name_or_path ./saved_models/${MODEL_TYPE}-${MODEL_SIZE}/$DATASET/two_stage \\\n-    --task_name $DATASET \\\n-    --do_eval \\\n-    --do_lower_case \\\n-    --data_dir $PATH_TO_DATA/$DATASET \\\n-    --output_dir ./saved_models/${MODEL_TYPE}-${MODEL_SIZE}/$DATASET/two_stage \\\n-    --plot_data_dir ./results/ \\\n-    --max_seq_length 128 \\\n-    --early_exit_entropy $ENTROPY \\\n-    --eval_highway \\\n-    --overwrite_cache \\\n-    --per_gpu_eval_batch_size=1\n-done"
        },
        {
            "sha": "adf4f652a9f7135657c5f5e8400aa47a65d907e0",
            "filename": "examples/research_projects/deebert/eval_deebert.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Feval_deebert.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Feval_deebert.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Feval_deebert.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,30 +0,0 @@\n-#!/bin/bash\n-export CUDA_VISIBLE_DEVICES=0\n-\n-PATH_TO_DATA=/h/xinji/projects/GLUE\n-\n-MODEL_TYPE=bert  # bert or roberta\n-MODEL_SIZE=base  # base or large\n-DATASET=MRPC  # SST-2, MRPC, RTE, QNLI, QQP, or MNLI\n-\n-MODEL_NAME=${MODEL_TYPE}-${MODEL_SIZE}\n-if [ $MODEL_TYPE = 'bert' ]\n-then\n-  MODEL_NAME=${MODEL_NAME}-uncased\n-fi\n-\n-\n-python -u run_glue_deebert.py  \\\n-  --model_type $MODEL_TYPE \\\n-  --model_name_or_path ./saved_models/${MODEL_TYPE}-${MODEL_SIZE}/$DATASET/two_stage \\\n-  --task_name $DATASET \\\n-  --do_eval \\\n-  --do_lower_case \\\n-  --data_dir $PATH_TO_DATA/$DATASET \\\n-  --output_dir ./saved_models/${MODEL_TYPE}-${MODEL_SIZE}/$DATASET/two_stage \\\n-  --plot_data_dir ./results/ \\\n-  --max_seq_length 128 \\\n-  --eval_each_highway \\\n-  --eval_highway \\\n-  --overwrite_cache \\\n-  --per_gpu_eval_batch_size=1"
        },
        {
            "sha": "99636a7fce1b8ebdd049db2bdeb2248c02791fce",
            "filename": "examples/research_projects/deebert/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1 +0,0 @@\n-transformers == 4.38.0"
        },
        {
            "sha": "6ca28ab5bc07bc082b4f424a06713e437d4e3be8",
            "filename": "examples/research_projects/deebert/run_glue_deebert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 735,
            "changes": 735,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Frun_glue_deebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Frun_glue_deebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Frun_glue_deebert.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,735 +0,0 @@\n-from __future__ import absolute_import, division, print_function\n-\n-import argparse\n-import glob\n-import logging\n-import os\n-import random\n-import time\n-\n-import numpy as np\n-import torch\n-from torch import nn\n-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm, trange\n-\n-import transformers\n-from src.modeling_highway_bert import DeeBertForSequenceClassification\n-from src.modeling_highway_roberta import DeeRobertaForSequenceClassification\n-from transformers import (\n-    WEIGHTS_NAME,\n-    AdamW,\n-    BertConfig,\n-    BertTokenizer,\n-    RobertaConfig,\n-    RobertaTokenizer,\n-    get_linear_schedule_with_warmup,\n-)\n-from transformers import glue_compute_metrics as compute_metrics\n-from transformers import glue_convert_examples_to_features as convert_examples_to_features\n-from transformers import glue_output_modes as output_modes\n-from transformers import glue_processors as processors\n-from transformers.trainer_utils import is_main_process\n-\n-\n-try:\n-    from torch.utils.tensorboard import SummaryWriter\n-except ImportError:\n-    from tensorboardX import SummaryWriter\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-MODEL_CLASSES = {\n-    \"bert\": (BertConfig, DeeBertForSequenceClassification, BertTokenizer),\n-    \"roberta\": (RobertaConfig, DeeRobertaForSequenceClassification, RobertaTokenizer),\n-}\n-\n-\n-def set_seed(args):\n-    random.seed(args.seed)\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    if args.n_gpu > 0:\n-        torch.cuda.manual_seed_all(args.seed)\n-\n-\n-def get_wanted_result(result):\n-    if \"spearmanr\" in result:\n-        print_result = result[\"spearmanr\"]\n-    elif \"f1\" in result:\n-        print_result = result[\"f1\"]\n-    elif \"mcc\" in result:\n-        print_result = result[\"mcc\"]\n-    elif \"acc\" in result:\n-        print_result = result[\"acc\"]\n-    else:\n-        raise ValueError(\"Primary metric unclear in the results\")\n-    return print_result\n-\n-\n-def train(args, train_dataset, model, tokenizer, train_highway=False):\n-    \"\"\"Train the model\"\"\"\n-    if args.local_rank in [-1, 0]:\n-        tb_writer = SummaryWriter()\n-\n-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n-\n-    if args.max_steps > 0:\n-        t_total = args.max_steps\n-        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n-    else:\n-        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n-\n-    # Prepare optimizer and schedule (linear warmup and decay)\n-    no_decay = [\"bias\", \"LayerNorm.weight\"]\n-    if train_highway:\n-        optimizer_grouped_parameters = [\n-            {\n-                \"params\": [\n-                    p\n-                    for n, p in model.named_parameters()\n-                    if (\"highway\" in n) and (not any(nd in n for nd in no_decay))\n-                ],\n-                \"weight_decay\": args.weight_decay,\n-            },\n-            {\n-                \"params\": [\n-                    p for n, p in model.named_parameters() if (\"highway\" in n) and (any(nd in n for nd in no_decay))\n-                ],\n-                \"weight_decay\": 0.0,\n-            },\n-        ]\n-    else:\n-        optimizer_grouped_parameters = [\n-            {\n-                \"params\": [\n-                    p\n-                    for n, p in model.named_parameters()\n-                    if (\"highway\" not in n) and (not any(nd in n for nd in no_decay))\n-                ],\n-                \"weight_decay\": args.weight_decay,\n-            },\n-            {\n-                \"params\": [\n-                    p\n-                    for n, p in model.named_parameters()\n-                    if (\"highway\" not in n) and (any(nd in n for nd in no_decay))\n-                ],\n-                \"weight_decay\": 0.0,\n-            },\n-        ]\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n-    scheduler = get_linear_schedule_with_warmup(\n-        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n-    )\n-    if args.fp16:\n-        try:\n-            from apex import amp\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n-\n-    # multi-gpu training (should be after apex fp16 initialization)\n-    if args.n_gpu > 1:\n-        model = nn.DataParallel(model)\n-\n-    # Distributed training (should be after apex fp16 initialization)\n-    if args.local_rank != -1:\n-        model = nn.parallel.DistributedDataParallel(\n-            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n-        )\n-\n-    # Train!\n-    logger.info(\"***** Running training *****\")\n-    logger.info(\"  Num examples = %d\", len(train_dataset))\n-    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n-    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n-    logger.info(\n-        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n-        args.train_batch_size\n-        * args.gradient_accumulation_steps\n-        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n-    )\n-    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n-    logger.info(\"  Total optimization steps = %d\", t_total)\n-\n-    global_step = 0\n-    tr_loss, logging_loss = 0.0, 0.0\n-    model.zero_grad()\n-    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n-    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n-    for _ in train_iterator:\n-        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n-        for step, batch in enumerate(epoch_iterator):\n-            model.train()\n-            batch = tuple(t.to(args.device) for t in batch)\n-            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n-            if args.model_type != \"distilbert\":\n-                inputs[\"token_type_ids\"] = (\n-                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n-                )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n-            inputs[\"train_highway\"] = train_highway\n-            outputs = model(**inputs)\n-            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n-\n-            if args.n_gpu > 1:\n-                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n-            if args.gradient_accumulation_steps > 1:\n-                loss = loss / args.gradient_accumulation_steps\n-\n-            if args.fp16:\n-                with amp.scale_loss(loss, optimizer) as scaled_loss:\n-                    scaled_loss.backward()\n-            else:\n-                loss.backward()\n-\n-            tr_loss += loss.item()\n-            if (step + 1) % args.gradient_accumulation_steps == 0:\n-                if args.fp16:\n-                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n-                else:\n-                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n-\n-                optimizer.step()\n-                scheduler.step()  # Update learning rate schedule\n-                model.zero_grad()\n-                global_step += 1\n-\n-                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n-                    # Log metrics\n-                    if (\n-                        args.local_rank == -1 and args.evaluate_during_training\n-                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n-                        results = evaluate(args, model, tokenizer)\n-                        for key, value in results.items():\n-                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n-                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n-                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n-                    logging_loss = tr_loss\n-\n-                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n-                    # Save model checkpoint\n-                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n-                    if not os.path.exists(output_dir):\n-                        os.makedirs(output_dir)\n-                    model_to_save = (\n-                        model.module if hasattr(model, \"module\") else model\n-                    )  # Take care of distributed/parallel training\n-                    model_to_save.save_pretrained(output_dir)\n-                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n-                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n-\n-            if args.max_steps > 0 and global_step > args.max_steps:\n-                epoch_iterator.close()\n-                break\n-        if args.max_steps > 0 and global_step > args.max_steps:\n-            train_iterator.close()\n-            break\n-\n-    if args.local_rank in [-1, 0]:\n-        tb_writer.close()\n-\n-    return global_step, tr_loss / global_step\n-\n-\n-def evaluate(args, model, tokenizer, prefix=\"\", output_layer=-1, eval_highway=False):\n-    # Loop to handle MNLI double evaluation (matched, mis-matched)\n-    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n-    eval_outputs_dirs = (args.output_dir, args.output_dir + \"-MM\") if args.task_name == \"mnli\" else (args.output_dir,)\n-\n-    results = {}\n-    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n-        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n-\n-        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n-            os.makedirs(eval_output_dir)\n-\n-        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n-        # Note that DistributedSampler samples randomly\n-        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n-        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n-\n-        # multi-gpu eval\n-        if args.n_gpu > 1:\n-            model = nn.DataParallel(model)\n-\n-        # Eval!\n-        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n-        logger.info(\"  Num examples = %d\", len(eval_dataset))\n-        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n-        eval_loss = 0.0\n-        nb_eval_steps = 0\n-        preds = None\n-        out_label_ids = None\n-        exit_layer_counter = {(i + 1): 0 for i in range(model.num_layers)}\n-        st = time.time()\n-        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n-            model.eval()\n-            batch = tuple(t.to(args.device) for t in batch)\n-\n-            with torch.no_grad():\n-                inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n-                if args.model_type != \"distilbert\":\n-                    inputs[\"token_type_ids\"] = (\n-                        batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n-                    )  # XLM, DistilBERT and RoBERTa don't use segment_ids\n-                if output_layer >= 0:\n-                    inputs[\"output_layer\"] = output_layer\n-                outputs = model(**inputs)\n-                if eval_highway:\n-                    exit_layer_counter[outputs[-1]] += 1\n-                tmp_eval_loss, logits = outputs[:2]\n-\n-                eval_loss += tmp_eval_loss.mean().item()\n-            nb_eval_steps += 1\n-            if preds is None:\n-                preds = logits.detach().cpu().numpy()\n-                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n-            else:\n-                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n-                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n-        eval_time = time.time() - st\n-        logger.info(\"Eval time: {}\".format(eval_time))\n-\n-        eval_loss = eval_loss / nb_eval_steps\n-        if args.output_mode == \"classification\":\n-            preds = np.argmax(preds, axis=1)\n-        elif args.output_mode == \"regression\":\n-            preds = np.squeeze(preds)\n-        result = compute_metrics(eval_task, preds, out_label_ids)\n-        results.update(result)\n-\n-        if eval_highway:\n-            logger.info(\"Exit layer counter: {}\".format(exit_layer_counter))\n-            actual_cost = sum([l * c for l, c in exit_layer_counter.items()])\n-            full_cost = len(eval_dataloader) * model.num_layers\n-            logger.info(\"Expected saving: {}\".format(actual_cost / full_cost))\n-            if args.early_exit_entropy >= 0:\n-                save_fname = (\n-                    args.plot_data_dir\n-                    + \"/\"\n-                    + args.model_name_or_path[2:]\n-                    + \"/entropy_{}.npy\".format(args.early_exit_entropy)\n-                )\n-                if not os.path.exists(os.path.dirname(save_fname)):\n-                    os.makedirs(os.path.dirname(save_fname))\n-                print_result = get_wanted_result(result)\n-                np.save(save_fname, np.array([exit_layer_counter, eval_time, actual_cost / full_cost, print_result]))\n-                logger.info(\"Entropy={}\\tResult={:.2f}\".format(args.early_exit_entropy, 100 * print_result))\n-\n-        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n-        with open(output_eval_file, \"w\") as writer:\n-            logger.info(\"***** Eval results {} *****\".format(prefix))\n-            for key in sorted(result.keys()):\n-                logger.info(\"  %s = %s\", key, str(result[key]))\n-                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n-\n-    return results\n-\n-\n-def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n-    if args.local_rank not in [-1, 0] and not evaluate:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-\n-    processor = processors[task]()\n-    output_mode = output_modes[task]\n-    # Load data features from cache or dataset file\n-    cached_features_file = os.path.join(\n-        args.data_dir,\n-        \"cached_{}_{}_{}_{}\".format(\n-            \"dev\" if evaluate else \"train\",\n-            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n-            str(args.max_seq_length),\n-            str(task),\n-        ),\n-    )\n-    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file)\n-    else:\n-        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n-        label_list = processor.get_labels()\n-        if task in [\"mnli\", \"mnli-mm\"] and args.model_type in [\"roberta\"]:\n-            # HACK(label indices are swapped in RoBERTa pretrained model)\n-            label_list[1], label_list[2] = label_list[2], label_list[1]\n-        examples = (\n-            processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n-        )\n-        features = convert_examples_to_features(\n-            examples,\n-            tokenizer,\n-            label_list=label_list,\n-            max_length=args.max_seq_length,\n-            output_mode=output_mode,\n-        )\n-        if args.local_rank in [-1, 0]:\n-            logger.info(\"Saving features into cached file %s\", cached_features_file)\n-            torch.save(features, cached_features_file)\n-\n-    if args.local_rank == 0 and not evaluate:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-\n-    # Convert to Tensors and build dataset\n-    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n-    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n-\n-    if features[0].token_type_ids is None:\n-        # For RoBERTa (a potential bug!)\n-        all_token_type_ids = torch.tensor([[0] * args.max_seq_length for f in features], dtype=torch.long)\n-    else:\n-        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n-    if output_mode == \"classification\":\n-        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n-    elif output_mode == \"regression\":\n-        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n-\n-    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n-    return dataset\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-\n-    # Required parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n-    )\n-    parser.add_argument(\n-        \"--model_type\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pre-trained model or shortcut name.\",\n-    )\n-    parser.add_argument(\n-        \"--task_name\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model predictions and checkpoints will be written.\",\n-    )\n-    parser.add_argument(\n-        \"--plot_data_dir\",\n-        default=\"./plotting/\",\n-        type=str,\n-        required=False,\n-        help=\"The directory to store data for plotting figures.\",\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\n-        \"--cache_dir\",\n-        default=\"\",\n-        type=str,\n-        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n-    )\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=128,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after tokenization. Sequences longer \"\n-            \"than this will be truncated, sequences shorter will be padded.\"\n-        ),\n-    )\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n-    parser.add_argument(\n-        \"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\"\n-    )\n-    parser.add_argument(\n-        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n-    )\n-    parser.add_argument(\"--eval_each_highway\", action=\"store_true\", help=\"Set this flag to evaluate each highway.\")\n-    parser.add_argument(\n-        \"--eval_after_first_stage\",\n-        action=\"store_true\",\n-        help=\"Set this flag to evaluate after training only bert (not highway).\",\n-    )\n-    parser.add_argument(\"--eval_highway\", action=\"store_true\", help=\"Set this flag if it's evaluating highway models\")\n-\n-    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n-    parser.add_argument(\n-        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n-    )\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n-    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n-    parser.add_argument(\n-        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n-    )\n-    parser.add_argument(\n-        \"--max_steps\",\n-        default=-1,\n-        type=int,\n-        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n-    )\n-    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-    parser.add_argument(\"--early_exit_entropy\", default=-1, type=float, help=\"Entropy threshold for early exit.\")\n-\n-    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n-    parser.add_argument(\"--save_steps\", type=int, default=50, help=\"Save checkpoint every X updates steps.\")\n-    parser.add_argument(\n-        \"--eval_all_checkpoints\",\n-        action=\"store_true\",\n-        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n-    )\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n-    )\n-    parser.add_argument(\n-        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O1\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n-    args = parser.parse_args()\n-\n-    if (\n-        os.path.exists(args.output_dir)\n-        and os.listdir(args.output_dir)\n-        and args.do_train\n-        and not args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n-                args.output_dir\n-            )\n-        )\n-\n-    # Setup distant debugging if needed\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup CUDA, GPU & distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = torch.cuda.device_count()\n-    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n-        torch.cuda.set_device(args.local_rank)\n-        device = torch.device(\"cuda\", args.local_rank)\n-        torch.distributed.init_process_group(backend=\"nccl\")\n-        args.n_gpu = 1\n-    args.device = device\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        args.local_rank,\n-        device,\n-        args.n_gpu,\n-        bool(args.local_rank != -1),\n-        args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    # Set seed\n-    set_seed(args)\n-\n-    # Prepare GLUE task\n-    args.task_name = args.task_name.lower()\n-    if args.task_name not in processors:\n-        raise ValueError(\"Task not found: %s\" % (args.task_name))\n-    processor = processors[args.task_name]()\n-    args.output_mode = output_modes[args.task_name]\n-    label_list = processor.get_labels()\n-    num_labels = len(label_list)\n-\n-    # Load pretrained model and tokenizer\n-    if args.local_rank not in [-1, 0]:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n-\n-    args.model_type = args.model_type.lower()\n-    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n-    config = config_class.from_pretrained(\n-        args.config_name if args.config_name else args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=args.task_name,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    tokenizer = tokenizer_class.from_pretrained(\n-        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n-        do_lower_case=args.do_lower_case,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    model = model_class.from_pretrained(\n-        args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n-        config=config,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-\n-    if args.model_type == \"bert\":\n-        model.bert.encoder.set_early_exit_entropy(args.early_exit_entropy)\n-        model.bert.init_highway_pooler()\n-    elif args.model_type == \"roberta\":\n-        model.roberta.encoder.set_early_exit_entropy(args.early_exit_entropy)\n-        model.roberta.init_highway_pooler()\n-    else:\n-        raise NotImplementedError()\n-\n-    if args.local_rank == 0:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n-\n-    model.to(args.device)\n-\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Training\n-    if args.do_train:\n-        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n-        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n-\n-        if args.eval_after_first_stage:\n-            result = evaluate(args, model, tokenizer, prefix=\"\")\n-            print_result = get_wanted_result(result)\n-\n-        train(args, train_dataset, model, tokenizer, train_highway=True)\n-\n-    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n-    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n-        # Create output directory if needed\n-        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n-            os.makedirs(args.output_dir)\n-\n-        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n-        # They can then be reloaded using `from_pretrained()`\n-        model_to_save = (\n-            model.module if hasattr(model, \"module\") else model\n-        )  # Take care of distributed/parallel training\n-        model_to_save.save_pretrained(args.output_dir)\n-        tokenizer.save_pretrained(args.output_dir)\n-\n-        # Good practice: save your training arguments together with the trained model\n-        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n-\n-        # Load a trained model and vocabulary that you have fine-tuned\n-        model = model_class.from_pretrained(args.output_dir)\n-        tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n-        model.to(args.device)\n-\n-    # Evaluation\n-    results = {}\n-    if args.do_eval and args.local_rank in [-1, 0]:\n-        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n-        checkpoints = [args.output_dir]\n-        if args.eval_all_checkpoints:\n-            checkpoints = [\n-                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n-            ]\n-\n-        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n-        for checkpoint in checkpoints:\n-            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n-            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n-\n-            model = model_class.from_pretrained(checkpoint)\n-            if args.model_type == \"bert\":\n-                model.bert.encoder.set_early_exit_entropy(args.early_exit_entropy)\n-            elif args.model_type == \"roberta\":\n-                model.roberta.encoder.set_early_exit_entropy(args.early_exit_entropy)\n-            else:\n-                raise NotImplementedError()\n-\n-            model.to(args.device)\n-            result = evaluate(args, model, tokenizer, prefix=prefix, eval_highway=args.eval_highway)\n-            print_result = get_wanted_result(result)\n-            logger.info(\"Result: {}\".format(print_result))\n-            if args.eval_each_highway:\n-                last_layer_results = print_result\n-                each_layer_results = []\n-                for i in range(model.num_layers):\n-                    logger.info(\"\\n\")\n-                    _result = evaluate(\n-                        args, model, tokenizer, prefix=prefix, output_layer=i, eval_highway=args.eval_highway\n-                    )\n-                    if i + 1 < model.num_layers:\n-                        each_layer_results.append(get_wanted_result(_result))\n-                each_layer_results.append(last_layer_results)\n-                save_fname = args.plot_data_dir + \"/\" + args.model_name_or_path[2:] + \"/each_layer.npy\"\n-                if not os.path.exists(os.path.dirname(save_fname)):\n-                    os.makedirs(os.path.dirname(save_fname))\n-                np.save(save_fname, np.array(each_layer_results))\n-                info_str = \"Score of each layer:\"\n-                for i in range(model.num_layers):\n-                    info_str += \" {:.2f}\".format(100 * each_layer_results[i])\n-                logger.info(info_str)\n-            result = {k + \"_{}\".format(global_step): v for k, v in result.items()}\n-            results.update(result)\n-\n-    return results\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/research_projects/deebert/src/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "b866ef0869c7586abb7ff5173f0f0bb8b63034a1",
            "filename": "examples/research_projects/deebert/src/modeling_highway_bert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 397,
            "changes": 397,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2Fmodeling_highway_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2Fmodeling_highway_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2Fmodeling_highway_bert.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,397 +0,0 @@\n-import torch\n-from torch import nn\n-from torch.nn import CrossEntropyLoss, MSELoss\n-\n-from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n-from transformers.models.bert.modeling_bert import (\n-    BERT_INPUTS_DOCSTRING,\n-    BERT_START_DOCSTRING,\n-    BertEmbeddings,\n-    BertLayer,\n-    BertPooler,\n-    BertPreTrainedModel,\n-)\n-\n-\n-def entropy(x):\n-    \"\"\"Calculate entropy of a pre-softmax logit Tensor\"\"\"\n-    exp_x = torch.exp(x)\n-    A = torch.sum(exp_x, dim=1)  # sum of exp(x_i)\n-    B = torch.sum(x * exp_x, dim=1)  # sum of x_i * exp(x_i)\n-    return torch.log(A) - B / A\n-\n-\n-class DeeBertEncoder(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.output_attentions = config.output_attentions\n-        self.output_hidden_states = config.output_hidden_states\n-        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.highway = nn.ModuleList([BertHighway(config) for _ in range(config.num_hidden_layers)])\n-\n-        self.early_exit_entropy = [-1 for _ in range(config.num_hidden_layers)]\n-\n-    def set_early_exit_entropy(self, x):\n-        if isinstance(x, (float, int)):\n-            for i in range(len(self.early_exit_entropy)):\n-                self.early_exit_entropy[i] = x\n-        else:\n-            self.early_exit_entropy = x\n-\n-    def init_highway_pooler(self, pooler):\n-        loaded_model = pooler.state_dict()\n-        for highway in self.highway:\n-            for name, param in highway.pooler.state_dict().items():\n-                param.copy_(loaded_model[name])\n-\n-    def forward(\n-        self,\n-        hidden_states,\n-        attention_mask=None,\n-        head_mask=None,\n-        encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n-    ):\n-        all_hidden_states = ()\n-        all_attentions = ()\n-        all_highway_exits = ()\n-        for i, layer_module in enumerate(self.layer):\n-            if self.output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            layer_outputs = layer_module(\n-                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n-            )\n-            hidden_states = layer_outputs[0]\n-\n-            if self.output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-            current_outputs = (hidden_states,)\n-            if self.output_hidden_states:\n-                current_outputs = current_outputs + (all_hidden_states,)\n-            if self.output_attentions:\n-                current_outputs = current_outputs + (all_attentions,)\n-\n-            highway_exit = self.highway[i](current_outputs)\n-            # logits, pooled_output\n-\n-            if not self.training:\n-                highway_logits = highway_exit[0]\n-                highway_entropy = entropy(highway_logits)\n-                highway_exit = highway_exit + (highway_entropy,)  # logits, hidden_states(?), entropy\n-                all_highway_exits = all_highway_exits + (highway_exit,)\n-\n-                if highway_entropy < self.early_exit_entropy[i]:\n-                    new_output = (highway_logits,) + current_outputs[1:] + (all_highway_exits,)\n-                    raise HighwayException(new_output, i + 1)\n-            else:\n-                all_highway_exits = all_highway_exits + (highway_exit,)\n-\n-        # Add last layer\n-        if self.output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        outputs = (hidden_states,)\n-        if self.output_hidden_states:\n-            outputs = outputs + (all_hidden_states,)\n-        if self.output_attentions:\n-            outputs = outputs + (all_attentions,)\n-\n-        outputs = outputs + (all_highway_exits,)\n-        return outputs  # last-layer hidden state, (all hidden states), (all attentions), all highway exits\n-\n-\n-@add_start_docstrings(\n-    \"The Bert Model transformer with early exiting (DeeBERT). \",\n-    BERT_START_DOCSTRING,\n-)\n-class DeeBertModel(BertPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.config = config\n-\n-        self.embeddings = BertEmbeddings(config)\n-        self.encoder = DeeBertEncoder(config)\n-        self.pooler = BertPooler(config)\n-\n-        self.init_weights()\n-\n-    def init_highway_pooler(self):\n-        self.encoder.init_highway_pooler(self.pooler)\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings.word_embeddings\n-\n-    def set_input_embeddings(self, value):\n-        self.embeddings.word_embeddings = value\n-\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"Prunes heads of the model.\n-        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n-        See base class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n-    ):\n-        r\"\"\"\n-        Return:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n-            last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n-                Sequence of hidden-states at the output of the last layer of the model.\n-            pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):\n-                Last layer hidden-state of the first token of the sequence (classification token)\n-                further processed by a Linear layer and a Tanh activation function. The Linear\n-                layer weights are trained from the next sentence prediction (classification)\n-                objective during pre-training.\n-\n-                This output is usually *not* a good summary\n-                of the semantic content of the input, you're often better with averaging or pooling\n-                the sequence of hidden-states for the whole input sequence.\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-            highway_exits (:obj:`tuple(tuple(torch.Tensor))`:\n-                Tuple of each early exit's results (total length: number of layers)\n-                Each tuple is again, a tuple of length 2 - the first entry is logits and the second entry is hidden states.\n-        \"\"\"\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        if encoder_attention_mask is None:\n-            encoder_attention_mask = torch.ones(input_shape, device=device)\n-        if token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n-\n-        # If a 2D ou 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if encoder_attention_mask.dim() == 3:\n-            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n-        if encoder_attention_mask.dim() == 2:\n-            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n-\n-        encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n-            dtype=next(self.parameters()).dtype\n-        )  # fp16 compatibility\n-        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n-\n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n-\n-        embedding_output = self.embeddings(\n-            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n-        )\n-        encoder_outputs = self.encoder(\n-            embedding_output,\n-            attention_mask=extended_attention_mask,\n-            head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n-        )\n-        sequence_output = encoder_outputs[0]\n-        pooled_output = self.pooler(sequence_output)\n-\n-        outputs = (\n-            sequence_output,\n-            pooled_output,\n-        ) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n-        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions), highway exits\n-\n-\n-class HighwayException(Exception):\n-    def __init__(self, message, exit_layer):\n-        self.message = message\n-        self.exit_layer = exit_layer  # start from 1!\n-\n-\n-class BertHighway(nn.Module):\n-    \"\"\"A module to provide a shortcut\n-    from (the output of one non-final BertLayer in BertEncoder) to (cross-entropy computation in BertForSequenceClassification)\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.pooler = BertPooler(config)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n-\n-    def forward(self, encoder_outputs):\n-        # Pooler\n-        pooler_input = encoder_outputs[0]\n-        pooler_output = self.pooler(pooler_input)\n-        # \"return\" pooler_output\n-\n-        # BertModel\n-        bmodel_output = (pooler_input, pooler_output) + encoder_outputs[1:]\n-        # \"return\" bmodel_output\n-\n-        # Dropout and classification\n-        pooled_output = bmodel_output[1]\n-\n-        pooled_output = self.dropout(pooled_output)\n-        logits = self.classifier(pooled_output)\n-\n-        return logits, pooled_output\n-\n-\n-@add_start_docstrings(\n-    \"\"\"Bert Model (with early exiting - DeeBERT) with a classifier on top,\n-    also takes care of multi-layer training. \"\"\",\n-    BERT_START_DOCSTRING,\n-)\n-class DeeBertForSequenceClassification(BertPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.num_layers = config.num_hidden_layers\n-\n-        self.bert = DeeBertModel(config)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n-\n-        self.init_weights()\n-\n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        labels=None,\n-        output_layer=-1,\n-        train_highway=False,\n-    ):\n-        r\"\"\"\n-            labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n-                Labels for computing the sequence classification/regression loss.\n-                Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n-                If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n-                If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-\n-        Returns:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n-            loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n-                Classification (or regression if config.num_labels==1) loss.\n-            logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n-                Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-            highway_exits (:obj:`tuple(tuple(torch.Tensor))`:\n-                Tuple of each early exit's results (total length: number of layers)\n-                Each tuple is again, a tuple of length 2 - the first entry is logits and the second entry is hidden states.\n-        \"\"\"\n-\n-        exit_layer = self.num_layers\n-        try:\n-            outputs = self.bert(\n-                input_ids,\n-                attention_mask=attention_mask,\n-                token_type_ids=token_type_ids,\n-                position_ids=position_ids,\n-                head_mask=head_mask,\n-                inputs_embeds=inputs_embeds,\n-            )\n-            # sequence_output, pooled_output, (hidden_states), (attentions), highway exits\n-\n-            pooled_output = outputs[1]\n-\n-            pooled_output = self.dropout(pooled_output)\n-            logits = self.classifier(pooled_output)\n-            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n-        except HighwayException as e:\n-            outputs = e.message\n-            exit_layer = e.exit_layer\n-            logits = outputs[0]\n-\n-        if not self.training:\n-            original_entropy = entropy(logits)\n-            highway_entropy = []\n-            highway_logits_all = []\n-        if labels is not None:\n-            if self.num_labels == 1:\n-                #  We are doing regression\n-                loss_fct = MSELoss()\n-                loss = loss_fct(logits.view(-1), labels.view(-1))\n-            else:\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-\n-            # work with highway exits\n-            highway_losses = []\n-            for highway_exit in outputs[-1]:\n-                highway_logits = highway_exit[0]\n-                if not self.training:\n-                    highway_logits_all.append(highway_logits)\n-                    highway_entropy.append(highway_exit[2])\n-                if self.num_labels == 1:\n-                    #  We are doing regression\n-                    loss_fct = MSELoss()\n-                    highway_loss = loss_fct(highway_logits.view(-1), labels.view(-1))\n-                else:\n-                    loss_fct = CrossEntropyLoss()\n-                    highway_loss = loss_fct(highway_logits.view(-1, self.num_labels), labels.view(-1))\n-                highway_losses.append(highway_loss)\n-\n-            if train_highway:\n-                outputs = (sum(highway_losses[:-1]),) + outputs\n-                # exclude the final highway, of course\n-            else:\n-                outputs = (loss,) + outputs\n-        if not self.training:\n-            outputs = outputs + ((original_entropy, highway_entropy), exit_layer)\n-            if output_layer >= 0:\n-                outputs = (\n-                    (outputs[0],) + (highway_logits_all[output_layer],) + outputs[2:]\n-                )  # use the highway of the last layer\n-\n-        return outputs  # (loss), logits, (hidden_states), (attentions), (highway_exits)"
        },
        {
            "sha": "c21fb32fde762a8269f1f5b78b0e51e07b17f606",
            "filename": "examples/research_projects/deebert/src/modeling_highway_roberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 154,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2Fmodeling_highway_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2Fmodeling_highway_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Fsrc%2Fmodeling_highway_roberta.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,154 +0,0 @@\n-from __future__ import absolute_import, division, print_function, unicode_literals\n-\n-from torch import nn\n-from torch.nn import CrossEntropyLoss, MSELoss\n-\n-from transformers import RobertaConfig\n-from transformers.file_utils import add_start_docstrings, add_start_docstrings_to_model_forward\n-from transformers.models.roberta.modeling_roberta import (\n-    ROBERTA_INPUTS_DOCSTRING,\n-    ROBERTA_START_DOCSTRING,\n-    RobertaEmbeddings,\n-)\n-\n-from .modeling_highway_bert import BertPreTrainedModel, DeeBertModel, HighwayException, entropy\n-\n-\n-@add_start_docstrings(\n-    \"The RoBERTa Model transformer with early exiting (DeeRoBERTa). \",\n-    ROBERTA_START_DOCSTRING,\n-)\n-class DeeRobertaModel(DeeBertModel):\n-    config_class = RobertaConfig\n-    base_model_prefix = \"roberta\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-\n-        self.embeddings = RobertaEmbeddings(config)\n-        self.init_weights()\n-\n-\n-@add_start_docstrings(\n-    \"\"\"RoBERTa Model (with early exiting - DeeRoBERTa) with a classifier on top,\n-    also takes care of multi-layer training. \"\"\",\n-    ROBERTA_START_DOCSTRING,\n-)\n-class DeeRobertaForSequenceClassification(BertPreTrainedModel):\n-    config_class = RobertaConfig\n-    base_model_prefix = \"roberta\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.num_layers = config.num_hidden_layers\n-\n-        self.roberta = DeeRobertaModel(config)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n-\n-    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING)\n-    def forward(\n-        self,\n-        input_ids=None,\n-        attention_mask=None,\n-        token_type_ids=None,\n-        position_ids=None,\n-        head_mask=None,\n-        inputs_embeds=None,\n-        labels=None,\n-        output_layer=-1,\n-        train_highway=False,\n-    ):\n-        r\"\"\"\n-            labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n-                Labels for computing the sequence classification/regression loss.\n-                Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n-                If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n-                If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-\n-        Returns:\n-            :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:\n-            loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n-                Classification (or regression if config.num_labels==1) loss.\n-            logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n-                Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-            hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n-                of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n-\n-                Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-            attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n-                Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n-                :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n-\n-                Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-                heads.\n-            highway_exits (:obj:`tuple(tuple(torch.Tensor))`:\n-                Tuple of each early exit's results (total length: number of layers)\n-                Each tuple is again, a tuple of length 2 - the first entry is logits and the second entry is hidden states.\n-        \"\"\"\n-\n-        exit_layer = self.num_layers\n-        try:\n-            outputs = self.roberta(\n-                input_ids,\n-                attention_mask=attention_mask,\n-                token_type_ids=token_type_ids,\n-                position_ids=position_ids,\n-                head_mask=head_mask,\n-                inputs_embeds=inputs_embeds,\n-            )\n-\n-            pooled_output = outputs[1]\n-\n-            pooled_output = self.dropout(pooled_output)\n-            logits = self.classifier(pooled_output)\n-            outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n-        except HighwayException as e:\n-            outputs = e.message\n-            exit_layer = e.exit_layer\n-            logits = outputs[0]\n-\n-        if not self.training:\n-            original_entropy = entropy(logits)\n-            highway_entropy = []\n-            highway_logits_all = []\n-        if labels is not None:\n-            if self.num_labels == 1:\n-                #  We are doing regression\n-                loss_fct = MSELoss()\n-                loss = loss_fct(logits.view(-1), labels.view(-1))\n-            else:\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-\n-            # work with highway exits\n-            highway_losses = []\n-            for highway_exit in outputs[-1]:\n-                highway_logits = highway_exit[0]\n-                if not self.training:\n-                    highway_logits_all.append(highway_logits)\n-                    highway_entropy.append(highway_exit[2])\n-                if self.num_labels == 1:\n-                    #  We are doing regression\n-                    loss_fct = MSELoss()\n-                    highway_loss = loss_fct(highway_logits.view(-1), labels.view(-1))\n-                else:\n-                    loss_fct = CrossEntropyLoss()\n-                    highway_loss = loss_fct(highway_logits.view(-1, self.num_labels), labels.view(-1))\n-                highway_losses.append(highway_loss)\n-\n-            if train_highway:\n-                outputs = (sum(highway_losses[:-1]),) + outputs\n-                # exclude the final highway, of course\n-            else:\n-                outputs = (loss,) + outputs\n-        if not self.training:\n-            outputs = outputs + ((original_entropy, highway_entropy), exit_layer)\n-            if output_layer >= 0:\n-                outputs = (\n-                    (outputs[0],) + (highway_logits_all[output_layer],) + outputs[2:]\n-                )  # use the highway of the last layer\n-\n-        return outputs  # (loss), logits, (hidden_states), (attentions), entropy"
        },
        {
            "sha": "7a5f059c8cedff8cea2874f9dd521c02ee97300e",
            "filename": "examples/research_projects/deebert/test_glue_deebert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 104,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Ftest_glue_deebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Ftest_glue_deebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Ftest_glue_deebert.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,104 +0,0 @@\n-import argparse\n-import logging\n-import sys\n-from unittest.mock import patch\n-\n-import run_glue_deebert\n-\n-from transformers.testing_utils import TestCasePlus, get_gpu_count, require_torch_non_multi_gpu, slow\n-\n-\n-logging.basicConfig(level=logging.DEBUG)\n-\n-logger = logging.getLogger()\n-\n-\n-def get_setup_file():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"-f\")\n-    args = parser.parse_args()\n-    return args.f\n-\n-\n-class DeeBertTests(TestCasePlus):\n-    def setup(self) -> None:\n-        stream_handler = logging.StreamHandler(sys.stdout)\n-        logger.addHandler(stream_handler)\n-\n-    def run_and_check(self, args):\n-        n_gpu = get_gpu_count()\n-\n-        if n_gpu > 1:\n-            pass\n-            # XXX: doesn't quite work with n_gpu > 1 https://github.com/huggingface/transformers/issues/10560\n-            # script = f\"{self.examples_dir_str}/research_projects/deebert/run_glue_deebert.py\"\n-            # distributed_args = f\"-m torch.distributed.launch --nproc_per_node={n_gpu} {script}\".split()\n-            # cmd = [sys.executable] + distributed_args + args\n-            # execute_subprocess_async(cmd, env=self.get_env())\n-            # XXX: test the results - need to save them first into .json file\n-        else:\n-            args.insert(0, \"run_glue_deebert.py\")\n-            with patch.object(sys, \"argv\", args):\n-                result = run_glue_deebert.main()\n-                for value in result.values():\n-                    self.assertGreaterEqual(value, 0.666)\n-\n-    @slow\n-    @require_torch_non_multi_gpu\n-    def test_glue_deebert_train(self):\n-        train_args = \"\"\"\n-            --model_type roberta\n-            --model_name_or_path FacebookAI/roberta-base\n-            --task_name MRPC\n-            --do_train\n-            --do_eval\n-            --do_lower_case\n-            --data_dir ./tests/fixtures/tests_samples/MRPC/\n-            --max_seq_length 128\n-            --per_gpu_eval_batch_size=1\n-            --per_gpu_train_batch_size=8\n-            --learning_rate 2e-4\n-            --num_train_epochs 3\n-            --overwrite_output_dir\n-            --seed 42\n-            --output_dir ./examples/deebert/saved_models/FacebookAI/roberta-base/MRPC/two_stage\n-            --plot_data_dir ./examples/deebert/results/\n-            --save_steps 0\n-            --overwrite_cache\n-            --eval_after_first_stage\n-            \"\"\".split()\n-        self.run_and_check(train_args)\n-\n-        eval_args = \"\"\"\n-            --model_type roberta\n-            --model_name_or_path ./examples/deebert/saved_models/FacebookAI/roberta-base/MRPC/two_stage\n-            --task_name MRPC\n-            --do_eval\n-            --do_lower_case\n-            --data_dir ./tests/fixtures/tests_samples/MRPC/\n-            --output_dir ./examples/deebert/saved_models/FacebookAI/roberta-base/MRPC/two_stage\n-            --plot_data_dir ./examples/deebert/results/\n-            --max_seq_length 128\n-            --eval_each_highway\n-            --eval_highway\n-            --overwrite_cache\n-            --per_gpu_eval_batch_size=1\n-            \"\"\".split()\n-        self.run_and_check(eval_args)\n-\n-        entropy_eval_args = \"\"\"\n-            --model_type roberta\n-            --model_name_or_path ./examples/deebert/saved_models/FacebookAI/roberta-base/MRPC/two_stage\n-            --task_name MRPC\n-            --do_eval\n-            --do_lower_case\n-            --data_dir ./tests/fixtures/tests_samples/MRPC/\n-            --output_dir ./examples/deebert/saved_models/FacebookAI/roberta-base/MRPC/two_stage\n-            --plot_data_dir ./examples/deebert/results/\n-            --max_seq_length 128\n-            --early_exit_entropy 0.1\n-            --eval_highway\n-            --overwrite_cache\n-            --per_gpu_eval_batch_size=1\n-            \"\"\".split()\n-        self.run_and_check(entropy_eval_args)"
        },
        {
            "sha": "32cdf5730f204e63ba9acf22c9d71656b701741a",
            "filename": "examples/research_projects/deebert/train_deebert.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Ftrain_deebert.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdeebert%2Ftrain_deebert.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdeebert%2Ftrain_deebert.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,38 +0,0 @@\n-#!/bin/bash\n-export CUDA_VISIBLE_DEVICES=0\n-\n-PATH_TO_DATA=/h/xinji/projects/GLUE\n-\n-MODEL_TYPE=bert  # bert or roberta\n-MODEL_SIZE=base  # base or large\n-DATASET=MRPC  # SST-2, MRPC, RTE, QNLI, QQP, or MNLI\n-\n-MODEL_NAME=${MODEL_TYPE}-${MODEL_SIZE}\n-EPOCHS=10\n-if [ $MODEL_TYPE = 'bert' ]\n-then\n-  EPOCHS=3\n-  MODEL_NAME=${MODEL_NAME}-uncased\n-fi\n-\n-\n-python -u run_glue_deebert.py \\\n-  --model_type $MODEL_TYPE \\\n-  --model_name_or_path $MODEL_NAME \\\n-  --task_name $DATASET \\\n-  --do_train \\\n-  --do_eval \\\n-  --do_lower_case \\\n-  --data_dir $PATH_TO_DATA/$DATASET \\\n-  --max_seq_length 128 \\\n-  --per_gpu_eval_batch_size=1 \\\n-  --per_gpu_train_batch_size=8 \\\n-  --learning_rate 2e-5 \\\n-  --num_train_epochs $EPOCHS \\\n-  --overwrite_output_dir \\\n-  --seed 42 \\\n-  --output_dir ./saved_models/${MODEL_TYPE}-${MODEL_SIZE}/$DATASET/two_stage \\\n-  --plot_data_dir ./results/ \\\n-  --save_steps 0 \\\n-  --overwrite_cache \\\n-  --eval_after_first_stage"
        },
        {
            "sha": "594e953f99d76d09235cf430118d19a47edf170a",
            "filename": "examples/research_projects/distillation/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 193,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,193 +0,0 @@\n-# Distil*\n-\n-Author: @VictorSanh\n-\n-This folder contains the original code used to train Distil* as well as examples showcasing how to use DistilBERT, DistilRoBERTa and DistilGPT2.\n-\n-**January 20, 2020 - Bug fixing** We have recently discovered and fixed [a bug](https://github.com/huggingface/transformers/commit/48cbf267c988b56c71a2380f748a3e6092ccaed3) in the evaluation of our `run_*.py` scripts that caused the reported metrics to be over-estimated on average. We have updated all the metrics with the latest runs.\n-\n-**December 6, 2019 - Update** We release **DistilmBERT**: 92% of `bert-base-multilingual-cased` on XNLI. The model supports 104 different languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\n-\n-**November 19, 2019 - Update** We release German **DistilBERT**: 98.8% of `bert-base-german-dbmdz-cased` on NER tasks.\n-\n-**October 23, 2019 - Update** We release **DistilRoBERTa**: 95% of `RoBERTa-base`'s performance on GLUE, twice as fast as RoBERTa while being 35% smaller.\n-\n-**October 3, 2019 - Update** We release our [NeurIPS workshop paper](https://arxiv.org/abs/1910.01108) explaining our approach on **DistilBERT**. It includes updated results and further experiments. We applied the same method to GPT2 and release the weights of **DistilGPT2**. DistilGPT2 is two times faster and 33% smaller than GPT2. **The paper supersedes our [previous blogpost](https://medium.com/huggingface/distilbert-8cf3380435b5) with a different distillation loss and better performances. Please use the paper as a reference when comparing/reporting results on DistilBERT.**\n-\n-**September 19, 2019 - Update:** We fixed bugs in the code and released an updated version of the weights trained with a modification of the distillation loss. DistilBERT now reaches 99% of `BERT-base`'s performance on GLUE, and 86.9 F1 score on SQuAD v1.1 dev set (compared to 88.5 for `BERT-base`). We will publish a formal write-up of our approach in the near future!\n-\n-\n-## What is Distil*\n-\n-Distil* is a class of compressed models that started with DistilBERT. DistilBERT stands for Distilled-BERT. DistilBERT is a small, fast, cheap and light Transformer model based on Bert architecture. It has 40% less parameters than `bert-base-uncased`, runs 60% faster while preserving 97% of BERT's performances as measured on the GLUE language understanding benchmark. DistilBERT is trained using knowledge distillation, a technique to compress a large model called the teacher into a smaller model called the student. By distillating Bert, we obtain a smaller Transformer model that bears a lot of similarities with the original BERT model while being lighter, smaller and faster to run. DistilBERT is thus an interesting option to put large-scaled trained Transformer model into production.\n-\n-We have applied the same method to other Transformer architectures and released the weights:\n-- GPT2: on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT2 reaches a perplexity on the test set of 16.3 compared to 21.1 for **DistilGPT2** (after fine-tuning on the train set).\n-- RoBERTa: **DistilRoBERTa** reaches 95% of `RoBERTa-base`'s performance on GLUE while being twice faster and 35% smaller.\n-- German BERT: **German DistilBERT** reaches 99% of `bert-base-german-dbmdz-cased`'s performance on German NER (CoNLL-2003).\n-- Multilingual BERT: **DistilmBERT** reaches 92% of Multilingual BERT's performance on XNLI while being twice faster and 25% smaller. The model supports 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\n-\n-For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https://arxiv.org/abs/1910.01108).\n-\n-Here are the results on the dev sets of GLUE:\n-\n-| Model                     | Macro-score                    | CoLA | MNLI | MRPC | QNLI | QQP  | RTE  | SST-2| STS-B| WNLI              |\n-| :---:                     |    :---:                       | :---:| :---:| :---:| :---:| :---:| :---:| :---:| :---:| :---:             |\n-| BERT-base-uncased         |  **79.5**                      | 56.3 | 84.7 | 88.6 | 91.8 | 89.6 | 69.3 | 92.7 | 89.0 | 53.5              |\n-| DistilBERT-base-uncased   |  **77.0**                      | 51.3 | 82.1 | 87.5 | 89.2 | 88.5 | 59.9 | 91.3 | 86.9 | 56.3              |\n-| BERT-base-cased           |  **78.2**                      | 58.2 | 83.9 | 87.8 | 91.0 | 89.2 | 66.1 | 91.7 | 89.2 | 46.5              |\n-| DistilBERT-base-cased     |  **75.9**                      | 47.2 | 81.5 | 85.6 | 88.2 | 87.8 | 60.6 | 90.4 | 85.5 | 56.3              |\n-| ---                       |    ---                         |  --- |  --- |  --- |  --- |  --- |  --- |  --- |  --- |  ---              |\n-| RoBERTa-base (reported)   |  **83.2**/**86.4**<sup>2</sup> | 63.6 | 87.6 | 90.2 | 92.8 | 91.9 | 78.7 | 94.8 | 91.2 | 57.7<sup>3</sup>  |\n-| DistilRoBERTa<sup>1</sup> |  **79.0**/**82.3**<sup>2</sup> | 59.3 | 84.0 | 86.6 | 90.8 | 89.4 | 67.9 | 92.5 | 88.3 | 52.1              |\n-\n-<sup>1</sup> We did not use the MNLI checkpoint for fine-tuning but directly perform transfer learning on the pre-trained DistilRoBERTa.\n-\n-<sup>2</sup> Macro-score computed without WNLI.\n-\n-<sup>3</sup> We compute this score ourselves for completeness.\n-\n-Here are the results on the *test* sets for 6 of the languages available in XNLI. The results are computed in the zero shot setting (trained on the English portion and evaluated on the target language portion):\n-\n-| Model                        | English | Spanish | Chinese | German | Arabic  | Urdu |\n-| :---:                        | :---:   | :---:   | :---:   | :---:  | :---:   | :---:|\n-| mBERT base cased (computed)  | 82.1    | 74.6    | 69.1    | 72.3   | 66.4    | 58.5 |\n-| mBERT base uncased (reported)| 81.4    | 74.3    | 63.8    | 70.5   | 62.1    | 58.3 |\n-| DistilmBERT                  | 78.2    | 69.1    | 64.0    | 66.3   | 59.1    | 54.7 |\n-\n-## Setup\n-\n-This part of the library has only be tested with Python3.6+. There are few specific dependencies to install before launching a distillation, you can install them with the command `pip install -r requirements.txt`.\n-\n-**Important note:** The training scripts have been updated to support PyTorch v1.2.0 (there are breaking changes compared to v1.1.0).\n-\n-\n-## How to use DistilBERT\n-\n-Transformers includes five pre-trained Distil* models, currently only provided for English and German (we are investigating the possibility to train and release a multilingual version of DistilBERT):\n-\n-- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\n-- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).\n-- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.\n-- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).\n-- `distilbert-base-german-cased`: DistilBERT German language model pretrained on 1/2 of the data used to pretrain Bert using distillation with the supervision of the `bert-base-german-dbmdz-cased` version of German DBMDZ Bert. For NER tasks the model reaches a F1 score of 83.49 on the CoNLL-2003 test set (for comparison, `bert-base-german-dbmdz-cased` reaches a 84.52 F1 score), and a F1 score of 85.23 on the GermEval 2014 test set (`bert-base-german-dbmdz-cased` reaches a 86.89 F1 score).\n-- `distilgpt2`: DistilGPT2 English language model pretrained with the supervision of `gpt2` (the smallest version of GPT2) on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset. The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 124M parameters for GPT2). On average, DistilGPT2 is two times faster than GPT2.\n-- `distilroberta-base`: DistilRoBERTa English language model pretrained with the supervision of `roberta-base` solely on [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), a reproduction of OpenAI's WebText dataset (it is ~4 times less training data than the teacher RoBERTa). The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). On average DistilRoBERTa is twice as fast as Roberta-base.\n-- `distilbert-base-multilingual-cased`: DistilmBERT multilingual model pretrained with the supervision of `bert-base-multilingual-cased` on the concatenation of Wikipedia in 104 different languages. The model supports the 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages). The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base). On average DistilmBERT is twice as fast as mBERT-base.\n-\n-Using DistilBERT is very similar to using BERT. DistilBERT share the same tokenizer as BERT's `bert-base-uncased` even though we provide a link to this tokenizer under the `DistilBertTokenizer` name to have a consistent naming between the library models.\n-\n-```python\n-tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n-model = DistilBertModel.from_pretrained('distilbert-base-cased')\n-\n-input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)\n-outputs = model(input_ids)\n-last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n-```\n-\n-Similarly, using the other Distil* models simply consists in calling the base classes with a different pretrained checkpoint:\n-- DistilBERT uncased: `model = DistilBertModel.from_pretrained('distilbert-base-uncased')`\n-- DistilGPT2: `model = GPT2Model.from_pretrained('distilgpt2')`\n-- DistilRoBERTa: `model = RobertaModel.from_pretrained('distilroberta-base')`\n-- DistilmBERT: `model = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')`\n-\n-\n-## How to train Distil*\n-\n-In the following, we will explain how you can train DistilBERT.\n-\n-### A. Preparing the data\n-\n-The weights we release are trained using a concatenation of Toronto Book Corpus and English Wikipedia (same training data as the English version of BERT).\n-\n-To avoid processing the data several time, we do it once and for all before the training. From now on, will suppose that you have a text file `dump.txt` which contains one sequence per line (a sequence being composed of one of several coherent sentences).\n-\n-First, we will binarize the data, i.e. tokenize the data and convert each token in an index in our model's vocabulary.\n-\n-```bash\n-python scripts/binarized_data.py \\\n-    --file_path data/dump.txt \\\n-    --tokenizer_type bert \\\n-    --tokenizer_name bert-base-uncased \\\n-    --dump_file data/binarized_text\n-```\n-\n-Our implementation of masked language modeling loss follows [XLM](https://github.com/facebookresearch/XLM)'s one and smooths the probability of masking with a factor that put more emphasis on rare words. Thus we count the occurrences of each tokens in the data:\n-\n-```bash\n-python scripts/token_counts.py \\\n-    --data_file data/binarized_text.bert-base-uncased.pickle \\\n-    --token_counts_dump data/token_counts.bert-base-uncased.pickle \\\n-    --vocab_size 30522\n-```\n-\n-### B. Training\n-\n-Training with distillation is really simple once you have pre-processed the data:\n-\n-```bash\n-python train.py \\\n-    --student_type distilbert \\\n-    --student_config training_configs/distilbert-base-uncased.json \\\n-    --teacher_type bert \\\n-    --teacher_name bert-base-uncased \\\n-    --alpha_ce 5.0 --alpha_mlm 2.0 --alpha_cos 1.0 --alpha_clm 0.0 --mlm \\\n-    --freeze_pos_embs \\\n-    --dump_path serialization_dir/my_first_training \\\n-    --data_file data/binarized_text.bert-base-uncased.pickle \\\n-    --token_counts data/token_counts.bert-base-uncased.pickle \\\n-    --force # overwrites the `dump_path` if it already exists.\n-```\n-\n-By default, this will launch a training on a single GPU (even if more are available on the cluster). Other parameters are available in the command line, please look in `train.py` or run `python train.py --help` to list them.\n-\n-We highly encourage you to use distributed training for training DistilBERT as the training corpus is quite large. Here's an example that runs a distributed training on a single node having 4 GPUs:\n-\n-```bash\n-export NODE_RANK=0\n-export N_NODES=1\n-\n-export N_GPU_NODE=4\n-export WORLD_SIZE=4\n-export MASTER_PORT=<AN_OPEN_PORT>\n-export MASTER_ADDR=<I.P.>\n-\n-pkill -f 'python -u train.py'\n-\n-python -m torch.distributed.launch \\\n-    --nproc_per_node=$N_GPU_NODE \\\n-    --nnodes=$N_NODES \\\n-    --node_rank $NODE_RANK \\\n-    --master_addr $MASTER_ADDR \\\n-    --master_port $MASTER_PORT \\\n-    train.py \\\n-        --force \\\n-        --n_gpu $WORLD_SIZE \\\n-        --student_type distilbert \\\n-        --student_config training_configs/distilbert-base-uncased.json \\\n-        --teacher_type bert \\\n-        --teacher_name bert-base-uncased \\\n-        --alpha_ce 0.33 --alpha_mlm 0.33 --alpha_cos 0.33 --alpha_clm 0.0 --mlm \\\n-        --freeze_pos_embs \\\n-        --dump_path serialization_dir/my_first_training \\\n-        --data_file data/binarized_text.bert-base-uncased.pickle \\\n-        --token_counts data/token_counts.bert-base-uncased.pickle\n-```\n-\n-**Tips:** Starting distilled training with good initialization of the model weights is crucial to reach decent performance. In our experiments, we initialized our model from a few layers of the teacher (Bert) itself! Please refer to `scripts/extract.py` and `scripts/extract_distilbert.py` to create a valid initialization checkpoint and use `--student_pretrained_weights` argument to use this initialization for the distilled training!\n-\n-Happy distillation!\n-\n-## Citation\n-\n-If you find the resource useful, you should cite the following paper:\n-\n-```bibtex\n-@inproceedings{sanh2019distilbert,\n-  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n-  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n-  booktitle={NeurIPS EMC^2 Workshop},\n-  year={2019}\n-}\n-```"
        },
        {
            "sha": "963af976f5a4a9e8b207e6a11e3b31b6cdb278a2",
            "filename": "examples/research_projects/distillation/distiller.py",
            "status": "removed",
            "additions": 0,
            "deletions": 601,
            "changes": 601,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fdistiller.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fdistiller.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Fdistiller.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,601 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"The distiller to distil the student.\n-Adapted in part from Facebook, Inc XLM model (https://github.com/facebookresearch/XLM)\n-\"\"\"\n-\n-import math\n-import os\n-import time\n-\n-import psutil\n-import torch\n-from grouped_batch_sampler import GroupedBatchSampler, create_lengths_groups\n-from lm_seqs_dataset import LmSeqsDataset\n-from torch import nn\n-from torch.optim import AdamW\n-from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm\n-\n-from transformers import get_linear_schedule_with_warmup\n-from utils import logger\n-\n-\n-try:\n-    from torch.utils.tensorboard import SummaryWriter\n-except ImportError:\n-    from tensorboardX import SummaryWriter\n-\n-\n-class Distiller:\n-    def __init__(\n-        self, params: dict, dataset: LmSeqsDataset, token_probs: torch.tensor, student: nn.Module, teacher: nn.Module\n-    ):\n-        logger.info(\"Initializing Distiller\")\n-        self.params = params\n-        self.dump_path = params.dump_path\n-        self.multi_gpu = params.multi_gpu\n-        self.fp16 = params.fp16\n-\n-        self.student = student\n-        self.teacher = teacher\n-\n-        self.student_config = student.config\n-        self.vocab_size = student.config.vocab_size\n-\n-        if params.n_gpu <= 1:\n-            sampler = RandomSampler(dataset)\n-        else:\n-            sampler = DistributedSampler(dataset)\n-\n-        if params.group_by_size:\n-            groups = create_lengths_groups(lengths=dataset.lengths, k=params.max_model_input_size)\n-            sampler = GroupedBatchSampler(sampler=sampler, group_ids=groups, batch_size=params.batch_size)\n-        else:\n-            sampler = BatchSampler(sampler=sampler, batch_size=params.batch_size, drop_last=False)\n-\n-        self.dataloader = DataLoader(dataset=dataset, batch_sampler=sampler, collate_fn=dataset.batch_sequences)\n-\n-        self.temperature = params.temperature\n-        assert self.temperature > 0.0\n-\n-        self.alpha_ce = params.alpha_ce\n-        self.alpha_mlm = params.alpha_mlm\n-        self.alpha_clm = params.alpha_clm\n-        self.alpha_mse = params.alpha_mse\n-        self.alpha_cos = params.alpha_cos\n-\n-        self.mlm = params.mlm\n-        if self.mlm:\n-            logger.info(\"Using MLM loss for LM step.\")\n-            self.mlm_mask_prop = params.mlm_mask_prop\n-            assert 0.0 <= self.mlm_mask_prop <= 1.0\n-            assert params.word_mask + params.word_keep + params.word_rand == 1.0\n-            self.pred_probs = torch.FloatTensor([params.word_mask, params.word_keep, params.word_rand])\n-            self.pred_probs = self.pred_probs.to(f\"cuda:{params.local_rank}\") if params.n_gpu > 0 else self.pred_probs\n-            self.token_probs = token_probs.to(f\"cuda:{params.local_rank}\") if params.n_gpu > 0 else token_probs\n-            if self.fp16:\n-                self.pred_probs = self.pred_probs.half()\n-                self.token_probs = self.token_probs.half()\n-        else:\n-            logger.info(\"Using CLM loss for LM step.\")\n-\n-        self.epoch = 0\n-        self.n_iter = 0\n-        self.n_total_iter = 0\n-        self.n_sequences_epoch = 0\n-        self.total_loss_epoch = 0\n-        self.last_loss = 0\n-        self.last_loss_ce = 0\n-        self.last_loss_mlm = 0\n-        self.last_loss_clm = 0\n-        if self.alpha_mse > 0.0:\n-            self.last_loss_mse = 0\n-        if self.alpha_cos > 0.0:\n-            self.last_loss_cos = 0\n-        self.last_log = 0\n-\n-        self.ce_loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n-        self.lm_loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n-        if self.alpha_mse > 0.0:\n-            self.mse_loss_fct = nn.MSELoss(reduction=\"sum\")\n-        if self.alpha_cos > 0.0:\n-            self.cosine_loss_fct = nn.CosineEmbeddingLoss(reduction=\"mean\")\n-\n-        logger.info(\"--- Initializing model optimizer\")\n-        assert params.gradient_accumulation_steps >= 1\n-        self.num_steps_epoch = len(self.dataloader)\n-        num_train_optimization_steps = (\n-            int(self.num_steps_epoch / params.gradient_accumulation_steps * params.n_epoch) + 1\n-        )\n-\n-        no_decay = [\"bias\", \"LayerNorm.weight\"]\n-        optimizer_grouped_parameters = [\n-            {\n-                \"params\": [\n-                    p for n, p in student.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n-                ],\n-                \"weight_decay\": params.weight_decay,\n-            },\n-            {\n-                \"params\": [\n-                    p for n, p in student.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n-                ],\n-                \"weight_decay\": 0.0,\n-            },\n-        ]\n-        logger.info(\n-            \"------ Number of trainable parameters (student): %i\"\n-            % sum([p.numel() for p in self.student.parameters() if p.requires_grad])\n-        )\n-        logger.info(\"------ Number of parameters (student): %i\" % sum([p.numel() for p in self.student.parameters()]))\n-        self.optimizer = AdamW(\n-            optimizer_grouped_parameters, lr=params.learning_rate, eps=params.adam_epsilon, betas=(0.9, 0.98)\n-        )\n-\n-        warmup_steps = math.ceil(num_train_optimization_steps * params.warmup_prop)\n-        self.scheduler = get_linear_schedule_with_warmup(\n-            self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_optimization_steps\n-        )\n-\n-        if self.fp16:\n-            try:\n-                from apex import amp\n-            except ImportError:\n-                raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-            logger.info(f\"Using fp16 training: {self.params.fp16_opt_level} level\")\n-            self.student, self.optimizer = amp.initialize(\n-                self.student, self.optimizer, opt_level=self.params.fp16_opt_level\n-            )\n-            self.teacher = self.teacher.half()\n-\n-        if self.multi_gpu:\n-            if self.fp16:\n-                from apex.parallel import DistributedDataParallel\n-\n-                logger.info(\"Using apex.parallel.DistributedDataParallel for distributed training.\")\n-                self.student = DistributedDataParallel(self.student)\n-            else:\n-                from torch.nn.parallel import DistributedDataParallel\n-\n-                logger.info(\"Using nn.parallel.DistributedDataParallel for distributed training.\")\n-                self.student = DistributedDataParallel(\n-                    self.student,\n-                    device_ids=[params.local_rank],\n-                    output_device=params.local_rank,\n-                    find_unused_parameters=True,\n-                )\n-\n-        self.is_master = params.is_master\n-        if self.is_master:\n-            logger.info(\"--- Initializing Tensorboard\")\n-            self.tensorboard = SummaryWriter(log_dir=os.path.join(self.dump_path, \"log\", \"train\"))\n-            self.tensorboard.add_text(tag=\"config/training\", text_string=str(self.params), global_step=0)\n-            self.tensorboard.add_text(tag=\"config/student\", text_string=str(self.student_config), global_step=0)\n-\n-    def prepare_batch_mlm(self, batch):\n-        \"\"\"\n-        Prepare the batch: from the token_ids and the lengths, compute the attention mask and the masked label for MLM.\n-\n-        Input:\n-        ------\n-            batch: `Tuple`\n-                token_ids: `torch.tensor(bs, seq_length)` - The token ids for each of the sequence. It is padded.\n-                lengths: `torch.tensor(bs)` - The lengths of each of the sequences in the batch.\n-\n-        Output:\n-        -------\n-            token_ids: `torch.tensor(bs, seq_length)` - The token ids after the modifications for MLM.\n-            attn_mask: `torch.tensor(bs, seq_length)` - The attention mask for the self-attention.\n-            mlm_labels: `torch.tensor(bs, seq_length)` - The masked language modeling labels. There is a -100 where there is nothing to predict.\n-        \"\"\"\n-        token_ids, lengths = batch\n-        token_ids, lengths = self.round_batch(x=token_ids, lengths=lengths)\n-        assert token_ids.size(0) == lengths.size(0)\n-\n-        attn_mask = torch.arange(token_ids.size(1), dtype=torch.long, device=lengths.device) < lengths[:, None]\n-\n-        bs, max_seq_len = token_ids.size()\n-        mlm_labels = token_ids.new(token_ids.size()).copy_(token_ids)\n-\n-        x_prob = self.token_probs[token_ids.flatten()]\n-        n_tgt = math.ceil(self.mlm_mask_prop * lengths.sum().item())\n-        tgt_ids = torch.multinomial(x_prob / x_prob.sum(), n_tgt, replacement=False)\n-        pred_mask = torch.zeros(\n-            bs * max_seq_len, dtype=torch.bool, device=token_ids.device\n-        )  # previously `dtype=torch.uint8`, cf pytorch 1.2.0 compatibility\n-        pred_mask[tgt_ids] = 1\n-        pred_mask = pred_mask.view(bs, max_seq_len)\n-\n-        pred_mask[token_ids == self.params.special_tok_ids[\"pad_token\"]] = 0\n-\n-        # mask a number of words == 0 [8] (faster with fp16)\n-        if self.fp16:\n-            n1 = pred_mask.sum().item()\n-            if n1 > 8:\n-                pred_mask = pred_mask.view(-1)\n-                n2 = max(n1 % 8, 8 * (n1 // 8))\n-                if n2 != n1:\n-                    pred_mask[torch.nonzero(pred_mask).view(-1)[: n1 - n2]] = 0\n-                pred_mask = pred_mask.view(bs, max_seq_len)\n-                assert pred_mask.sum().item() % 8 == 0, pred_mask.sum().item()\n-\n-        _token_ids_real = token_ids[pred_mask]\n-        _token_ids_rand = _token_ids_real.clone().random_(self.vocab_size)\n-        _token_ids_mask = _token_ids_real.clone().fill_(self.params.special_tok_ids[\"mask_token\"])\n-        probs = torch.multinomial(self.pred_probs, len(_token_ids_real), replacement=True)\n-        _token_ids = (\n-            _token_ids_mask * (probs == 0).long()\n-            + _token_ids_real * (probs == 1).long()\n-            + _token_ids_rand * (probs == 2).long()\n-        )\n-        token_ids = token_ids.masked_scatter(pred_mask, _token_ids)\n-\n-        mlm_labels[~pred_mask] = -100  # previously `mlm_labels[1-pred_mask] = -1`, cf pytorch 1.2.0 compatibility\n-\n-        # sanity checks\n-        assert 0 <= token_ids.min() <= token_ids.max() < self.vocab_size\n-\n-        return token_ids, attn_mask, mlm_labels\n-\n-    def prepare_batch_clm(self, batch):\n-        \"\"\"\n-        Prepare the batch: from the token_ids and the lengths, compute the attention mask and the labels for CLM.\n-\n-        Input:\n-        ------\n-            batch: `Tuple`\n-                token_ids: `torch.tensor(bs, seq_length)` - The token ids for each of the sequence. It is padded.\n-                lengths: `torch.tensor(bs)` - The lengths of each of the sequences in the batch.\n-\n-        Output:\n-        -------\n-            token_ids: `torch.tensor(bs, seq_length)` - The token ids after the modifications for MLM.\n-            attn_mask: `torch.tensor(bs, seq_length)` - The attention mask for the self-attention.\n-            clm_labels: `torch.tensor(bs, seq_length)` - The causal language modeling labels. There is a -100 where there is nothing to predict.\n-        \"\"\"\n-        token_ids, lengths = batch\n-        token_ids, lengths = self.round_batch(x=token_ids, lengths=lengths)\n-        assert token_ids.size(0) == lengths.size(0)\n-\n-        attn_mask = torch.arange(token_ids.size(1), dtype=torch.long, device=lengths.device) < lengths[:, None]\n-        clm_labels = token_ids.new(token_ids.size()).copy_(token_ids)\n-        clm_labels[~attn_mask] = -100  # previously `clm_labels[1-attn_mask] = -1`, cf pytorch 1.2.0 compatibility\n-\n-        # sanity checks\n-        assert 0 <= token_ids.min() <= token_ids.max() < self.vocab_size\n-\n-        return token_ids, attn_mask, clm_labels\n-\n-    def round_batch(self, x: torch.tensor, lengths: torch.tensor):\n-        \"\"\"\n-        For float16 only.\n-        Sub-sample sentences in a batch, and add padding, so that each dimension is a multiple of 8.\n-\n-        Input:\n-        ------\n-            x: `torch.tensor(bs, seq_length)` - The token ids.\n-            lengths: `torch.tensor(bs, seq_length)` - The lengths of each of the sequence in the batch.\n-\n-        Output:\n-        -------\n-            x:  `torch.tensor(new_bs, new_seq_length)` - The updated token ids.\n-            lengths: `torch.tensor(new_bs, new_seq_length)` - The updated lengths.\n-        \"\"\"\n-        if not self.fp16 or len(lengths) < 8:\n-            return x, lengths\n-\n-        # number of sentences == 0 [8]\n-        bs1 = len(lengths)\n-        bs2 = 8 * (bs1 // 8)\n-        assert bs2 > 0 and bs2 % 8 == 0\n-        if bs1 != bs2:\n-            idx = torch.randperm(bs1)[:bs2]\n-            lengths = lengths[idx]\n-            slen = lengths.max().item()\n-            x = x[idx, :slen]\n-        else:\n-            idx = None\n-\n-        # sequence length == 0 [8]\n-        ml1 = x.size(1)\n-        if ml1 % 8 != 0:\n-            pad = 8 - (ml1 % 8)\n-            ml2 = ml1 + pad\n-            if self.mlm:\n-                pad_id = self.params.special_tok_ids[\"pad_token\"]\n-            else:\n-                pad_id = self.params.special_tok_ids[\"unk_token\"]\n-            padding_tensor = torch.zeros(bs2, pad, dtype=torch.long, device=x.device).fill_(pad_id)\n-            x = torch.cat([x, padding_tensor], 1)\n-            assert x.size() == (bs2, ml2)\n-\n-        assert x.size(0) % 8 == 0\n-        assert x.size(1) % 8 == 0\n-        return x, lengths\n-\n-    def train(self):\n-        \"\"\"\n-        The real training loop.\n-        \"\"\"\n-        if self.is_master:\n-            logger.info(\"Starting training\")\n-        self.last_log = time.time()\n-        self.student.train()\n-        self.teacher.eval()\n-\n-        for _ in range(self.params.n_epoch):\n-            if self.is_master:\n-                logger.info(f\"--- Starting epoch {self.epoch}/{self.params.n_epoch-1}\")\n-            if self.multi_gpu:\n-                torch.distributed.barrier()\n-\n-            iter_bar = tqdm(self.dataloader, desc=\"-Iter\", disable=self.params.local_rank not in [-1, 0])\n-            for batch in iter_bar:\n-                if self.params.n_gpu > 0:\n-                    batch = tuple(t.to(f\"cuda:{self.params.local_rank}\") for t in batch)\n-\n-                if self.mlm:\n-                    token_ids, attn_mask, lm_labels = self.prepare_batch_mlm(batch=batch)\n-                else:\n-                    token_ids, attn_mask, lm_labels = self.prepare_batch_clm(batch=batch)\n-                self.step(input_ids=token_ids, attention_mask=attn_mask, lm_labels=lm_labels)\n-\n-                iter_bar.update()\n-                iter_bar.set_postfix(\n-                    {\"Last_loss\": f\"{self.last_loss:.2f}\", \"Avg_cum_loss\": f\"{self.total_loss_epoch/self.n_iter:.2f}\"}\n-                )\n-            iter_bar.close()\n-\n-            if self.is_master:\n-                logger.info(f\"--- Ending epoch {self.epoch}/{self.params.n_epoch-1}\")\n-            self.end_epoch()\n-\n-        if self.is_master:\n-            logger.info(\"Save very last checkpoint as `pytorch_model.bin`.\")\n-            self.save_checkpoint(checkpoint_name=\"pytorch_model.bin\")\n-            logger.info(\"Training is finished\")\n-\n-    def step(self, input_ids: torch.tensor, attention_mask: torch.tensor, lm_labels: torch.tensor):\n-        \"\"\"\n-        One optimization step: forward of student AND teacher, backward on the loss (for gradient accumulation),\n-        and possibly a parameter update (depending on the gradient accumulation).\n-\n-        Input:\n-        ------\n-        input_ids: `torch.tensor(bs, seq_length)` - The token ids.\n-        attention_mask: `torch.tensor(bs, seq_length)` - The attention mask for self attention.\n-        lm_labels: `torch.tensor(bs, seq_length)` - The language modeling labels (mlm labels for MLM and clm labels for CLM).\n-        \"\"\"\n-        if self.mlm:\n-            student_outputs = self.student(\n-                input_ids=input_ids, attention_mask=attention_mask\n-            )  # (bs, seq_length, voc_size)\n-            with torch.no_grad():\n-                teacher_outputs = self.teacher(\n-                    input_ids=input_ids, attention_mask=attention_mask\n-                )  # (bs, seq_length, voc_size)\n-        else:\n-            student_outputs = self.student(input_ids=input_ids, attention_mask=None)  # (bs, seq_length, voc_size)\n-            with torch.no_grad():\n-                teacher_outputs = self.teacher(input_ids=input_ids, attention_mask=None)  # (bs, seq_length, voc_size)\n-        s_logits, s_hidden_states = student_outputs[\"logits\"], student_outputs[\"hidden_states\"]\n-        t_logits, t_hidden_states = teacher_outputs[\"logits\"], teacher_outputs[\"hidden_states\"]\n-        assert s_logits.size() == t_logits.size()\n-\n-        # https://github.com/peterliht/knowledge-distillation-pytorch/blob/master/model/net.py#L100\n-        # https://github.com/peterliht/knowledge-distillation-pytorch/issues/2\n-        if self.params.restrict_ce_to_mask:\n-            mask = (lm_labels > -1).unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)\n-        else:\n-            mask = attention_mask.unsqueeze(-1).expand_as(s_logits)  # (bs, seq_length, voc_size)\n-        s_logits_slct = torch.masked_select(s_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n-        s_logits_slct = s_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n-        t_logits_slct = torch.masked_select(t_logits, mask)  # (bs * seq_length * voc_size) modulo the 1s in mask\n-        t_logits_slct = t_logits_slct.view(-1, s_logits.size(-1))  # (bs * seq_length, voc_size) modulo the 1s in mask\n-        assert t_logits_slct.size() == s_logits_slct.size()\n-\n-        loss_ce = (\n-            self.ce_loss_fct(\n-                nn.functional.log_softmax(s_logits_slct / self.temperature, dim=-1),\n-                nn.functional.softmax(t_logits_slct / self.temperature, dim=-1),\n-            )\n-            * (self.temperature) ** 2\n-        )\n-        loss = self.alpha_ce * loss_ce\n-\n-        if self.alpha_mlm > 0.0:\n-            loss_mlm = self.lm_loss_fct(s_logits.view(-1, s_logits.size(-1)), lm_labels.view(-1))\n-            loss += self.alpha_mlm * loss_mlm\n-        if self.alpha_clm > 0.0:\n-            shift_logits = s_logits[..., :-1, :].contiguous()\n-            shift_labels = lm_labels[..., 1:].contiguous()\n-            loss_clm = self.lm_loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n-            loss += self.alpha_clm * loss_clm\n-\n-        if self.alpha_mse > 0.0:\n-            loss_mse = self.mse_loss_fct(s_logits_slct, t_logits_slct) / s_logits_slct.size(\n-                0\n-            )  # Reproducing batchmean reduction\n-            loss += self.alpha_mse * loss_mse\n-        if self.alpha_cos > 0.0:\n-            s_hidden_states = s_hidden_states[-1]  # (bs, seq_length, dim)\n-            t_hidden_states = t_hidden_states[-1]  # (bs, seq_length, dim)\n-            mask = attention_mask.unsqueeze(-1).expand_as(s_hidden_states)  # (bs, seq_length, dim)\n-            assert s_hidden_states.size() == t_hidden_states.size()\n-            dim = s_hidden_states.size(-1)\n-\n-            s_hidden_states_slct = torch.masked_select(s_hidden_states, mask)  # (bs * seq_length * dim)\n-            s_hidden_states_slct = s_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n-            t_hidden_states_slct = torch.masked_select(t_hidden_states, mask)  # (bs * seq_length * dim)\n-            t_hidden_states_slct = t_hidden_states_slct.view(-1, dim)  # (bs * seq_length, dim)\n-\n-            target = s_hidden_states_slct.new(s_hidden_states_slct.size(0)).fill_(1)  # (bs * seq_length,)\n-            loss_cos = self.cosine_loss_fct(s_hidden_states_slct, t_hidden_states_slct, target)\n-            loss += self.alpha_cos * loss_cos\n-\n-        self.total_loss_epoch += loss.item()\n-        self.last_loss = loss.item()\n-        self.last_loss_ce = loss_ce.item()\n-        if self.alpha_mlm > 0.0:\n-            self.last_loss_mlm = loss_mlm.item()\n-        if self.alpha_clm > 0.0:\n-            self.last_loss_clm = loss_clm.item()\n-        if self.alpha_mse > 0.0:\n-            self.last_loss_mse = loss_mse.item()\n-        if self.alpha_cos > 0.0:\n-            self.last_loss_cos = loss_cos.item()\n-\n-        self.optimize(loss)\n-\n-        self.n_sequences_epoch += input_ids.size(0)\n-\n-    def optimize(self, loss):\n-        \"\"\"\n-        Normalization on the loss (gradient accumulation or distributed training), followed by\n-        backward pass on the loss, possibly followed by a parameter update (depending on the gradient accumulation).\n-        Also update the metrics for tensorboard.\n-        \"\"\"\n-        # Check for NaN\n-        if (loss != loss).data.any():\n-            logger.error(\"NaN detected\")\n-            exit()\n-\n-        if self.multi_gpu:\n-            loss = loss.mean()\n-        if self.params.gradient_accumulation_steps > 1:\n-            loss = loss / self.params.gradient_accumulation_steps\n-\n-        if self.fp16:\n-            from apex import amp\n-\n-            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n-                scaled_loss.backward()\n-        else:\n-            loss.backward()\n-\n-        self.iter()\n-        if self.n_iter % self.params.gradient_accumulation_steps == 0:\n-            if self.fp16:\n-                nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), self.params.max_grad_norm)\n-            else:\n-                nn.utils.clip_grad_norm_(self.student.parameters(), self.params.max_grad_norm)\n-            self.optimizer.step()\n-            self.optimizer.zero_grad()\n-            self.scheduler.step()\n-\n-    def iter(self):\n-        \"\"\"\n-        Update global counts, write to tensorboard and save checkpoint.\n-        \"\"\"\n-        self.n_iter += 1\n-        self.n_total_iter += 1\n-\n-        if self.n_total_iter % self.params.log_interval == 0:\n-            self.log_tensorboard()\n-            self.last_log = time.time()\n-        if self.n_total_iter % self.params.checkpoint_interval == 0:\n-            self.save_checkpoint()\n-\n-    def log_tensorboard(self):\n-        \"\"\"\n-        Log into tensorboard. Only by the master process.\n-        \"\"\"\n-        if not self.is_master:\n-            return\n-\n-        for param_name, param in self.student.named_parameters():\n-            self.tensorboard.add_scalar(\n-                tag=\"parameter_mean/\" + param_name, scalar_value=param.data.mean(), global_step=self.n_total_iter\n-            )\n-            self.tensorboard.add_scalar(\n-                tag=\"parameter_std/\" + param_name, scalar_value=param.data.std(), global_step=self.n_total_iter\n-            )\n-            if param.grad is None:\n-                continue\n-            self.tensorboard.add_scalar(\n-                tag=\"grad_mean/\" + param_name, scalar_value=param.grad.data.mean(), global_step=self.n_total_iter\n-            )\n-            self.tensorboard.add_scalar(\n-                tag=\"grad_std/\" + param_name, scalar_value=param.grad.data.std(), global_step=self.n_total_iter\n-            )\n-\n-        self.tensorboard.add_scalar(\n-            tag=\"losses/cum_avg_loss_epoch\",\n-            scalar_value=self.total_loss_epoch / self.n_iter,\n-            global_step=self.n_total_iter,\n-        )\n-        self.tensorboard.add_scalar(tag=\"losses/loss\", scalar_value=self.last_loss, global_step=self.n_total_iter)\n-        self.tensorboard.add_scalar(\n-            tag=\"losses/loss_ce\", scalar_value=self.last_loss_ce, global_step=self.n_total_iter\n-        )\n-        if self.alpha_mlm > 0.0:\n-            self.tensorboard.add_scalar(\n-                tag=\"losses/loss_mlm\", scalar_value=self.last_loss_mlm, global_step=self.n_total_iter\n-            )\n-        if self.alpha_clm > 0.0:\n-            self.tensorboard.add_scalar(\n-                tag=\"losses/loss_clm\", scalar_value=self.last_loss_clm, global_step=self.n_total_iter\n-            )\n-        if self.alpha_mse > 0.0:\n-            self.tensorboard.add_scalar(\n-                tag=\"losses/loss_mse\", scalar_value=self.last_loss_mse, global_step=self.n_total_iter\n-            )\n-        if self.alpha_cos > 0.0:\n-            self.tensorboard.add_scalar(\n-                tag=\"losses/loss_cos\", scalar_value=self.last_loss_cos, global_step=self.n_total_iter\n-            )\n-        self.tensorboard.add_scalar(\n-            tag=\"learning_rate/lr\", scalar_value=self.scheduler.get_lr()[0], global_step=self.n_total_iter\n-        )\n-\n-        self.tensorboard.add_scalar(\n-            tag=\"global/memory_usage\",\n-            scalar_value=psutil.virtual_memory()._asdict()[\"used\"] / 1_000_000,\n-            global_step=self.n_total_iter,\n-        )\n-        self.tensorboard.add_scalar(\n-            tag=\"global/speed\", scalar_value=time.time() - self.last_log, global_step=self.n_total_iter\n-        )\n-\n-    def end_epoch(self):\n-        \"\"\"\n-        Finally arrived at the end of epoch (full pass on dataset).\n-        Do some tensorboard logging and checkpoint saving.\n-        \"\"\"\n-        logger.info(f\"{self.n_sequences_epoch} sequences have been trained during this epoch.\")\n-\n-        if self.is_master:\n-            self.save_checkpoint(checkpoint_name=f\"model_epoch_{self.epoch}.pth\")\n-            self.tensorboard.add_scalar(\n-                tag=\"epoch/loss\", scalar_value=self.total_loss_epoch / self.n_iter, global_step=self.epoch\n-            )\n-\n-        self.epoch += 1\n-        self.n_sequences_epoch = 0\n-        self.n_iter = 0\n-        self.total_loss_epoch = 0\n-\n-    def save_checkpoint(self, checkpoint_name: str = \"checkpoint.pth\"):\n-        \"\"\"\n-        Save the current state. Only by the master process.\n-        \"\"\"\n-        if not self.is_master:\n-            return\n-        mdl_to_save = self.student.module if hasattr(self.student, \"module\") else self.student\n-        mdl_to_save.config.save_pretrained(self.dump_path)\n-        state_dict = mdl_to_save.state_dict()\n-        torch.save(state_dict, os.path.join(self.dump_path, checkpoint_name))"
        },
        {
            "sha": "e25def738a8483ce74ea179a6211e9b429965e6e",
            "filename": "examples/research_projects/distillation/grouped_batch_sampler.py",
            "status": "removed",
            "additions": 0,
            "deletions": 108,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fgrouped_batch_sampler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fgrouped_batch_sampler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Fgrouped_batch_sampler.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,108 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Adapted from PyTorch Vision (https://github.com/pytorch/vision/blob/master/references/detection/group_by_aspect_ratio.py)\"\"\"\n-\n-import bisect\n-import copy\n-from collections import defaultdict\n-\n-import numpy as np\n-from torch.utils.data import BatchSampler, Sampler\n-\n-from utils import logger\n-\n-\n-def _quantize(x, bins):\n-    bins = copy.deepcopy(bins)\n-    bins = sorted(bins)\n-    quantized = [bisect.bisect_right(bins, y) for y in x]\n-    return quantized\n-\n-\n-def create_lengths_groups(lengths, k=0):\n-    bins = np.arange(start=3, stop=k, step=4).tolist() if k > 0 else [10]\n-    groups = _quantize(lengths, bins)\n-    # count number of elements per group\n-    counts = np.unique(groups, return_counts=True)[1]\n-    fbins = [0] + bins + [np.inf]\n-    logger.info(\"Using {} as bins for aspect lengths quantization\".format(fbins))\n-    logger.info(\"Count of instances per bin: {}\".format(counts))\n-    return groups\n-\n-\n-class GroupedBatchSampler(BatchSampler):\n-    \"\"\"\n-    Wraps another sampler to yield a mini-batch of indices.\n-    It enforces that the batch only contain elements from the same group.\n-    It also tries to provide mini-batches which follows an ordering which is\n-    as close as possible to the ordering from the original sampler.\n-    Arguments:\n-        sampler (Sampler): Base sampler.\n-        group_ids (list[int]): If the sampler produces indices in range [0, N),\n-            `group_ids` must be a list of `N` ints which contains the group id of each sample.\n-            The group ids must be a continuous set of integers starting from\n-            0, i.e. they must be in the range [0, num_groups).\n-        batch_size (int): Size of mini-batch.\n-    \"\"\"\n-\n-    def __init__(self, sampler, group_ids, batch_size):\n-        if not isinstance(sampler, Sampler):\n-            raise TypeError(\n-                \"sampler should be an instance of torch.utils.data.Sampler, but got sampler={}\".format(sampler)\n-            )\n-        self.sampler = sampler\n-        self.group_ids = group_ids\n-        self.batch_size = batch_size\n-\n-    def __iter__(self):\n-        buffer_per_group = defaultdict(list)\n-        samples_per_group = defaultdict(list)\n-\n-        num_batches = 0\n-        for idx in self.sampler:\n-            group_id = self.group_ids[idx]\n-            buffer_per_group[group_id].append(idx)\n-            samples_per_group[group_id].append(idx)\n-            if len(buffer_per_group[group_id]) == self.batch_size:\n-                yield buffer_per_group[group_id]  # TODO\n-                num_batches += 1\n-                del buffer_per_group[group_id]\n-            assert len(buffer_per_group[group_id]) < self.batch_size\n-\n-        # now we have run out of elements that satisfy\n-        # the group criteria, let's return the remaining\n-        # elements so that the size of the sampler is\n-        # deterministic\n-        expected_num_batches = len(self)\n-        num_remaining = expected_num_batches - num_batches\n-        if num_remaining > 0:\n-            # for the remaining batches, group the batches by similar lengths\n-            batch_idx = []\n-            for group_id, idxs in sorted(buffer_per_group.items(), key=lambda x: x[0]):\n-                batch_idx.extend(idxs)\n-                if len(batch_idx) >= self.batch_size:\n-                    yield batch_idx[: self.batch_size]\n-                    batch_idx = batch_idx[self.batch_size :]\n-                    num_remaining -= 1\n-            if len(batch_idx) > 0:\n-                yield batch_idx\n-                num_remaining -= 1\n-        assert num_remaining == 0\n-\n-    def __len__(self):\n-        \"\"\"\n-        Return the number of mini-batches rather than the number of samples.\n-        \"\"\"\n-        return (len(self.sampler) + self.batch_size - 1) // self.batch_size"
        },
        {
            "sha": "647c8f464f7eb807cd859a21f33f29bcbb64b502",
            "filename": "examples/research_projects/distillation/lm_seqs_dataset.py",
            "status": "removed",
            "additions": 0,
            "deletions": 167,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Flm_seqs_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Flm_seqs_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Flm_seqs_dataset.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,167 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Dataset to distilled models\n-adapted in part from Facebook, Inc XLM model (https://github.com/facebookresearch/XLM)\n-\"\"\"\n-\n-import numpy as np\n-import torch\n-from torch.utils.data import Dataset\n-\n-from utils import logger\n-\n-\n-class LmSeqsDataset(Dataset):\n-    \"\"\"Custom Dataset wrapping language modeling sequences.\n-\n-    Each sample will be retrieved by indexing the list of token_ids and their corresponding lengths.\n-\n-    Input:\n-    ------\n-        params: `NameSpace` parameters\n-        data: `List[np.array[int]]\n-    \"\"\"\n-\n-    def __init__(self, params, data):\n-        self.params = params\n-\n-        self.token_ids = np.array(data)\n-        self.lengths = np.array([len(t) for t in data])\n-\n-        self.check()\n-        self.remove_long_sequences()\n-        self.remove_empty_sequences()\n-        self.remove_unknown_sequences()\n-        self.check()\n-        self.print_statistics()\n-\n-    def __getitem__(self, index):\n-        return (self.token_ids[index], self.lengths[index])\n-\n-    def __len__(self):\n-        return len(self.lengths)\n-\n-    def check(self):\n-        \"\"\"\n-        Some sanity checks\n-        \"\"\"\n-        assert len(self.token_ids) == len(self.lengths)\n-        assert all(self.lengths[i] == len(self.token_ids[i]) for i in range(len(self.lengths)))\n-\n-    def remove_long_sequences(self):\n-        \"\"\"\n-        Sequences that are too long are split by chunk of max_model_input_size.\n-        \"\"\"\n-        max_len = self.params.max_model_input_size\n-        indices = self.lengths > max_len\n-        logger.info(f\"Splitting {sum(indices)} too long sequences.\")\n-\n-        def divide_chunks(l, n):\n-            return [l[i : i + n] for i in range(0, len(l), n)]\n-\n-        new_tok_ids = []\n-        new_lengths = []\n-        if self.params.mlm:\n-            cls_id, sep_id = self.params.special_tok_ids[\"cls_token\"], self.params.special_tok_ids[\"sep_token\"]\n-        else:\n-            cls_id, sep_id = self.params.special_tok_ids[\"bos_token\"], self.params.special_tok_ids[\"eos_token\"]\n-\n-        for seq_, len_ in zip(self.token_ids, self.lengths):\n-            assert (seq_[0] == cls_id) and (seq_[-1] == sep_id), seq_\n-            if len_ <= max_len:\n-                new_tok_ids.append(seq_)\n-                new_lengths.append(len_)\n-            else:\n-                sub_seqs = []\n-                for sub_s in divide_chunks(seq_, max_len - 2):\n-                    if sub_s[0] != cls_id:\n-                        sub_s = np.insert(sub_s, 0, cls_id)\n-                    if sub_s[-1] != sep_id:\n-                        sub_s = np.insert(sub_s, len(sub_s), sep_id)\n-                    assert len(sub_s) <= max_len\n-                    assert (sub_s[0] == cls_id) and (sub_s[-1] == sep_id), sub_s\n-                    sub_seqs.append(sub_s)\n-\n-                new_tok_ids.extend(sub_seqs)\n-                new_lengths.extend([len(l) for l in sub_seqs])\n-\n-        self.token_ids = np.array(new_tok_ids)\n-        self.lengths = np.array(new_lengths)\n-\n-    def remove_empty_sequences(self):\n-        \"\"\"\n-        Too short sequences are simply removed. This could be tuned.\n-        \"\"\"\n-        init_size = len(self)\n-        indices = self.lengths > 11\n-        self.token_ids = self.token_ids[indices]\n-        self.lengths = self.lengths[indices]\n-        new_size = len(self)\n-        logger.info(f\"Remove {init_size - new_size} too short (<=11 tokens) sequences.\")\n-\n-    def remove_unknown_sequences(self):\n-        \"\"\"\n-        Remove sequences with a (too) high level of unknown tokens.\n-        \"\"\"\n-        if \"unk_token\" not in self.params.special_tok_ids:\n-            return\n-        else:\n-            unk_token_id = self.params.special_tok_ids[\"unk_token\"]\n-        init_size = len(self)\n-        unk_occs = np.array([np.count_nonzero(a == unk_token_id) for a in self.token_ids])\n-        indices = (unk_occs / self.lengths) < 0.5\n-        self.token_ids = self.token_ids[indices]\n-        self.lengths = self.lengths[indices]\n-        new_size = len(self)\n-        logger.info(f\"Remove {init_size - new_size} sequences with a high level of unknown tokens (50%).\")\n-\n-    def print_statistics(self):\n-        \"\"\"\n-        Print some statistics on the corpus. Only the master process.\n-        \"\"\"\n-        if not self.params.is_master:\n-            return\n-        logger.info(f\"{len(self)} sequences\")\n-        # data_len = sum(self.lengths)\n-        # nb_unique_tokens = len(Counter(list(chain(*self.token_ids))))\n-        # logger.info(f'{data_len} tokens ({nb_unique_tokens} unique)')\n-\n-        # unk_idx = self.params.special_tok_ids['unk_token']\n-        # nb_unknown = sum([(t==unk_idx).sum() for t in self.token_ids])\n-        # logger.info(f'{nb_unknown} unknown tokens (covering {100*nb_unknown/data_len:.2f}% of the data)')\n-\n-    def batch_sequences(self, batch):\n-        \"\"\"\n-        Do the padding and transform into torch.tensor.\n-        \"\"\"\n-        token_ids = [t[0] for t in batch]\n-        lengths = [t[1] for t in batch]\n-        assert len(token_ids) == len(lengths)\n-\n-        # Max for paddings\n-        max_seq_len_ = max(lengths)\n-\n-        # Pad token ids\n-        if self.params.mlm:\n-            pad_idx = self.params.special_tok_ids[\"pad_token\"]\n-        else:\n-            pad_idx = self.params.special_tok_ids[\"unk_token\"]\n-        tk_ = [list(t.astype(int)) + [pad_idx] * (max_seq_len_ - len(t)) for t in token_ids]\n-        assert len(tk_) == len(token_ids)\n-        assert all(len(t) == max_seq_len_ for t in tk_)\n-\n-        tk_t = torch.tensor(tk_)  # (bs, max_seq_len_)\n-        lg_t = torch.tensor(lengths)  # (bs)\n-        return tk_t, lg_t"
        },
        {
            "sha": "4a2ed783a7c984174a9576f62a254066f03d7a0a",
            "filename": "examples/research_projects/distillation/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,7 +0,0 @@\n-transformers\n-\n-gitpython==3.1.41\n-tensorboard>=1.14.0\n-tensorboardX==1.8\n-psutil==5.6.6\n-scipy>=1.4.1"
        },
        {
            "sha": "a1150f6b437e5976c4f1b55f107d5d7b92a08d99",
            "filename": "examples/research_projects/distillation/run_squad_w_distillation.py",
            "status": "removed",
            "additions": 0,
            "deletions": 877,
            "changes": 877,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Frun_squad_w_distillation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Frun_squad_w_distillation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Frun_squad_w_distillation.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,877 +0,0 @@\n-# coding=utf-8\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"This is the exact same script as `examples/question-answering/run_squad.py` (as of 2020, January 8th) with an additional and optional step of distillation.\"\"\"\n-\n-import argparse\n-import glob\n-import logging\n-import os\n-import random\n-import timeit\n-\n-import numpy as np\n-import torch\n-from torch import nn\n-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm, trange\n-\n-import transformers\n-from transformers import (\n-    WEIGHTS_NAME,\n-    AdamW,\n-    BertConfig,\n-    BertForQuestionAnswering,\n-    BertTokenizer,\n-    DistilBertConfig,\n-    DistilBertForQuestionAnswering,\n-    DistilBertTokenizer,\n-    RobertaConfig,\n-    RobertaForQuestionAnswering,\n-    RobertaTokenizer,\n-    XLMConfig,\n-    XLMForQuestionAnswering,\n-    XLMTokenizer,\n-    XLNetConfig,\n-    XLNetForQuestionAnswering,\n-    XLNetTokenizer,\n-    get_linear_schedule_with_warmup,\n-    squad_convert_examples_to_features,\n-)\n-from transformers.data.metrics.squad_metrics import (\n-    compute_predictions_log_probs,\n-    compute_predictions_logits,\n-    squad_evaluate,\n-)\n-from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n-from transformers.trainer_utils import is_main_process\n-\n-\n-try:\n-    from torch.utils.tensorboard import SummaryWriter\n-except ImportError:\n-    from tensorboardX import SummaryWriter\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-MODEL_CLASSES = {\n-    \"bert\": (BertConfig, BertForQuestionAnswering, BertTokenizer),\n-    \"xlnet\": (XLNetConfig, XLNetForQuestionAnswering, XLNetTokenizer),\n-    \"xlm\": (XLMConfig, XLMForQuestionAnswering, XLMTokenizer),\n-    \"distilbert\": (DistilBertConfig, DistilBertForQuestionAnswering, DistilBertTokenizer),\n-    \"roberta\": (RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer),\n-}\n-\n-\n-def set_seed(args):\n-    random.seed(args.seed)\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    if args.n_gpu > 0:\n-        torch.cuda.manual_seed_all(args.seed)\n-\n-\n-def to_list(tensor):\n-    return tensor.detach().cpu().tolist()\n-\n-\n-def train(args, train_dataset, model, tokenizer, teacher=None):\n-    \"\"\"Train the model\"\"\"\n-    if args.local_rank in [-1, 0]:\n-        tb_writer = SummaryWriter()\n-\n-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n-\n-    if args.max_steps > 0:\n-        t_total = args.max_steps\n-        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n-    else:\n-        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n-\n-    # Prepare optimizer and schedule (linear warmup and decay)\n-    no_decay = [\"bias\", \"LayerNorm.weight\"]\n-    optimizer_grouped_parameters = [\n-        {\n-            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n-            \"weight_decay\": args.weight_decay,\n-        },\n-        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n-    ]\n-    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n-    scheduler = get_linear_schedule_with_warmup(\n-        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n-    )\n-\n-    # Check if saved optimizer or scheduler states exist\n-    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n-        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n-    ):\n-        # Load in optimizer and scheduler states\n-        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n-        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n-\n-    if args.fp16:\n-        try:\n-            from apex import amp\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-\n-        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n-\n-    # multi-gpu training (should be after apex fp16 initialization)\n-    if args.n_gpu > 1:\n-        model = nn.DataParallel(model)\n-\n-    # Distributed training (should be after apex fp16 initialization)\n-    if args.local_rank != -1:\n-        model = nn.parallel.DistributedDataParallel(\n-            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n-        )\n-\n-    # Train!\n-    logger.info(\"***** Running training *****\")\n-    logger.info(\"  Num examples = %d\", len(train_dataset))\n-    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n-    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n-    logger.info(\n-        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n-        args.train_batch_size\n-        * args.gradient_accumulation_steps\n-        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n-    )\n-    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n-    logger.info(\"  Total optimization steps = %d\", t_total)\n-\n-    global_step = 1\n-    epochs_trained = 0\n-    steps_trained_in_current_epoch = 0\n-    # Check if continuing training from a checkpoint\n-    if os.path.exists(args.model_name_or_path):\n-        try:\n-            # set global_step to global_step of last saved checkpoint from model path\n-            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n-            global_step = int(checkpoint_suffix)\n-            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n-            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n-\n-            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n-            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n-            logger.info(\"  Continuing training from global step %d\", global_step)\n-            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n-        except ValueError:\n-            logger.info(\"  Starting fine-tuning.\")\n-\n-    tr_loss, logging_loss = 0.0, 0.0\n-    model.zero_grad()\n-    train_iterator = trange(\n-        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n-    )\n-    # Added here for reproducibility\n-    set_seed(args)\n-\n-    for _ in train_iterator:\n-        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n-        for step, batch in enumerate(epoch_iterator):\n-            # Skip past any already trained steps if resuming training\n-            if steps_trained_in_current_epoch > 0:\n-                steps_trained_in_current_epoch -= 1\n-                continue\n-\n-            model.train()\n-            if teacher is not None:\n-                teacher.eval()\n-            batch = tuple(t.to(args.device) for t in batch)\n-\n-            inputs = {\n-                \"input_ids\": batch[0],\n-                \"attention_mask\": batch[1],\n-                \"start_positions\": batch[3],\n-                \"end_positions\": batch[4],\n-            }\n-            if args.model_type != \"distilbert\":\n-                inputs[\"token_type_ids\"] = None if args.model_type == \"xlm\" else batch[2]\n-            if args.model_type in [\"xlnet\", \"xlm\"]:\n-                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n-                if args.version_2_with_negative:\n-                    inputs.update({\"is_impossible\": batch[7]})\n-            outputs = model(**inputs)\n-            loss, start_logits_stu, end_logits_stu = outputs\n-\n-            # Distillation loss\n-            if teacher is not None:\n-                if \"token_type_ids\" not in inputs:\n-                    inputs[\"token_type_ids\"] = None if args.teacher_type == \"xlm\" else batch[2]\n-                with torch.no_grad():\n-                    start_logits_tea, end_logits_tea = teacher(\n-                        input_ids=inputs[\"input_ids\"],\n-                        token_type_ids=inputs[\"token_type_ids\"],\n-                        attention_mask=inputs[\"attention_mask\"],\n-                    )\n-                assert start_logits_tea.size() == start_logits_stu.size()\n-                assert end_logits_tea.size() == end_logits_stu.size()\n-\n-                loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n-                loss_start = loss_fct(\n-                    nn.functional.log_softmax(start_logits_stu / args.temperature, dim=-1),\n-                    nn.functional.softmax(start_logits_tea / args.temperature, dim=-1),\n-                ) * (args.temperature**2)\n-                loss_end = loss_fct(\n-                    nn.functional.log_softmax(end_logits_stu / args.temperature, dim=-1),\n-                    nn.functional.softmax(end_logits_tea / args.temperature, dim=-1),\n-                ) * (args.temperature**2)\n-                loss_ce = (loss_start + loss_end) / 2.0\n-\n-                loss = args.alpha_ce * loss_ce + args.alpha_squad * loss\n-\n-            if args.n_gpu > 1:\n-                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n-            if args.gradient_accumulation_steps > 1:\n-                loss = loss / args.gradient_accumulation_steps\n-\n-            if args.fp16:\n-                with amp.scale_loss(loss, optimizer) as scaled_loss:\n-                    scaled_loss.backward()\n-            else:\n-                loss.backward()\n-\n-            tr_loss += loss.item()\n-            if (step + 1) % args.gradient_accumulation_steps == 0:\n-                if args.fp16:\n-                    nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n-                else:\n-                    nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n-\n-                optimizer.step()\n-                scheduler.step()  # Update learning rate schedule\n-                model.zero_grad()\n-                global_step += 1\n-\n-                # Log metrics\n-                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n-                    # Only evaluate when single GPU otherwise metrics may not average well\n-                    if args.local_rank == -1 and args.evaluate_during_training:\n-                        results = evaluate(args, model, tokenizer)\n-                        for key, value in results.items():\n-                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n-                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n-                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n-                    logging_loss = tr_loss\n-\n-                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n-                    # Save model checkpoint\n-                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n-                    if not os.path.exists(output_dir):\n-                        os.makedirs(output_dir)\n-                    model_to_save = (\n-                        model.module if hasattr(model, \"module\") else model\n-                    )  # Take care of distributed/parallel training\n-                    model_to_save.save_pretrained(output_dir)\n-                    tokenizer.save_pretrained(output_dir)\n-\n-                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n-                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n-\n-                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n-                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n-                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n-\n-            if args.max_steps > 0 and global_step > args.max_steps:\n-                epoch_iterator.close()\n-                break\n-        if args.max_steps > 0 and global_step > args.max_steps:\n-            train_iterator.close()\n-            break\n-\n-    if args.local_rank in [-1, 0]:\n-        tb_writer.close()\n-\n-    return global_step, tr_loss / global_step\n-\n-\n-def evaluate(args, model, tokenizer, prefix=\"\"):\n-    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n-\n-    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n-        os.makedirs(args.output_dir)\n-\n-    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n-\n-    # Note that DistributedSampler samples randomly\n-    eval_sampler = SequentialSampler(dataset)\n-    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n-\n-    # multi-gpu evaluate\n-    if args.n_gpu > 1 and not isinstance(model, nn.DataParallel):\n-        model = nn.DataParallel(model)\n-\n-    # Eval!\n-    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n-    logger.info(\"  Num examples = %d\", len(dataset))\n-    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n-\n-    all_results = []\n-    start_time = timeit.default_timer()\n-\n-    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n-        model.eval()\n-        batch = tuple(t.to(args.device) for t in batch)\n-\n-        with torch.no_grad():\n-            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n-            if args.model_type != \"distilbert\":\n-                inputs[\"token_type_ids\"] = None if args.model_type == \"xlm\" else batch[2]  # XLM don't use segment_ids\n-            example_indices = batch[3]\n-            if args.model_type in [\"xlnet\", \"xlm\"]:\n-                inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n-\n-            outputs = model(**inputs)\n-\n-        for i, example_index in enumerate(example_indices):\n-            eval_feature = features[example_index.item()]\n-            unique_id = int(eval_feature.unique_id)\n-\n-            output = [to_list(output[i]) for output in outputs]\n-\n-            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n-            # models only use two.\n-            if len(output) >= 5:\n-                start_logits = output[0]\n-                start_top_index = output[1]\n-                end_logits = output[2]\n-                end_top_index = output[3]\n-                cls_logits = output[4]\n-\n-                result = SquadResult(\n-                    unique_id,\n-                    start_logits,\n-                    end_logits,\n-                    start_top_index=start_top_index,\n-                    end_top_index=end_top_index,\n-                    cls_logits=cls_logits,\n-                )\n-\n-            else:\n-                start_logits, end_logits = output\n-                result = SquadResult(unique_id, start_logits, end_logits)\n-\n-            all_results.append(result)\n-\n-    evalTime = timeit.default_timer() - start_time\n-    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n-\n-    # Compute predictions\n-    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(prefix))\n-    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(prefix))\n-\n-    if args.version_2_with_negative:\n-        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(prefix))\n-    else:\n-        output_null_log_odds_file = None\n-\n-    if args.model_type in [\"xlnet\", \"xlm\"]:\n-        # XLNet uses a more complex post-processing procedure\n-        predictions = compute_predictions_log_probs(\n-            examples,\n-            features,\n-            all_results,\n-            args.n_best_size,\n-            args.max_answer_length,\n-            output_prediction_file,\n-            output_nbest_file,\n-            output_null_log_odds_file,\n-            model.config.start_n_top,\n-            model.config.end_n_top,\n-            args.version_2_with_negative,\n-            tokenizer,\n-            args.verbose_logging,\n-        )\n-    else:\n-        predictions = compute_predictions_logits(\n-            examples,\n-            features,\n-            all_results,\n-            args.n_best_size,\n-            args.max_answer_length,\n-            args.do_lower_case,\n-            output_prediction_file,\n-            output_nbest_file,\n-            output_null_log_odds_file,\n-            args.verbose_logging,\n-            args.version_2_with_negative,\n-            args.null_score_diff_threshold,\n-            tokenizer,\n-        )\n-\n-    # Compute the F1 and exact scores.\n-    results = squad_evaluate(examples, predictions)\n-    return results\n-\n-\n-def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n-    if args.local_rank not in [-1, 0] and not evaluate:\n-        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-        torch.distributed.barrier()\n-\n-    # Load data features from cache or dataset file\n-    input_file = args.predict_file if evaluate else args.train_file\n-    cached_features_file = os.path.join(\n-        os.path.dirname(input_file),\n-        \"cached_distillation_{}_{}_{}\".format(\n-            \"dev\" if evaluate else \"train\",\n-            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n-            str(args.max_seq_length),\n-        ),\n-    )\n-    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features_and_dataset = torch.load(cached_features_file)\n-\n-        try:\n-            features, dataset, examples = (\n-                features_and_dataset[\"features\"],\n-                features_and_dataset[\"dataset\"],\n-                features_and_dataset[\"examples\"],\n-            )\n-        except KeyError:\n-            raise DeprecationWarning(\n-                \"You seem to be loading features from an older version of this script please delete the \"\n-                \"file %s in order for it to be created again\" % cached_features_file\n-            )\n-    else:\n-        logger.info(\"Creating features from dataset file at %s\", input_file)\n-        processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n-        if evaluate:\n-            examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n-        else:\n-            examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n-\n-        features, dataset = squad_convert_examples_to_features(\n-            examples=examples,\n-            tokenizer=tokenizer,\n-            max_seq_length=args.max_seq_length,\n-            doc_stride=args.doc_stride,\n-            max_query_length=args.max_query_length,\n-            is_training=not evaluate,\n-            return_dataset=\"pt\",\n-            threads=args.threads,\n-        )\n-\n-        if args.local_rank in [-1, 0]:\n-            logger.info(\"Saving features into cached file %s\", cached_features_file)\n-            torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n-\n-    if args.local_rank == 0 and not evaluate:\n-        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-        torch.distributed.barrier()\n-\n-    if output_examples:\n-        return dataset, examples, features\n-    return dataset\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-\n-    # Required parameters\n-    parser.add_argument(\n-        \"--model_type\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model checkpoints and predictions will be written.\",\n-    )\n-\n-    # Distillation parameters (optional)\n-    parser.add_argument(\n-        \"--teacher_type\",\n-        default=None,\n-        type=str,\n-        help=(\n-            \"Teacher type. Teacher tokenizer and student (model) tokenizer must output the same tokenization. Only for\"\n-            \" distillation.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--teacher_name_or_path\",\n-        default=None,\n-        type=str,\n-        help=\"Path to the already SQuAD fine-tuned teacher model. Only for distillation.\",\n-    )\n-    parser.add_argument(\n-        \"--alpha_ce\", default=0.5, type=float, help=\"Distillation loss linear weight. Only for distillation.\"\n-    )\n-    parser.add_argument(\n-        \"--alpha_squad\", default=0.5, type=float, help=\"True SQuAD loss linear weight. Only for distillation.\"\n-    )\n-    parser.add_argument(\n-        \"--temperature\", default=2.0, type=float, help=\"Distillation temperature. Only for distillation.\"\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        help=\"The input data dir. Should contain the .json files for the task.\"\n-        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--train_file\",\n-        default=None,\n-        type=str,\n-        help=\"The input training file. If a data dir is specified, will look for the file there\"\n-        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--predict_file\",\n-        default=None,\n-        type=str,\n-        help=\"The input evaluation file. If a data dir is specified, will look for the file there\"\n-        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\n-        \"--cache_dir\",\n-        default=\"\",\n-        type=str,\n-        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n-    )\n-\n-    parser.add_argument(\n-        \"--version_2_with_negative\",\n-        action=\"store_true\",\n-        help=\"If true, the SQuAD examples contain some that do not have an answer.\",\n-    )\n-    parser.add_argument(\n-        \"--null_score_diff_threshold\",\n-        type=float,\n-        default=0.0,\n-        help=\"If null_score - best_non_null is greater than the threshold predict null.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=384,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n-            \"longer than this will be truncated, and sequences shorter than this will be padded.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--doc_stride\",\n-        default=128,\n-        type=int,\n-        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\",\n-    )\n-    parser.add_argument(\n-        \"--max_query_length\",\n-        default=64,\n-        type=int,\n-        help=(\n-            \"The maximum number of tokens for the question. Questions longer than this will \"\n-            \"be truncated to this length.\"\n-        ),\n-    )\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n-    parser.add_argument(\n-        \"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\"\n-    )\n-    parser.add_argument(\n-        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n-    )\n-\n-    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n-    parser.add_argument(\n-        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n-    )\n-    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n-    parser.add_argument(\n-        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n-    )\n-    parser.add_argument(\n-        \"--max_steps\",\n-        default=-1,\n-        type=int,\n-        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n-    )\n-    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-    parser.add_argument(\n-        \"--n_best_size\",\n-        default=20,\n-        type=int,\n-        help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n-    )\n-    parser.add_argument(\n-        \"--max_answer_length\",\n-        default=30,\n-        type=int,\n-        help=(\n-            \"The maximum length of an answer that can be generated. This is needed because the start \"\n-            \"and end predictions are not conditioned on one another.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--verbose_logging\",\n-        action=\"store_true\",\n-        help=(\n-            \"If true, all of the warnings related to data processing will be printed. \"\n-            \"A number of warnings are expected for a normal SQuAD evaluation.\"\n-        ),\n-    )\n-\n-    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n-    parser.add_argument(\"--save_steps\", type=int, default=50, help=\"Save checkpoint every X updates steps.\")\n-    parser.add_argument(\n-        \"--eval_all_checkpoints\",\n-        action=\"store_true\",\n-        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n-    )\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n-    )\n-    parser.add_argument(\n-        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O1\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-\n-    parser.add_argument(\"--threads\", type=int, default=1, help=\"multiple threads for converting example to features\")\n-    args = parser.parse_args()\n-\n-    if (\n-        os.path.exists(args.output_dir)\n-        and os.listdir(args.output_dir)\n-        and args.do_train\n-        and not args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n-                args.output_dir\n-            )\n-        )\n-\n-    # Setup distant debugging if needed\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup CUDA, GPU & distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n-    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n-        torch.cuda.set_device(args.local_rank)\n-        device = torch.device(\"cuda\", args.local_rank)\n-        torch.distributed.init_process_group(backend=\"nccl\")\n-        args.n_gpu = 1\n-    args.device = device\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        args.local_rank,\n-        device,\n-        args.n_gpu,\n-        bool(args.local_rank != -1),\n-        args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    # Set seed\n-    set_seed(args)\n-\n-    # Load pretrained model and tokenizer\n-    if args.local_rank not in [-1, 0]:\n-        # Make sure only the first process in distributed training will download model & vocab\n-        torch.distributed.barrier()\n-\n-    args.model_type = args.model_type.lower()\n-    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n-    config = config_class.from_pretrained(\n-        args.config_name if args.config_name else args.model_name_or_path,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    tokenizer = tokenizer_class.from_pretrained(\n-        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n-        do_lower_case=args.do_lower_case,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    model = model_class.from_pretrained(\n-        args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n-        config=config,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-\n-    if args.teacher_type is not None:\n-        assert args.teacher_name_or_path is not None\n-        assert args.alpha_ce > 0.0\n-        assert args.alpha_ce + args.alpha_squad > 0.0\n-        assert args.teacher_type != \"distilbert\", \"We constraint teachers not to be of type DistilBERT.\"\n-        teacher_config_class, teacher_model_class, _ = MODEL_CLASSES[args.teacher_type]\n-        teacher_config = teacher_config_class.from_pretrained(\n-            args.teacher_name_or_path, cache_dir=args.cache_dir if args.cache_dir else None\n-        )\n-        teacher = teacher_model_class.from_pretrained(\n-            args.teacher_name_or_path, config=teacher_config, cache_dir=args.cache_dir if args.cache_dir else None\n-        )\n-        teacher.to(args.device)\n-    else:\n-        teacher = None\n-\n-    if args.local_rank == 0:\n-        # Make sure only the first process in distributed training will download model & vocab\n-        torch.distributed.barrier()\n-\n-    model.to(args.device)\n-\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n-    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n-    # remove the need for this code, but it is still valid.\n-    if args.fp16:\n-        try:\n-            import apex\n-\n-            apex.amp.register_half_function(torch, \"einsum\")\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-\n-    # Training\n-    if args.do_train:\n-        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n-        global_step, tr_loss = train(args, train_dataset, model, tokenizer, teacher=teacher)\n-        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n-\n-    # Save the trained model and the tokenizer\n-    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n-        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n-        # They can then be reloaded using `from_pretrained()`\n-        model_to_save = (\n-            model.module if hasattr(model, \"module\") else model\n-        )  # Take care of distributed/parallel training\n-        model_to_save.save_pretrained(args.output_dir)\n-        tokenizer.save_pretrained(args.output_dir)\n-\n-        # Good practice: save your training arguments together with the trained model\n-        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n-\n-        # Load a trained model and vocabulary that you have fine-tuned\n-        model = model_class.from_pretrained(args.output_dir)\n-        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n-        model.to(args.device)\n-\n-    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n-    results = {}\n-    if args.do_eval and args.local_rank in [-1, 0]:\n-        if args.do_train:\n-            logger.info(\"Loading checkpoints saved during training for evaluation\")\n-        checkpoints = [args.output_dir]\n-        if args.eval_all_checkpoints:\n-            checkpoints = [\n-                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n-            ]\n-\n-        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n-\n-        for checkpoint in checkpoints:\n-            # Reload the model\n-            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n-            model = model_class.from_pretrained(checkpoint)\n-            model.to(args.device)\n-\n-            # Evaluate\n-            result = evaluate(args, model, tokenizer, prefix=global_step)\n-\n-            result = {k + (\"_{}\".format(global_step) if global_step else \"\"): v for k, v in result.items()}\n-            results.update(result)\n-\n-    logger.info(\"Results: {}\".format(results))\n-\n-    return results\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "3fc3214acf7ff9ea9543a04a86595f93d7b113ab",
            "filename": "examples/research_projects/distillation/scripts/binarized_data.py",
            "status": "removed",
            "additions": 0,
            "deletions": 97,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fbinarized_data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fbinarized_data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fbinarized_data.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,97 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Preprocessing script before distillation.\n-\"\"\"\n-\n-import argparse\n-import logging\n-import pickle\n-import random\n-import time\n-\n-import numpy as np\n-\n-from transformers import BertTokenizer, GPT2Tokenizer, RobertaTokenizer\n-\n-\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n-)\n-logger = logging.getLogger(__name__)\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser(\n-        description=\"Preprocess the data to avoid re-doing it several times by (tokenization + token_to_ids).\"\n-    )\n-    parser.add_argument(\"--file_path\", type=str, default=\"data/dump.txt\", help=\"The path to the data.\")\n-    parser.add_argument(\"--tokenizer_type\", type=str, default=\"bert\", choices=[\"bert\", \"roberta\", \"gpt2\"])\n-    parser.add_argument(\"--tokenizer_name\", type=str, default=\"bert-base-uncased\", help=\"The tokenizer to use.\")\n-    parser.add_argument(\"--dump_file\", type=str, default=\"data/dump\", help=\"The dump file prefix.\")\n-    args = parser.parse_args()\n-\n-    logger.info(f\"Loading Tokenizer ({args.tokenizer_name})\")\n-    if args.tokenizer_type == \"bert\":\n-        tokenizer = BertTokenizer.from_pretrained(args.tokenizer_name)\n-        bos = tokenizer.special_tokens_map[\"cls_token\"]  # `[CLS]`\n-        sep = tokenizer.special_tokens_map[\"sep_token\"]  # `[SEP]`\n-    elif args.tokenizer_type == \"roberta\":\n-        tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n-        bos = tokenizer.special_tokens_map[\"cls_token\"]  # `<s>`\n-        sep = tokenizer.special_tokens_map[\"sep_token\"]  # `</s>`\n-    elif args.tokenizer_type == \"gpt2\":\n-        tokenizer = GPT2Tokenizer.from_pretrained(args.tokenizer_name)\n-        bos = tokenizer.special_tokens_map[\"bos_token\"]  # `<|endoftext|>`\n-        sep = tokenizer.special_tokens_map[\"eos_token\"]  # `<|endoftext|>`\n-\n-    logger.info(f\"Loading text from {args.file_path}\")\n-    with open(args.file_path, \"r\", encoding=\"utf8\") as fp:\n-        data = fp.readlines()\n-\n-    logger.info(\"Start encoding\")\n-    logger.info(f\"{len(data)} examples to process.\")\n-\n-    rslt = []\n-    iter = 0\n-    interval = 10000\n-    start = time.time()\n-    for text in data:\n-        text = f\"{bos} {text.strip()} {sep}\"\n-        token_ids = tokenizer.encode(text, add_special_tokens=False)\n-        rslt.append(token_ids)\n-\n-        iter += 1\n-        if iter % interval == 0:\n-            end = time.time()\n-            logger.info(f\"{iter} examples processed. - {(end-start):.2f}s/{interval}expl\")\n-            start = time.time()\n-    logger.info(\"Finished binarization\")\n-    logger.info(f\"{len(data)} examples processed.\")\n-\n-    dp_file = f\"{args.dump_file}.{args.tokenizer_name}.pickle\"\n-    vocab_size = tokenizer.vocab_size\n-    if vocab_size < (1 << 16):\n-        rslt_ = [np.uint16(d) for d in rslt]\n-    else:\n-        rslt_ = [np.int32(d) for d in rslt]\n-    random.shuffle(rslt_)\n-    logger.info(f\"Dump to {dp_file}\")\n-    with open(dp_file, \"wb\") as handle:\n-        pickle.dump(rslt_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "c45821d187312a3e5b644c3f2eab13183d5cbd0f",
            "filename": "examples/research_projects/distillation/scripts/extract.py",
            "status": "removed",
            "additions": 0,
            "deletions": 106,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fextract.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fextract.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fextract.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,106 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Preprocessing script before training the distilled model.\n-Specific to RoBERTa -> DistilRoBERTa and GPT2 -> DistilGPT2.\n-\"\"\"\n-\n-import argparse\n-\n-import torch\n-\n-from transformers import GPT2LMHeadModel, RobertaForMaskedLM\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(\n-        description=(\n-            \"Extraction some layers of the full RobertaForMaskedLM or GPT2LMHeadModel for Transfer Learned\"\n-            \" Distillation\"\n-        )\n-    )\n-    parser.add_argument(\"--model_type\", default=\"roberta\", choices=[\"roberta\", \"gpt2\"])\n-    parser.add_argument(\"--model_name\", default=\"roberta-large\", type=str)\n-    parser.add_argument(\"--dump_checkpoint\", default=\"serialization_dir/tf_roberta_048131723.pth\", type=str)\n-    parser.add_argument(\"--vocab_transform\", action=\"store_true\")\n-    args = parser.parse_args()\n-\n-    if args.model_type == \"roberta\":\n-        model = RobertaForMaskedLM.from_pretrained(args.model_name)\n-        prefix = \"roberta\"\n-    elif args.model_type == \"gpt2\":\n-        model = GPT2LMHeadModel.from_pretrained(args.model_name)\n-        prefix = \"transformer\"\n-\n-    state_dict = model.state_dict()\n-    compressed_sd = {}\n-\n-    # Embeddings #\n-    if args.model_type == \"gpt2\":\n-        for param_name in [\"wte.weight\", \"wpe.weight\"]:\n-            compressed_sd[f\"{prefix}.{param_name}\"] = state_dict[f\"{prefix}.{param_name}\"]\n-    else:\n-        for w in [\"word_embeddings\", \"position_embeddings\", \"token_type_embeddings\"]:\n-            param_name = f\"{prefix}.embeddings.{w}.weight\"\n-            compressed_sd[param_name] = state_dict[param_name]\n-        for w in [\"weight\", \"bias\"]:\n-            param_name = f\"{prefix}.embeddings.LayerNorm.{w}\"\n-            compressed_sd[param_name] = state_dict[param_name]\n-\n-    # Transformer Blocks #\n-    std_idx = 0\n-    for teacher_idx in [0, 2, 4, 7, 9, 11]:\n-        if args.model_type == \"gpt2\":\n-            for layer in [\"ln_1\", \"attn.c_attn\", \"attn.c_proj\", \"ln_2\", \"mlp.c_fc\", \"mlp.c_proj\"]:\n-                for w in [\"weight\", \"bias\"]:\n-                    compressed_sd[f\"{prefix}.h.{std_idx}.{layer}.{w}\"] = state_dict[\n-                        f\"{prefix}.h.{teacher_idx}.{layer}.{w}\"\n-                    ]\n-            compressed_sd[f\"{prefix}.h.{std_idx}.attn.bias\"] = state_dict[f\"{prefix}.h.{teacher_idx}.attn.bias\"]\n-        else:\n-            for layer in [\n-                \"attention.self.query\",\n-                \"attention.self.key\",\n-                \"attention.self.value\",\n-                \"attention.output.dense\",\n-                \"attention.output.LayerNorm\",\n-                \"intermediate.dense\",\n-                \"output.dense\",\n-                \"output.LayerNorm\",\n-            ]:\n-                for w in [\"weight\", \"bias\"]:\n-                    compressed_sd[f\"{prefix}.encoder.layer.{std_idx}.{layer}.{w}\"] = state_dict[\n-                        f\"{prefix}.encoder.layer.{teacher_idx}.{layer}.{w}\"\n-                    ]\n-        std_idx += 1\n-\n-    # Language Modeling Head ###s\n-    if args.model_type == \"roberta\":\n-        for layer in [\"lm_head.decoder.weight\", \"lm_head.bias\"]:\n-            compressed_sd[f\"{layer}\"] = state_dict[f\"{layer}\"]\n-        if args.vocab_transform:\n-            for w in [\"weight\", \"bias\"]:\n-                compressed_sd[f\"lm_head.dense.{w}\"] = state_dict[f\"lm_head.dense.{w}\"]\n-                compressed_sd[f\"lm_head.layer_norm.{w}\"] = state_dict[f\"lm_head.layer_norm.{w}\"]\n-    elif args.model_type == \"gpt2\":\n-        for w in [\"weight\", \"bias\"]:\n-            compressed_sd[f\"{prefix}.ln_f.{w}\"] = state_dict[f\"{prefix}.ln_f.{w}\"]\n-        compressed_sd[\"lm_head.weight\"] = state_dict[\"lm_head.weight\"]\n-\n-    print(f\"N layers selected for distillation: {std_idx}\")\n-    print(f\"Number of params transferred for distillation: {len(compressed_sd.keys())}\")\n-\n-    print(f\"Save transferred checkpoint to {args.dump_checkpoint}.\")\n-    torch.save(compressed_sd, args.dump_checkpoint)"
        },
        {
            "sha": "8637970c5117c5e9072ff4b18bc7d36acdd6c398",
            "filename": "examples/research_projects/distillation/scripts/extract_distilbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 96,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fextract_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fextract_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Fextract_distilbert.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,96 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Preprocessing script before training DistilBERT.\n-Specific to BERT -> DistilBERT.\n-\"\"\"\n-\n-import argparse\n-\n-import torch\n-\n-from transformers import BertForMaskedLM\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(\n-        description=(\n-            \"Extraction some layers of the full BertForMaskedLM or RObertaForMaskedLM for Transfer Learned\"\n-            \" Distillation\"\n-        )\n-    )\n-    parser.add_argument(\"--model_type\", default=\"bert\", choices=[\"bert\"])\n-    parser.add_argument(\"--model_name\", default=\"bert-base-uncased\", type=str)\n-    parser.add_argument(\"--dump_checkpoint\", default=\"serialization_dir/tf_bert-base-uncased_0247911.pth\", type=str)\n-    parser.add_argument(\"--vocab_transform\", action=\"store_true\")\n-    args = parser.parse_args()\n-\n-    if args.model_type == \"bert\":\n-        model = BertForMaskedLM.from_pretrained(args.model_name)\n-        prefix = \"bert\"\n-    else:\n-        raise ValueError('args.model_type should be \"bert\".')\n-\n-    state_dict = model.state_dict()\n-    compressed_sd = {}\n-\n-    for w in [\"word_embeddings\", \"position_embeddings\"]:\n-        compressed_sd[f\"distilbert.embeddings.{w}.weight\"] = state_dict[f\"{prefix}.embeddings.{w}.weight\"]\n-    for w in [\"weight\", \"bias\"]:\n-        compressed_sd[f\"distilbert.embeddings.LayerNorm.{w}\"] = state_dict[f\"{prefix}.embeddings.LayerNorm.{w}\"]\n-\n-    std_idx = 0\n-    for teacher_idx in [0, 2, 4, 7, 9, 11]:\n-        for w in [\"weight\", \"bias\"]:\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.attention.q_lin.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.attention.self.query.{w}\"\n-            ]\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.attention.k_lin.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.attention.self.key.{w}\"\n-            ]\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.attention.v_lin.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.attention.self.value.{w}\"\n-            ]\n-\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.attention.out_lin.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.attention.output.dense.{w}\"\n-            ]\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.sa_layer_norm.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.attention.output.LayerNorm.{w}\"\n-            ]\n-\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.ffn.lin1.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.intermediate.dense.{w}\"\n-            ]\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.ffn.lin2.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.output.dense.{w}\"\n-            ]\n-            compressed_sd[f\"distilbert.transformer.layer.{std_idx}.output_layer_norm.{w}\"] = state_dict[\n-                f\"{prefix}.encoder.layer.{teacher_idx}.output.LayerNorm.{w}\"\n-            ]\n-        std_idx += 1\n-\n-    compressed_sd[\"vocab_projector.weight\"] = state_dict[\"cls.predictions.decoder.weight\"]\n-    compressed_sd[\"vocab_projector.bias\"] = state_dict[\"cls.predictions.bias\"]\n-    if args.vocab_transform:\n-        for w in [\"weight\", \"bias\"]:\n-            compressed_sd[f\"vocab_transform.{w}\"] = state_dict[f\"cls.predictions.transform.dense.{w}\"]\n-            compressed_sd[f\"vocab_layer_norm.{w}\"] = state_dict[f\"cls.predictions.transform.LayerNorm.{w}\"]\n-\n-    print(f\"N layers selected for distillation: {std_idx}\")\n-    print(f\"Number of params transferred for distillation: {len(compressed_sd.keys())}\")\n-\n-    print(f\"Save transferred checkpoint to {args.dump_checkpoint}.\")\n-    torch.save(compressed_sd, args.dump_checkpoint)"
        },
        {
            "sha": "2f80bf31f47778b280944f4307e633d32e7923ce",
            "filename": "examples/research_projects/distillation/scripts/token_counts.py",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Ftoken_counts.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Ftoken_counts.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Fscripts%2Ftoken_counts.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,57 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Preprocessing script before training the distilled model.\n-\"\"\"\n-\n-import argparse\n-import logging\n-import pickle\n-from collections import Counter\n-\n-\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n-)\n-logger = logging.getLogger(__name__)\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(\n-        description=\"Token Counts for smoothing the masking probabilities in MLM (cf XLM/word2vec)\"\n-    )\n-    parser.add_argument(\n-        \"--data_file\", type=str, default=\"data/dump.bert-base-uncased.pickle\", help=\"The binarized dataset.\"\n-    )\n-    parser.add_argument(\n-        \"--token_counts_dump\", type=str, default=\"data/token_counts.bert-base-uncased.pickle\", help=\"The dump file.\"\n-    )\n-    parser.add_argument(\"--vocab_size\", default=30522, type=int)\n-    args = parser.parse_args()\n-\n-    logger.info(f\"Loading data from {args.data_file}\")\n-    with open(args.data_file, \"rb\") as fp:\n-        data = pickle.load(fp)\n-\n-    logger.info(\"Counting occurrences for MLM.\")\n-    counter = Counter()\n-    for tk_ids in data:\n-        counter.update(tk_ids)\n-    counts = [0] * args.vocab_size\n-    for k, v in counter.items():\n-        counts[k] = v\n-\n-    logger.info(f\"Dump to {args.token_counts_dump}\")\n-    with open(args.token_counts_dump, \"wb\") as handle:\n-        pickle.dump(counts, handle, protocol=pickle.HIGHEST_PROTOCOL)"
        },
        {
            "sha": "15d98ace09b56c7aa0e4ea85a18524d4c10f4b5f",
            "filename": "examples/research_projects/distillation/train.py",
            "status": "removed",
            "additions": 0,
            "deletions": 325,
            "changes": 325,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftrain.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftrain.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Ftrain.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,325 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Training the distilled model.\n-Supported architectures include: BERT -> DistilBERT, RoBERTa -> DistilRoBERTa, GPT2 -> DistilGPT2.\n-\"\"\"\n-\n-import argparse\n-import json\n-import os\n-import pickle\n-import shutil\n-\n-import numpy as np\n-import torch\n-from distiller import Distiller\n-from lm_seqs_dataset import LmSeqsDataset\n-\n-from transformers import (\n-    BertConfig,\n-    BertForMaskedLM,\n-    BertTokenizer,\n-    DistilBertConfig,\n-    DistilBertForMaskedLM,\n-    DistilBertTokenizer,\n-    GPT2Config,\n-    GPT2LMHeadModel,\n-    GPT2Tokenizer,\n-    RobertaConfig,\n-    RobertaForMaskedLM,\n-    RobertaTokenizer,\n-)\n-from utils import git_log, init_gpu_params, logger, set_seed\n-\n-\n-MODEL_CLASSES = {\n-    \"distilbert\": (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer),\n-    \"roberta\": (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n-    \"bert\": (BertConfig, BertForMaskedLM, BertTokenizer),\n-    \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n-}\n-\n-\n-def sanity_checks(args):\n-    \"\"\"\n-    A bunch of args sanity checks to perform even starting...\n-    \"\"\"\n-    assert (args.mlm and args.alpha_mlm > 0.0) or (not args.mlm and args.alpha_mlm == 0.0)\n-    assert (args.alpha_mlm > 0.0 and args.alpha_clm == 0.0) or (args.alpha_mlm == 0.0 and args.alpha_clm > 0.0)\n-    if args.mlm:\n-        assert os.path.isfile(args.token_counts)\n-        assert (args.student_type in [\"roberta\", \"distilbert\"]) and (args.teacher_type in [\"roberta\", \"bert\"])\n-    else:\n-        assert (args.student_type in [\"gpt2\"]) and (args.teacher_type in [\"gpt2\"])\n-\n-    assert args.teacher_type == args.student_type or (\n-        args.student_type == \"distilbert\" and args.teacher_type == \"bert\"\n-    )\n-    assert os.path.isfile(args.student_config)\n-    if args.student_pretrained_weights is not None:\n-        assert os.path.isfile(args.student_pretrained_weights)\n-\n-    if args.freeze_token_type_embds:\n-        assert args.student_type in [\"roberta\"]\n-\n-    assert args.alpha_ce >= 0.0\n-    assert args.alpha_mlm >= 0.0\n-    assert args.alpha_clm >= 0.0\n-    assert args.alpha_mse >= 0.0\n-    assert args.alpha_cos >= 0.0\n-    assert args.alpha_ce + args.alpha_mlm + args.alpha_clm + args.alpha_mse + args.alpha_cos > 0.0\n-\n-\n-def freeze_pos_embeddings(student, args):\n-    if args.student_type == \"roberta\":\n-        student.roberta.embeddings.position_embeddings.weight.requires_grad = False\n-    elif args.student_type == \"gpt2\":\n-        student.transformer.wpe.weight.requires_grad = False\n-\n-\n-def freeze_token_type_embeddings(student, args):\n-    if args.student_type == \"roberta\":\n-        student.roberta.embeddings.token_type_embeddings.weight.requires_grad = False\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser(description=\"Training\")\n-    parser.add_argument(\"--force\", action=\"store_true\", help=\"Overwrite dump_path if it already exists.\")\n-\n-    parser.add_argument(\n-        \"--dump_path\", type=str, required=True, help=\"The output directory (log, checkpoints, parameters, etc.)\"\n-    )\n-    parser.add_argument(\n-        \"--data_file\",\n-        type=str,\n-        required=True,\n-        help=\"The binarized file (tokenized + tokens_to_ids) and grouped by sequence.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--student_type\",\n-        type=str,\n-        choices=[\"distilbert\", \"roberta\", \"gpt2\"],\n-        required=True,\n-        help=\"The student type (DistilBERT, RoBERTa).\",\n-    )\n-    parser.add_argument(\"--student_config\", type=str, required=True, help=\"Path to the student configuration.\")\n-    parser.add_argument(\n-        \"--student_pretrained_weights\", default=None, type=str, help=\"Load student initialization checkpoint.\"\n-    )\n-\n-    parser.add_argument(\n-        \"--teacher_type\", choices=[\"bert\", \"roberta\", \"gpt2\"], required=True, help=\"Teacher type (BERT, RoBERTa).\"\n-    )\n-    parser.add_argument(\"--teacher_name\", type=str, required=True, help=\"The teacher model.\")\n-\n-    parser.add_argument(\"--temperature\", default=2.0, type=float, help=\"Temperature for the softmax temperature.\")\n-    parser.add_argument(\n-        \"--alpha_ce\", default=0.5, type=float, help=\"Linear weight for the distillation loss. Must be >=0.\"\n-    )\n-    parser.add_argument(\n-        \"--alpha_mlm\",\n-        default=0.0,\n-        type=float,\n-        help=\"Linear weight for the MLM loss. Must be >=0. Should be used in conjunction with `mlm` flag.\",\n-    )\n-    parser.add_argument(\"--alpha_clm\", default=0.5, type=float, help=\"Linear weight for the CLM loss. Must be >=0.\")\n-    parser.add_argument(\"--alpha_mse\", default=0.0, type=float, help=\"Linear weight of the MSE loss. Must be >=0.\")\n-    parser.add_argument(\n-        \"--alpha_cos\", default=0.0, type=float, help=\"Linear weight of the cosine embedding loss. Must be >=0.\"\n-    )\n-\n-    parser.add_argument(\n-        \"--mlm\", action=\"store_true\", help=\"The LM step: MLM or CLM. If `mlm` is True, the MLM is used over CLM.\"\n-    )\n-    parser.add_argument(\n-        \"--mlm_mask_prop\",\n-        default=0.15,\n-        type=float,\n-        help=\"Proportion of tokens for which we need to make a prediction.\",\n-    )\n-    parser.add_argument(\"--word_mask\", default=0.8, type=float, help=\"Proportion of tokens to mask out.\")\n-    parser.add_argument(\"--word_keep\", default=0.1, type=float, help=\"Proportion of tokens to keep.\")\n-    parser.add_argument(\"--word_rand\", default=0.1, type=float, help=\"Proportion of tokens to randomly replace.\")\n-    parser.add_argument(\n-        \"--mlm_smoothing\",\n-        default=0.7,\n-        type=float,\n-        help=\"Smoothing parameter to emphasize more rare tokens (see XLM, similar to word2vec).\",\n-    )\n-    parser.add_argument(\"--token_counts\", type=str, help=\"The token counts in the data_file for MLM.\")\n-\n-    parser.add_argument(\n-        \"--restrict_ce_to_mask\",\n-        action=\"store_true\",\n-        help=\"If true, compute the distillation loss only the [MLM] prediction distribution.\",\n-    )\n-    parser.add_argument(\n-        \"--freeze_pos_embs\",\n-        action=\"store_true\",\n-        help=\"Freeze positional embeddings during distillation. For student_type in ['roberta', 'gpt2'] only.\",\n-    )\n-    parser.add_argument(\n-        \"--freeze_token_type_embds\",\n-        action=\"store_true\",\n-        help=\"Freeze token type embeddings during distillation if existent. For student_type in ['roberta'] only.\",\n-    )\n-\n-    parser.add_argument(\"--n_epoch\", type=int, default=3, help=\"Number of pass on the whole dataset.\")\n-    parser.add_argument(\"--batch_size\", type=int, default=5, help=\"Batch size (for each process).\")\n-    parser.add_argument(\n-        \"--group_by_size\",\n-        action=\"store_false\",\n-        help=\"If true, group sequences that have similar length into the same batch. Default is true.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=50,\n-        help=\"Gradient accumulation for larger training batches.\",\n-    )\n-    parser.add_argument(\"--warmup_prop\", default=0.05, type=float, help=\"Linear warmup proportion.\")\n-    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-    parser.add_argument(\"--learning_rate\", default=5e-4, type=float, help=\"The initial learning rate for Adam.\")\n-    parser.add_argument(\"--adam_epsilon\", default=1e-6, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", default=5.0, type=float, help=\"Max gradient norm.\")\n-    parser.add_argument(\"--initializer_range\", default=0.02, type=float, help=\"Random initialization range.\")\n-\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O1\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\"--n_gpu\", type=int, default=1, help=\"Number of GPUs in the node.\")\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Distributed training - Local rank\")\n-    parser.add_argument(\"--seed\", type=int, default=56, help=\"Random seed\")\n-\n-    parser.add_argument(\"--log_interval\", type=int, default=500, help=\"Tensorboard logging interval.\")\n-    parser.add_argument(\"--checkpoint_interval\", type=int, default=4000, help=\"Checkpoint interval.\")\n-    args = parser.parse_args()\n-    sanity_checks(args)\n-\n-    # ARGS #\n-    init_gpu_params(args)\n-    set_seed(args)\n-    if args.is_master:\n-        if os.path.exists(args.dump_path):\n-            if not args.force:\n-                raise ValueError(\n-                    f\"Serialization dir {args.dump_path} already exists, but you have not precised wheter to overwrite\"\n-                    \" itUse `--force` if you want to overwrite it\"\n-                )\n-            else:\n-                shutil.rmtree(args.dump_path)\n-\n-        if not os.path.exists(args.dump_path):\n-            os.makedirs(args.dump_path)\n-        logger.info(f\"Experiment will be dumped and logged in {args.dump_path}\")\n-\n-        # SAVE PARAMS #\n-        logger.info(f\"Param: {args}\")\n-        with open(os.path.join(args.dump_path, \"parameters.json\"), \"w\") as f:\n-            json.dump(vars(args), f, indent=4)\n-        git_log(args.dump_path)\n-\n-    student_config_class, student_model_class, _ = MODEL_CLASSES[args.student_type]\n-    teacher_config_class, teacher_model_class, teacher_tokenizer_class = MODEL_CLASSES[args.teacher_type]\n-\n-    # TOKENIZER #\n-    tokenizer = teacher_tokenizer_class.from_pretrained(args.teacher_name)\n-    special_tok_ids = {}\n-    for tok_name, tok_symbol in tokenizer.special_tokens_map.items():\n-        idx = tokenizer.all_special_tokens.index(tok_symbol)\n-        special_tok_ids[tok_name] = tokenizer.all_special_ids[idx]\n-    logger.info(f\"Special tokens {special_tok_ids}\")\n-    args.special_tok_ids = special_tok_ids\n-    args.max_model_input_size = tokenizer.max_model_input_sizes[args.teacher_name]\n-\n-    # DATA LOADER #\n-    logger.info(f\"Loading data from {args.data_file}\")\n-    with open(args.data_file, \"rb\") as fp:\n-        data = pickle.load(fp)\n-\n-    if args.mlm:\n-        logger.info(f\"Loading token counts from {args.token_counts} (already pre-computed)\")\n-        with open(args.token_counts, \"rb\") as fp:\n-            counts = pickle.load(fp)\n-\n-        token_probs = np.maximum(counts, 1) ** -args.mlm_smoothing\n-        for idx in special_tok_ids.values():\n-            token_probs[idx] = 0.0  # do not predict special tokens\n-        token_probs = torch.from_numpy(token_probs)\n-    else:\n-        token_probs = None\n-\n-    train_lm_seq_dataset = LmSeqsDataset(params=args, data=data)\n-    logger.info(\"Data loader created.\")\n-\n-    # STUDENT #\n-    logger.info(f\"Loading student config from {args.student_config}\")\n-    stu_architecture_config = student_config_class.from_pretrained(args.student_config)\n-    stu_architecture_config.output_hidden_states = True\n-\n-    if args.student_pretrained_weights is not None:\n-        logger.info(f\"Loading pretrained weights from {args.student_pretrained_weights}\")\n-        student = student_model_class.from_pretrained(args.student_pretrained_weights, config=stu_architecture_config)\n-    else:\n-        student = student_model_class(stu_architecture_config)\n-\n-    if args.n_gpu > 0:\n-        student.to(f\"cuda:{args.local_rank}\")\n-    logger.info(\"Student loaded.\")\n-\n-    # TEACHER #\n-    teacher = teacher_model_class.from_pretrained(args.teacher_name, output_hidden_states=True)\n-    if args.n_gpu > 0:\n-        teacher.to(f\"cuda:{args.local_rank}\")\n-    logger.info(f\"Teacher loaded from {args.teacher_name}.\")\n-\n-    # FREEZING #\n-    if args.freeze_pos_embs:\n-        freeze_pos_embeddings(student, args)\n-    if args.freeze_token_type_embds:\n-        freeze_token_type_embeddings(student, args)\n-\n-    # SANITY CHECKS #\n-    assert student.config.vocab_size == teacher.config.vocab_size\n-    assert student.config.hidden_size == teacher.config.hidden_size\n-    assert student.config.max_position_embeddings == teacher.config.max_position_embeddings\n-    if args.mlm:\n-        assert token_probs.size(0) == stu_architecture_config.vocab_size\n-\n-    # DISTILLER #\n-    torch.cuda.empty_cache()\n-    distiller = Distiller(\n-        params=args, dataset=train_lm_seq_dataset, token_probs=token_probs, student=student, teacher=teacher\n-    )\n-    distiller.train()\n-    logger.info(\"Let's go get some drinks.\")\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "d4f524d704c3fa7135f77e54a333e62c3b4ce227",
            "filename": "examples/research_projects/distillation/training_configs/distilbert-base-cased.json",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-cased.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-cased.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-cased.json?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,15 +0,0 @@\n-{\n-\t\"activation\": \"gelu\",\n-\t\"attention_dropout\": 0.1,\n-\t\"dim\": 768,\n-\t\"dropout\": 0.1,\n-\t\"hidden_dim\": 3072,\n-\t\"initializer_range\": 0.02,\n-\t\"max_position_embeddings\": 512,\n-\t\"n_heads\": 12,\n-\t\"n_layers\": 6,\n-\t\"sinusoidal_pos_embds\": true,\n-\t\"tie_weights_\": true,\n-\t\"vocab_size\": 28996\n-  }\n-  \n\\ No newline at end of file"
        },
        {
            "sha": "f76e7febcba536f7ee6137e70ffca0acae649bea",
            "filename": "examples/research_projects/distillation/training_configs/distilbert-base-multilingual-cased.json",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-multilingual-cased.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-multilingual-cased.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-multilingual-cased.json?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,15 +0,0 @@\n-{\n-\t\"activation\": \"gelu\",\n-\t\"attention_dropout\": 0.1,\n-\t\"dim\": 768,\n-\t\"dropout\": 0.1,\n-\t\"hidden_dim\": 3072,\n-\t\"initializer_range\": 0.02,\n-\t\"max_position_embeddings\": 512,\n-\t\"n_heads\": 12,\n-\t\"n_layers\": 6,\n-\t\"sinusoidal_pos_embds\": true,\n-\t\"tie_weights_\": true,\n-\t\"vocab_size\": 119547\n-  }\n-  \n\\ No newline at end of file"
        },
        {
            "sha": "15d1e7fe00e63100b602a0d7db0cdbf16f7e6ff0",
            "filename": "examples/research_projects/distillation/training_configs/distilbert-base-uncased.json",
            "status": "removed",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-uncased.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-uncased.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilbert-base-uncased.json?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,15 +0,0 @@\n-{\n-\t\"activation\": \"gelu\",\n-\t\"attention_dropout\": 0.1,\n-\t\"dim\": 768,\n-\t\"dropout\": 0.1,\n-\t\"hidden_dim\": 3072,\n-\t\"initializer_range\": 0.02,\n-\t\"max_position_embeddings\": 512,\n-\t\"n_heads\": 12,\n-\t\"n_layers\": 6,\n-\t\"sinusoidal_pos_embds\": true,\n-\t\"tie_weights_\": true,\n-\t\"vocab_size\": 30522\n-  }\n-  \n\\ No newline at end of file"
        },
        {
            "sha": "9820ac93b8c72d9c1629a6470f4f3efa5e58f56b",
            "filename": "examples/research_projects/distillation/training_configs/distilgpt2.json",
            "status": "removed",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilgpt2.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilgpt2.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilgpt2.json?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,9 +0,0 @@\n-{\n-\t\"initializer_range\": 0.02,\n-\t\"layer_norm_epsilon\": 0.00001,\n-\t\"n_embd\": 768,\n-\t\"n_head\": 12,\n-\t\"n_layer\": 6,\n-\t\"n_positions\": 1024,\n-\t\"vocab_size\": 50257\n-}\n\\ No newline at end of file"
        },
        {
            "sha": "2d90ef6380a0e4d54dbab8b1a151f7162665c0da",
            "filename": "examples/research_projects/distillation/training_configs/distilroberta-base.json",
            "status": "removed",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilroberta-base.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilroberta-base.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Ftraining_configs%2Fdistilroberta-base.json?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,14 +0,0 @@\n-{\n-    \"vocab_size\": 50265,\n-    \"hidden_size\": 768,\n-    \"num_hidden_layers\": 6,\n-    \"num_attention_heads\": 12,\n-    \"intermediate_size\": 3072,\n-    \"hidden_act\": \"gelu\",\n-    \"hidden_dropout_prob\": 0.1,\n-    \"attention_probs_dropout_prob\": 0.1,\n-    \"max_position_embeddings\": 514,\n-    \"type_vocab_size\": 1,\n-    \"initializer_range\": 0.02,\n-    \"layer_norm_eps\": 0.00001\n-}\n\\ No newline at end of file"
        },
        {
            "sha": "e86d2593bbd9918c1a1ae1822807235ca4b5519c",
            "filename": "examples/research_projects/distillation/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 134,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fdistillation%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fdistillation%2Futils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,134 +0,0 @@\n-# coding=utf-8\n-# Copyright 2019-present, the HuggingFace Inc. team and Facebook, Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Utils to train DistilBERT\n-adapted in part from Facebook, Inc XLM model (https://github.com/facebookresearch/XLM)\n-\"\"\"\n-\n-import json\n-import logging\n-import os\n-import socket\n-\n-import git\n-import numpy as np\n-import torch\n-\n-\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - PID: %(process)d -  %(message)s\",\n-    datefmt=\"%m/%d/%Y %H:%M:%S\",\n-    level=logging.INFO,\n-)\n-logger = logging.getLogger(__name__)\n-\n-\n-def git_log(folder_path: str):\n-    \"\"\"\n-    Log commit info.\n-    \"\"\"\n-    repo = git.Repo(search_parent_directories=True)\n-    repo_infos = {\n-        \"repo_id\": str(repo),\n-        \"repo_sha\": str(repo.head.object.hexsha),\n-        \"repo_branch\": str(repo.active_branch),\n-    }\n-\n-    with open(os.path.join(folder_path, \"git_log.json\"), \"w\") as f:\n-        json.dump(repo_infos, f, indent=4)\n-\n-\n-def init_gpu_params(params):\n-    \"\"\"\n-    Handle single and multi-GPU / multi-node.\n-    \"\"\"\n-    if params.n_gpu <= 0:\n-        params.local_rank = 0\n-        params.master_port = -1\n-        params.is_master = True\n-        params.multi_gpu = False\n-        return\n-\n-    assert torch.cuda.is_available()\n-\n-    logger.info(\"Initializing GPUs\")\n-    if params.n_gpu > 1:\n-        assert params.local_rank != -1\n-\n-        params.world_size = int(os.environ[\"WORLD_SIZE\"])\n-        params.n_gpu_per_node = int(os.environ[\"N_GPU_NODE\"])\n-        params.global_rank = int(os.environ[\"RANK\"])\n-\n-        # number of nodes / node ID\n-        params.n_nodes = params.world_size // params.n_gpu_per_node\n-        params.node_id = params.global_rank // params.n_gpu_per_node\n-        params.multi_gpu = True\n-\n-        assert params.n_nodes == int(os.environ[\"N_NODES\"])\n-        assert params.node_id == int(os.environ[\"NODE_RANK\"])\n-\n-    # local job (single GPU)\n-    else:\n-        assert params.local_rank == -1\n-\n-        params.n_nodes = 1\n-        params.node_id = 0\n-        params.local_rank = 0\n-        params.global_rank = 0\n-        params.world_size = 1\n-        params.n_gpu_per_node = 1\n-        params.multi_gpu = False\n-\n-    # sanity checks\n-    assert params.n_nodes >= 1\n-    assert 0 <= params.node_id < params.n_nodes\n-    assert 0 <= params.local_rank <= params.global_rank < params.world_size\n-    assert params.world_size == params.n_nodes * params.n_gpu_per_node\n-\n-    # define whether this is the master process / if we are in multi-node distributed mode\n-    params.is_master = params.node_id == 0 and params.local_rank == 0\n-    params.multi_node = params.n_nodes > 1\n-\n-    # summary\n-    PREFIX = f\"--- Global rank: {params.global_rank} - \"\n-    logger.info(PREFIX + \"Number of nodes: %i\" % params.n_nodes)\n-    logger.info(PREFIX + \"Node ID        : %i\" % params.node_id)\n-    logger.info(PREFIX + \"Local rank     : %i\" % params.local_rank)\n-    logger.info(PREFIX + \"World size     : %i\" % params.world_size)\n-    logger.info(PREFIX + \"GPUs per node  : %i\" % params.n_gpu_per_node)\n-    logger.info(PREFIX + \"Master         : %s\" % str(params.is_master))\n-    logger.info(PREFIX + \"Multi-node     : %s\" % str(params.multi_node))\n-    logger.info(PREFIX + \"Multi-GPU      : %s\" % str(params.multi_gpu))\n-    logger.info(PREFIX + \"Hostname       : %s\" % socket.gethostname())\n-\n-    # set GPU device\n-    torch.cuda.set_device(params.local_rank)\n-\n-    # initialize multi-GPU\n-    if params.multi_gpu:\n-        logger.info(\"Initializing PyTorch distributed\")\n-        torch.distributed.init_process_group(\n-            init_method=\"env://\",\n-            backend=\"nccl\",\n-        )\n-\n-\n-def set_seed(args):\n-    \"\"\"\n-    Set the random seed.\n-    \"\"\"\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    if args.n_gpu > 0:\n-        torch.cuda.manual_seed_all(args.seed)"
        },
        {
            "sha": "5ebcee07fcb684b27c57bea865d89006536a9682",
            "filename": "examples/research_projects/fsner/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,88 +0,0 @@\n-<p align=\"center\"> <img src=\"http://sayef.tech:8082/uploads/FSNER-LOGO-2.png\" alt=\"FSNER LOGO\"> </p>\n-\n-<p align=\"center\">\n-  Implemented by <a href=\"https://huggingface.co/sayef\"> sayef </a>. \n-</p>\n-\n-## Overview\n-\n-The FSNER model was proposed in [Example-Based Named Entity Recognition](https://arxiv.org/abs/2008.10570) by Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it uses a train-free few-shot learning approach inspired by question-answering.\n-\n-\n-\n-## Abstract\n-----\n-> We present a novel approach to named entity recognition (NER) in the presence of scarce data that we call example-based NER. Our train-free few-shot learning approach takes inspiration from question-answering to identify entity spans in a new and unseen domain. In comparison with the current state-of-the-art, the proposed method performs significantly better, especially when using a low number of support examples.\n-\n-\n-\n-## Model Training Details\n------\n-\n-| identifier        | epochs           | datasets  |\n-| ---------- |:----------:| :-----:|\n-| [sayef/fsner-bert-base-uncased](https://huggingface.co/sayef/fsner-bert-base-uncased)      | 10 | ontonotes5, conll2003, wnut2017, and fin (Alvarado et al.). |\n-\n-\n-## Installation and Example Usage\n-------\n-\n-You can use the FSNER model in 3 ways:\n-\n-1. Install directly from PyPI: `pip install fsner` and import the model as shown in the code example below\n-\n-    or\n-\n-2. Install from source: `python setup.py install` and import the model as shown in the code example below\n-\n-    or\n-\n-3. Clone repo and change directory to `src` and import the model as shown in the code example below\n-\n-\n-\n-```python\n-from fsner import FSNERModel, FSNERTokenizerUtils\n-\n-model = FSNERModel(\"sayef/fsner-bert-base-uncased\")\n-\n-tokenizer = FSNERTokenizerUtils(\"sayef/fsner-bert-base-uncased\")\n-\n-# size of query and supports must be the same. If you want to find all the entitites in one particular query, just repeat the same query n times where n is equal to the number of supports (or entities).\n-\n-\n-query = [\n-    'KWE 4000 can reach with a maximum speed from up to 450 P/min an accuracy from 50 mg',\n-    'I would like to order a computer from eBay.',\n-]\n-\n-# each list in supports are the examples of one entity type\n-# wrap entities around with [E] and [/E] in the examples\n-\n-supports = [\n-        [\n-           'Horizontal flow wrapper [E] Pack 403 [/E] features the new retrofit-kit â€paper-ON-formâ€œ',\n-           '[E] Paloma Pick-and-Place-Roboter [/E] arranges the bakery products for the downstream tray-forming equipment',\n-           'Finally, the new [E] Kliklok ACE [/E] carton former forms cartons and trays without the use of glue',\n-           'We set up our pilot plant with the right [E] FibreFormÂ® [/E] configuration to make prototypes for your marketing tests and package validation',\n-           'The [E] CAR-T5 [/E] is a reliable, purely mechanically driven cartoning machine for versatile application fields'\n-        ],\n-        [\n-            \"[E] Walmart [/E] is a leading e-commerce company\",\n-            \"I recently ordered a book from [E] Amazon [/E]\",\n-            \"I ordered this from [E] ShopClues [/E]\",\n-            \"[E] Flipkart [/E] started it's journey from zero\"\n-        ]\n-   ]\n-\n-device = 'cpu'\n-\n-W_query = tokenizer.tokenize(query).to(device)\n-W_supports = tokenizer.tokenize(supports).to(device)\n-\n-start_prob, end_prob = model(W_query, W_supports)\n-\n-output = tokenizer.extract_entity_from_scores(query, W_query, start_prob, end_prob, thresh=0.50)\n-\n-print(output)\n-```"
        },
        {
            "sha": "f00ba2f7a92b99c3cf2c67ec9d480ef312d47ff8",
            "filename": "examples/research_projects/fsner/pyproject.toml",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fpyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fpyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2Fpyproject.toml?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,7 +0,0 @@\n-[build-system]\n-requires = [\n-    \"setuptools>=57.4.0\",\n-    \"wheel>=0.37.0\",\n-    \"transformers>=4.9.2\"\n-]\n-build-backend = \"setuptools.build_meta\"\n\\ No newline at end of file"
        },
        {
            "sha": "f77cb020b2c1faf87572e2f705b36ebcaf2ed82e",
            "filename": "examples/research_projects/fsner/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1 +0,0 @@\n-transformers>=4.9.2\n\\ No newline at end of file"
        },
        {
            "sha": "8ce34d0f7d9053b36d3cde98d251dfbc0ffe5a25",
            "filename": "examples/research_projects/fsner/setup.py",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsetup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsetup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2Fsetup.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,27 +0,0 @@\n-import setuptools\n-\n-\n-with open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n-    long_description = fh.read()\n-\n-setuptools.setup(\n-    name=\"fsner\",\n-    version=\"0.0.1\",\n-    author=\"msi sayef\",\n-    author_email=\"msi.sayef@gmail.com\",\n-    description=\"Few-shot Named Entity Recognition\",\n-    long_description=long_description,\n-    long_description_content_type=\"text/markdown\",\n-    url=\"https://github.com/huggingface/transformers/tree/main/examples/research_projects/fsner\",\n-    project_urls={\n-        \"Bug Tracker\": \"https://github.com/huggingface/transformers/issues\",\n-    },\n-    classifiers=[\n-        \"Programming Language :: Python :: 3\",\n-        \"Operating System :: OS Independent\",\n-    ],\n-    package_dir={\"\": \"src\"},\n-    packages=setuptools.find_packages(where=\"src\"),\n-    python_requires=\">=3.6\",\n-    install_requires=[\"torch>=1.9.0\", \"transformers>=4.9.2\"],\n-)"
        },
        {
            "sha": "130813cc119c1689912b3de28abb59cb18a92045",
            "filename": "examples/research_projects/fsner/src/fsner/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,5 +0,0 @@\n-from .model import FSNERModel\n-from .tokenizer_utils import FSNERTokenizerUtils\n-\n-\n-__all__ = [\"FSNERModel\", \"FSNERTokenizerUtils\"]"
        },
        {
            "sha": "0410340c4a9467113152a7486c4856a0e5b04ba3",
            "filename": "examples/research_projects/fsner/src/fsner/model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2Fmodel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2Fmodel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2Fmodel.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,80 +0,0 @@\n-import torch\n-\n-from transformers import AutoModel\n-\n-\n-class FSNERModel(torch.nn.Module):\n-    \"\"\"\n-    The FSNER model implements a few-shot named entity recognition method from the paper `Example-Based Named Entity Recognition <https://arxiv.org/abs/2008.10570>`__ by\n-    Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, Weizhu Chen. To identify entity spans in a new domain, it\n-    uses a train-free few-shot learning approach inspired by question-answering.\n-    \"\"\"\n-\n-    def __init__(self, pretrained_model_name_or_path=\"sayef/fsner-bert-base-uncased\"):\n-        super(FSNERModel, self).__init__()\n-\n-        self.bert = AutoModel.from_pretrained(pretrained_model_name_or_path, return_dict=True)\n-        self.cos = torch.nn.CosineSimilarity(3, 1e-08)\n-        self.softmax = torch.nn.Softmax(dim=1)\n-\n-    def BERT(self, **inputs):\n-        return self.bert(**inputs).last_hidden_state\n-\n-    def VectorSum(self, token_embeddings):\n-        return token_embeddings.sum(2, keepdim=True)\n-\n-    def Atten(self, q_rep, S_rep, T=1):\n-        return self.softmax(T * self.cos(q_rep, S_rep))\n-\n-    def forward(self, W_query, W_supports):\n-        \"\"\"\n-        Find scores of each token being start and end token for an entity.\n-        Args:\n-            W_query (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of query sequence tokens in the vocabulary.\n-            W_supports (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of support sequence tokens in the vocabulary.\n-        Returns:\n-            p_start (`torch.FloatTensor` of shape `(batch_size, sequence_length)`): Scores of each token as\n-            being start token of an entity\n-            p_end (`torch.FloatTensor` of shape `(batch_size, sequence_length)`): Scores of each token as\n-            being end token of an entity\n-        \"\"\"\n-\n-        support_sizes = W_supports[\"sizes\"].tolist()\n-        start_token_id = W_supports[\"start_token_id\"].item()\n-        end_token_id = W_supports[\"end_token_id\"].item()\n-\n-        del W_supports[\"sizes\"]\n-        del W_supports[\"start_token_id\"]\n-        del W_supports[\"end_token_id\"]\n-\n-        q = self.BERT(**W_query)\n-        S = self.BERT(**W_supports)\n-\n-        p_starts = None\n-        p_ends = None\n-\n-        start_token_masks = W_supports[\"input_ids\"] == start_token_id\n-        end_token_masks = W_supports[\"input_ids\"] == end_token_id\n-\n-        for i, size in enumerate(support_sizes):\n-            if i == 0:\n-                s = 0\n-            else:\n-                s = support_sizes[i - 1]\n-\n-            s_start = S[s : s + size][start_token_masks[s : s + size]]\n-            s_end = S[s : s + size][end_token_masks[s : s + size]]\n-\n-            p_start = torch.matmul(q[i], s_start.T).sum(1).softmax(0)\n-            p_end = torch.matmul(q[i], s_end.T).sum(1).softmax(0)\n-\n-            if p_starts is not None:\n-                p_starts = torch.vstack((p_starts, p_start))\n-                p_ends = torch.vstack((p_ends, p_end))\n-            else:\n-                p_starts = p_start\n-                p_ends = p_end\n-\n-        return p_starts, p_ends"
        },
        {
            "sha": "7169e23dbe490d53decb69add6c9d54c1f351b5a",
            "filename": "examples/research_projects/fsner/src/fsner/tokenizer_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 102,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2Ftokenizer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2Ftokenizer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ffsner%2Fsrc%2Ffsner%2Ftokenizer_utils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,102 +0,0 @@\n-import torch\n-\n-from transformers import AutoTokenizer\n-\n-\n-class FSNERTokenizerUtils:\n-    def __init__(self, pretrained_model_name_or_path):\n-        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n-\n-    def tokenize(self, x):\n-        \"\"\"\n-        Wrapper function for tokenizing query and supports\n-        Args:\n-            x (`List[str] or List[List[str]]`):\n-                List of strings for query or list of lists of strings for supports.\n-        Returns:\n-            `transformers.tokenization_utils_base.BatchEncoding` dict with additional keys and values for start_token_id, end_token_id and sizes of example lists for each entity type\n-        \"\"\"\n-\n-        if isinstance(x, list) and all(isinstance(_x, list) for _x in x):\n-            d = None\n-            for l in x:\n-                t = self.tokenizer(\n-                    l,\n-                    padding=\"max_length\",\n-                    max_length=384,\n-                    truncation=True,\n-                    return_tensors=\"pt\",\n-                )\n-                t[\"sizes\"] = torch.tensor([len(l)])\n-                if d is not None:\n-                    for k in d.keys():\n-                        d[k] = torch.cat((d[k], t[k]), 0)\n-                else:\n-                    d = t\n-\n-            d[\"start_token_id\"] = torch.tensor(self.tokenizer.convert_tokens_to_ids(\"[E]\"))\n-            d[\"end_token_id\"] = torch.tensor(self.tokenizer.convert_tokens_to_ids(\"[/E]\"))\n-\n-        elif isinstance(x, list) and all(isinstance(_x, str) for _x in x):\n-            d = self.tokenizer(\n-                x,\n-                padding=\"max_length\",\n-                max_length=384,\n-                truncation=True,\n-                return_tensors=\"pt\",\n-            )\n-\n-        else:\n-            raise Exception(\n-                \"Type of parameter x was not recognized! Only `list of strings` for query or `list of lists of\"\n-                \" strings` for supports are supported.\"\n-            )\n-\n-        return d\n-\n-    def extract_entity_from_scores(self, query, W_query, p_start, p_end, thresh=0.70):\n-        \"\"\"\n-        Extracts entities from query and scores given a threshold.\n-        Args:\n-            query (`List[str]`):\n-                List of query strings.\n-            W_query (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of query sequence tokens in the vocabulary.\n-            p_start (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-                Scores of each token as being start token of an entity\n-            p_end (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-                Scores of each token as being end token of an entity\n-            thresh (`float`):\n-                Score threshold value\n-        Returns:\n-            A list of lists of tuples(decoded entity, score)\n-        \"\"\"\n-\n-        final_outputs = []\n-        for idx in range(len(W_query[\"input_ids\"])):\n-            start_indexes = end_indexes = range(p_start.shape[1])\n-\n-            output = []\n-            for start_id in start_indexes:\n-                for end_id in end_indexes:\n-                    if start_id < end_id:\n-                        output.append(\n-                            (\n-                                start_id,\n-                                end_id,\n-                                p_start[idx][start_id].item(),\n-                                p_end[idx][end_id].item(),\n-                            )\n-                        )\n-\n-            output.sort(key=lambda tup: (tup[2] * tup[3]), reverse=True)\n-            temp = []\n-            for k in range(len(output)):\n-                if output[k][2] * output[k][3] >= thresh:\n-                    c_start_pos, c_end_pos = output[k][0], output[k][1]\n-                    decoded = self.tokenizer.decode(W_query[\"input_ids\"][idx][c_start_pos:c_end_pos])\n-                    temp.append((decoded, output[k][2] * output[k][3]))\n-\n-            final_outputs.append(temp)\n-\n-        return final_outputs"
        },
        {
            "sha": "f685a512509f0deafbf2b78be0aa917e5dca5655",
            "filename": "examples/research_projects/information-gain-filtration/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 100,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Finformation-gain-filtration%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,100 +0,0 @@\n-\n-# Information Gain Filtration(IGF)\n-\n-Authors @Tuko @mraunak\n-\n-This folder contains the code how to implement IGF for finetuning on GPT-2.\n-\n-## What is IGF?\n-\n-Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final\n-performance of language model fine-tuning(see paper below). The method is an alternative fine-tuning method that trains\n-a secondary model (e.g., a simple convolutional network) to predict the amount of information\n-gained over a given pre-trained model. The secondary model is lightweight and trained to\n-predict the Information Gain measure. Information Gain is defined as the change in a loss\n-function for a model before and after an SGD update with a sample (Equation X in the paper).\n-A small subset of the training set named the â€œobjectiveâ€ set, is used to measure information\n-gain on the pre-trained model, and consequently to train the secondary model. After \n-training, the model is used for filtering samples for the fine-tuning process. Therefore, \n-a high information gain value would suggest a sample is informative, whereas a low value\n-would suggest a non-informative sample that should be filtered out. Thus, a thresholding\n-strategy is defined to select informative samples. With such a strategy, samples are filtered\n-and once enough samples are selected to form a mini-batch and a usual fine-tuning/optimization\n-step is applied. The filtration process is repeated until the fine-tuning process is over. \n-\n-Paper [Selecting Informative Contexts Improves Language Model Finetuning](https://arxiv.org/abs/2005.00175)\n-\n-# Results\n-\n-Several experiments were conducted to show the robustness of the IGF method versus the\n-standard fine-tuning process. For example, we achieve a median perplexity of 54.0 on the \n-Books dataset compared to 57.3 for standard fine-tuning on GPT-2 Small. The code was\n-implemented using the Transformers library and Pytorch. While the method may seem more\n-expensive, we saw enough evidence that it may lead to a performance benefit in the final models.   \n-\n-![IGF performance](result_igf.png)\n-\n-Figure 1: Comparing IGF to Standard Fine-tuning:\n-IGF with constant (p < 10âˆ’3 , t-test) and shifting(p < 10âˆ’6 , t-test) thresholding significantly outperform standard fine-tuning. The left-hand figure shows\n-test-set perplexity after each fine-tuning batch, averaged over 50 runs (error bars denote Â± one standard error). The right-hand figure shows the perplexity of each\n-method after 60 batches. IGF with shifting thresholding (red) clearly improves over standard batched fine-tuning with Adam\n-\n-## How to use this project?\n-\n-To fine-tune a transformer model with IGF on a language modeling task, use the following script:\n-\n-- `model_name_or_path`: Path to pretrained model or model identifier from huggingface.co/models\n-- `data_file`: A jbl file containing tokenized data which can be split as objective dataset,\n-    train_dataset and test_dataset\n-- `igf_data_file`: A jbl file containing the context and information gain pairs to train secondary learner.  \n-- `context_len`: The maximum total input sequence length after tokenization. Sequences longer \n-    than this will be truncated, sequences shorter will be padded.\n-- `size_objective_set`: Number of articles that are long enough to be used as our objective set\"\n-- `min_len`: The minimum length of the article to be used as objective set\n-- `trim`: Truncate the example if it exceeds context length\n-- `eval_freq`: Secondary model evaluation can be triggered at eval_freq\n-- `max_steps`: To calculate training epochs\n-- `number`: The number of examples split to be used as objective_set/test_data\n-- `secondary_learner_batch_size`: The batch size of training data for secondary learner\n-- `secondary_learner_max_epochs`: The number of epochs to train secondary learner\n-- `recopy_model`: Reset the model to the original pretrained GPT-2 weights after each iteration\n-- `eval_interval`: Decay the selectivity of our secondary learner filter from\"\n-    1 standard deviation above average to 1 below average after eval_interval(10) batches\"\n-\n-  \n-```python\n-python run_clm_igf.py\\\n---model_name_or_path \"openai-community/gpt2\" \\\n---data_file=\"data/tokenized_stories_train_wikitext103\" \\\n---igf_data_file=\"data/IGF_values\" \\\n---context_len 32 \\\n---size_objective_set 100 \\\n---min_len 1026 \\\n---trim True \\\n---eval_freq 100 \\\n---max_steps 1000 \\\n---secondary_learner_batch_size 128 \\\n---secondary_learner_max_epochs 15 \\\n---number 100 \\\n---recopy_model \\\n---eval_interval 10 \\\n-```\n-\n-## Citation\n-\n-If you find the resource useful, please cite the following paper\n-\n-```bibtex\n-@inproceedings{antonello-etal-2021-selecting,\n-    title = \"Selecting Informative Contexts Improves Language Model Fine-tuning\",\n-    author = \"Antonello, Richard and Beckage, Nicole and Turek, Javier and Huth, Alexander\",\n-    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\",\n-    month = aug,\n-    year = \"2021\",\n-    address = \"Online\",\n-    publisher = \"Association for Computational Linguistics\",\n-    url = \"https://aclanthology.org/2021.acl-long.87\",\n-    doi = \"10.18653/v1/2021.acl-long.87\",\n-    pages = \"1072--1085\",\n-}\n-```"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "examples/research_projects/information-gain-filtration/igf/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Figf%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Figf%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Finformation-gain-filtration%2Figf%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "4c5aefd9584e16468d303372018ca66d7fc030b1",
            "filename": "examples/research_projects/information-gain-filtration/igf/igf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 416,
            "changes": 416,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Figf%2Figf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Figf%2Figf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Finformation-gain-filtration%2Figf%2Figf.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,416 +0,0 @@\n-# Copyright 2022 - Intel Corp. All rights reserved.\n-# Authors: Mayank Kumar Raunak, Javier Turek, Nicole Backage\n-\n-import copy\n-import logging\n-import random\n-\n-import joblib\n-import numpy as np\n-import torch\n-import torch.nn as nn\n-from torch.utils.data import DataLoader\n-from tqdm import tqdm\n-\n-from transformers import AdamW, GPT2LMHeadModel, get_linear_schedule_with_warmup\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def set_seed(seed):\n-    \"\"\"\n-    For reproducible training\n-\n-    Args:\n-        seed: A seed for reproducible training\n-\n-    \"\"\"\n-    random.seed(seed)\n-    np.random.seed(seed)\n-    torch.manual_seed(seed)\n-    torch.cuda.manual_seed_all(seed)\n-\n-\n-def compute_perplexity(model, test_data, context_len):\n-    \"\"\"\n-    Computes perplexity of the transformer model on data in test_data\n-\n-    Args:\n-        model: Pre-trained GPT2 model\n-        test_data: Data on which perplexity calculation is required\n-        context_len: The maximum total input sequence length after tokenization. Sequences longer\n-                     than this will be truncated, sequences shorter will be padded\n-\n-    Returns:\n-        Perplexity on input test data\n-\n-    \"\"\"\n-\n-    model.eval()\n-    device = next(model.parameters()).device\n-    eval_batch_size = 1\n-    context = torch.zeros((eval_batch_size, context_len), dtype=torch.long, device=device)\n-    eval_dataloader = DataLoader(test_data, shuffle=False, batch_size=eval_batch_size)\n-    eval_loss = torch.zeros(1, device=device)\n-    nb_eval_examples = 0\n-    for batch in eval_dataloader:\n-        batch.to(device)\n-        # pad\n-        context.zero_()\n-        for i in range(eval_batch_size):\n-            context[i, :] = batch[i]\n-        outputs = model(context, labels=context)\n-        eval_loss += outputs[0].sum().item()\n-        nb_eval_examples += batch.size(0)\n-    eval_loss = eval_loss / nb_eval_examples\n-    perplexity = torch.exp(eval_loss)\n-    model.train()\n-    return perplexity\n-\n-\n-def load_gpt2(model_name=\"openai-community/gpt2\"):\n-    \"\"\"\n-    load original openai-community/gpt2 and save off for quicker loading\n-\n-    Args:\n-        model_name: GPT-2\n-\n-    Returns:\n-        GPT-2 model\n-\n-    \"\"\"\n-\n-    model = GPT2LMHeadModel.from_pretrained(model_name, output_hidden_states=True)\n-    torch.save(model.state_dict(), model_name + \"local.pt\")\n-    return model\n-\n-\n-def recopy_gpt2(orig_model, device, max_steps):\n-    \"\"\"\n-    Reset the model to the original pretrained GPT-2 weights after each iteration\n-\n-    Args:\n-        orig_model: Original pretrained GPT-2 model imported from Transformers library\n-        device: CPU/GPU\n-        max_steps: number of training steps\n-\n-    Returns:\n-        Original PreTrained GPT-2 model,\n-        lm_optimizer: Adam optimizer with Decoupled weight decay\n-        lm_scheduler: linear scheduler with the appropriate schedule\n-\n-    \"\"\"\n-    model = copy.deepcopy(orig_model)\n-    model.to(device)\n-\n-    no_decay = [\"bias\", \"LayerNorm.weight\"]\n-    optimizer_grouped_parameters = [\n-        {\n-            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n-            \"weight_decay\": 0.0,\n-        },\n-        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n-    ]\n-    lm_optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n-    lm_scheduler = get_linear_schedule_with_warmup(lm_optimizer, 0, max_steps)\n-    torch.cuda.empty_cache()\n-    return model, lm_optimizer, lm_scheduler\n-\n-\n-def intermittent_save(contexts, real_perps, past_perps, filename):\n-    \"\"\"\n-    save the perplexity differences to filename\n-\n-    Args:\n-        contexts: Example on which the perplexity is calculated\n-        real_perps: Perplexity after back-propagating on the selected context\n-        past_perps: Perplexity of model before training on the context\n-        filename: File to store perplexity differences\n-\n-    Returns:\n-        file with perplexity differences\n-\n-    \"\"\"\n-    # save the perplexity differences to filename\n-    avg = np.array(real_perps).mean()\n-    std = np.array(real_perps).std()\n-    perp_diff = (real_perps - avg) / std\n-    data_final = list(zip(contexts, perp_diff, past_perps))\n-    joblib.dump(data_final, filename)\n-\n-\n-def collect_objective_set(\n-    model,\n-    orig_perp,\n-    context_len,\n-    train_data,\n-    objective_set,\n-    max_steps,\n-    device,\n-    filename=\"dev.jbl\",\n-    recopy_model=recopy_gpt2,\n-):\n-    \"\"\"\n-    Collect individual IGF values from pre-trained transformer model\n-    max_steps samples of training data to train secondary model\n-\n-    Args:\n-        model: Pre-trained GPT2 model\n-        orig_perp: Perplexity of original pretrained GPT-2 model\n-        context_len: The maximum total input sequence length after tokenization. Sequences longer\n-                    than this will be truncated, sequences shorter will be padded\n-        train_data: Data to train model\n-        objective_set: Contexts used to create (X,IG(X)) pairs which is the training data for secondary learner\n-        max_steps: To calculate training epochs of model\n-        device: GPU/CPU\n-        filename: To store intermediate perplexity differences\n-        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\n-\n-    Returns:\n-        file stored intermediate perplexity differences in intermediate stages\n-\n-    \"\"\"\n-\n-    # initialize variables to record relevant information\n-    contexts = []\n-    real_perps = []\n-    past_perps = []\n-\n-    # Initialize the transformer model\n-    orig_model = copy.deepcopy(model)\n-    orig_model.to(device=\"cpu\")\n-    torch.cuda.empty_cache()\n-\n-    # Compute perplexity of initial transformer model for comparison\n-    model.train()\n-    model, lm_optimizer, lm_scheduler = recopy_model(orig_model, device, max_steps)\n-\n-    for step in tqdm(range(max_steps)):\n-        context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n-        story = random.choice(train_data)\n-        start = random.randint(0, len(story[0]) - context_len - 1)\n-        context[0, :] = story[0][start : start + context_len]\n-        lm_optimizer.zero_grad()\n-        outputs = model(context, labels=context)\n-        lm_loss = outputs[0]\n-        past_perp = compute_perplexity(model, context, context_len)\n-        model.train()\n-        lm_loss.backward()\n-        # Do LM backprop\n-        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n-        lm_optimizer.step()\n-        lm_scheduler.step()  # Update learning rate schedule\n-\n-        # Compute perplexity after back-propagating on the selected context\n-        real_perp = compute_perplexity(model, objective_set, context_len)\n-\n-        # Periodically save the stored (X, IG(X)) pairs\n-        if step % 1000 == 0 and step > 1:\n-            intermittent_save(contexts, real_perps, past_perps, filename)\n-\n-        # Reset the pretrained model to the original pretrained GPT-2 weights after each iteration\n-        model, lm_optimizer, lm_scheduler = recopy_model(orig_model, device, max_steps)\n-\n-        past_perps.append(past_perp.item())\n-        real_perps.append(orig_perp - real_perp.item())\n-        contexts.append(np.array(context.cpu()))\n-\n-    intermittent_save(contexts, real_perps, past_perps, filename)\n-\n-\n-def generate_datasets(\n-    context_len, file=\"data/tokenized_stories_train_wikitext103.jbl\", number=100, min_len=1026, trim=True\n-):\n-    \"\"\"\n-    Generate objective set and training set\n-\n-    Args:\n-        context_len: The maximum total input sequence length after tokenization. Sequences longer\n-                than this will be truncated, sequences shorter will be padded\n-        file: Tokenized data split into training set and objective set\n-        number: size of objective dataset\n-        min_len: minimum length of a context in objective set\n-        trim: If True truncate the context if it exceeds context length\n-\n-    Returns:\n-        Generated objective set and training data\n-\n-\n-    \"\"\"\n-    # Generate objective set and training set\n-    # Designate the first number (100) articles that are long enough to be used\n-    # as our objective set, rest (that are long enough) are training data for\n-    # secondary learner\n-\n-    data = joblib.load(file)\n-    print(\"data loaded\")\n-    objective_set = []\n-    if trim:\n-        for i, example in enumerate(data):\n-            if len(example[0]) > min_len:\n-                start = random.randint(0, len(example[0]) - context_len - 1)\n-                objective_set.append(example[0, start : start + context_len])\n-            if len(objective_set) >= number:\n-                break\n-        train_data = []\n-        for j in range(i + 1, len(data)):\n-            if len(data[j][0]) > min_len:\n-                train_data.append(data[j])\n-    else:\n-        objective_set = data[0:number]\n-        train_data = data[number:]\n-\n-    joblib.dump(objective_set, \"objective_set.jbl\")\n-    print(\"objective set saved\")\n-    return train_data, objective_set\n-\n-\n-def train_secondary_learner(\n-    secondary_learner, train_dataset, max_epochs, batch_size, eval_freq=50, igf_model_path=\"secondary_learner.pt\"\n-):\n-    \"\"\"\n-    Train the secondary learner (igf_model)\n-\n-    Args:\n-        secondary_learner: secondary learner\n-        train_dataset: data to train secondary learner\n-        max_epochs: number of epochs to train secondary learner\n-        batch_size: batch size of training data of secondary learner\n-        eval_freq: secondary model evaluation can be triggered at eval_freq\n-        igf_model_path: path to store trained secondary learner\n-\n-    Returns:\n-        Trained secondary learner\n-\n-    \"\"\"\n-    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    # We will use the first 512 pairs from our dataset as a test set for\n-    # our secondary learner and the rest to train\n-    test_dataset = train_dataset[:512]\n-    train_dataset = train_dataset[512:]\n-    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n-    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n-\n-    # secondary learner model set up\n-    loss = nn.MSELoss()\n-    test_loss = nn.MSELoss(reduction=\"sum\")\n-    secondary_learner.to(device)\n-    q_optimizer = torch.optim.Adam(secondary_learner.parameters(), lr=0.00001)\n-    secondary_learner.train()\n-\n-    # TODO in original code this is written as number of actual batches seen\n-    # not number of items seen but other places it is number of items instead.\n-    # improve consistency! changed this to epochs for clarity\n-    best_test_loss = float(\"inf\")\n-    # Iterate through batches until we've used max_steps batches\n-    for epoch in range(int(max_epochs)):\n-        tr_q_loss = 0.0\n-        secondary_learner.train()\n-        for step, batch in enumerate(train_dataloader):\n-            context = batch[0].to(device)\n-            real_q = batch[1].to(device)\n-            predicted_q = secondary_learner(context)\n-            q_optimizer.zero_grad()\n-            q_loss = loss(predicted_q, real_q.float())\n-            q_loss.backward()\n-            q_optimizer.step()\n-            tr_q_loss += q_loss.item()\n-\n-            # model trains fairly quickly so we won't wait for a full epoch\n-            # eval is triggered at eval_freq and end of epochs\n-            if (step % eval_freq == 0 and step > 0) or ((step + 1) == len(train_dataloader)):\n-                tr_loss = tr_q_loss / (step + 1)\n-\n-                secondary_learner.eval()\n-                q_loss2 = 0.0\n-                sum_q2 = 0.0\n-                predicted = []\n-                actual = []\n-                # Compute performance of the secondary learner after this batch\n-                for step2, batch2 in enumerate(test_dataloader):\n-                    features2 = batch2[0].to(device)\n-                    real_q2 = batch2[1].to(device)\n-                    predicted_q2 = secondary_learner(features2)\n-                    q_loss2 += test_loss(predicted_q2, real_q2).item()\n-                    sum_q2 += torch.sum(predicted_q2).item()\n-                    for ei, i in enumerate(predicted_q2.cpu().detach().numpy()):\n-                        predicted.append(i.item())\n-                    for ei, i in enumerate(real_q2.cpu().detach().numpy()):\n-                        actual.append(i.item())\n-\n-                q_loss2 /= len(test_dataset)\n-                print(\n-                    \"Epoch: \",\n-                    epoch,\n-                    \"step: \",\n-                    step,\n-                    \"Avg. q:\",\n-                    sum_q2 / len(test_dataset),\n-                    \"Train Loss: \",\n-                    tr_loss,\n-                    \"Test Loss: \",\n-                    q_loss2,\n-                )\n-                if q_loss2 < best_test_loss:\n-                    joblib.dump((predicted, actual), \"pred_vs_actual.jbl\")\n-                    torch.save(secondary_learner.state_dict(), igf_model_path)\n-                    best_test_loss = q_loss2\n-\n-            secondary_learner.train()\n-    return secondary_learner\n-\n-\n-class SecondaryLearner(nn.Module):\n-    \"\"\"\n-    Our secondary learner\n-    \"\"\"\n-\n-    def __init__(self, model):\n-        \"\"\"\n-        We use a simple convolutional network as our secondary learner\n-\n-        Args:\n-            model: Pre-trained GPT2 model\n-        \"\"\"\n-        # embeddings are from the pretrained model\n-        super(SecondaryLearner, self).__init__()\n-        self.embeddings = model.transformer.wte\n-        self.embeddings.weight = copy.deepcopy(model.transformer.wte.weight)\n-        self.conv = nn.Conv1d(self.embeddings.weight.size(1), 256, 3, padding=1)\n-        self.fc = nn.Sequential(nn.Linear(256, 32), nn.Dropout(p=0.1), nn.Linear(32, 32), nn.Linear(32, 1))\n-\n-    def forward(self, context):\n-        \"\"\"\n-        Forward pass through the secondary learner\n-\n-        Args:\n-            context: Context input to the secondary learner\n-\n-        Returns:\n-            tensor after squeeze operation\n-\n-        \"\"\"\n-        pooled = torch.max(self.conv(self.embeddings(context).squeeze(1).transpose(1, 2)), 2)[0]\n-        qs = self.fc(pooled)\n-        return qs.squeeze(1)\n-\n-    @classmethod\n-    def from_pretrained(cls, state_path, model):\n-        \"\"\"\n-        Load the secondary learner\n-\n-        Args:\n-            state_path: Path to save secondary learner\n-            model: Pretrained GPT-2\n-\n-        Returns:\n-            secondary learner\n-        \"\"\"\n-\n-        secondary_learner = cls(model)  # this calls __init__\n-        state_dict = torch.load(state_path)\n-        secondary_learner.load_state_dict(state_dict)\n-        secondary_learner.embeddings = model.transformer.wte\n-        secondary_learner.embeddings.weight = copy.deepcopy(model.transformer.wte.weight)\n-        return secondary_learner"
        },
        {
            "sha": "2aa3227637c888310bf557baa3b952a5aa680248",
            "filename": "examples/research_projects/information-gain-filtration/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Finformation-gain-filtration%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,6 +0,0 @@\n-matplotlib\n-numpy>=1.17.2\n-joblib>=0.13.2\n-scipy\n-torch>=1.10.1\n-transformers>=3.5\n\\ No newline at end of file"
        },
        {
            "sha": "10bb0b7d681630c668d11dec6c6606b9934f168e",
            "filename": "examples/research_projects/information-gain-filtration/result_igf.png",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Fresult_igf.png",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Fresult_igf.png",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Finformation-gain-filtration%2Fresult_igf.png?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "74973309c4e16b4aa6fc6e3ba4ed5e1e95a86ee0",
            "filename": "examples/research_projects/information-gain-filtration/run_clm_igf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 450,
            "changes": 450,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Frun_clm_igf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Finformation-gain-filtration%2Frun_clm_igf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Finformation-gain-filtration%2Frun_clm_igf.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,450 +0,0 @@\n-# Copyright 2022 - Intel Corp. All rights reserved.\n-# Authors: Mayank Kumar Raunak, Javier Turek, Nicole Beckage\n-\n-\"\"\"\n-Implementation of a new method for fine-tuning transformer models that we call\n-Information Gain Filtration 'IGF' on WikiText data set and compared the results\n-with the standard fine-tuning method\n-\n-Steps followed in the code:\n-\n-1) Generate a objective dataset of pairs (X, IG(X)). IG(X)--Informativeness of context 'X'.\n-Our IG (information gain) model is learning to predict the â€˜informativenessâ€™ of a particular\n-context. Informativeness is the change in metric between the modelâ€™s accuracy on an\n-objective set before and after seeing that context. For casual language modeling, the\n-metric is perplexity.\n-\n-2) A secondary learner is trained to infer a function approximation for IG using the dataset\n-created in (1).\n-\n-3) The learner created in (2) is used to inform the fine-tuning process and filter out low informative samples.\n-\n-Last, a plot is generated to compare the performance of IGF to standard fine-tuning without any filtering\n-\n-\"\"\"\n-\n-# Prerequisite libraries:\n-\n-import argparse\n-import random\n-\n-import joblib\n-import numpy as np\n-import torch\n-from igf.igf import (\n-    SecondaryLearner,\n-    collect_objective_set,\n-    compute_perplexity,\n-    generate_datasets,\n-    load_gpt2,\n-    recopy_gpt2,\n-    set_seed,\n-    train_secondary_learner,\n-)\n-from torch.utils.data import DataLoader, RandomSampler\n-\n-from transformers import GPT2LMHeadModel\n-\n-\n-def generate_n_pairs(\n-    context_len=32,\n-    max_steps=10,\n-    size_objective_set=100,\n-    min_len=1026,\n-    trim=True,\n-    data_file=\"data/tokenized_stories_train_wikitext103.jbl\",\n-    igf_data_file=\"igf_context_pairs.jbl\",\n-):\n-    \"\"\"\n-    Collecting *n* pairs for training the secondary learner\n-    Args:\n-        context_len: The maximum total input sequence length after tokenization. Sequences longer\n-                    than this will be truncated, sequences shorter will be padded\n-        max_steps: To calculate training epochs of secondary learner\n-        size_objective_set: size of objective data set used to create (X,IG(X)) pairs which is the training data for secondary learner\n-        min_len: The minimum length of the article to be used as objective set\n-        trim: If True truncate the context if it exceeds context length\n-        data_file: Tokenized data set split for training and evaluation of model\n-        igf_data_file: file to store (I,IG(X)) paired data set to train secondary learner\n-\n-    Returns:\n-        Data stored in igf_data_file\n-\n-    \"\"\"\n-    # generates same data everytime\n-    set_seed(3)\n-    # generate train_data and objective_set\n-    train_data, objective_set = generate_datasets(\n-        context_len, data_file, number=size_objective_set, min_len=1026, trim=True\n-    )\n-    # keeps model same across runs\n-    set_seed(4)\n-    # model, lm_optimizer, lm_scheduler = recopy_gpt2(model, device, max_steps) # store original model weights\n-    # can we train on GPU?\n-    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-\n-    # load pretrained model\n-    model = load_gpt2(\"openai-community/gpt2\").to(device)\n-    print(\"computing perplexity on objective set\")\n-    orig_perp = compute_perplexity(model, objective_set, context_len).item()\n-    print(\"perplexity on objective set:\", orig_perp)\n-\n-    # collect igf pairs and save to file demo.jbl\n-    collect_objective_set(model, orig_perp, context_len, train_data, objective_set, max_steps, device, igf_data_file)\n-\n-    # clean up, delete model and data we don't need anymore\n-    del model, train_data, objective_set\n-    torch.cuda.empty_cache()\n-\n-\n-def training_secondary_learner(\n-    secondary_learner_train_data,\n-    secondary_learner_max_epochs=15,\n-    secondary_learner_batch_size=128,\n-    eval_freq=100,\n-    igf_model_path=\"igf_model.pt\",\n-):\n-    \"\"\"\n-    Train the secondary learner\n-\n-    Args:\n-        secondary_learner_train_data: Data set with (X,IG(X)) pairs to train secondary learner where IG(X) - measure of informativeness and X- context\n-        secondary_learner_max_epochs: Number of epochs to train secondary learner\n-        secondary_learner_batch_size: Batch size to train secondary learner\n-        eval_freq (object): secondary model evaluation can be triggered at eval_freq\n-        igf_model_path: path to store trained secondary learner\n-\n-    Returns:\n-        Trained secondary learner\n-    \"\"\"\n-\n-    set_seed(42)\n-\n-    # Load pre-trained model\n-    model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-\n-    # Initialize secondary learner to use embedding weights of model\n-    secondary_learner = SecondaryLearner(model)\n-\n-    # Train secondary learner\n-    secondary_learner = train_secondary_learner(\n-        secondary_learner,\n-        secondary_learner_train_data,\n-        max_epochs=secondary_learner_max_epochs,\n-        batch_size=secondary_learner_batch_size,\n-        eval_freq=100,\n-        igf_model_path=igf_model_path,\n-    )\n-\n-    del model, secondary_learner_train_data\n-    torch.cuda.empty_cache()\n-\n-    return secondary_learner\n-\n-\n-def finetune(\n-    model,\n-    train_dataset,\n-    test_dataset,\n-    context_len=32,\n-    max_steps=1000,\n-    batch_size=16,\n-    threshold=1.0,\n-    recopy_model=recopy_gpt2,\n-    secondary_learner=None,\n-    eval_interval=10,\n-    finetuned_model_name=\"openai-community/gpt2_finetuned.pt\",\n-):\n-    \"\"\"\n-    fine-tune with IGF if secondary_learner is not None, else standard fine-tuning\n-\n-    Args:\n-        model: pre-trained GPT-2 model\n-        train_dataset: Data set to train GPT-2 model\n-        test_dataset: Evaluate GPT-2 model\n-        context_len: The maximum total input sequence length after tokenization. Sequences longer\n-                    than this will be truncated, sequences shorter will be padded\n-        max_steps: To calculate training epochs\n-        batch_size: Batch size to train GPT-2 model\n-        threshold: The threshold value used by secondary learner to filter the train_data and allow only\"\n-                    informative data as input to the model\n-        recopy_model: Reset the model to the original pretrained GPT-2 weights after each iteration\n-        secondary_learner: Selection of IGF as fine-tuning method if not None\n-        eval_interval: number of batches after which decay the selectivity of our secondary learner filter from\n-                        1 standard deviation above average to 1 below average\n-        fine-tuned_model_name: name of the final final-tuned GPT-2 model\n-\n-    Returns:\n-        Fine-tuned GPT-2 model\n-\n-    \"\"\"\n-\n-    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-    train_sampler = RandomSampler(train_dataset)\n-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler)\n-\n-    num_train_epochs = max_steps // (len(train_dataset)) + 1\n-    global_step = 0\n-    context = torch.zeros((1, context_len), dtype=torch.long, device=device)\n-    model, lm_optimizer, lm_scheduler = recopy_model(model, device, max_steps)\n-\n-    model.train()\n-    if secondary_learner is not None:\n-        secondary_learner.to(device)\n-        secondary_learner.eval()\n-    contexts = []\n-    examples = 0\n-\n-    observed_qs = []\n-    test_perps = []\n-\n-    # Compute the performance of the transformer model at the beginning\n-    real_perp = compute_perplexity(model, test_dataset, context_len)\n-    test_perps.append(real_perp)\n-    print(\"Test perplexity, step\", global_step, \":\", real_perp)\n-    for epoch in range(int(num_train_epochs)):\n-        for step, example in enumerate(train_dataloader):\n-            torch.cuda.empty_cache()\n-            start = random.randint(0, example.size(2) - context_len - 1)\n-            context[0, :] = example[0, 0, start : start + context_len]\n-            lm_optimizer.zero_grad()\n-            outputs = model(context, labels=context)\n-            do_backprop = True\n-\n-            if secondary_learner is not None:\n-                predicted_q = secondary_learner.forward(\n-                    torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0)\n-                )[0].item()\n-                observed_qs.append(float(predicted_q))\n-\n-                # Here we implement the simple non-constant threshold for the predicted IG(X) value\n-                # We will decay the selectivity of our secondary learner filter from\n-                # 1 standard deviation above average to 1 below average after 10 batches.\n-\n-                if global_step == 10:\n-                    threshold = -1\n-                if predicted_q < threshold:\n-                    do_backprop = False\n-\n-            # If we passed the filter, add the context to the batch!\n-            if do_backprop:\n-                contexts.append(np.array(context.cpu()))\n-                lm_loss = outputs[0]\n-                lm_loss.backward()\n-                examples += 1\n-\n-            del outputs\n-\n-            # Once the batch is filled with enough contexts, backprop on the batch.\n-            if examples == batch_size:\n-                torch.cuda.empty_cache()\n-                examples = 0\n-                # Do LM backprop\n-                torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n-                lm_optimizer.step()\n-                lm_scheduler.step()  # Update learning rate schedule\n-                global_step += 1\n-                # Compute the performance of the transformer model at this batch\n-                if global_step % eval_interval == 0:\n-                    real_perp = compute_perplexity(model, test_dataset, context_len)\n-                    test_perps.append(real_perp)\n-\n-                    print(\"Test perplexity, step\", global_step, \":\", real_perp)\n-            # Break out of the loop after 60 batches\n-            if max_steps > 0 and global_step > 60:\n-                break\n-        if max_steps > 0 and global_step > 60:\n-            break\n-\n-    # save finetuned transformer model\n-    torch.save(model.state_dict(), finetuned_model_name)\n-    torch.cuda.empty_cache()\n-    # Do some cleaning up so we can reinitialize for the next run of this function\n-    del lm_optimizer\n-    del lm_scheduler\n-    return model\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser(description=\"Fine-tune a transformer model with IGF on a language modeling task\")\n-\n-    # Required parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The input data dir. Should contain data files for WikiText.\",\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-    )\n-    parser.add_argument(\n-        \"--data_file\",\n-        type=str,\n-        default=None,\n-        help=(\n-            \"A jbl file containing tokenized data which can be split as objective dataset, \"\n-            \"train_dataset and test_dataset.\"\n-        ),\n-    )\n-\n-    parser.add_argument(\n-        \"--igf_data_file\",\n-        type=str,\n-        default=None,\n-        help=\"A jbl file containing the context and information gain pairs to train secondary learner.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the final fine-tuned model is stored.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=None,\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n-\n-    parser.add_argument(\n-        \"--context_len\",\n-        default=32,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after tokenization. Sequences longer \"\n-            \"than this will be truncated, sequences shorter will be padded.\"\n-        ),\n-    )\n-\n-    parser.add_argument(\n-        \"--size_objective_set\",\n-        default=100,\n-        type=int,\n-        help=\"number of articles that are long enough to be used as our objective set\",\n-    )\n-    parser.add_argument(\n-        \"--eval_freq\", default=100, type=int, help=\"secondary model evaluation is triggered at eval_freq\"\n-    )\n-\n-    parser.add_argument(\"--max_steps\", default=1000, type=int, help=\"To calculate training epochs\")\n-\n-    parser.add_argument(\n-        \"--secondary_learner_batch_size\",\n-        default=128,\n-        type=int,\n-        help=\"batch size of training data for secondary learner\",\n-    )\n-\n-    parser.add_argument(\n-        \"--batch_size\",\n-        default=16,\n-        type=int,\n-        help=\"batch size of training data of language model(openai-community/gpt2) \",\n-    )\n-\n-    parser.add_argument(\n-        \"--eval_interval\",\n-        default=10,\n-        type=int,\n-        help=(\n-            \"decay the selectivity of our secondary learner filter from \"\n-            \"1 standard deviation above average to 1 below average after 10 batches\"\n-        ),\n-    )\n-\n-    parser.add_argument(\n-        \"--number\", default=100, type=int, help=\"The number of examples split to be used as objective_set/test_data\"\n-    )\n-\n-    parser.add_argument(\n-        \"--min_len\", default=1026, type=int, help=\"The minimum length of the article to be used as objective set\"\n-    )\n-\n-    parser.add_argument(\n-        \"--secondary_learner_max_epochs\", default=15, type=int, help=\"number of epochs to train secondary learner\"\n-    )\n-\n-    parser.add_argument(\"--trim\", default=True, type=bool, help=\"truncate the example if it exceeds context length\")\n-\n-    parser.add_argument(\n-        \"--threshold\",\n-        default=1.0,\n-        type=float,\n-        help=(\n-            \"The threshold value used by secondary learner to filter the train_data and allow only\"\n-            \" informative data as input to the model\"\n-        ),\n-    )\n-\n-    parser.add_argument(\n-        \"--finetuned_model_name\", default=\"openai-community/gpt2_finetuned.pt\", type=str, help=\"finetuned_model_name\"\n-    )\n-\n-    parser.add_argument(\n-        \"--recopy_model\",\n-        default=recopy_gpt2,\n-        type=str,\n-        help=\"Reset the model to the original pretrained GPT-2 weights after each iteration\",\n-    )\n-\n-    # function calls\n-    # Collecting *n* pairs of context and information gain(X, IG(X)) for training the secondary learner\n-    generate_n_pairs(\n-        context_len=32,\n-        max_steps=10,\n-        size_objective_set=100,\n-        min_len=1026,\n-        trim=True,\n-        data_file=\"data/tokenized_stories_train_wikitext103.jbl\",\n-        igf_data_file=\"igf_context_pairs.jbl\",\n-    )\n-\n-    # Load train data for secondary learner\n-    secondary_learner_train_data = joblib.load(\"data/IGF_values.jbl\")\n-\n-    # Train secondary learner\n-    secondary_learner = training_secondary_learner(\n-        secondary_learner_train_data,\n-        secondary_learner_max_epochs=15,\n-        secondary_learner_batch_size=128,\n-        eval_freq=100,\n-        igf_model_path=\"igf_model.pt\",\n-    )\n-\n-    # load pretrained openai-community/gpt2 model\n-    model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-    set_seed(42)\n-\n-    # Generate train and test data to train and evaluate openai-community/gpt2 model\n-    train_dataset, test_dataset = generate_datasets(\n-        context_len=32, file=\"data/tokenized_stories_train_wikitext103.jbl\", number=100, min_len=1026, trim=True\n-    )\n-\n-    # fine-tuning of the openai-community/gpt2 model using igf (Information Gain Filtration)\n-    finetune(\n-        model,\n-        train_dataset,\n-        test_dataset,\n-        context_len=32,\n-        max_steps=1000,\n-        batch_size=16,\n-        threshold=1.0,\n-        recopy_model=recopy_gpt2,\n-        secondary_learner=secondary_learner,\n-        eval_interval=10,\n-        finetuned_model_name=\"openai-community/gpt2_finetuned.pt\",\n-    )\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "08e05f38931943134ac8c4457ded19da7d41abc4",
            "filename": "examples/research_projects/jax-projects/HOW_TO_PROPOSE_PROJECT.md",
            "status": "removed",
            "additions": 0,
            "deletions": 109,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2FHOW_TO_PROPOSE_PROJECT.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2FHOW_TO_PROPOSE_PROJECT.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2FHOW_TO_PROPOSE_PROJECT.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,109 +0,0 @@\n-# How to propose a Flax/JAX + Transformers project \n-\n-Great that you've opened this document! \n-While we at ğŸ¤— are proposing a couple of projects, we strongly \n-believe that the community can come up with much more **creative**, **fun**, and \n-**impactful** projects on their own. This being said, we are really looking forward\n-to seeing your project proposal! \n-\n-## What a project should be about\n-\n-The proposed project should fall into the machine learning fields of **Natural Language Processing (NLP)** and/or **Computer Vision (CV)** (possibly also **Speech Recognition (ASR)** depending on whether Speech Recognition models are available in Flax in due time) and aim at solving a specific task. \n-Possible tasks can belong to: \n-\n- * text classification\n- * text generation\n- * image recognition\n- * image processing\n- * image captioning\n- * audio classification\n- * and other tasks you can think of!\n-\n-The clearer a task is defined, the better your project proposal is.\n-*E.g.* \"Using a T5 model to learn grammar correction in French\" or \"Adapting a pre-trained CLIP model for zero-shot image classification in Spanish\" are **well-defined and clear** project proposals, while something like \"Train a language model\" or \"Image classification\" are **too vague**.\n-\n-There is no limit to your creativity as long as the project is feasible and ethical.\n-The more creative & specific your project proposal, the more interesting it will be, \n-and the more likely will you find motivated team members to work on your project!\n-To get an idea of how to formulate your project proposals, you can browse through \n-existing project proposals on the [forum](https://discuss.huggingface.co/c/flax-jax-projects/22).\n-\n-## How to submit a project proposal\n-\n-First, you should make sure that you are [logged in](https://huggingface.co/login?sso=bm9uY2U9OTRlNjZjZmZhYjMwMmJmMWMyYjc5MmFiMTMyMzY5ODYmcmV0dXJuX3Nzb191cmw9aHR0cHMlM0ElMkYlMkZkaXNjdXNzLmh1Z2dpbmdmYWNlLmNvJTJGc2Vzc2lvbiUyRnNzb19sb2dpbg%3D%3D&sig=429ad8924bcb33c40f9823027ea749abb55d393f4f58924f36a2dba3ab0a48da) with your Hugging Face account on the forum. \n-\n-Second, make sure that your project idea doesn't already exist by checking [existing projects](https://discuss.huggingface.co/c/flax-jax-projects/22). \n-If your project already exists - great! This means that you can comment and improve\n-the existing idea and join the project to form a team! If your project idea already \n-exists for a different language, feel free to submit the same project idea, just in \n-a different language.\n-\n-Third, having ensured that your project doesn't exist, click on the *\"New Topic\"*\n-button on the [Flax/JAX Projects Forum category](https://discuss.huggingface.co/c/flax-jax-projects/22) to create a new project proposal.\n-\n-Fourth, make sure that your project proposal includes the following information:\n-\n-1. *A clear description of the project*\n-2. *In which language should the project be conducted?* English, German, Chinese, ...? It can also be a multi-lingual project\n-3. *Which model should be used?* If you want to adapt an existing model, you can add the link to one of the 4000 available checkpoints in JAX [here](https://huggingface.co/models?filter=jax) If you want to train a model from scratch, you can simply state the model architecture to be used, *e.g.* BERT, CLIP, etc. You can also base your project on a model that is not part of transformers. For an overview of libraries based on JAX, you can take a look at [awesome-jax](https://github.com/n2cholas/awesome-jax#awesome-jax-). **Note** that for a project that is not based on Transformers it will be more difficult for the ğŸ¤— team to help you. Also have a look at the section [Quickstart Flax & Jax in Transformers](https://github.com/huggingface/transformers/tree/main/examples/research_projects/jax-projects#quickstart-flax-and-jax-in-transformers) to see what model architectures are currently supported in ğŸ¤— Transformers.\n-4. *What data should be used?* It is important to state at least what kind of data you would like to use. Ideally, you can already point to publicly available data or a dataset in the ğŸ¤— Datasets library.\n-5. *Are similar training scripts available in Flax/JAX?* It would be important to find similar training scripts that already exist in Flax/JAX. *E.g.* if you are working on a Seq-to-Seq task, you can make use of the [`run_summarization_flax.py`](https://github.com/huggingface/transformers/blob/main/examples/flax/summarization/run_summarization_flax.py) script which is very similar to any seq2seq training. Also have a look at the section [Quickstart Flax & Jax in Transformers](https://github.com/huggingface/transformers/tree/main/examples/research_projects/jax-projects#quickstart-flax-and-jax-in-transformers) to see what training scripts are currently supported in ğŸ¤— Transformers.\n-6. *(Optionally) What are possible challenges?* List possible difficulties with your project. *E.g.* If you know that training convergence usually takes a lot of time, it is worth stating this here!\n-7. *(Optionally) What is the desired project outcome?* - How would you like to demo your project? One could *e.g.* create a Streamlit application.\n-8. *(Optionally) Links to read upon* - Can you provide any links that would help the reader to better understand your project idea?\n-\n-Feel free to copy-paste the following format for your project proposal and fill out the respective sections: \n-\n-```\n-# <FILL ME: Name of project>\n-\n-<FILL ME: A clear description of the project>\n-\n-## 2. Language\n-\n-The model will be trained in <FILL ME: which language?>.\n-\n-## 3. Model\n-\n-<FILL ME: 3. Which model should be used?>\n-\n-## 4. Datasets\n-\n-<FILL ME: 4. Which data should be used?>\n-\n-Possible links to publicly available datasets include:\n-- <FILL ME: Link 1 to dataset> \n-- <FILL ME: Link 2 to dataset> \n-- <FILL ME: Link 3 to dataset> \n-\n-## 5. Training scripts\n-\n-<FILL ME: 5. Are there publicly available training scripts that can be used/tweaked for the project?>\n-\n-We can make use of <FILL ME: link to training script> to train the model.>\n-\n-## 6. (Optional) Challenges\n-\n-<(Optionally) FILL ME: 6. What are possible challenges?>\n-\n-## 7. (Optional) Desired project outcome\n-\n-<(Optionally) FILL ME: 7. What is the desired project outcome? A demo?>\n-\n-## 8. (Optional) Reads\n-\n-The following links can be useful to better understand the project and \n-what has previously been done.\n-\n-- <FILL ME: Link 1 to read> \n-- <FILL ME: Link 2 to read> \n-- <FILL ME: Link 3 to read> \n-```\n-\n-To see how a proposed project looks like, please have a look at submitted project \n-proposals [here](https://discuss.huggingface.co/c/flax-jax-projects/22).\n-\n-## Will my project proposal be selected?\n-\n-Having submitted a project proposal, you can now promote your idea in the Slack channel `#flax-jax-community-week` to try to convince other participants to join your project! \n-Once other people have joined your project, one of the organizers (`@Suzana, @valhalla, @osanseviero, @patrickvonplaten`) will officially create a team for your project and add your project to [this google sheet](https://docs.google.com/spreadsheets/d/1GpHebL7qrwJOc9olTpIPgjf8vOS0jNb6zR_B8x_Jtik/edit?usp=sharing)."
        },
        {
            "sha": "88d8d7f9eba9269ed9659463a690b6b76d890d42",
            "filename": "examples/research_projects/jax-projects/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 1295,
            "changes": 1295,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,1295 +0,0 @@\n-# Flax/JAX community week ğŸ¤—\n-\n-Welcome to the Flax/JAX community week! The goal of this week is to make compute-intensive NLP and CV projects (like pre-training BERT, GPT2, CLIP, ViT) \n-practicable for a wider audience of engineers and researchers. \n-To do so, we will try to teach **you** how to effectively use JAX/Flax on TPU and help you to complete a fun NLP and/or CV project in JAX/Flax during the community week. \n-\n-Free access to a TPUv3-8 will kindly be provided by the Google Cloud team!\n-\n-In this document, we list all the important information that you will need during the Flax/JAX community week.\n-\n-Don't forget to sign up [here](https://forms.gle/tVGPhjKXyEsSgUcs8)! \n-\n-## Table of Contents\n-\n-- [Organization](#organization)\n-- [Important dates](#important-dates)\n-- [Communication](#communication)\n-- [Projects](#projects)\n-\t- [How to propose](#how-to-propose-a-project)\n-\t- [How to form a team](#how-to-form-a-team-around-a-project)\n-- [Tips & Tricks for project](#tips-on-how-to-organize-the-project)\n-- [How to install flax, jax, optax, transformers, datasets](#how-to-install-relevant-libraries)\n-- [Quickstart Flax/JAX](#quickstart-flax-and-jax)\n-- [Quickstart Flax/JAX in ğŸ¤— Transformers](#quickstart-flax-and-jax-in-transformers)\n-    - [Flax design philosophy in ğŸ¤— Transformers](#flax-design-philosophy-in-transformers)\n-    - [How to use flax models & scripts](#how-to-use-flax-models-and-example-scripts)\n-- [Talks](#talks)\n-- [How to use the ğŸ¤— Hub for training](#how-to-use-the-hub-for-collaboration)\n-- [How to setup TPU VM](#how-to-setup-tpu-vm)\n-- [How to build a demo](#how-to-build-a-demo)\n-    - [Using the Hugging Face Widgets](#using-the-hugging-face-widgets)\n-    - [Using a Streamlit demo](#using-a-streamlit-demo)\n-    - [Using a Gradio demo](#using-a-gradio-demo)\n-- [Project evaluation](#project-evaluation)\n-- [General Tips & Tricks](#general-tips-and-tricks)\n-- [FAQ](#faq)\n-\n-## Organization\n-\n-Participants can propose ideas for an interesting NLP and/or CV project. Teams of 3 to 5 will then be formed around the most promising and interesting projects. Make sure to read through the [Projects](#projects) section on how to propose projects, comment on other participants' project ideas, and create a team.\n-\n-To help each team successfully finish their project, we have organized talks by leading scientists and engineers from Google, Hugging Face, and the open-source NLP & CV community. The talks will take place before the community week from June 30th to July 2nd. Make sure to attend the talks to get the most out of your participation! Check out the [Talks](#talks) section to get an overview of the talks, including the speaker and the time of the talk.\n-\n-Each team is then given **free access to a TPUv3-8 VM** from July 7th to July 14th. In addition, we will provide training examples in JAX/Flax for a variety of NLP and Vision models to kick-start your project. During the week, we'll make sure to answer any questions you might have about JAX/Flax and Transformers and help each team as much as possible to complete their project!\n-\n-At the end of the community week, each team should submit a demo of their project. All demonstrations will be evaluated by a jury and the top-3 demos will be awarded a prize. Check out the [How to submit a demo](#how-to-submit-a-demo) section for more information and suggestions on how to submit your project.\n-\n-## Important dates\n-\n-- **23.06.** Official announcement of the community week. Make sure to sign-up in [this google form](https://forms.gle/tVGPhjKXyEsSgUcs8).\n-- **23.06. - 30.06.** Participants will be added to an internal Slack channel. Project ideas can be proposed here and groups of 3-5 are formed. Read this document for more information. \n-- **30.06.** Release of all relevant training scripts in JAX/Flax as well as other documents on how to set up a TPU, how to use the training scripts, how to submit a demo, tips & tricks for JAX/Flax, tips & tricks for efficient use of the hub.\n-- **30.06. - 2.07.** Talks about JAX/Flax, TPU, Transformers, Computer Vision & NLP will be held. \n-- **7.07.** Start of the community week! Access to TPUv3-8 will be given to each team.\n-- **7.07. - 14.07.** The Hugging Face & JAX/Flax & Cloud team will be available for any questions, problems the teams might run into.\n-- **15.07.** Access to TPU is deactivated and community week officially ends.\n-- **16.07.** Deadline for each team to submit a demo. \n-\n-## Communication\n-\n-All important communication will take place in an internal Slack channel, called `#flax-jax-community-week`. \n-Important announcements of the Hugging Face, Flax/JAX, and Google Cloud team will be posted there. \n-Such announcements include general information about the community week (Dates, Rules, ...), release of relevant training scripts (Flax/JAX example scripts for NLP and Vision), release of other important documents (How to access the TPU), etc. \n-The Slack channel will also be the central place for participants to post about their results, share their learning experiences, ask questions, etc.\n-\n-For issues with Flax/JAX, Transformers, Datasets or for questions that are specific to your project we would be **very happy** if you could use the following public repositories and forums:\n-\n-- Flax: [Issues](https://github.com/google/flax/issues), [Questions](https://github.com/google/flax/discussions)\n-- JAX: [Issues](https://github.com/google/jax/issues), [Questions](https://github.com/google/jax/discussions)\n-- ğŸ¤— Transformers: [Issues](https://github.com/huggingface/transformers/issues), [Questions](https://discuss.huggingface.co/c/transformers/9)\n-- ğŸ¤— Datasets: [Issues](https://github.com/huggingface/datasets/issues), [Questions](https://discuss.huggingface.co/c/datasets/10)\n-- Project specific questions: [Forum](https://discuss.huggingface.co/c/flax-jax-projects/22)\n-- TPU related questions: [TODO]()\n-\n-Please do **not** post the complete issue/project-specific question in the Slack channel, but instead a link to your issue/question that we will try to answer as soon as possible. \n-This way, we make sure that the everybody in the community can benefit from your questions - even after the community week - and that the same question is not answered twice.\n-\n-To be invited to the Slack channel, please make sure you have signed up [on the Google form](https://forms.gle/tVGPhjKXyEsSgUcs8). \n-\n-**Note**: If you have signed up on the google form, but you are not in the Slack channel, please leave a message on [(TODO) the official forum announcement]( ) and ping `@Suzana` and `@patrickvonplaten`.\n-\n-## Projects\n-\n-During the first week after the community week announcement, **23.06. - 30.06.**, teams will be formed around the most promising and interesting project ideas. Each team can consist of 2 to 10 participants. Projects can be accessed [here](https://discuss.huggingface.co/c/flax-jax-projects/22).\n-\n-All officially defined projects can be seen [here](https://docs.google.com/spreadsheets/d/1GpHebL7qrwJOc9olTpIPgjf8vOS0jNb6zR_B8x_Jtik/edit?usp=sharing).\n-\n-### How to propose a project\n-\n-Some default project ideas are given by the organizers. **However, we strongly encourage participants to submit their own project ideas!**\n-Check out the [HOW_TO_PROPOSE_PROJECT.md](https://github.com/huggingface/transformers/tree/main/examples/research_projects/jax-projects/HOW_TO_PROPOSE_PROJECT.md) for more information on how to propose a new project.\n-\n-### How to form a team around a project\n-\n-You can check out all existing projects ideas on the forum under [Flax/JAX projects category](https://discuss.huggingface.co/c/flax-jax-projects/22).\n-Make sure to quickly check out each project idea and leave a â¤ï¸  if you like an idea. \n-Feel free to leave comments, suggestions for improvement, or questions about more details directly on the discussion thread. \n-If you have found the project that you â¤ï¸  the most, leave a message \"I would like to join this project\" on the discussion thread. \n-We strongly advise you to also shortly state who you are, which time zone you are in and why you would like to work on this project, how you can contribute to the project and what your vision is for the project.\n-For projects that see a lot of interest and for which enough participants have expressed interest in joining, an official team will be created by the organizers. \n-One of the organizers (`@Suzana`, `@valhalla`, `@osanseviero`, `@patrickvonplaten`) will leave a message \"For this project the team: `<team_name>`, `<team_members>` , is officially created\" on the thread and note down the teams on [this google sheet](https://docs.google.com/spreadsheets/d/1GpHebL7qrwJOc9olTpIPgjf8vOS0jNb6zR_B8x_Jtik/edit?usp=sharing).\n-\n-Once created, the team can start refining their project:\n-\n-- What is the goal of the project? *E.g.*, Present a language model that writes poetry in Russian.\n-- What model will we use? *E.g.*, FlaxGPT2\n-- What data will we use? *E.g.* Russian dataset of OSCAR & publicly available book on poetry\n-- Should we use a pre-trained model or train a model from scratch? E.g. Train a model from scratch\n-- What training scripts do we need? *E.g.* `transformers/examples/flax/run_clm_flax.py` can be used\n-- What kind of demo would we like to present? E.g. Text-generation API of the ğŸ¤— Hub in combination with a Streamlit demo that lets the user generate a poem of a given length\n-- How will the work be divided? *E.g.* Team member 1 works on data preprocessing, Team member 2 works on adapting the Flax script, ...\n-\n-We highly recommend that each team discusses all relevant ideas for their project directly on the forum thread. \n-This way valuable learning experiences are shared and accessible by the whole community in the future. \n-Additionally, the organizers, other participants, or anybody in the community really can read through your discussions and leave comments/tips for improvement. Obviously, you can also create private chats, ... to discuss more sensitive topics, etc.\n-\n-**Important**:\n-\n-- For project ideas that see a lot of interest, we are more than happy to create more than one team.\n-- Participants are welcome to join multiple teams, even though we encourage them to only work on a single project.\n-- Under special circumstances, participants can change/create new teams. Please note that we would like to keep this the exception. If however, you would like to change/leave existing teams, please leave a post on the project's thread where you ping the corresponding organizer that created the group.\n- - It is often easy to propose/join a project that is done in your native language. Feel free to reach out to existing [language-specific groups](https://discuss.huggingface.co/c/languages-at-hugging-face/15) to look for community members that might be interested in joining your project.\n-\n-## Tips on how to organize the project\n-\n-This section gives you some tips on how to most efficiently & effectively \n-work as a team to achieve your goal. It is by no means a strict recipe to follow, \n-but rather a collection of tips from the ğŸ¤— team.\n-\n-Once your team is defined, you can start working on the project as soon as possible. \n-\n-\n-### Communication\n-\n-At first, it is always useful to get to know each other and to set up a means of communication.\n-While we recommend that all technical aspects of work can be discussed directly on the [forum](https://discuss.huggingface.co/c/flax-jax-projects/22) under your project thread, \n-it can be very helpful to have a more direct way of communicating, *e.g.* in a channel. \n-For this we have created a discord that you can access [here](https://discord.com/channels/858019234139602994/858019234139602997). \n-This discord will not be managed by anybody and is just there so that you can communicate more effectively with your team members. \n-Feel free to create a new channel for you and your team where you can discuss everything. If you and your team have already set up other ways of communicating, it is absolutely not required to make use of the discord. However, we do recommend each team to set up some kind \n-of channel or group for quick discussions.\n-\n-### Project definition\n-\n-In the very beginning, you should make sure your project is well-defined and that \n-everybody in the team understands the goal of the project and the work that needs to be \n-done in order to achieve the goal. A well-defined project:\n-\n-- has defined the task on which the model will be trained\n-- has defined the model that will be trained\n-- has defined the datasets that will be used for training\n-- has defined the type of training scripts that need to be written\n-- has defined the desired outcome of the project\n-- has defined the workflows\n-\n-By \"has defined\" we don't meant that the corresponding code already has to be written and ready \n-to be used, but that everybody in team is on the same page on what type of model, data and training script should be used.\n-\n-To give an example, a well-defined project would be the following:\n-\n-- task: summarization\n-- model: [google-t5/t5-small](https://huggingface.co/google-t5/t5-small)\n-- dataset: [CNN/Daily mail](https://huggingface.co/datasets/cnn_dailymail)\n-- training script: [run_summarization_flax.py](https://github.com/huggingface/transformers/blob/main/examples/flax/summarization/run_summarization_flax.py)\n-- outcome: t5 model that can summarize news\n-- work flow: adapt `run_summarization_flax.py` to work with `google-t5/t5-small`.\n-\n-This example is a very easy and not the most interesting project since a `google-t5/t5-small`\n-summarization model exists already for CNN/Daily mail and pretty much no code has to be \n-written. \n-A well-defined project does not need to have the dataset be part of \n-the `datasets` library and the training script already be pre-written, however it should \n-be clear how the desired dataset can be accessed and how the training script can be \n-written. \n-\n-It is also important to have a clear plan regarding the workflow. Usually, the \n-data processing is done in a first step. Once the data is in a format that the model can \n-work with, the training script can be written, etc. These steps should be more detailed \n-once the team has a clearly defined project. It can be helpful to set deadlines for each step.\n-\n-### Workload division\n-\n-To effectively work as a team, it is crucial to divide the workload among everybody.\n-Some team members will be more motivated and experienced than others and \n-some team members simply want to participate to learn more and cannot contribute that \n-much to the team. This is totally fine! One cannot expect everybody in the team to have the same level of experience and time/motivation during the community week.\n-\n-As a conclusion, being honest about one's expected involvement is crucial so that \n-the workload can be divided accordingly. If someone doesn't think her/his tasks are feasible - let \n-the team know early on so that someone else can take care of it!\n-\n-It is recommended that the motivated and experienced team members take the lead in dividing the work and are ready to take over the tasks of another team member if necessary. \n-\n-The workload can often be divided according to:\n-\n-- data preprocessing (load the data and preprocess data in the correct format)\n-- data tokenization / data collator (process data samples into tokens or images)\n-- model configuration (writing the code that defines the model)\n-- model forward pass (make sure input / output work correctly)\n-- loss function (define the loss function)\n-- putting the pieces together in a training script\n-\n-Many of the steps above require other steps to be finished, so it often makes sense \n-to use dummy data in the expected format to start, *e.g.*, with the model forward pass \n-before the data preprocessing is done.\n-\n-### Expectations\n-\n-It is also very important to stay realistic with the scope of your project. Each team \n-has access to a TPUv3-8 for only *ca.* 10 days, so it's important to keep the scope of \n-the project reasonable. While we do want each team to work on interesting projects, each \n-team should make sure that the project goals can be achieved within the provided compute \n-time on TPU. For instance, pretraining a 11 billion parameters T5 model is not really a realistic \n-task with just 10 days of TPUv3-8 compute. \n-Also, it might be difficult to finish a project where the whole modeling, dataset and training code has to be written from scratch.\n-\n-Having defined your project, feel free to reach out on Slack or the forum for feedback from the organizers. We can surely give you our opinion on whether the project is feasible and what can be done to improve it.\n-the project is feasible.\n-\n-### Other tips\n-\n-Here is a collection of some more tips:\n-\n-- We strongly recommend to work as publicly and collaboratively as possible during the week so that other teams \n-and the organizers can best help you. This includes publishing important discussions on \n-the forum and making use of the [ğŸ¤— hub](http://huggingface.co/) to have a version \n-control for your models and training logs.\n-- When debugging, it is important that the debugging cycle is kept as short as possible to \n-be able to effectively debug. *E.g.* if there is a problem with your training script, \n-you should run it with just a couple of hundreds of examples and not the whole dataset script. This can be done by either making use of [datasets streaming](https://huggingface.co/docs/datasets/master/dataset_streaming?highlight=streaming) or by selecting just the first \n-X number of data samples after loading:\n-\n-```python\n-datasets[\"train\"] = datasets[\"train\"].select(range(1000))\n-```\n-- Ask for help. If you are stuck, use the public Slack channel or the [forum](https://discuss.huggingface.co/c/flax-jax-projects/22) to ask for help.\n-\n-## How to install relevant libraries\n-\n-In the following we will explain how to install all relevant libraries on your local computer and on TPU VM.\n-\n-It is recommended to install all relevant libraries both on your local machine \n-and on the TPU virtual machine. This way, quick prototyping and testing can be done on\n-your local machine and the actual training can be done on the TPU VM.\n-\n-### Local computer\n-\n-The following libraries are required to train a JAX/Flax model with ğŸ¤— Transformers and ğŸ¤— Datasets:\n-\n-- [JAX](https://github.com/google/jax/)\n-- [Flax](https://github.com/google/flax)\n-- [Optax](https://github.com/deepmind/optax)\n-- [Transformers](https://github.com/huggingface/transformers)\n-- [Datasets](https://github.com/huggingface/datasets)\n-\n-You should install the above libraries in a [virtual environment](https://docs.python.org/3/library/venv.html). \n-If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). Create a virtual environment with the version of Python you're going\n-to use and activate it.\n-\n-You should be able to run the command:\n-\n-```bash\n-python3 -m venv <your-venv-name>\n-```\n-\n-You can activate your venv by running\n-\n-```bash\n-source ~/<your-venv-name>/bin/activate\n-```\n-\n-We strongly recommend to make use of the provided JAX/Flax examples scripts in [transformers/examples/flax](https://github.com/huggingface/transformers/tree/main/examples/flax) even if you want to train a JAX/Flax model of another github repository that is not integrated into ğŸ¤— Transformers.\n-In all likelihood, you will need to adapt one of the example scripts, so we recommend forking and cloning the ğŸ¤— Transformers repository as follows. \n-Doing so will allow you to share your fork of the Transformers library with your team members so that the team effectively works on the same code base. It will also automatically install the newest versions of `flax`, `jax` and `optax`.\n-\n-1. Fork the [repository](https://github.com/huggingface/transformers) by\n-   clicking on the 'Fork' button on the repository's page. This creates a copy of the code\n-   under your GitHub user account.\n-\n-2. Clone your fork to your local disk, and add the base repository as a remote:\n-\n-   ```bash\n-   $ git clone https://github.com/<your Github handle>/transformers.git\n-   $ cd transformers\n-   $ git remote add upstream https://github.com/huggingface/transformers.git\n-   ```\n-\n-3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n-\n-   ```bash\n-   $ git checkout -b a-descriptive-name-for-my-project\n-   ```\n-\n-4. Set up a flax environment by running the following command in a virtual environment:\n-\n-   ```bash\n-   $ pip install -e \".[flax]\"\n-   ```\n-\n-   (If transformers was already installed in the virtual environment, remove\n-   it with `pip uninstall transformers` before reinstalling it in editable\n-   mode with the `-e` flag.)\n-\n-   If you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\n-   library.\n-\n-   Running this command will automatically install `flax`, `jax` and `optax`.\n-\n-Next, you should also install the ğŸ¤— Datasets library. We strongly recommend installing the \n-library from source to profit from the most current additions during the community week.\n-\n-Simply run the following steps:\n-\n-```bash\n-$ cd ~/\n-$ git clone https://github.com/huggingface/datasets.git\n-$ cd datasets\n-$ pip install -e \".[streaming]\"\n-```\n-\n-If you plan on contributing a specific dataset during \n-the community week, please fork the datasets repository and follow the instructions \n-[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request).\n-\n-To verify that all libraries are correctly installed, you can run the following command.\n-It assumes that both `transformers` and `datasets` were installed from main - otherwise\n-datasets streaming will not work correctly.\n-\n-```python\n-from transformers import FlaxRobertaModel, RobertaTokenizerFast\n-from datasets import load_dataset\n-import jax\n-\n-dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n-\n-dummy_input = next(iter(dataset))[\"text\"]\n-\n-tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n-input_ids = tokenizer(dummy_input, return_tensors=\"np\").input_ids[:, :10]\n-\n-model = FlaxRobertaModel.from_pretrained(\"julien-c/dummy-unknown\")\n-\n-# run a forward pass, should return an object `FlaxBaseModelOutputWithPooling`\n-model(input_ids)\n-```\n-\n-### TPU VM\n-\n-**VERY IMPORTANT** - Only one process can access the TPU cores at a time. This means that if multiple team members \n-are trying to connect to the TPU cores errors, such as:\n-\n-```\n-libtpu.so already in used by another process. Not attempting to load libtpu.so in this process.\n-```\n-\n-are thrown. As a conclusion, we recommend every team member to create her/his own virtual environment, but only one \n-person should run the heavy training processes. Also, please take turns when setting up the TPUv3-8 so that everybody \n-can verify that JAX is correctly installed.\n-\n-The following libraries are required to train a JAX/Flax model with ğŸ¤— Transformers and ğŸ¤— Datasets on TPU VM:\n-\n-- [JAX](https://github.com/google/jax/)\n-- [Flax](https://github.com/google/flax)\n-- [Optax](https://github.com/deepmind/optax)\n-- [Transformers](https://github.com/huggingface/transformers)\n-- [Datasets](https://github.com/huggingface/datasets)\n-\n-You should install the above libraries in a [virtual environment](https://docs.python.org/3/library/venv.html). \n-If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). Create a virtual environment with the version of Python you're going\n-to use and activate it.\n-\n-You should be able to run the command:\n-\n-```bash\n-python3 -m venv <your-venv-name>\n-```\n-\n-If this doesn't work, you first might to have install `python3-venv`. You can do this as follows:\n-\n-```bash\n-sudo apt-get install python3-venv\n-```\n-\n-You can activate your venv by running\n-\n-```bash\n-source ~/<your-venv-name>/bin/activate\n-```\n-\n-Next you should install JAX's TPU version on TPU by running the following command: \n-\n-```bash\n-$ pip install requests\n-```\n-\n-and then:\n-\n-```bash\n-$ pip install \"jax[tpu]>=0.2.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n-```\n-\n-**Note**: Running this command might actually throw an error, such as:\n-```\n- Building wheel for jax (setup.py) ... error\n-  ERROR: Command errored out with exit status 1:\n-   command: /home/patrick/patrick/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-lwseckn1/jax/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-lwseckn1/jax/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-pydotzlo\n-       cwd: /tmp/pip-install-lwseckn1/jax/\n-  Complete output (6 lines):\n-  usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n-     or: setup.py --help [cmd1 cmd2 ...]\n-     or: setup.py --help-commands\n-     or: setup.py cmd --help\n-  \n-  error: invalid command 'bdist_wheel'\n-  ----------------------------------------\n-  ERROR: Failed building wheel for jax\n-```\n-Jax should have been installed correctly nevertheless.\n-\n-To verify that JAX was correctly installed, you can run the following command:\n-\n-```python\n-import jax\n-jax.device_count()\n-```\n-\n-This should display the number of TPU cores, which should be 8 on a TPUv3-8 VM.\n-\n-We strongly recommend to make use of the provided JAX/Flax examples scripts in [transformers/examples/flax](https://github.com/huggingface/transformers/tree/main/examples/flax) even if you want to train a JAX/Flax model of another github repository that is not integrated into ğŸ¤— Transformers.\n-In all likelihood, you will need to adapt one of the example scripts, so we recommend forking and cloning the ğŸ¤— Transformers repository as follows. \n-Doing so will allow you to share your fork of the Transformers library with your team members so that the team effectively works on the same code base. It will also automatically install the newest versions of `flax`, `jax` and `optax`.\n-\n-1. Fork the [repository](https://github.com/huggingface/transformers) by\n-   clicking on the 'Fork' button on the repository's page. This creates a copy of the code\n-   under your GitHub user account.\n-\n-2. Clone your fork to your local disk, and add the base repository as a remote:\n-\n-   ```bash\n-   $ git clone https://github.com/<your Github handle>/transformers.git\n-   $ cd transformers\n-   $ git remote add upstream https://github.com/huggingface/transformers.git\n-   ```\n-\n-3. Create a new branch to hold your development changes. This is especially useful to share code changes with your team:\n-\n-   ```bash\n-   $ git checkout -b a-descriptive-name-for-my-project\n-   ```\n-\n-4. Set up a flax environment by running the following command in a virtual environment:\n-\n-   ```bash\n-   $ pip install -e \".[flax]\"\n-   ```\n-\n-   (If transformers was already installed in the virtual environment, remove\n-   it with `pip uninstall transformers` before reinstalling it in editable\n-   mode with the `-e` flag.)\n-\n-   If you have already cloned that repo, you might need to `git pull` to get the most recent changes in the `datasets`\n-   library.\n-\n-   Running this command will automatically install `flax`, `jax` and `optax`.\n-\n-Next, you should also install the ğŸ¤— Datasets library. We strongly recommend installing the \n-library from source to profit from the most current additions during the community week.\n-\n-Simply run the following steps:\n-\n-```bash\n-$ cd ~/\n-$ git clone https://github.com/huggingface/datasets.git\n-$ cd datasets\n-$ pip install -e \".[streaming]\"\n-```\n-\n-If you plan on contributing a specific dataset during \n-the community week, please fork the datasets repository and follow the instructions \n-[here](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-create-a-pull-request).\n-\n-To verify that all libraries are correctly installed, you can run the following command.\n-It assumes that both `transformers` and `datasets` were installed from main - otherwise\n-datasets streaming will not work correctly.\n-\n-```python\n-from transformers import FlaxRobertaModel, RobertaTokenizerFast\n-from datasets import load_dataset\n-import jax\n-\n-dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\n-\n-dummy_input = next(iter(dataset))[\"text\"]\n-\n-tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n-input_ids = tokenizer(dummy_input, return_tensors=\"np\").input_ids[:, :10]\n-\n-model = FlaxRobertaModel.from_pretrained(\"julien-c/dummy-unknown\")\n-\n-# run a forward pass, should return an object `FlaxBaseModelOutputWithPooling`\n-model(input_ids)\n-```\n-\n-## Quickstart flax and jax\n-\n-[JAX](https://jax.readthedocs.io/en/latest/index.html) is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more. A great place for getting started with JAX is the [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html).\n-\n-[Flax](https://flax.readthedocs.io/en/latest/index.html) is a high-performance neural network library designed for flexibility built on top of JAX. It aims to provide users with full control of their training code and is carefully designed to work well with JAX transformations such as `grad` and `pmap` (see the [Flax philosophy](https://flax.readthedocs.io/en/latest/philosophy.html)). For an introduction to Flax see the [Flax Basics Colab](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html) or the list of curated [Flax examples](https://flax.readthedocs.io/en/latest/examples.html).\n-\n-## Quickstart flax and jax in transformers\n-\n-Currently, we support the following models in Flax. \n-Note that some models are about to be merged to `main` and will \n-be available in a couple of days.\n-\n-- [BART](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/modeling_flax_bart.py)\n-- [BERT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_flax_bert.py)\n-- [BigBird](https://github.com/huggingface/transformers/blob/main/src/transformers/models/big_bird/modeling_flax_big_bird.py)\n-- [CLIP](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py)\n-- [ELECTRA](https://github.com/huggingface/transformers/blob/main/src/transformers/models/electra/modeling_flax_electra.py)\n-- [GPT2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai-community/gpt2/modeling_flax_gpt2.py)\n-- [(TODO) MBART](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mbart/modeling_flax_mbart.py)\n-- [RoBERTa](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py)\n-- [T5](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_flax_t5.py)\n-- [ViT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/vit/modeling_flax_vit.py)\n-- [Wav2Vec2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py)\n-\n-You can find all available training scripts for JAX/Flax under the \n-official [flax example folder](https://github.com/huggingface/transformers/tree/main/examples/flax). Note that a couple of training scripts will be released in the following week.\n-\n-- [Causal language modeling (GPT2)](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_clm_flax.py)\n-- [Masked language modeling (BERT, RoBERTa, ELECTRA, BigBird)](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py)\n-- [Text classification (BERT, RoBERTa, ELECTRA, BigBird)](https://github.com/huggingface/transformers/blob/main/examples/flax/text-classification/run_flax_glue.py)\n-- [Summarization / Seq2Seq (BART, MBART, T5)](https://github.com/huggingface/transformers/blob/main/examples/flax/summarization/run_summarization_flax.py)\n-- [Masked Seq2Seq pret-training (T5)](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_t5_mlm_flax.py)\n-- [Contrastive Loss pretraining for Wav2Vec2](https://github.com/huggingface/transformers/blob/main/examples/research_projects/jax-projects/wav2vec2)\n-- [Fine-tuning long-range QA for BigBird](https://github.com/huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird)\n-- [(TODO) Image classification (ViT)]( )\n-- [(TODO) CLIP pretraining, fine-tuning (CLIP)]( )\n-\n-\n-### **Flax design philosophy in Transformers**\n-\n-This section will explain how Flax models are implemented in Transformers and how the design differs from PyTorch.\n-\n-Let's first go over the difference between Flax and PyTorch.\n-\n-In JAX, most transformations (notably `jax.jit`) require functions that are transformed to be stateless so that they have no side effects. This is because any such side-effects will only be executed once when the transformed function is run during compilation and all subsequent calls of the compiled function would re-use the same side-effects of the compiled run instead of the \"actual\" side-effects (see [Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html)). As a consequence, Flax models, which are designed to work well with JAX transformations, are stateless. This means that when running a model in inference, both the inputs and the model weights are passed to the forward pass. In contrast, PyTorch model are very much stateful with the weights being stored within the model instance and the user just passing the inputs to the forward pass.\n-\n-Let's illustrate the difference between stateful models in PyTorch and stateless models in Flax.\n-\n-For simplicity, let's assume the language model consists simply of a single attention layer [`key_proj`, `value_proj`, `query_proj`] and a linear layer `logits_proj` to project the transformed word embeddings to the output logit vectors.\n-\n-#### **Stateful models in PyTorch**\n-\n-In PyTorch, the weights matrices would be stored as `torch.nn.Linear` objects alongside the model's config inside the model class `ModelPyTorch`:\n-\n-```python\n-class ModelPyTorch:\n- \n-  def __init__(self, config):\n-    self.config = config\n-    self.key_proj = torch.nn.Linear(config)\n-    self.value_proj = torch.nn.Linear(config)\n-    self.query_proj = torch.nn.Linear(config)\n-    self.logits_proj = torch.nn.Linear(config)\n-```\n-\n-Instantiating an object `model_pytorch` of the class `ModelPyTorch` would actually allocate memory for the model weights and attach them to the attributes `self.key_proj`, `self.value_proj`, `self.query_proj`, and `self.logits.proj`. We could access the weights via:\n-\n-```python\n-key_projection_matrix = model_pytorch.key_proj.weight.data\n-```\n-\n-Visually, we would represent an object of `model_pytorch` therefore as follows:\n-\n-![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/lm_pytorch_def.png)\n-\n-Executing a forward pass then simply corresponds to passing the `input_ids` to the object `model_pytorch`:\n-\n-```python\n-sequences = model_pytorch(input_ids)\n-```\n-\n-In a more abstract way, this can be represented as passing the word embeddings to the model function to get the output logits:\n-\n-![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/lm_pt_inference.png)\n-\n-This design is called **stateful** because the output logits, the `sequences`, can change even if the word embeddings, the `input_ids`, stay the same. Hence, the function's output does not only depend on its inputs, but also on its **state**, `[self.key_proj, self.value_proj, self.query_proj, self.logits_proj]`, which makes `model_pytorch` stateful.\n-\n-#### **Stateless models in Flax/JAX**\n-\n-Now, let's see how the mathematically equivalent model would be written in JAX/Flax. The model class `ModelFlax` would define the self-attention and logits projection weights as [**`flax.linen.Dense`**](https://flax.readthedocs.io/en/latest/_autosummary/flax.linen.Dense.html#flax.linen.Dense) objects:\n-\n-```python\n-class ModelFlax:\n-\n-  def __init__(self, config):\n-    self.config = config\n-    self.key_proj = flax.linen.Dense(config)\n-    self.value_proj = flax.linen.Dense(config)\n-    self.query_proj = flax.linen.Dense(config)\n-    self.logits_proj = flax.linen.Dense(config)\n-```\n-\n-At first glance the linear layer class `flax.linen.Dense` looks very similar to PyTorch's `torch.nn.Linear` class. However, instantiating an object `model_flax` only defines the linear transformation functions and does **not** allocate memory to store the linear transformation weights. In a way, the attribute `self.key_proj` tell the instantiated object `model_flax` to perform a linear transformation on some input and force it to expect a weight, called `key_proj`, as an input.\n-\n-This time we would illustrate the object `model_flax` without the weight matrices:\n-\n-![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/lm_flax_def.png)\n-\n-\n-Accordingly, the forward pass requires both `input_ids` as well as a dictionary consisting of the model's weights (called `state` here) to compute the `sequences`:\n-\n-To get the initial `state` we need to explicitly do a forward pass by passing a dummy input:\n-\n-```python\n-state = model_flax.init(rng, dummy_input_ids)\n-```\n-\n-and then we can do the forward pass.\n-\n-```python\n-sequences = model_flax.apply(state, input_ids)\n-```\n-\n-Visually, the forward pass would now be represented as passing all tensors required for the computation to the model's object:\n-\n-![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/lm_flax_inference.png)\n-\n-This design is called **stateless** because the output logits, the `sequences`, **cannot** change if the word embeddings, the `input_ids`, stay the same. Hence, the function's output only depends on its inputs, being the `input_ids` and the `state` dictionary consisting of the weights **state**, `[key_proj, value_proj, query_proj, logits_proj]`. \n-\n-Another term which is often used to describe the design difference between Flax/JAX and PyTorch is **immutable** vs **mutable**. A instantiated Flax model, `model_flax`, is **immutable** as a logical consequence of `model_flax`'s output being fully defined by its input: If calling `model_flax` could mutate `model_flax`, then calling `model_flax` twice with the same inputs could lead to different results which would violate the \"*statelessness*\" of Flax models.\n-\n-#### **Flax models in Transformers**\n-\n-Now let us see how this is handled in `Transformers.` If you have used a Flax model in Transformers already, you might wonder how come you don't always have to pass the parameters to the function of the forward pass. This is because the `FlaxPreTrainedModel` class abstracts it away. \n-It is designed this way so that the Flax models in Transformers will have a similar API to PyTorch and Tensorflow models.\n-\n-The `FlaxPreTrainedModel` is an abstract class that holds a Flax module, handles weights initialization, and provides a simple interface for downloading and loading pre-trained weights i.e. the `save_pretrained` and `from_pretrained` methods. Each Flax model then defines its own subclass of `FlaxPreTrainedModel`; *e.g.* the BERT model has `FlaxBertPreTrainedModel`. Each such class provides two important methods, `init_weights` and `__call__`. Let's see what each of those methods do:\n-\n-- The `init_weights` method takes the expected input shape and a [`PRNGKey`](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html) (and any other arguments that are required to get initial weights) and calls `module.init` by passing it a random example to get the initial weights with the given `dtype` (for ex. `fp32` or `bf16` etc). This method is called when we create an instance of the model class, so the weights are already initialized when you create a model i.e., when you do \n-\n-      model = FlaxBertModel(config)\n-\n-- The `__call__` method defines forward pass. It takes all necessary model inputs and parameters (and any other arguments required for the forward pass). The parameters are optional; when no parameters are passed, it uses the previously initialized or loaded parameters which can be accessed using `model.params`. It then calls the `module.apply` method, passing it the parameters and inputs to do the actual forward pass. So we can do a forward pass using\n-\n-      output = model(inputs, params=params)\n-\n-\n-Let's look at an example to see how this works. We will write a simple two-layer MLP model.\n-\n-First, write a Flax module that will declare the layers and computation.\n-\n-```python\n-import flax.linen as nn\n-import jax.numpy as jnp\n-\n-class MLPModule(nn.Module):\n-   config: MLPConfig\n-   dtype: jnp.dtype = jnp.float32\n-\n-   def setup(self):\n-      self.dense1 = nn.Dense(self.config.hidden_dim, dtype=self.dtype)\n-      self.dense2 = nn.Desne(self.config.hidden_dim, dtype=self.dtype)\n-   \n-   def __call__(self, inputs):\n-      hidden_states = self.dense1(inputs)\n-      hidden_states = nn.relu(hidden_states)\n-      hidden_states = self.dense2(hidden_states)\n-      return hidden_states\n-```\n-\n-Now let's define the `FlaxPreTrainedModel` model class.\n-\n-```python\n-from transformers.modeling_flax_utils import FlaxPreTrainedModel\n-\n-class FlaxMLPPreTrainedModel(FlaxPreTrainedModel):\n-   config_class = MLPConfig\n-   base_model_prefix = \"model\"\n-   module_class: nn.Module = None\n-\n-   def __init__(self, config: BertConfig, input_shape: Tuple = (1, 8), seed: int = 0, dtype: jnp.dtype = jnp.float32, **kwargs):\n-      # initialize the flax module\n-      module = self.module_class(config=config, dtype=dtype, **kwargs)\n-      super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype)\n-   \n-   def init_weights(self, rng, input_shape):\n-      # init input tensors\n-      inputs = jnp.zeros(input_shape, dtype=\"i4\")\n-      \n-      params_rng, dropout_rng = jax.random.split(rng)\n-      rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n-      \n-      params = self.module.init(rngs, inputs)[\"params\"]\n-      return params\n-   \n-   def __call__(self, inputs, params: dict = None):\n-      params = {\"params\": params or self.params}\n-      outputs = self.module.apply(params, jnp.array(inputs))\n-      return outputs\n-```\n-\n-\n-Now we can define our model class as follows.\n-\n-```python\n-class FlaxMLPModel(FlaxMLPPreTrainedModel):\n-   module_class = FlaxMLPModule\n-```\n-\n-Now the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows us to attach loaded or randomly initialized weights to the model instance.\n-\n-So the important point to remember is that the `model` is not an instance of `nn.Module`; it's an abstract class, like a container that holds a Flax module, its parameters and provides convenient methods for initialization and forward pass. The key take-away here is that an instance of `FlaxMLPModel` is very much stateful now since it holds all the model parameters, whereas the underlying Flax module `FlaxMLPModule` is still stateless. Now to make `FlaxMLPModel` fully compliant with JAX transformations, it is always possible to pass the parameters to `FlaxMLPModel` as well to make it stateless and easier to work with during training. Feel free to take a look at the code to see how exactly this is implemented for ex. [`modeling_flax_bert.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_flax_bert.py#L536)\n-\n-Another significant difference between Flax and PyTorch models is that, we can pass the `labels` directly to PyTorch's forward pass to compute the loss, whereas Flax models never accept `labels` as an input argument. In PyTorch, gradient backpropagation is performed by simply calling `.backward()` on the computed loss which makes it very handy for the user to be able to pass the `labels`. In Flax however, gradient backpropagation cannot be done by simply calling `.backward()` on the loss output, but the loss function itself has to be transformed by `jax.grad` or `jax.value_and_grad` to return the gradients of all parameters. This transformation cannot happen under-the-hood when one passes the `labels` to Flax's forward function, so that in Flax, we simply don't allow `labels` to be passed by design and force the user to implement the loss function oneself. As a conclusion, you will see that all training-related code is decoupled from the modeling code and always defined in the training scripts themselves.\n-\n-### **How to use flax models and example scripts**\n-\n-\n-#### **How to do a forward pass**\n-\n-Let's first see how to load, save and do inference with Flax models. As explained in the above section, all Flax models in Transformers have similar API to PyTorch models, so we can use the familiar `from_pretrained` and `save_pretrained` methods to load and save Flax models.\n-\n-Let's use the base `FlaxRobertaModel` without any heads as an example.\n-\n-```python\n-from transformers import FlaxRobertaModel, RobertaTokenizerFast\n-import jax\n-\n-tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n-inputs = tokenizer(\"JAX/Flax is amazing \", padding=\"max_length\", max_length=128, return_tensors=\"np\")\n-\n-model = FlaxRobertaModel.from_pretrained(\"julien-c/dummy-unknown\")\n-\n-@jax.jit\n-def run_model(input_ids, attention_mask):\n-   # run a forward pass, should return an object `FlaxBaseModelOutputWithPooling`\n-   return model(input_ids, attention_mask)\n-\n-outputs = run_model(**inputs)\n-```\n-\n-We use `jax.jit` to compile the function to get maximum performance. Note that in the above example, we set `padding=max_length` to pad all examples to the same length. We do this because JAX's compiler has to recompile a function everytime its input shape changes - in a sense a compiled function is not only defined by its code but also by its input and output shape. It is usually much more effective to pad the input to be of a fixed static shape than having to recompile every the function multiple times.\n-\n-\n-#### **How to write a training loop**\n-\n-Now let's see how we can write a simple training loop to train Flax models, we will use `FlaxGPT2ForCausalLM` as an example. \n-\n-A training loop for Flax models typically consists of\n-- A loss function that takes the parameters and inputs, runs the forward pass and returns the loss. \n-- We then transform the loss function using `jax.grad` or `jax.value_and_grad`  so that we get the gradients of all parameters.\n-- An optimizer to update the paramteres using the gradients returned by the transformed loss function.\n-- A train step function which combines the loss function and optimizer update, does the forward and backward pass and returns the updated parameters.\n-\n-Lets see how that looks like in code:\n-\n-First initialize our model\n-\n-```python\n-import jax\n-import jax.numpy as jnp\n-\n-from transformers import FlaxGPT2ForCausalLM\n-\n-model = FlaxGPT2ForCausalLM(config) \n-```\n-\n-As explained above we don't compute the loss inside the model, but rather in the task-specific training script.\n-For demonstration purposes, we write a pseudo training script for causal language modeling in the following.\n-\n-```python\n-from flax.training.common_utils import onehot\n-\n-def cross_entropy(logits, labels):\n-   return -jnp.sum(labels * jax.nn.log_softmax(logits, axis=-1), axis=-1)\n-\n-# define a function which will run the forward pass return loss\n-def compute_loss(params, input_ids, labels):\n-   logits = model(input_ids, params=params, train=True)\n-   num_classes = logits.shape[-1]\n-   loss = cross_entropy(logits, onehot(labels, num_classes)).mean()\n-   return loss\n-```\n-\n-Now we transform the loss function with `jax.value_and_grad`.\n-\n-```python\n-# transform the loss function to get the gradients\n-grad_fn = jax.value_and_grad(compute_loss)\n-```\n-\n-We use the [optax](https://github.com/deepmind/optax) library to Initialize the optimizer. \n-\n-```python\n-import optax\n-\n-params = model.params\n-tx = optax.sgd(learning_rate=3e-3)\n-opt_state = tx.init(params)\n-```\n-\n-Now we define a single training step which will do a forward and a backward pass.\n-\n-```python\n-def _train_step(params, opt_state, input_ids, labels)\n-   # do the forward pass and get the loss and gradients\n-   loss, grads = grad_fn(params, input_ids, labels)\n-\n-   # use the gradients to update parameters\n-   updates, opt_state = tx.update(grads, opt_state)\n-   updated_params = optax.apply_updates(params, updates)\n-\n-   return updates_params, opt_state, loss\n-\n-train_step = jax.jit(_train_step)\n-```\n-\n-Finally, let's run our training loop.\n-\n-```python\n-# train loop\n-for i in range(10):\n-   params, opt_state, loss = train_step(params, opt_state, input_ids, labels)\n-```\n-\n-Note how we always pass the `params` and `opt_state` to the `train_step` which then returns the updated `params` and `opt_state`. This is because of the staless nature of JAX/Flax models, all the state\n-like parameters, optimizer state is kept external.\n-\n-We can now save the model with the trained parameters using\n-\n-```python\n-model.save_pretrained(\"awesome-flax-model\", params=params)\n-```\n-\n-Note that, as JAX is backed by the [XLA](https://www.tensorflow.org/xla) compiler any JAX/Flax code can run on all `XLA` compliant device without code change!\n-That menas you could use the same training script on CPUs, GPUs, TPUs.\n-\n-To know more about how to train the Flax models on different devices (GPU, multi-GPUs, TPUs) and use the example scripts, please look at the [examples README](https://github.com/huggingface/transformers/tree/main/examples/flax).\n-\n-## Talks\n-\n-3 days of talks around JAX / Flax, Transformers, large-scale language modeling and other great topics during our community event!\n-\n-### Wednesday, June 30th\n-- [Watch the talks on YouTube](https://www.youtube.com/watch?v=fuAyUQcVzTY)\n-- [Chat history](https://docs.google.com/spreadsheets/d/1PZ5xYV2hVwlAVQSqDag65ympv5YNCSDmXyG-eWTaZ_o/edit?usp=sharing)\n-\n- Speaker        | Topic                           | Time                  |  Video |\n-|-------------|---------------------------------|------------------------|------------------------|\n-| Skye Wanderman-Milne, Google Brain | Intro to JAX on Cloud TPUs      | 6.00pm-6.45pm CEST / 9.00am-9.45am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=fuAyUQcVzTY) |\n-| Marc van Zee, Google Brain | Introduction to Flax      | 6.45pm-7.30pm CEST / 9.45am-10.30am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/fuAyUQcVzTY?t=2569) |\n-| Pablo Castro, Google Brain | Using Jax & Flax for RL with the Dopamine library      | 7.30pm-8.00pm CEST / 10.30am-11.00am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/fuAyUQcVzTY?t=5306) |\n-\n-### Thursday, July 1st\n-- [Watch the talks on YouTube](https://www.youtube.com/watch?v=__eG63ZP_5g)\n-- [Chat history](https://docs.google.com/spreadsheets/d/1PZ5xYV2hVwlAVQSqDag65ympv5YNCSDmXyG-eWTaZ_o/edit#gid=1515796400)\n-\n- Speaker        | Topic                           | Time                  | Video |\n-|-------------|---------------------------------|------------------------|------------------------|\n-| Suraj Patil & Patrick von Platen, Hugging Face | How to use JAX/Flax with Transformers      | 5.30pm-6.00pm CEST / 8.30am-9.00am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=__eG63ZP_5g) |\n-| Sabrina J. Mielke, Johns Hopkins University & HuggingFace | From stateful code to purified JAX: how to build your neural net framework | 6.00pm-6.30pm CEST / 9.00am-9.30am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/__eG63ZP_5g?t=1576) |\n-| Mostafa Dehghani, Google Brain | Long Range Arena: Benchmarking Efficient Transformers      | 6.30pm-7.00pm CEST / 9.30am-10.00am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/__eG63ZP_5g?t=3695) |\n-| Rohan Anil, Google Brain | Scalable Second Order Optimization for Deep Learning      | 7.00pm-7.30pm CEST / 10.00am-10.30am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/__eG63ZP_5g?t=5285) |\n-\n-\n-### Friday, July 2nd\n-- [Watch the talks on YouTube](https://www.youtube.com/watch?v=ZCMOPkcTu3s)\n-- [Chat history](https://docs.google.com/spreadsheets/d/1PZ5xYV2hVwlAVQSqDag65ympv5YNCSDmXyG-eWTaZ_o/edit#gid=1166061401)\n-\n- Speaker        | Topic                           | Time                  |  Video |\n-|-------------|---------------------------------|------------------------|------------------------|\n-| Lucas Beyer, Google Brain | Vision Transformer      | 5.00pm-5.30 CEST / 8.00am-8.30 PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://www.youtube.com/watch?v=ZCMOPkcTu3s) |\n-| Ben Wang, EleutherAI | Multihost Training in Mesh Transformer JAX      | 5.30pm-6.00 CEST / 8.30am-9.00 PST       | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/ZCMOPkcTu3s?t=1803) |\n-| Iurii Kemaev, SoÅˆa MokrÃ¡, Junhyuk Oh, DeepMind | DeepMind JAX Ecosystem      |    6.00pm-6.30 CEST / 9.00am-9.30am PST   | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/ZCMOPkcTu3s?t=3388) |\n-| Siddhartha Kamalakara, Joanna Yoo & JoÃ£o G M AraÃºjo, Cohere | Training large scale language models      | 6:30pm-7.00pm CEST / 9:30am-10.00am PST      | [![Youtube](https://www.youtube.com/s/desktop/f506bd45/img/favicon_32.png)](https://youtu.be/ZCMOPkcTu3s?t=5095) |\n-\n-### Talks & Speakers\n-\n-#### Skye Wanderman-Milne, JAX developer, Google Brain\n-- Talk: Intro to JAX on Cloud TPUs\n-- Abstract: JAX is a system for high-performance machine-learning research that combines the familiarity of Python + NumPy together with the power of hardware acceleration on CPUs, GPUs, and TPUs. It offers composable function transformations for automatic differentiation, automatic batching, end-to-end compilation, and both data and model parallelism. This talk will show you how to get up and running with JAX on a Cloud TPU VM. \n-- Speaker info: Skye Wanderman-Milne is a software engineer working on JAX. She has previously worked on TensorFlow and Apache Impala, a high-performance distributed database.\n-\n-#### Marc van Zee, Research SWE, Google Brain (Flax team)\n-- Talk: Introduction to Flax\n-- Abstract: In this talk I will provide a high-level introduction to the neural network library Flax. I will discuss the Flax philosophy, talk about the ecosystem around Flax and provide a high-level introduction to the code. I explain the Module abstraction and how to use it to train your models.\n-- Speaker info: Marc is at Google Research for over 4 years. First he worked on conceptual AI, developing a next generation language understanding and reasoning prototype and he authored the CFQ dataset for compositional generalization. Currently, Marc works as a research software engineer in the Flax team.\n-\n-#### Pablo Castro, Staff Research Software Developer; Google Research, Brain Team\n-- Talk: Using Jax & Flax for RL with the Dopamine library\n-- Abstract: The Dopamine library was launched with TensorFlow in 2018 and we added a Jax/Flax variant of it last year. Internally, Jax's flexibility has facilitated our RL research tremendously, and we are excited to demonstrate its potential.\n-- Speaker info: Pablo Samuel has been at Google for over 9 years, and is currently a researcher with the Brain team, focusing on fundamental reinforcement learning, as well as machine learning and creativity. Aside from his research, Pablo Samuel is an active musician (with a channel exploring the intersection of music and computer science), and is helping increase the representation of the LatinX community in the research world.\n-- Dopamine repo: https://github.com/google/dopamine \n-- Homepage: https://psc-g.github.io/\n-- Twitter: https://twitter.com/pcastr\n-\n-#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face\n-- Talk: How to use JAX/Flax with Transformers\n-- Abstract: Transformers is one of the most popular open-source ML libraries and supports PyTorch, Tensorflow, and JAX/Flax. In this talk, we will explain how JAX/Flax models should be used in Transformers and compare their design in Transformers with the design of PyTorch models in Transformers. In the second part, we will give you a hands-on presentation of how a model can be trained end-to-end with the official JAX/Flax example scripts using Transformers & Datasets. Along the way, we want to give you some tips and tricks on how to best realize your project.\n-- Speaker info: Suraj and Patrick are part of Hugging Faceâ€™s open source team and lead the integration of JAX/Flax into Transformers.\n-- GitHub: https://github.com/patil-suraj & https://github.com/patrickvonplaten\n-\n-#### Sabrina J. Mielke, PhD student at The Johns Hopkins University & Part-time research intern at HuggingFace\n-- Talk: From stateful code to purified JAX: how to build your neural net framework\n-- Abstract: Moving from object-oriented (and stateful) PyTorch- or TF2-code with tape-based backprop to JAX isn't easy---and while running grad() on numpy-oneliners is cool and all, you do wonder... how do I build actual big neural nets? Libraries like flax, trax, or haiku make it easy---but how could you build machinery like that yourself?\n-- Speaker info: Sabrina is a PhD student at the Johns Hopkins University and a part-time research intern at HuggingFace, researching open-vocabulary language models for segmentation and tokenization. She has published and co-organized workshops and shared tasks on these topics as well as on morphology and typological analysis in ACL, NAACL, EMNLP, LREC, and AAAI. You can find her reminisce for a time when formal language theory played a bigger role in NLP on Twitter at @sjmielke.\n-- Links: The 2020 blogpost this talk will be based on: https://sjmielke.com/jax-purify.htm, leading to our experiment Parallax and eventually Haiku\n-\n-#### Mostafa Dehghani, Research Scientist, Google Brain\n-- Talk: Long Range Arena: Benchmarking Efficient Transformers\n-- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on LRA. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. \n-- Speaker info: https://mostafadehghani.com/\n-\n-#### Rohan Anil, Senior Staff Software Engineer, Google Research, Brain Team\n-- Talk: Scalable Second Order Optimization for Deep Learning\n-- Abstract: Optimization in machine learning, both theoretical and applied, is presently dominated by first-order gradient methods such as stochastic gradient descent. Second-order optimization methods, that involve second derivatives and/or second order statistics of the data, are far less prevalent despite strong theoretical properties, due to their prohibitive computation, memory and communication costs. In an attempt to bridge this gap between theoretical and practical optimization, we present a scalable implementation of a second-order preconditioned method (concretely, a variant of full-matrix Adagrad), that along with several critical algorithmic and numerical improvements, provides significant convergence and wall-clock time improvements compared to conventional first-order methods on state-of-the-art deep models. Our novel design effectively utilizes the prevalent heterogeneous hardware architecture for training deep models, consisting of a multicore CPU coupled with multiple accelerator units. We demonstrate superior performance compared to state-of-the-art on very large learning tasks such as machine translation with Transformers, language modeling with BERT, click-through rate prediction on Criteo, and image classification on ImageNet with ResNet-50.\n-- Speaker info: Rohan Anil is a software engineer at Google Research, Mountain View. Lately, he has been working on scalable and practical optimization techniques for efficient training of neural networks in various regimes.\n-- Resources:\n-  - https://arxiv.org/abs/2002.09018\n-  - https://arxiv.org/abs/1901.11150\n-  - https://arxiv.org/abs/2106.06199\n-\n-\n-#### Lucas Beyer, Senior Research Engineer, Google Brain\n-- Talk: Vision Transformer\n-- Abstract: This talk will discuss the learning of general visual representations via large-scale pre-training and few-shot transfer, with a special focus on the Vision Transformer (ViT) architecture, which popularized transformers for the visual domain.\n-- Speaker info: Lucas Beyer is a self-taught hacker and studied engineer. He went on to do his PhD in robotic perception at RWTH Aachen and is currently on a quest to find the ultimate visual representation at Google Brain in ZÃ¼rich\n-\n-#### Ben Wang, Independent AI Researcher, EleutherAI\n-- Talk: Multihost Training in Mesh Transformer JAX\n-- Abstract: As models become larger, training must be scaled across multiple nodes. This talk discusses some design decisions and tradeoffs made for scaling to multiple nodes in Mesh Transformer JAX, a library for running model parallel transformers on TPU pods.\n-- Speaker info: Ben is an independent AI researcher who contributes to EleutherAI, an open source research collective centered around democratizing access to powerful AI models. Recently he has released GPT-J-6B, a 6 billion parameter transformer which is the most powerful autoregressive language model in terms of zero-shot performance with public weights.\n-- Website: https://www.eleuther.ai/\n-\n-#### Iurii Kemaev, Research Engineer, SoÅˆa MokrÃ¡, Research Engineer, and Junhyuk Oh, Research Scientist, DeepMind\n-- Talk: DeepMind JAX Ecosystem\n-- Abstract: The DeepMind JAX Ecosystem is an effort to build a shared substrate of components to enable all aspects of AGI Research. In this talk, our researchers and engineers will give a high-level overview of our Ecosystem goals and design philosophies, using our Haiku (neural network), Optax (optimization) and RLax (reinforcement learning) libraries as examples. We will then deep dive on two examples of recent DeepMind research that have been enabled by JAX and these libraries: generative models and meta-gradient reinforcement learning.\n-- Speaker info:\n-  - Iurii Kemaev is a Research Engineer at DeepMind. He has been using JAX for 2 years advancing RL research. Iurii is one of the DM JAX ecosystem leads.\n-  - SoÅˆa MokrÃ¡ is a Research Engineer at DeepMind. She has a background in machine translation and has been using JAX as the main ML framework for the past 6 months.\n-  - Junhyuk Oh is a Research Scientist at DeepMind, working on reinforcement learning and meta-learning. More information is available at https://junhyuk.com/\n-\n-#### Siddhartha Kamalakara, Joanna Yoo, JoÃ£o G M AraÃºjo, MLE at Cohere\n-- Talk: Training large scale language models\n-- Abstract: A journey through Cohereâ€™s experiences with training large scale language models. Join us in our exploration of pipeline and model parallelism as strategies for efficient training of large language models. We will present and motivate our recent transition to JAX+Flax as our choice of internal tech stack.\n-- Speaker info: \n-   - JoÃ£o G M AraÃºjo is a Brazilian college student with a passion for mathematics and a fascination for Deep Learning. JoÃ£o conducted research on representation learning and spent 3 months in Japan working on NeuroEvolution. JoÃ£o likes reading fantasy books and spending quality time with family and friends, and also runs a YouTube series on theoretical understanding of Deep Learning where researchers talk about their findings\n-   - Joanna Yoo is one of the founding engineers at Cohere, working on scaling language models for the last year and half. Joanna loves live concerts and rock climbing!\n-   - Siddhartha Rao Kamalakara is an MLE at Cohere and a researcher at FOR.ai with research interests at the intersection of efficient training and empirical understanding of DL.\n-- Website: https://cohere.ai/\n-\n-\n-## How to use the hub for collaboration\n-\n-In this section, we will explain how a team can use the ğŸ¤— hub to collaborate on a project.\n-The ğŸ¤— hub allows each team to create a repository with integrated git version control that \n-should be used for their project.\n-The advantages of using a repository on the ğŸ¤— hub are:\n-\n-- easy collaboration - each team member has write access to the model repository\n-- integrated git version control - code scripts as well as large model files are tracked using git version control\n-- easy sharing - the hub allows each team to easily share their work during and after the event\n-- integrated tensorboard functionality - uploaded tensorboard traces are automatically displayed on an integrated tensorboard tab\n-\n-We highly recommend each team to make use of the ğŸ¤— hub during the event.\n-To better understand how the repository and the hub in general functions, please take a look at the documentation and the videos [here](https://huggingface.co/docs/hub).\n-\n-Now let's explain in more detail how a project can be created on the hub. Having an officially defined project on [this](https://docs.google.com/spreadsheets/d/1GpHebL7qrwJOc9olTpIPgjf8vOS0jNb6zR_B8x_Jtik/edit?usp=sharing) Google Sheet you should be part of [the Flax Community organization on the hub](https://huggingface.co/flax-community). All repositories should be created under this organization so that write access can be shared and everybody can easily access other participants'\n-work ğŸ¤—. Note that we are giving each team member access to all repositories created under [flax-community](https://huggingface.co/flax-community), but we encourage participants to only clone and edit repositories corresponding to one's teams. If you want to help other teams, please ask them before changing files in their repository! The integrated git version control keeps track of \n-all changes, so in case a file was deleted by mistake, it is trivial to re-create it.\n-\n-Awesome! Now, let's first go over a simple example where most of the required we'll pre-train a RoBERTa model on a low-resource language. To begin with, we create a repository \n-under [the Flax Community organization on the hub](https://huggingface.co/flax-community) by logging in to the hub and going to [*\"Add model\"*](https://huggingface.co/new). By default \n-the username should be displayed under \"*Owner*\", which we want to change to *flax-community*. Next, we give our repository a fitting name for the project - here we'll just call it \n-*roberta-base-als* because we'll be pretraining a RoBERTa model on the super low-resource language *Alemannic* (`als`). We make sure that the model is a public repository and create it!\n-It should then be displayed on [the Flax Community organization on the hub](https://huggingface.co/flax-community).\n-\n-Great, now we have a project directory with integrated git version control and a public model page, which we can access under [flax-community/roberta-base-als](https://huggingface.co/flax-community/roberta-base-als). Let's create a short README so that other participants know what this model is about. You can create the README.md directly on the model page as a markdown file.\n-Let's now make use of the repository for training.\n-\n-We assume that the ğŸ¤— Transformers library and [git-lfs](https://git-lfs.github.com/) are correctly installed on our machine or the TPU attributed to us. \n-If this is not the case, please refer to the [Installation guide](#how-to-install-relevant-libraries) and the official [git-lfs](https://git-lfs.github.com/) website.\n-\n-At first we should log in:\n-\n-```bash\n-$ huggingface-cli login\n-```\n-\n-Next we can clone the repo:\n-\n-```bash\n-$ git clone https://huggingface.co/flax-community/roberta-base-als\n-```\n-\n-We have now cloned the model's repository and it should be under `roberta-base-als`. As you can see,\n-we have all the usual git functionalities in this repo - when adding a file, we can do `git add .`, `git commit -m \"add file\"` and `git push` \n-as usual. Let's try it out by adding the model's config.\n-\n-We go into the folder:\n-\n-```bash\n-$ cd ./roberta-base-als\n-```\n-\n-and run the following commands in a Python shell to save a config.\n-\n-```python\n-from transformers import RobertaConfig\n-\n-config = RobertaConfig.from_pretrained(\"FacebookAI/roberta-base\")\n-config.save_pretrained(\"./\")\n-```\n-\n-Now we've added a `config.json` file and can upload it by running \n-\n-```bash\n-$ git add . && git commit -m \"add config\" && git push\n-```\n-\n-Cool! The file is now displayed on the model page under the [files tab](https://huggingface.co/flax-community/roberta-base-als/tree/main).\n-We encourage you to upload all files except maybe the actual data files to the repository. This includes training scripts, model weights,\n-model configurations, training logs, etc...\n-\n-Next, let's create a tokenizer and save it to the model dir by following the instructions of the [official Flax MLM README](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#train-tokenizer). We can again use a simple Python shell.\n-\n-```python\n-from datasets import load_dataset\n-from tokenizers import ByteLevelBPETokenizer\n-\n-# load dataset\n-dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_als\", split=\"train\")\n-\n-# Instantiate tokenizer\n-tokenizer = ByteLevelBPETokenizer()\n-\n-def batch_iterator(batch_size=1000):\n-    for i in range(0, len(dataset), batch_size):\n-        yield dataset[i: i + batch_size][\"text\"]\n-\n-# Customized training\n-tokenizer.train_from_iterator(batch_iterator(), vocab_size=50265, min_frequency=2, special_tokens=[\n-    \"<s>\",\n-    \"<pad>\",\n-    \"</s>\",\n-    \"<unk>\",\n-    \"<mask>\",\n-])\n-\n-# Save files to disk\n-tokenizer.save(\"./tokenizer.json\")\n-```\n-\n-This creates and saves our tokenizer directly in the cloned repository.\n-Finally, we can start training. For now, we'll simply use the official [`run_mlm_flax`](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py)\n-script, but we might make some changes later. So let's copy the script into our model repository.\n-\n-```bash\n-$ cp ~/transformers/examples/flax/language-modeling/run_mlm_flax.py ./\n-```\n-\n-This way we are certain to have all the code used to train the model tracked in our repository.\n-Let's start training by running:\n-\n-```bash\n-./run_mlm_flax.py \\\n-    --output_dir=\"./\" \\\n-    --model_type=\"roberta\" \\\n-    --config_name=\"./\" \\\n-    --tokenizer_name=\"./\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_als\" \\\n-    --max_seq_length=\"128\" \\\n-    --per_device_train_batch_size=\"4\" \\\n-    --per_device_eval_batch_size=\"4\" \\\n-    --learning_rate=\"3e-4\" \\\n-    --warmup_steps=\"1000\" \\\n-    --overwrite_output_dir \\\n-    --num_train_epochs=\"8\" \\\n-    --push_to_hub\n-```\n-\n-Since the dataset is tiny this command should actually run in less than 5 minutes. Note that we attach \n-the flag ``--push_to_hub`` so that both model weights and tensorboard traces are automatically uploaded to the hub.\n-You can see the tensorboard directly on the model page, under the [Training metrics tab](https://huggingface.co/flax-community/roberta-base-als/tensorboard).\n-\n-As you can see, it is pretty simple to upload model weights and training logs to the model hub. Since the repository \n-has git version control, you & your team probably already have the necessary skills to collaborate. Thanks \n-to `git-lfs` being integrated into the hub, model weights and other larger file can just as easily be uploaded \n-and changed. Finally, at Hugging Face, we believe that the model hub is a great platform to share your project \n-while you are still working on it:\n-\n-- Bugs in training scripts can be found and corrected by anybody participating in the event\n-- Loss curves can be analyzed directly on the model page\n-- Model weights can be accessed and analyzed by everybody from the model repository\n-\n-If you are not using a transformers model, don't worry - you should still be able to make use of the hub's functionalities!\n-The [huggingface_hub](https://github.com/huggingface/huggingface_hub) allows you to upload essentially any JAX/Flax model to the hub with \n-just a couple of lines of code. *E.g.* assuming you want to call your model simply `flax-model-dummy`, you can upload it to the hub with \n-just three lines of code:\n-\n-\n-```python\n-from flax import serialization\n-from jax import random\n-from flax import linen as nn\n-from huggingface_hub import Repository\n-\n-model = nn.Dense(features=5)\n-\n-key1, key2 = random.split(random.PRNGKey(0))\n-x = random.normal(key1, (10,))\n-params = model.init(key2, x)\n-\n-bytes_output = serialization.to_bytes(params)\n-\n-repo = Repository(\"flax-model\", clone_from=\"flax-community/flax-model-dummy\", token=True)\n-with repo.commit(\"My cool Flax model :)\"):\n-    with open(\"flax_model.msgpack\", \"wb\") as f:\n-        f.write(bytes_output)\n-\n-# Repo is created and available here: https://huggingface.co/flax-community/flax-model-dummy\n-```\n-\n-**Note**: Make sure to have `huggingface_hub >= 0.0.13` to make this command work.\n-\n-For more information, check out [this PR](https://github.com/huggingface/huggingface_hub/pull/143) on how to upload any framework to the hub.\n-\n-## How to setup TPU VM\n-\n-In this section we will explain how you can ssh into a TPU VM that has been given to your team.\n-If your username is in one of the officially defined projects [here](https://docs.google.com/spreadsheets/d/1GpHebL7qrwJOc9olTpIPgjf8vOS0jNb6zR_B8x_Jtik/edit?usp=sharing), you should have received two emails: \n-\n-- one that states that you have been granted the role \"Community Week Participants\" for the project hf-flax, and\n-- one (or more if you are in multiple projects) that gives you the TPU name and the TPU zone for the TPU of your team\n-\n-You should click on \"Open Cloud Console\" on the first mail and agree to the pop up windows that follows. It will allow you to use a TPU VM. Don't worry if you cannot access the actual project `hf-flax` visually on the google cloud console and receive an error:\n-\n-```\n-You don't have sufficient permission to view this page\n-```\n-- this is expected! \n-\n-Great, now you and your team can access your TPU VM!\n-\n-In the following, we will describe how to do so using a standard console, but you should also be able to connect to the TPU VM via IDEs, like Visual Studio Code, etc.\n-\n-1. You need to install the Google Cloud SDK. Please follow the instructions on [cloud.google.com/sdk](https://cloud.google.com/sdk/docs/install#linux).\n-\n-2. Once you've installed the google cloud sdk, you should set your account by running the following command. Make sure that `<your-email-address>` corresponds to the gmail address you used to sign up for this event.\n-\n-```bash\n-$ gcloud config set account <your-email-address>\n-```\n-\n-3. Let's also make sure the correct project is set in case your email is used for multiple gcloud projects:\n-\n-```bash\n-$ gcloud config set project hf-flax\n-```\n-\n-4. Next, you will need to authenticate yourself. You can do so by running: \n-\n-```bash\n-$ gcloud auth login\n-```\n-\n-This should give you a link to a website, where you can authenticate your gmail account.\n-\n-5. Finally, you can ssh into the TPU VM! Please run the following command by setting <zone> to either `europe-west4-a` or `us-central1-a` (depending on what is stated in the second email you received) and <tpu-name> to the TPU name also sent to you in the second email.\n-\t\n-```bash\n-$ gcloud alpha compute tpus tpu-vm ssh <tpu-name> --zone <zone> --project hf-flax\n-```\n-\t\n-This should ssh you into the TPU VM!\n-Now you can follow the steps of the section [How to install relevant libraries](#how-to-install-relevant-libraries) to install all necessary \n-libraries. Make sure to carefully follow the explanations of the \"**IMPORTANT**\" statement to correctly install JAX on TPU.\n-Also feel free to install other `python` or `apt` packages on your machine if it helps you to work more efficiently!\n-\n-\n-## How to build a demo\n- \n-### Using the Hugging Face Widgets\n-\n-Hugging Face has over [15 widgets](https://huggingface-widgets.netlify.app/) for different use cases using ğŸ¤— Transformers library. Some of them also support [3rd party libraries](https://huggingface.co/docs/hub/libraries) such as [Sentence Similarity](https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1) with Sentence Transformers and [Text to Speech](https://huggingface.co/julien-c/ljspeech_tts_train_tacotron2_raw_phn_tacotron_g2p_en_no_space_train) with [ESPnet](https://github.com/espnet/espnet).\n-\n-All the widgets are open sourced in the `huggingface_hub` [repo](https://github.com/huggingface/huggingface_hub/tree/main/widgets). Here is a summary of existing widgets:\n-\n-**NLP**\n-* **Conversational:** To have the best conversations!. [Example](https://huggingface.co/microsoft/DialoGPT-large?).\n-* **Feature Extraction:** Retrieve the input embeddings. [Example](https://huggingface.co/sentence-transformers/distilbert-base-nli-mean-tokens?text=test).\n-* **Fill Mask:** Predict potential words for a mask token. [Example](https://huggingface.co/google-bert/bert-base-uncased?).\n-* **Question Answering:** Given a context and a question, predict the answer. [Example](https://huggingface.co/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad).\n-* **Sentence Simmilarity:** Predict how similar a set of sentences are. Useful for Sentence Transformers.\n-* **Summarization:** Given a text, output a summary of it. [Example](https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n-* **Table Question Answering:** Given a table and a question, predict the answer. [Example](https://huggingface.co/google/tapas-base-finetuned-wtq).\n-* **Text Generation:** Generate text based on a prompt. [Example](https://huggingface.co/openai-community/gpt2)\n-* **Token Classification:** Useful for tasks such as Named Entity Recognition and Part of Speech. [Example](https://huggingface.co/dslim/bert-base-NER).\n-* **Zero-Shot Classification:** Too cool to explain with words. Here is an [example](https://huggingface.co/typeform/distilbert-base-uncased-mnli)\n-* ([WIP](https://github.com/huggingface/huggingface_hub/issues/99)) **Table to Text Generation**.\n-\n-**Speech**\n-* **Audio to Audio:** For tasks such as audio source separation or speech enhancement. \n-* **Automatic Speech Recognition:** Convert audio to text. [Example](https://huggingface.co/facebook/wav2vec2-base-960h)\n-* **Text to Speech**: Convert text to audio.\n-\n-**Image**\n-* **Image Classification:** Given an image, predict its class. [Example](https://huggingface.co/osanseviero/llamastic).\n-* ([WIP](https://github.com/huggingface/huggingface_hub/issues/100)) **Zero Shot Image Classification**\n-* ([WIP](https://github.com/huggingface/huggingface_hub/issues/112)) **Image Captioning**\n-* ([WIP](https://github.com/huggingface/huggingface_hub/issues/113)) **Text to Image Generation**\n-* ([Proposed](https://github.com/huggingface/huggingface_hub/issues/127)) **Visual Question Answering**\n-\n-You can propose and implement new widgets by [opening an issue](https://github.com/huggingface/huggingface_hub/issues). Contributions are welcomed!\n-\n-\n-### Using a Streamlit demo\n-\n-Sometimes you might be using different libraries or a very specific application that is not well supported by the current widgets. In this case, [Streamlit](https://streamlit.io/) can be an excellent option to build a cool visual demo. Setting up a Streamlit application is straightforward and in Python!\n-\n-A common use case is how to load files you have in your model repository in the Hub from the Streamlit demo. The `huggingface_hub` library is here to help you!\n-\n-```bash\n-pip install huggingface_hub\n-```\n-\n-Here is an example downloading (and caching!) a specific file directly from the Hub\n-```python\n-from huggingface_hub import hf_hub_download\n-filepath = hf_hub_download(\"flax-community/roberta-base-als\", \"flax_model.msgpack\");\n-```\n-\n-In many cases you will want to download the full repository. Here is an example downloading all the files from a repo. You can even specify specific revisions!\n-\n-```python\n-from huggingface_hub import snapshot_download\n-local_path = snapshot_download(\"flax-community/roberta-base-als\");\n-```\n-\n-Note that if you're using ğŸ¤— Transformers library, you can quickly load the model and tokenizer as follows\n-```python\n-from transformers import AutoTokenizer, AutoModelForMaskedLM\n-  \n-tokenizer = AutoTokenizer.from_pretrained(\"REPO_ID\")\n-model = AutoModelForMaskedLM.from_pretrained(\"REPO_ID\")\n-```\n-\n-\n-We'll provide more examples on Streamlit demos next week. Stay tuned!\n-\n-### Using a Gradio demo\n-\n-You can also use [Gradio](https://gradio.app/) to share your demos! [Here](https://huggingface.co/blog/gradio) is an example using the Gradio library to create a GUI for a Hugging Face model.\n-\n-More to come!\n-\n-## Project evaluation\n-\n-For your project to be evaluated, please fill out [this google form](https://forms.gle/jQaMkj3JJdD4Xcwn9).\n-Please make sure that your submitted project includes a demo as well as information about the model, data, training methods, etc.\n-\n-### Criteria\n-\n-* **Demo.** All projects are required to have a demo. Itâ€™s open ended, but we provide some ideas on how to build demos in the [How to build a demo](#how-to-build-a-demo) section.\n-* **Technical difficulty.** Difficulty has different aspects, such as working with complex architectures, obtaining better evaluation metrics than existing models, or implementing models for low-resource languages. \n-* **Social impact.** The project is expected to have a positive social impact, e.g. by tackling under-explored area of practical interest for minorities or under-represented group (low-ressources languages, specific focus on bias, fairness or ethical issues in ML) or by tackling general societal challenges, e.g. health or climate related challenges.\n-* **Innovativeness.** Projects that propose novel applications or bring new ideas will be rewarded more.\n-\n-### Jury\n-\n-* [Niki Parmar](https://research.google/people/NikiParmar/): Staff Research Scientist at Google.\n-* [Ross Wightman](https://www.linkedin.com/in/wightmanr/): Angel Investor.\n-* [Thomas Wolf](https://www.linkedin.com/in/thomas-wolf-a056857/): Co-founder and CSO at Hugging Face.\n-* [Ashish Vaswani](https://research.google/people/AshishVaswani/): Staff Research Scientist at Google Brain.\n-\n-### Process\n-\n-* **July 17, 12h00 CEST**: TPU VM access closes.\n-* **July 19, 12h00 CEST**: Project completition ends (including demo).\n-* **July 19-21** A group of event organizers (Suraj, Patrick, Suzana, and Omar) will do an initial filter to find the top 15 projects.\n-* **July 22-26** The jury will go over the 15 projects and pick the top three projects out of them.\n-* **July 27.** Winner projects are announced\n-\n-\n-## General tips and tricks\n-\n-TODO (will be filled continuously)...\n-\n-## FAQ\n-\n-TODO (will be filled continuously)..."
        },
        {
            "sha": "42586e49580ebba3f9143d9303536f3c1580db5b",
            "filename": "examples/research_projects/jax-projects/big_bird/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,60 +0,0 @@\n-\n-Author: [@vasudevgupta7](https://github.com/thevasudevgupta/)\n-\n-## Intro\n-\n-In this project, we fine-tuned [**BigBird**](https://arxiv.org/abs/2007.14062) on [**natural-questions**](https://huggingface.co/datasets/natural_questions) dataset for **question-answering** task on long documents. **BigBird**, is a **sparse-attention based transformer** which extends Transformer based models, such as BERT to much **longer sequences**.\n-\n-Read more about BigBird at https://huggingface.co/blog/big-bird\n-\n-## Fine-tuning\n-\n-**Setup**\n-\n-You need to install jax yourself by following the official docs ([refer this](https://github.com/google/jax#installation)). Other requirements for this project can be installed by running following command:\n-\n-```shell\n-pip3 install -qr requirements.txt\n-```\n-\n-**Download & prepare dataset**\n-\n-The Natural Questions corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. This corpus takes ~100 GB on disk. We have used HuggingFace datasets to download & process the dataset.\n-\n-```shell\n-# just run following CMD\n-python3 prepare_natural_questions.py\n-\n-# this will download the whole dataset from HuggingFace Hub & will make it ready for training\n-# this script takes ~3 hours to process the dataset\n-```\n-\n-**Launch Training**\n-\n-We have trained on Cloud's TPU v3-8. Each epoch took around 4.5 hours and the model got converged in just 2 epochs. You can see complete training args in [this script](bigbird_flax.py).\n-\n-```shell\n-# just run following CMD\n-python3 train.py\n-\n-# In case, you want to try hparams tuning, you can run wandb sweep\n-wandb sweep --project=bigbird sweep_flax.yaml\n-wandb agent <agent-id-obtained-by-above-CMD>\n-```\n-\n-## Evaluation\n-\n-Our evaluation script is different from the original script and we are evaluating sequences with length up to 4096 for simplicity. We managed to get the **EM score of ~55.2** using our evaluation script.\n-\n-```shell\n-# download validation-dataset first\n-mkdir natural-questions-validation\n-wget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation.arrow -P natural-questions-validation\n-wget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P natural-questions-validation\n-wget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/state.json -P natural-questions-validation\n-\n-# simply run following command\n-python3 evaluate.py\n-```\n-\n-You can find our checkpoint on HuggingFace Hub ([see this](https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions)). In case you are interested in PyTorch BigBird fine-tuning, you can refer to [this repository](https://github.com/thevasudevgupta/bigbird)."
        },
        {
            "sha": "af5e11c83a6ad2f4f2afa55f316c4e06b493b351",
            "filename": "examples/research_projects/jax-projects/big_bird/bigbird_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 323,
            "changes": 323,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fbigbird_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fbigbird_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fbigbird_flax.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,323 +0,0 @@\n-import json\n-import os\n-from dataclasses import dataclass\n-from functools import partial\n-from typing import Callable\n-\n-import flax.linen as nn\n-import jax\n-import jax.numpy as jnp\n-import joblib\n-import optax\n-import wandb\n-from flax import jax_utils, struct, traverse_util\n-from flax.serialization import from_bytes, to_bytes\n-from flax.training import train_state\n-from flax.training.common_utils import shard\n-from tqdm.auto import tqdm\n-\n-from transformers import BigBirdConfig, FlaxBigBirdForQuestionAnswering\n-from transformers.models.big_bird.modeling_flax_big_bird import FlaxBigBirdForQuestionAnsweringModule\n-\n-\n-class FlaxBigBirdForNaturalQuestionsModule(FlaxBigBirdForQuestionAnsweringModule):\n-    \"\"\"\n-    BigBirdForQuestionAnswering with CLS Head over the top for predicting category\n-\n-    This way we can load its weights with FlaxBigBirdForQuestionAnswering\n-    \"\"\"\n-\n-    config: BigBirdConfig\n-    dtype: jnp.dtype = jnp.float32\n-    add_pooling_layer: bool = True\n-\n-    def setup(self):\n-        super().setup()\n-        self.cls = nn.Dense(5, dtype=self.dtype)\n-\n-    def __call__(self, *args, **kwargs):\n-        outputs = super().__call__(*args, **kwargs)\n-        cls_out = self.cls(outputs[2])\n-        return outputs[:2] + (cls_out,)\n-\n-\n-class FlaxBigBirdForNaturalQuestions(FlaxBigBirdForQuestionAnswering):\n-    module_class = FlaxBigBirdForNaturalQuestionsModule\n-\n-\n-def calculate_loss_for_nq(start_logits, start_labels, end_logits, end_labels, pooled_logits, pooler_labels):\n-    def cross_entropy(logits, labels, reduction=None):\n-        \"\"\"\n-        Args:\n-            logits: bsz, seqlen, vocab_size\n-            labels: bsz, seqlen\n-        \"\"\"\n-        vocab_size = logits.shape[-1]\n-        labels = (labels[..., None] == jnp.arange(vocab_size)[None]).astype(\"f4\")\n-        logits = jax.nn.log_softmax(logits, axis=-1)\n-        loss = -jnp.sum(labels * logits, axis=-1)\n-        if reduction is not None:\n-            loss = reduction(loss)\n-        return loss\n-\n-    cross_entropy = partial(cross_entropy, reduction=jnp.mean)\n-    start_loss = cross_entropy(start_logits, start_labels)\n-    end_loss = cross_entropy(end_logits, end_labels)\n-    pooled_loss = cross_entropy(pooled_logits, pooler_labels)\n-    return (start_loss + end_loss + pooled_loss) / 3\n-\n-\n-@dataclass\n-class Args:\n-    model_id: str = \"google/bigbird-roberta-base\"\n-    logging_steps: int = 3000\n-    save_steps: int = 10500\n-\n-    block_size: int = 128\n-    num_random_blocks: int = 3\n-\n-    batch_size_per_device: int = 1\n-    max_epochs: int = 5\n-\n-    # tx_args\n-    lr: float = 3e-5\n-    init_lr: float = 0.0\n-    warmup_steps: int = 20000\n-    weight_decay: float = 0.0095\n-\n-    save_dir: str = \"bigbird-roberta-natural-questions\"\n-    base_dir: str = \"training-expt\"\n-    tr_data_path: str = \"data/nq-training.jsonl\"\n-    val_data_path: str = \"data/nq-validation.jsonl\"\n-\n-    def __post_init__(self):\n-        os.makedirs(self.base_dir, exist_ok=True)\n-        self.save_dir = os.path.join(self.base_dir, self.save_dir)\n-        self.batch_size = self.batch_size_per_device * jax.device_count()\n-\n-\n-@dataclass\n-class DataCollator:\n-    pad_id: int\n-    max_length: int = 4096  # no dynamic padding on TPUs\n-\n-    def __call__(self, batch):\n-        batch = self.collate_fn(batch)\n-        batch = jax.tree_util.tree_map(shard, batch)\n-        return batch\n-\n-    def collate_fn(self, features):\n-        input_ids, attention_mask = self.fetch_inputs(features[\"input_ids\"])\n-        batch = {\n-            \"input_ids\": jnp.array(input_ids, dtype=jnp.int32),\n-            \"attention_mask\": jnp.array(attention_mask, dtype=jnp.int32),\n-            \"start_labels\": jnp.array(features[\"start_token\"], dtype=jnp.int32),\n-            \"end_labels\": jnp.array(features[\"end_token\"], dtype=jnp.int32),\n-            \"pooled_labels\": jnp.array(features[\"category\"], dtype=jnp.int32),\n-        }\n-        return batch\n-\n-    def fetch_inputs(self, input_ids: list):\n-        inputs = [self._fetch_inputs(ids) for ids in input_ids]\n-        return zip(*inputs)\n-\n-    def _fetch_inputs(self, input_ids: list):\n-        attention_mask = [1 for _ in range(len(input_ids))]\n-        while len(input_ids) < self.max_length:\n-            input_ids.append(self.pad_id)\n-            attention_mask.append(0)\n-        return input_ids, attention_mask\n-\n-\n-def get_batched_dataset(dataset, batch_size, seed=None):\n-    if seed is not None:\n-        dataset = dataset.shuffle(seed=seed)\n-    for i in range(len(dataset) // batch_size):\n-        batch = dataset[i * batch_size : (i + 1) * batch_size]\n-        yield dict(batch)\n-\n-\n-@partial(jax.pmap, axis_name=\"batch\")\n-def train_step(state, drp_rng, **model_inputs):\n-    def loss_fn(params):\n-        start_labels = model_inputs.pop(\"start_labels\")\n-        end_labels = model_inputs.pop(\"end_labels\")\n-        pooled_labels = model_inputs.pop(\"pooled_labels\")\n-\n-        outputs = state.apply_fn(**model_inputs, params=params, dropout_rng=drp_rng, train=True)\n-        start_logits, end_logits, pooled_logits = outputs\n-\n-        return state.loss_fn(\n-            start_logits,\n-            start_labels,\n-            end_logits,\n-            end_labels,\n-            pooled_logits,\n-            pooled_labels,\n-        )\n-\n-    drp_rng, new_drp_rng = jax.random.split(drp_rng)\n-    grad_fn = jax.value_and_grad(loss_fn)\n-    loss, grads = grad_fn(state.params)\n-    metrics = jax.lax.pmean({\"loss\": loss}, axis_name=\"batch\")\n-    grads = jax.lax.pmean(grads, \"batch\")\n-\n-    state = state.apply_gradients(grads=grads)\n-    return state, metrics, new_drp_rng\n-\n-\n-@partial(jax.pmap, axis_name=\"batch\")\n-def val_step(state, **model_inputs):\n-    start_labels = model_inputs.pop(\"start_labels\")\n-    end_labels = model_inputs.pop(\"end_labels\")\n-    pooled_labels = model_inputs.pop(\"pooled_labels\")\n-\n-    outputs = state.apply_fn(**model_inputs, params=state.params, train=False)\n-    start_logits, end_logits, pooled_logits = outputs\n-\n-    loss = state.loss_fn(start_logits, start_labels, end_logits, end_labels, pooled_logits, pooled_labels)\n-    metrics = jax.lax.pmean({\"loss\": loss}, axis_name=\"batch\")\n-    return metrics\n-\n-\n-class TrainState(train_state.TrainState):\n-    loss_fn: Callable = struct.field(pytree_node=False)\n-\n-\n-@dataclass\n-class Trainer:\n-    args: Args\n-    data_collator: Callable\n-    train_step_fn: Callable\n-    val_step_fn: Callable\n-    model_save_fn: Callable\n-    logger: wandb\n-    scheduler_fn: Callable = None\n-\n-    def create_state(self, model, tx, num_train_steps, ckpt_dir=None):\n-        params = model.params\n-        state = TrainState.create(\n-            apply_fn=model.__call__,\n-            params=params,\n-            tx=tx,\n-            loss_fn=calculate_loss_for_nq,\n-        )\n-        if ckpt_dir is not None:\n-            params, opt_state, step, args, data_collator = restore_checkpoint(ckpt_dir, state)\n-            tx_args = {\n-                \"lr\": args.lr,\n-                \"init_lr\": args.init_lr,\n-                \"warmup_steps\": args.warmup_steps,\n-                \"num_train_steps\": num_train_steps,\n-                \"weight_decay\": args.weight_decay,\n-            }\n-            tx, lr = build_tx(**tx_args)\n-            state = train_state.TrainState(\n-                step=step,\n-                apply_fn=model.__call__,\n-                params=params,\n-                tx=tx,\n-                opt_state=opt_state,\n-            )\n-            self.args = args\n-            self.data_collator = data_collator\n-            self.scheduler_fn = lr\n-            model.params = params\n-        state = jax_utils.replicate(state)\n-        return state\n-\n-    def train(self, state, tr_dataset, val_dataset):\n-        args = self.args\n-        total = len(tr_dataset) // args.batch_size\n-\n-        rng = jax.random.PRNGKey(0)\n-        drp_rng = jax.random.split(rng, jax.device_count())\n-        for epoch in range(args.max_epochs):\n-            running_loss = jnp.array(0, dtype=jnp.float32)\n-            tr_dataloader = get_batched_dataset(tr_dataset, args.batch_size, seed=epoch)\n-            i = 0\n-            for batch in tqdm(tr_dataloader, total=total, desc=f\"Running EPOCH-{epoch}\"):\n-                batch = self.data_collator(batch)\n-                state, metrics, drp_rng = self.train_step_fn(state, drp_rng, **batch)\n-                running_loss += jax_utils.unreplicate(metrics[\"loss\"])\n-                i += 1\n-                if i % args.logging_steps == 0:\n-                    state_step = jax_utils.unreplicate(state.step)\n-                    tr_loss = running_loss.item() / i\n-                    lr = self.scheduler_fn(state_step - 1)\n-\n-                    eval_loss = self.evaluate(state, val_dataset)\n-                    logging_dict = {\n-                        \"step\": state_step.item(),\n-                        \"eval_loss\": eval_loss.item(),\n-                        \"tr_loss\": tr_loss,\n-                        \"lr\": lr.item(),\n-                    }\n-                    tqdm.write(str(logging_dict))\n-                    self.logger.log(logging_dict, commit=True)\n-\n-                if i % args.save_steps == 0:\n-                    self.save_checkpoint(args.save_dir + f\"-e{epoch}-s{i}\", state=state)\n-\n-    def evaluate(self, state, dataset):\n-        dataloader = get_batched_dataset(dataset, self.args.batch_size)\n-        total = len(dataset) // self.args.batch_size\n-        running_loss = jnp.array(0, dtype=jnp.float32)\n-        i = 0\n-        for batch in tqdm(dataloader, total=total, desc=\"Evaluating ... \"):\n-            batch = self.data_collator(batch)\n-            metrics = self.val_step_fn(state, **batch)\n-            running_loss += jax_utils.unreplicate(metrics[\"loss\"])\n-            i += 1\n-        return running_loss / i\n-\n-    def save_checkpoint(self, save_dir, state):\n-        state = jax_utils.unreplicate(state)\n-        print(f\"SAVING CHECKPOINT IN {save_dir}\", end=\" ... \")\n-        self.model_save_fn(save_dir, params=state.params)\n-        with open(os.path.join(save_dir, \"opt_state.msgpack\"), \"wb\") as f:\n-            f.write(to_bytes(state.opt_state))\n-        joblib.dump(self.args, os.path.join(save_dir, \"args.joblib\"))\n-        joblib.dump(self.data_collator, os.path.join(save_dir, \"data_collator.joblib\"))\n-        with open(os.path.join(save_dir, \"training_state.json\"), \"w\") as f:\n-            json.dump({\"step\": state.step.item()}, f)\n-        print(\"DONE\")\n-\n-\n-def restore_checkpoint(save_dir, state):\n-    print(f\"RESTORING CHECKPOINT FROM {save_dir}\", end=\" ... \")\n-    with open(os.path.join(save_dir, \"flax_model.msgpack\"), \"rb\") as f:\n-        params = from_bytes(state.params, f.read())\n-\n-    with open(os.path.join(save_dir, \"opt_state.msgpack\"), \"rb\") as f:\n-        opt_state = from_bytes(state.opt_state, f.read())\n-\n-    args = joblib.load(os.path.join(save_dir, \"args.joblib\"))\n-    data_collator = joblib.load(os.path.join(save_dir, \"data_collator.joblib\"))\n-\n-    with open(os.path.join(save_dir, \"training_state.json\"), \"r\") as f:\n-        training_state = json.load(f)\n-    step = training_state[\"step\"]\n-\n-    print(\"DONE\")\n-    return params, opt_state, step, args, data_collator\n-\n-\n-def scheduler_fn(lr, init_lr, warmup_steps, num_train_steps):\n-    decay_steps = num_train_steps - warmup_steps\n-    warmup_fn = optax.linear_schedule(init_value=init_lr, end_value=lr, transition_steps=warmup_steps)\n-    decay_fn = optax.linear_schedule(init_value=lr, end_value=1e-7, transition_steps=decay_steps)\n-    lr = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[warmup_steps])\n-    return lr\n-\n-\n-def build_tx(lr, init_lr, warmup_steps, num_train_steps, weight_decay):\n-    def weight_decay_mask(params):\n-        params = traverse_util.flatten_dict(params)\n-        mask = {k: (v[-1] != \"bias\" and v[-2:] != (\"LayerNorm\", \"scale\")) for k, v in params.items()}\n-        return traverse_util.unflatten_dict(mask)\n-\n-    lr = scheduler_fn(lr, init_lr, warmup_steps, num_train_steps)\n-\n-    tx = optax.adamw(learning_rate=lr, weight_decay=weight_decay, mask=weight_decay_mask)\n-    return tx, lr"
        },
        {
            "sha": "3c5123efeba5d61afcd540d569e1be2d3d15109a",
            "filename": "examples/research_projects/jax-projects/big_bird/evaluate.py",
            "status": "removed",
            "additions": 0,
            "deletions": 164,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fevaluate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fevaluate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fevaluate.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,164 +0,0 @@\n-import jax\n-import jax.numpy as jnp\n-from bigbird_flax import FlaxBigBirdForNaturalQuestions\n-from datasets import load_from_disk\n-\n-from transformers import BigBirdTokenizerFast\n-\n-\n-CATEGORY_MAPPING = {0: \"null\", 1: \"short\", 2: \"long\", 3: \"yes\", 4: \"no\"}\n-PUNCTUATION_SET_TO_EXCLUDE = set(\"\".join([\"â€˜\", \"â€™\", \"Â´\", \"`\", \".\", \",\", \"-\", '\"']))\n-\n-\n-def get_sub_answers(answers, begin=0, end=None):\n-    return [\" \".join(x.split(\" \")[begin:end]) for x in answers if len(x.split(\" \")) > 1]\n-\n-\n-def expand_to_aliases(given_answers, make_sub_answers=False):\n-    if make_sub_answers:\n-        # if answers are longer than one word, make sure a predictions is correct if it coresponds to the complete 1: or :-1 sub word\n-        # *e.g.* if the correct answer contains a prefix such as \"the\", or \"a\"\n-        given_answers = (\n-            given_answers + get_sub_answers(given_answers, begin=1) + get_sub_answers(given_answers, end=-1)\n-        )\n-    answers = []\n-    for answer in given_answers:\n-        alias = answer.replace(\"_\", \" \").lower()\n-        alias = \"\".join(c if c not in PUNCTUATION_SET_TO_EXCLUDE else \" \" for c in alias)\n-        answers.append(\" \".join(alias.split()).strip())\n-    return set(answers)\n-\n-\n-def get_best_valid_start_end_idx(start_scores, end_scores, top_k=1, max_size=100):\n-    best_start_scores, best_start_idx = jax.lax.top_k(start_scores, top_k)\n-    best_end_scores, best_end_idx = jax.lax.top_k(end_scores, top_k)\n-\n-    widths = best_end_idx[:, None] - best_start_idx[None, :]\n-    mask = jnp.logical_or(widths < 0, widths > max_size)\n-    scores = (best_end_scores[:, None] + best_start_scores[None, :]) - (1e8 * mask)\n-    best_score = jnp.argmax(scores).item()\n-\n-    return best_start_idx[best_score % top_k], best_end_idx[best_score // top_k]\n-\n-\n-def format_dataset(sample):\n-    question = sample[\"question\"][\"text\"]\n-    context = sample[\"document\"][\"tokens\"][\"token\"]\n-    is_html = sample[\"document\"][\"tokens\"][\"is_html\"]\n-    long_answers = sample[\"annotations\"][\"long_answer\"]\n-    short_answers = sample[\"annotations\"][\"short_answers\"]\n-\n-    context_string = \" \".join([context[i] for i in range(len(context)) if not is_html[i]])\n-\n-    # 0 - No ; 1 - Yes\n-    for answer in sample[\"annotations\"][\"yes_no_answer\"]:\n-        if answer == 0 or answer == 1:\n-            return {\n-                \"question\": question,\n-                \"context\": context_string,\n-                \"short\": [],\n-                \"long\": [],\n-                \"category\": \"no\" if answer == 0 else \"yes\",\n-            }\n-\n-    short_targets = []\n-    for s in short_answers:\n-        short_targets.extend(s[\"text\"])\n-    short_targets = list(set(short_targets))\n-\n-    long_targets = []\n-    for s in long_answers:\n-        if s[\"start_token\"] == -1:\n-            continue\n-        answer = context[s[\"start_token\"] : s[\"end_token\"]]\n-        html = is_html[s[\"start_token\"] : s[\"end_token\"]]\n-        new_answer = \" \".join([answer[i] for i in range(len(answer)) if not html[i]])\n-        if new_answer not in long_targets:\n-            long_targets.append(new_answer)\n-\n-    category = \"long_short\" if len(short_targets + long_targets) > 0 else \"null\"\n-\n-    return {\n-        \"question\": question,\n-        \"context\": context_string,\n-        \"short\": short_targets,\n-        \"long\": long_targets,\n-        \"category\": category,\n-    }\n-\n-\n-def main():\n-    dataset = load_from_disk(\"natural-questions-validation\")\n-    dataset = dataset.map(format_dataset).remove_columns([\"annotations\", \"document\", \"id\"])\n-    print(dataset)\n-\n-    short_validation_dataset = dataset.filter(lambda x: (len(x[\"question\"]) + len(x[\"context\"])) < 4 * 4096)\n-    short_validation_dataset = short_validation_dataset.filter(lambda x: x[\"category\"] != \"null\")\n-\n-    model_id = \"vasudevgupta/flax-bigbird-natural-questions\"\n-    model = FlaxBigBirdForNaturalQuestions.from_pretrained(model_id)\n-    tokenizer = BigBirdTokenizerFast.from_pretrained(model_id)\n-\n-    @jax.jit\n-    def forward(*args, **kwargs):\n-        start_logits, end_logits, pooled_logits = model(*args, **kwargs)\n-        return start_logits, end_logits, jnp.argmax(pooled_logits, axis=-1)\n-\n-    def evaluate(example):\n-        # encode question and context so that they are separated by a tokenizer.sep_token and cut at max_length\n-        inputs = tokenizer(\n-            example[\"question\"],\n-            example[\"context\"],\n-            return_tensors=\"np\",\n-            max_length=4096,\n-            padding=\"max_length\",\n-            truncation=True,\n-        )\n-\n-        start_scores, end_scores, category = forward(**inputs)\n-\n-        predicted_category = CATEGORY_MAPPING[category.item()]\n-\n-        example[\"targets\"] = example[\"long\"] + example[\"short\"]\n-        if example[\"category\"] in [\"yes\", \"no\", \"null\"]:\n-            example[\"targets\"] = [example[\"category\"]]\n-        example[\"has_tgt\"] = example[\"category\"] != \"null\"\n-        # Now target can be: \"yes\", \"no\", \"null\", \"list of long & short answers\"\n-\n-        if predicted_category in [\"yes\", \"no\", \"null\"]:\n-            example[\"output\"] = [predicted_category]\n-            example[\"match\"] = example[\"output\"] == example[\"targets\"]\n-            example[\"has_pred\"] = predicted_category != \"null\"\n-            return example\n-\n-        max_size = 38 if predicted_category == \"short\" else 1024\n-        start_score, end_score = get_best_valid_start_end_idx(\n-            start_scores[0], end_scores[0], top_k=8, max_size=max_size\n-        )\n-\n-        input_ids = inputs[\"input_ids\"][0].tolist()\n-        example[\"output\"] = [tokenizer.decode(input_ids[start_score : end_score + 1])]\n-\n-        answers = expand_to_aliases(example[\"targets\"], make_sub_answers=True)\n-        predictions = expand_to_aliases(example[\"output\"])\n-\n-        # some preprocessing to both prediction and answer\n-        answers = {\"\".join(a.split()) for a in answers}\n-        predictions = {\"\".join(p.split()) for p in predictions}\n-        predictions = {s for s in predictions if s not in [\"``\", \"''\", \"`\", \"'\"]}\n-\n-        # if there is a common element, it's a exact match\n-        example[\"match\"] = len(list(answers & predictions)) > 0\n-        example[\"has_pred\"] = predicted_category != \"null\" and len(predictions) > 0\n-\n-        return example\n-\n-    short_validation_dataset = short_validation_dataset.map(evaluate)\n-\n-    total = len(short_validation_dataset)\n-    matched = len(short_validation_dataset.filter(lambda x: x[\"match\"] == 1))\n-    print(\"EM score:\", (matched / total) * 100, \"%\")\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "ebbb184ccb6b6ba7f29c16b0a903958bb365f62b",
            "filename": "examples/research_projects/jax-projects/big_bird/prepare_natural_questions.py",
            "status": "removed",
            "additions": 0,
            "deletions": 329,
            "changes": 329,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fprepare_natural_questions.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fprepare_natural_questions.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fprepare_natural_questions.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,329 +0,0 @@\n-import os\n-\n-import jsonlines\n-import numpy as np\n-from tqdm import tqdm\n-\n-\n-DOC_STRIDE = 2048\n-MAX_LENGTH = 4096\n-SEED = 42\n-PROCESS_TRAIN = os.environ.pop(\"PROCESS_TRAIN\", \"false\")\n-CATEGORY_MAPPING = {\"null\": 0, \"short\": 1, \"long\": 2, \"yes\": 3, \"no\": 4}\n-\n-\n-def _get_single_answer(example):\n-    def choose_first(answer, is_long_answer=False):\n-        assert isinstance(answer, list)\n-        if len(answer) == 1:\n-            answer = answer[0]\n-            return {k: [answer[k]] for k in answer} if is_long_answer else answer\n-        for a in answer:\n-            if is_long_answer:\n-                a = {k: [a[k]] for k in a}\n-            if len(a[\"start_token\"]) > 0:\n-                break\n-        return a\n-\n-    answer = {\"id\": example[\"id\"]}\n-    annotation = example[\"annotations\"]\n-    yes_no_answer = annotation[\"yes_no_answer\"]\n-    if 0 in yes_no_answer or 1 in yes_no_answer:\n-        answer[\"category\"] = [\"yes\"] if 1 in yes_no_answer else [\"no\"]\n-        answer[\"start_token\"] = answer[\"end_token\"] = []\n-        answer[\"start_byte\"] = answer[\"end_byte\"] = []\n-        answer[\"text\"] = [\"<cls>\"]\n-    else:\n-        answer[\"category\"] = [\"short\"]\n-        out = choose_first(annotation[\"short_answers\"])\n-        if len(out[\"start_token\"]) == 0:\n-            # answer will be long if short is not available\n-            answer[\"category\"] = [\"long\"]\n-            out = choose_first(annotation[\"long_answer\"], is_long_answer=True)\n-            out[\"text\"] = []\n-        answer.update(out)\n-\n-    # disregard some samples\n-    if len(answer[\"start_token\"]) > 1 or answer[\"start_token\"] == answer[\"end_token\"]:\n-        answer[\"remove_it\"] = True\n-    else:\n-        answer[\"remove_it\"] = False\n-\n-    cols = [\"start_token\", \"end_token\", \"start_byte\", \"end_byte\", \"text\"]\n-    if not all(isinstance(answer[k], list) for k in cols):\n-        raise ValueError(\"Issue in ID\", example[\"id\"])\n-\n-    return answer\n-\n-\n-def get_context_and_ans(example, assertion=False):\n-    \"\"\"Gives new context after removing <html> & new answer tokens as per new context\"\"\"\n-    answer = _get_single_answer(example)\n-    # bytes are of no use\n-    del answer[\"start_byte\"]\n-    del answer[\"end_byte\"]\n-\n-    # handle yes_no answers explicitly\n-    if answer[\"category\"][0] in [\"yes\", \"no\"]:  # category is list with one element\n-        doc = example[\"document\"][\"tokens\"]\n-        context = []\n-        for i in range(len(doc[\"token\"])):\n-            if not doc[\"is_html\"][i]:\n-                context.append(doc[\"token\"][i])\n-        return {\n-            \"context\": \" \".join(context),\n-            \"answer\": {\n-                \"start_token\": -100,  # ignore index in cross-entropy\n-                \"end_token\": -100,  # ignore index in cross-entropy\n-                \"category\": answer[\"category\"],\n-                \"span\": answer[\"category\"],  # extra\n-            },\n-        }\n-\n-    # later, help in removing all no answers\n-    if answer[\"start_token\"] == [-1]:\n-        return {\n-            \"context\": \"None\",\n-            \"answer\": {\n-                \"start_token\": -1,\n-                \"end_token\": -1,\n-                \"category\": \"null\",\n-                \"span\": \"None\",  # extra\n-            },\n-        }\n-\n-    # handling normal samples\n-\n-    cols = [\"start_token\", \"end_token\"]\n-    answer.update({k: answer[k][0] if len(answer[k]) > 0 else answer[k] for k in cols})  # e.g. [10] == 10\n-\n-    doc = example[\"document\"][\"tokens\"]\n-    start_token = answer[\"start_token\"]\n-    end_token = answer[\"end_token\"]\n-\n-    context = []\n-    for i in range(len(doc[\"token\"])):\n-        if not doc[\"is_html\"][i]:\n-            context.append(doc[\"token\"][i])\n-        else:\n-            if answer[\"start_token\"] > i:\n-                start_token -= 1\n-            if answer[\"end_token\"] > i:\n-                end_token -= 1\n-    new = \" \".join(context[start_token:end_token])\n-\n-    # checking above code\n-    if assertion:\n-        \"\"\"checking if above code is working as expected for all the samples\"\"\"\n-        is_html = doc[\"is_html\"][answer[\"start_token\"] : answer[\"end_token\"]]\n-        old = doc[\"token\"][answer[\"start_token\"] : answer[\"end_token\"]]\n-        old = \" \".join([old[i] for i in range(len(old)) if not is_html[i]])\n-        if new != old:\n-            print(\"ID:\", example[\"id\"])\n-            print(\"New:\", new, end=\"\\n\")\n-            print(\"Old:\", old, end=\"\\n\\n\")\n-\n-    return {\n-        \"context\": \" \".join(context),\n-        \"answer\": {\n-            \"start_token\": start_token,\n-            \"end_token\": end_token - 1,  # this makes it inclusive\n-            \"category\": answer[\"category\"],  # either long or short\n-            \"span\": new,  # extra\n-        },\n-    }\n-\n-\n-def get_strided_contexts_and_ans(example, tokenizer, doc_stride=2048, max_length=4096, assertion=True):\n-    # overlap will be of doc_stride - q_len\n-\n-    out = get_context_and_ans(example, assertion=assertion)\n-    answer = out[\"answer\"]\n-\n-    # later, removing these samples\n-    if answer[\"start_token\"] == -1:\n-        return {\n-            \"example_id\": example[\"id\"],\n-            \"input_ids\": [[-1]],\n-            \"labels\": {\n-                \"start_token\": [-1],\n-                \"end_token\": [-1],\n-                \"category\": [\"null\"],\n-            },\n-        }\n-\n-    input_ids = tokenizer(example[\"question\"][\"text\"], out[\"context\"]).input_ids\n-    q_len = input_ids.index(tokenizer.sep_token_id) + 1\n-\n-    # return yes/no\n-    if answer[\"category\"][0] in [\"yes\", \"no\"]:  # category is list with one element\n-        inputs = []\n-        category = []\n-        q_indices = input_ids[:q_len]\n-        doc_start_indices = range(q_len, len(input_ids), max_length - doc_stride)\n-        for i in doc_start_indices:\n-            end_index = i + max_length - q_len\n-            slice = input_ids[i:end_index]\n-            inputs.append(q_indices + slice)\n-            category.append(answer[\"category\"][0])\n-            if slice[-1] == tokenizer.sep_token_id:\n-                break\n-\n-        return {\n-            \"example_id\": example[\"id\"],\n-            \"input_ids\": inputs,\n-            \"labels\": {\n-                \"start_token\": [-100] * len(category),\n-                \"end_token\": [-100] * len(category),\n-                \"category\": category,\n-            },\n-        }\n-\n-    splitted_context = out[\"context\"].split()\n-    complete_end_token = splitted_context[answer[\"end_token\"]]\n-    answer[\"start_token\"] = len(\n-        tokenizer(\n-            \" \".join(splitted_context[: answer[\"start_token\"]]),\n-            add_special_tokens=False,\n-        ).input_ids\n-    )\n-    answer[\"end_token\"] = len(\n-        tokenizer(\" \".join(splitted_context[: answer[\"end_token\"]]), add_special_tokens=False).input_ids\n-    )\n-\n-    answer[\"start_token\"] += q_len\n-    answer[\"end_token\"] += q_len\n-\n-    # fixing end token\n-    num_sub_tokens = len(tokenizer(complete_end_token, add_special_tokens=False).input_ids)\n-    if num_sub_tokens > 1:\n-        answer[\"end_token\"] += num_sub_tokens - 1\n-\n-    old = input_ids[answer[\"start_token\"] : answer[\"end_token\"] + 1]  # right & left are inclusive\n-    start_token = answer[\"start_token\"]\n-    end_token = answer[\"end_token\"]\n-\n-    if assertion:\n-        \"\"\"This won't match exactly because of extra gaps => visaully inspect everything\"\"\"\n-        new = tokenizer.decode(old)\n-        if answer[\"span\"] != new:\n-            print(\"ISSUE IN TOKENIZATION\")\n-            print(\"OLD:\", answer[\"span\"])\n-            print(\"NEW:\", new, end=\"\\n\\n\")\n-\n-    if len(input_ids) <= max_length:\n-        return {\n-            \"example_id\": example[\"id\"],\n-            \"input_ids\": [input_ids],\n-            \"labels\": {\n-                \"start_token\": [answer[\"start_token\"]],\n-                \"end_token\": [answer[\"end_token\"]],\n-                \"category\": answer[\"category\"],\n-            },\n-        }\n-\n-    q_indices = input_ids[:q_len]\n-    doc_start_indices = range(q_len, len(input_ids), max_length - doc_stride)\n-\n-    inputs = []\n-    answers_start_token = []\n-    answers_end_token = []\n-    answers_category = []  # null, yes, no, long, short\n-    for i in doc_start_indices:\n-        end_index = i + max_length - q_len\n-        slice = input_ids[i:end_index]\n-        inputs.append(q_indices + slice)\n-        assert len(inputs[-1]) <= max_length, \"Issue in truncating length\"\n-\n-        if start_token >= i and end_token <= end_index - 1:\n-            start_token = start_token - i + q_len\n-            end_token = end_token - i + q_len\n-            answers_category.append(answer[\"category\"][0])  # [\"short\"] -> \"short\"\n-        else:\n-            start_token = -100\n-            end_token = -100\n-            answers_category.append(\"null\")\n-        new = inputs[-1][start_token : end_token + 1]\n-\n-        answers_start_token.append(start_token)\n-        answers_end_token.append(end_token)\n-        if assertion:\n-            \"\"\"checking if above code is working as expected for all the samples\"\"\"\n-            if new != old and new != [tokenizer.cls_token_id]:\n-                print(\"ISSUE in strided for ID:\", example[\"id\"])\n-                print(\"New:\", tokenizer.decode(new))\n-                print(\"Old:\", tokenizer.decode(old), end=\"\\n\\n\")\n-        if slice[-1] == tokenizer.sep_token_id:\n-            break\n-\n-    return {\n-        \"example_id\": example[\"id\"],\n-        \"input_ids\": inputs,\n-        \"labels\": {\n-            \"start_token\": answers_start_token,\n-            \"end_token\": answers_end_token,\n-            \"category\": answers_category,\n-        },\n-    }\n-\n-\n-def prepare_inputs(example, tokenizer, doc_stride=2048, max_length=4096, assertion=False):\n-    example = get_strided_contexts_and_ans(\n-        example,\n-        tokenizer,\n-        doc_stride=doc_stride,\n-        max_length=max_length,\n-        assertion=assertion,\n-    )\n-\n-    return example\n-\n-\n-def save_to_disk(hf_data, file_name):\n-    with jsonlines.open(file_name, \"a\") as writer:\n-        for example in tqdm(hf_data, total=len(hf_data), desc=\"Saving samples ... \"):\n-            labels = example[\"labels\"]\n-            for ids, start, end, cat in zip(\n-                example[\"input_ids\"],\n-                labels[\"start_token\"],\n-                labels[\"end_token\"],\n-                labels[\"category\"],\n-            ):\n-                if start == -1 and end == -1:\n-                    continue  # leave waste samples with no answer\n-                if cat == \"null\" and np.random.rand() < 0.6:\n-                    continue  # removing 50 % samples\n-                writer.write(\n-                    {\n-                        \"input_ids\": ids,\n-                        \"start_token\": start,\n-                        \"end_token\": end,\n-                        \"category\": CATEGORY_MAPPING[cat],\n-                    }\n-                )\n-\n-\n-if __name__ == \"__main__\":\n-    \"\"\"Running area\"\"\"\n-    from datasets import load_dataset\n-\n-    from transformers import BigBirdTokenizer\n-\n-    data = load_dataset(\"natural_questions\")\n-    tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n-\n-    data = data[\"train\" if PROCESS_TRAIN == \"true\" else \"validation\"]\n-\n-    fn_kwargs = {\n-        \"tokenizer\": tokenizer,\n-        \"doc_stride\": DOC_STRIDE,\n-        \"max_length\": MAX_LENGTH,\n-        \"assertion\": False,\n-    }\n-    data = data.map(prepare_inputs, fn_kwargs=fn_kwargs)\n-    data = data.remove_columns([\"annotations\", \"document\", \"id\", \"question\"])\n-    print(data)\n-\n-    np.random.seed(SEED)\n-    cache_file_name = \"nq-training.jsonl\" if PROCESS_TRAIN == \"true\" else \"nq-validation.jsonl\"\n-    save_to_disk(data, file_name=cache_file_name)"
        },
        {
            "sha": "b1bc8a7ace24b44fb6805108b4bc11293eab3ed2",
            "filename": "examples/research_projects/jax-projects/big_bird/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,6 +0,0 @@\n-git+https://github.com/huggingface/transformers@main\n-datasets\n-sentencepiece\n-wandb\n-flax\n-jsonlines"
        },
        {
            "sha": "d804f61b3e16f063368c187e3de598dc1094ffe6",
            "filename": "examples/research_projects/jax-projects/big_bird/sweep_flax.yaml",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fsweep_flax.yaml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fsweep_flax.yaml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Fsweep_flax.yaml?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,16 +0,0 @@\n-command: \n-        - python3\n-        - train.py\n-method: random\n-parameters:\n-        lr:\n-                values: [4e-5, 3e-5]\n-        warmup_steps:\n-                values: [20000, 15000, 10000, 5000]\n-        weight_decay:\n-                distribution: normal\n-                mu: 1e-2\n-                sigma: 2e-3\n-metric:\n-        name: eval_loss\n-        goal: minimize"
        },
        {
            "sha": "ce37b7f975bb3aceb41df88ef1b51bfe71098408",
            "filename": "examples/research_projects/jax-projects/big_bird/train.py",
            "status": "removed",
            "additions": 0,
            "deletions": 78,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Ftrain.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Ftrain.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fbig_bird%2Ftrain.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,78 +0,0 @@\n-import os\n-from dataclasses import replace\n-\n-import jax\n-import wandb\n-from bigbird_flax import Args, DataCollator, FlaxBigBirdForNaturalQuestions, Trainer, build_tx, train_step, val_step\n-from datasets import load_dataset\n-from flax import jax_utils\n-\n-from transformers import BigBirdTokenizerFast\n-\n-\n-if __name__ == \"__main__\":\n-    print(\"#################### AVAILABLE DEVICES ####################\")\n-    print(jax.devices())\n-    print(\"###########################################################\")\n-\n-    # setup for wandb sweep\n-    args = Args()\n-    logger = wandb.init(project=\"bigbird-natural-questions\", config=args.__dict__)\n-    wandb_args = dict(logger.config)\n-    del wandb_args[\"batch_size\"]\n-    args = replace(args, **wandb_args)\n-    base_dir = args.base_dir + \"-\" + wandb.run.id\n-    args = replace(args, base_dir=base_dir)\n-    print(args)\n-\n-    tr_dataset = load_dataset(\"json\", data_files=args.tr_data_path)[\"train\"]\n-    val_dataset = load_dataset(\"json\", data_files=args.val_data_path)[\"train\"]\n-\n-    # drop extra batch for now\n-    indices = range(len(tr_dataset) - len(tr_dataset) % args.batch_size)\n-    tr_dataset = tr_dataset.shuffle().select(indices)\n-    indices = range(len(val_dataset) - len(val_dataset) % args.batch_size)\n-    val_dataset = val_dataset.shuffle().select(indices)\n-\n-    if os.environ.get(\"TRAIN_ON_SMALL\", \"false\") == \"true\":\n-        tr_dataset = tr_dataset.shuffle().select(range(80000))\n-        val_dataset = val_dataset.shuffle().select(range(8000))\n-\n-    print(tr_dataset)\n-    print(val_dataset)\n-\n-    model = FlaxBigBirdForNaturalQuestions.from_pretrained(\n-        args.model_id, block_size=args.block_size, num_random_blocks=args.num_random_blocks\n-    )\n-    tokenizer = BigBirdTokenizerFast.from_pretrained(args.model_id)\n-    data_collator = DataCollator(pad_id=tokenizer.pad_token_id, max_length=4096)\n-\n-    tx_args = {\n-        \"lr\": args.lr,\n-        \"init_lr\": args.init_lr,\n-        \"warmup_steps\": args.warmup_steps,\n-        \"num_train_steps\": args.max_epochs * (len(tr_dataset) // args.batch_size),\n-        \"weight_decay\": args.weight_decay,\n-    }\n-    tx, lr = build_tx(**tx_args)\n-\n-    trainer = Trainer(\n-        args=args,\n-        data_collator=data_collator,\n-        model_save_fn=model.save_pretrained,\n-        train_step_fn=train_step,\n-        val_step_fn=val_step,\n-        logger=logger,\n-        scheduler_fn=lr,\n-    )\n-\n-    ckpt_dir = None\n-    state = trainer.create_state(model, tx, num_train_steps=tx_args[\"num_train_steps\"], ckpt_dir=ckpt_dir)\n-    try:\n-        trainer.train(state, tr_dataset, val_dataset)\n-    except KeyboardInterrupt:\n-        print(\"Oooops; TRAINING STOPPED UNFORTUNATELY\")\n-\n-    print(\"SAVING WEIGHTS IN `final-weights`\")\n-    params = jax_utils.unreplicate(state.params)\n-    model.save_pretrained(os.path.join(args.base_dir, \"final-weights\"), params=params)"
        },
        {
            "sha": "bdb6629e509c6fa7f5a6987a9a184fe581af4cd0",
            "filename": "examples/research_projects/jax-projects/dataset-streaming/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 121,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fdataset-streaming%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fdataset-streaming%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fdataset-streaming%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,121 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Language model training examples in streaming mode\n-\n-The following examples showcase how to train a language model from scratch \n-using the JAX/Flax backend.\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-All of the following examples make use of [dataset streaming](https://huggingface.co/docs/datasets/master/dataset_streaming), therefore allowing to train models on massive datasets\\\n-without ever having to download the full dataset.\n-\n-## Masked language modeling\n-\n-In the following, we demonstrate how to train a bi-directional transformer model \n-using masked language modeling objective as introduced in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n-More specifically, we demonstrate how JAX/Flax and dataset streaming can be leveraged \n-to pre-train [**`FacebookAI/roberta-base`**](https://huggingface.co/FacebookAI/roberta-base)\n-in English on a single TPUv3-8 pod for 10000 update steps.\n-\n-The example script uses the ğŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n-\n-Let's start by creating a model repository to save the trained model and logs.\n-Here we call the model `\"english-roberta-base-dummy\"`, but you can change the model name as you like.\n-\n-You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\n-you are logged in) or via the command line:\n-\n-```bash\n-huggingface-cli repo create english-roberta-base-dummy\n-```\n-\n-Next we clone the model repository to add the tokenizer and model files.\n-\n-```bash\n-git clone https://huggingface.co/<your-username>/english-roberta-base-dummy\n-```\n-\n-To ensure that all tensorboard traces will be uploaded correctly, we need to \n-track them. You can run the following command inside your model repo to do so.\n-\n-```bash\n-cd english-roberta-base-dummy\n-git lfs track \"*tfevents*\"\n-```\n-\n-Great, we have set up our model repository. During training, we will automatically\n-push the training logs and model weights to the repo.\n-\n-Next, let's add a symbolic link to the `run_mlm_flax.py`.\n-\n-```bash\n-export MODEL_DIR=\"./english-roberta-base-dummy\"\n-ln -s ~/transformers/examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py ./\n-```\n-\n-### Copy config and tokenizer of existing model\n-\n-In this example, we will simply copy an existing config and tokenizer in English.\n-You can run the following code in a Python shell to do so.\n-\n-```python\n-from transformers import RobertaTokenizerFast, RobertaConfig\n-\n-model_dir = \"./english-roberta-base-dummy\"\n-\n-tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n-config = RobertaConfig.from_pretrained(\"FacebookAI/roberta-base\")\n-\n-tokenizer.save_pretrained(model_dir)\n-config.save_pretrained(model_dir)\n-```\n-\n-### Train model\n-\n-Next we can run the example script to pretrain the model.\n-Compared to the default [`run_mlm_flax`](https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py), we introduced 4 new training settings:\n-- `num_train_steps` - how many update steps should be run.\n-- `num_eval_samples` - how many training samples should be taken for evaluation.\n-- `logging_steps` - at what rate should the training loss be logged.\n-- `eval_steps` - at what rate should evaluation be run.\n-10K update steps \n-\n-```bash\n-./run_mlm_flax_stream.py \\\n-    --output_dir=\"${MODEL_DIR}\" \\\n-    --model_type=\"roberta\" \\\n-    --config_name=\"${MODEL_DIR}\" \\\n-    --tokenizer_name=\"${MODEL_DIR}\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_en\" \\\n-    --max_seq_length=\"128\" \\\n-    --per_device_train_batch_size=\"128\" \\\n-    --per_device_eval_batch_size=\"128\" \\\n-    --learning_rate=\"3e-4\" \\\n-    --warmup_steps=\"1000\" \\\n-    --overwrite_output_dir \\\n-    --adam_beta1=\"0.9\" \\\n-    --adam_beta2=\"0.98\" \\\n-    --num_train_steps=\"10000\" \\\n-    --num_eval_samples=\"5000\" \\\n-    --logging_steps=\"250\" \\\n-    --eval_steps=\"1000\" \\\n-    --push_to_hub\n-```"
        },
        {
            "sha": "8940fab5bda34b6652eac43e13cca849a3c517da",
            "filename": "examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py",
            "status": "removed",
            "additions": 0,
            "deletions": 637,
            "changes": 637,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fdataset-streaming%2Frun_mlm_flax_stream.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fdataset-streaming%2Frun_mlm_flax_stream.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fdataset-streaming%2Frun_mlm_flax_stream.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,637 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...) with whole word masking on a\n-text file or a dataset.\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=fill-mask\n-\"\"\"\n-\n-import logging\n-import os\n-import sys\n-import time\n-from collections import defaultdict\n-from dataclasses import dataclass, field\n-\n-# You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.\n-from pathlib import Path\n-from typing import Dict, List, Optional, Tuple\n-\n-import datasets\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from tqdm import tqdm\n-\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_MASKED_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForMaskedLM,\n-    HfArgumentParser,\n-    PreTrainedTokenizerBase,\n-    TensorType,\n-    TrainingArguments,\n-    is_tensorboard_available,\n-    set_seed,\n-)\n-\n-\n-if datasets.__version__ <= \"1.8.0\":\n-    raise ValueError(\"Make sure to upgrade `datasets` to a version >= 1.9.0 to use dataset streaming\")\n-\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_MASKED_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    train_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input train ref data file for whole word masking in Chinese.\"},\n-    )\n-    validation_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input validation ref data file for whole word masking in Chinese.\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated. Default to the max input length of the model.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    mlm_probability: float = field(\n-        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to `max_seq_length`. \"\n-                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n-            )\n-        },\n-    )\n-    line_by_line: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n-    )\n-    text_column_name: str = field(\n-        default=\"text\", metadata={\"help\": \"The name of the column to retrieve the training text.\"}\n-    )\n-    shuffle_buffer_size: int = field(\n-        default=10000, metadata={\"help\": \"The number of examples to pre-load for shuffling.\"}\n-    )\n-    num_train_steps: int = field(default=50000, metadata={\"help\": \"The number of training steps.\"})\n-    num_eval_samples: int = field(default=50000, metadata={\"help\": \"The number of samples to be used for evaluation\"})\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n-\n-\n-@flax.struct.dataclass\n-class FlaxDataCollatorForLanguageModeling:\n-    \"\"\"\n-    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n-    are not all of the same length.\n-\n-    Args:\n-        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n-            The tokenizer used for encoding the data.\n-        mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n-            The probability with which to (randomly) mask tokens in the input.\n-\n-    .. note::\n-\n-        For best performance, this data collator should be used with a dataset having items that are dictionaries or\n-        BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n-        :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n-        argument :obj:`return_special_tokens_mask=True`.\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    mlm_probability: float = 0.15\n-\n-    def __post_init__(self):\n-        if self.tokenizer.mask_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n-                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n-            )\n-\n-    def __call__(self, examples: List[Dict[str, np.ndarray]]) -> Dict[str, np.ndarray]:\n-        # Handle dict or lists with proper padding and conversion to tensor.\n-        batch = self.tokenizer.pad(examples, return_tensors=TensorType.NUMPY)\n-\n-        # If special token mask has been preprocessed, pop it from the dict.\n-        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n-\n-        batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n-            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n-        )\n-        return batch\n-\n-    def mask_tokens(\n-        self, inputs: np.ndarray, special_tokens_mask: Optional[np.ndarray]\n-    ) -> Tuple[jnp.ndarray, jnp.ndarray]:\n-        \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n-        \"\"\"\n-        labels = inputs.copy()\n-        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n-        probability_matrix = np.full(labels.shape, self.mlm_probability)\n-        special_tokens_mask = special_tokens_mask.astype(\"bool\")\n-\n-        probability_matrix[special_tokens_mask] = 0.0\n-        masked_indices = np.random.binomial(1, probability_matrix).astype(\"bool\")\n-        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n-\n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = np.random.binomial(1, np.full(labels.shape, 0.8)).astype(\"bool\") & masked_indices\n-        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n-\n-        # 10% of the time, we replace masked input tokens with random word\n-        indices_random = np.random.binomial(1, np.full(labels.shape, 0.5)).astype(\"bool\")\n-        indices_random &= masked_indices & ~indices_replaced\n-\n-        random_words = np.random.randint(self.tokenizer.vocab_size, size=labels.shape, dtype=\"i4\")\n-        inputs[indices_random] = random_words[indices_random]\n-\n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n-        return inputs, labels\n-\n-\n-def generate_batch_splits(samples_idx: np.ndarray, batch_size: int) -> np.ndarray:\n-    num_samples = len(samples_idx)\n-    samples_to_remove = num_samples % batch_size\n-\n-    if samples_to_remove != 0:\n-        samples_idx = samples_idx[:-samples_to_remove]\n-    sections_split = num_samples // batch_size\n-    batch_idx = np.split(samples_idx, sections_split)\n-    return batch_idx\n-\n-\n-def advance_iter_and_group_samples(train_iterator, num_samples, max_seq_length):\n-    \"\"\"\n-    The training iterator is advanced so that after groupifying the samples,\n-    `num_samples` of length `max_seq_length` are returned.\n-    \"\"\"\n-    num_total_tokens = max_seq_length * num_samples\n-    samples = defaultdict(list)\n-\n-    i = 0\n-    while i < num_total_tokens:\n-        tokenized_samples = next(train_iterator)\n-        i += len(tokenized_samples[\"input_ids\"])\n-\n-        # concatenate tokenized samples to list (excluding \"id\" and \"text\")\n-        samples = {\n-            k: samples[k] + tokenized_samples[k] for k in [\"input_ids\", \"attention_mask\", \"special_tokens_mask\"]\n-        }\n-\n-    # Concatenated tokens are split to lists of length `max_seq_length`.\n-    # Note that remainedr of % max_seq_length are thrown away.\n-    def group_texts(examples):\n-        result = {\n-            k: [t[i : i + max_seq_length] for i in range(0, num_total_tokens, max_seq_length)]\n-            for k, t in examples.items()\n-        }\n-        return result\n-\n-    grouped_samples = group_texts(samples)\n-    return grouped_samples\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-if __name__ == \"__main__\":\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        level=\"INFO\",\n-        datefmt=\"[%X]\",\n-    )\n-\n-    # Log on each process the small summary:\n-    logger = logging.getLogger(__name__)\n-    logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n-        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n-    )\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        dataset = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            streaming=True,\n-            split=\"train\",\n-        )\n-\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n-    # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n-    # efficient when it receives the `special_tokens_mask`.\n-    def tokenize_function(examples):\n-        return tokenizer(examples[data_args.text_column_name], return_special_tokens_mask=True)\n-\n-    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=list(dataset.features.keys()))\n-\n-    shuffle_seed = training_args.seed\n-    tokenized_datasets = tokenized_datasets.shuffle(buffer_size=data_args.shuffle_buffer_size, seed=shuffle_seed)\n-\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-\n-        summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-\n-    # Data collator\n-    # This one will take care of randomly masking the tokens.\n-    data_collator = FlaxDataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=data_args.mlm_probability)\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxAutoModelForMaskedLM.from_pretrained(\n-            model_args.model_name_or_path, config=config, seed=training_args.seed, dtype=getattr(jnp, model_args.dtype)\n-        )\n-    else:\n-        model = FlaxAutoModelForMaskedLM.from_config(\n-            config, seed=training_args.seed, dtype=getattr(jnp, model_args.dtype)\n-        )\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    eval_batch_size = int(training_args.per_device_eval_batch_size) * jax.device_count()\n-\n-    # define number steps per stream epoch\n-    num_train_steps = data_args.num_train_steps\n-\n-    # Create learning rate schedule\n-    warmup_fn = optax.linear_schedule(\n-        init_value=0.0, end_value=training_args.learning_rate, transition_steps=training_args.warmup_steps\n-    )\n-    decay_fn = optax.linear_schedule(\n-        init_value=training_args.learning_rate,\n-        end_value=0,\n-        transition_steps=num_train_steps - training_args.warmup_steps,\n-    )\n-    linear_decay_lr_schedule_fn = optax.join_schedules(\n-        schedules=[warmup_fn, decay_fn], boundaries=[training_args.warmup_steps]\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    # Note that this mask is specifically adapted for FlaxBERT-like models.\n-    # For other models, one should correct the layer norm parameter naming\n-    # accordingly.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    # Setup train state\n-    state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw)\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, dropout_rng):\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-\n-        def loss_fn(params):\n-            labels = batch.pop(\"labels\")\n-\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-\n-            # compute loss, ignore padded input tokens\n-            label_mask = jnp.where(labels > 0, 1.0, 0.0)\n-            loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n-\n-            # take average\n-            loss = loss.sum() / label_mask.sum()\n-\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(loss_fn)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-        new_state = state.apply_gradients(grads=grad)\n-\n-        metrics = jax.lax.pmean(\n-            {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n-        )\n-\n-        return new_state, metrics, new_dropout_rng\n-\n-    # Create parallel version of the train step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        labels = batch.pop(\"labels\")\n-\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        # compute loss, ignore padded input tokens\n-        label_mask = jnp.where(labels > 0, 1.0, 0.0)\n-        loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n-\n-        # compute accuracy\n-        accuracy = jnp.equal(jnp.argmax(logits, axis=-1), labels) * label_mask\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss.sum(), \"accuracy\": accuracy.sum(), \"normalizer\": label_mask.sum()}\n-        metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n-\n-        return metrics\n-\n-    p_eval_step = jax.pmap(eval_step, \"batch\", donate_argnums=(0,))\n-\n-    # Replicate the train state on each device\n-    state = jax_utils.replicate(state)\n-\n-    train_time = 0\n-    train_start = time.time()\n-    train_metrics = []\n-    eval_metrics = []\n-\n-    training_iter = iter(tokenized_datasets)\n-\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-    eval_samples = advance_iter_and_group_samples(training_iter, data_args.num_eval_samples, max_seq_length)\n-\n-    steps = tqdm(range(num_train_steps), desc=\"Training...\", position=0)\n-    for step in range(num_train_steps):\n-        # ======================== Training ================================\n-        try:\n-            samples = advance_iter_and_group_samples(training_iter, train_batch_size, max_seq_length)\n-        except StopIteration:\n-            # Once the end of the dataset stream is reached, the training iterator\n-            # is reinitialized and reshuffled and a new eval dataset is randomly chosen.\n-            shuffle_seed += 1\n-            tokenized_datasets.set_epoch(shuffle_seed)\n-\n-            training_iter = iter(tokenized_datasets)\n-\n-            eval_dataset = advance_iter_and_group_samples(training_iter, data_args.num_eval_samples, max_seq_length)\n-            samples = advance_iter_and_group_samples(training_iter, train_batch_size, max_seq_length)\n-\n-        # process input samples\n-        model_inputs = data_collator(samples)\n-\n-        # Model forward\n-        model_inputs = shard(model_inputs.data)\n-        state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rngs)\n-\n-        train_metrics.append(train_metric)\n-\n-        if step % training_args.logging_steps == 0 and step > 0:\n-            steps.write(\n-                f\"Step... ({step} | Loss: {train_metric['loss'].mean()}, Learning Rate:\"\n-                f\" {train_metric['learning_rate'].mean()})\"\n-            )\n-            train_time += time.time() - train_start\n-            if has_tensorboard and jax.process_index() == 0:\n-                write_train_metric(summary_writer, train_metrics, train_time, step)\n-            train_metrics = []\n-\n-        # ======================== Evaluating ==============================\n-        if step % training_args.eval_steps == 0 and step > 0:\n-            # Avoid using jax.numpy here in case of TPU training\n-            eval_samples_idx = np.arange(data_args.num_eval_samples)\n-            eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size)\n-\n-            for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=1)):\n-                # process input samples\n-                batch_eval_samples = {k: [v[idx] for idx in batch_idx] for k, v in eval_samples.items()}\n-                model_inputs = data_collator(batch_eval_samples)\n-\n-                # Model forward\n-                model_inputs = shard(model_inputs.data)\n-                metrics = p_eval_step(state.params, model_inputs)\n-                eval_metrics.append(metrics)\n-\n-            # normalize eval metrics\n-            eval_metrics = get_metrics(eval_metrics)\n-            eval_metrics = jax.tree_util.tree_map(jnp.sum, eval_metrics)\n-            eval_normalizer = eval_metrics.pop(\"normalizer\")\n-            eval_metrics = jax.tree_util.tree_map(lambda x: x / eval_normalizer, eval_metrics)\n-\n-            # Update progress bar\n-            steps.desc = (\n-                f\"Step... ({step + 1}/{num_train_steps} | Loss: {eval_metrics['loss']}, Acc:\"\n-                f\" {eval_metrics['accuracy']})\"\n-            )\n-\n-            if has_tensorboard and jax.process_index() == 0:\n-                write_eval_metric(summary_writer, eval_metrics, step)\n-            eval_metrics = []\n-\n-            # save checkpoint after each epoch and push checkpoint to the hub\n-            if jax.process_index() == 0:\n-                params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-                model.save_pretrained(\n-                    training_args.output_dir,\n-                    params=params,\n-                    push_to_hub=training_args.push_to_hub,\n-                    commit_message=f\"Saving weights and logs of step {step+1}\",\n-                )\n-\n-        # update tqdm bar\n-        steps.update(1)"
        },
        {
            "sha": "72d3db1935895f4c53732e0c8dd554a8ca0840e5",
            "filename": "examples/research_projects/jax-projects/hybrid_clip/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 172,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,172 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Vision-Text dual encoder model training examples\n-\n-> Note: This example is experimental and might not give the best possible results\n-\n-The following example showcases how to train a CLIP like vision-text dual encoder model\n-using a pre-trained vision and text encoder using the JAX/Flax backend.\n-\n-Such a model can be used for natural language image search and potentially zero-shot image classification.\n-The model is inspired by the [CLIP](https://openai.com/blog/clip/) approach, introduced by Alec Radford et al.\n-The idea is to train a vision encoder and a text encoder jointly to project the representation of images and their\n-captions into the same embedding space, such that the caption embeddings are located near the embeddings\n-of the images they describe.\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-In this example we will use the vision model from [CLIP](https://huggingface.co/models?filter=clip)\n-as the image encoder and [`FacebookAI/roberta-base`](https://huggingface.co/FacebookAI/roberta-base) as the text encoder.\n-Note that one can also use the [ViT](https://huggingface.co/models?filter=vit) model as image encoder and any other BERT or ROBERTa model as text encoder.\n-To train the model on languages other than English one should choose a text encoder trained on the desired\n-language and a image-text dataset in that language. One such dataset is [WIT](https://github.com/google-research-datasets/wit).\t\n-\n-Let's start by creating a model repository to save the trained model and logs.\n-Here we call the model `\"clip-roberta-base\"`, but you can change the model name as you like.\n-\n-You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\n-you are logged in) or via the command line:\n-\n-```bash\n-huggingface-cli repo create clip-roberta-base\n-```\n-Next we clone the model repository to add the tokenizer and model files.\n-```bash\n-git clone https://huggingface.co/<your-username>/clip-roberta-base\n-```\n-To ensure that all tensorboard traces will be uploaded correctly, we need to \n-track them. You can run the following command inside your model repo to do so.\n-\n-```bash\n-cd clip-roberta-base\n-git lfs track \"*tfevents*\"\n-```\n-\n-Great, we have set up our model repository. During training, we will automatically\n-push the training logs and model weights to the repo.\n-\n-Next, let's add a symbolic link to the `run_hybrid_clip.py`.\n-\n-```bash\n-export MODEL_DIR=\"./clip-roberta-base\n-ln -s ~/transformers/examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py run_hybrid_clip.py\n-```\n-\n-## How to use the `FlaxHybridCLIP` model:\n-\n-The `FlaxHybridCLIP` class let's you load any text and vision encoder model to create a dual encoder. \n-Here is an example of how to load the model using pre-trained text and vision models.\n-\n-```python\n-from modeling_hybrid_clip import FlaxHybridCLIP\n-\n-model = FlaxHybridCLIP.from_text_vision_pretrained(\"google-bert/bert-base-uncased\", \"openai/clip-vit-base-patch32\")\n-\n-# save the model\n-model.save_pretrained(\"bert-clip\")\n-\n-# load the saved model\n-model = FlaxHybridCLIP.from_pretrained(\"bert-clip\")\n-```\n-\n-If the checkpoints are in PyTorch then one could pass `text_from_pt=True` and `vision_from_pt=True`. This will load the model\n-PyTorch checkpoints convert them to flax and load the model.\n-\n-```python\n-model = FlaxHybridCLIP.from_text_vision_pretrained(\"google-bert/bert-base-uncased\", \"openai/clip-vit-base-patch32\", text_from_pt=True, vision_from_pt=True)\n-```\n-\n-This loads both the text and vision encoders using pre-trained weights, the projection layers are randomly\n-initialized except for CLIP's vision model. If you use CLIP to initialize the vision model then the vision projection weights are also\n-loaded using the pre-trained weights.\n-\n-## Prepare the dataset\n-\n-We will use the MS-COCO dataset to train our dual encoder model. MS-COCO contains over 82,000 images, each of which has at least 5 different caption annotations. The dataset is usually used for image captioning tasks, but we can repurpose the image-caption pairs to train our dual encoder model for image search.\n-\n-### Download and extract the data.\n-\n-It consists of two compressed folders: one with images, and the otherâ€”with associated image captions. Note that the compressed images folder is 13GB in size.\n-\n-```bash\n-wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n-wget http://images.cocodataset.org/zips/train2014.zip\n-\n-unzip annotations_trainval2014.zip\n-unzip train2014.zip\n-\n-mkdir coco_dataset\n-mv train2014 coco_dataset/\n-mv annotations coco_dataset/\n-```\n-\n-### Prepare dataset files and split the dataset.\n-\n-```python\n-import json\n-import collections\n-\n-images_dir = \"coco_dataset/train2014\"\n-annotation_file = \"coco_dataset/annotations/captions_train2014.json\"\n-with open(annotation_file, \"r\") as f:\n-    annotations = json.load(f)[\"annotations\"]\n-\n-image_path_to_caption = collections.defaultdict(list)\n-for element in annotations:\n-    caption = f\"{element['caption'].lower().rstrip('.')}\"\n-    image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n-    image_path_to_caption[image_path].append(caption)\n-\n-lines = []\n-for image_path, captions in image_path_to_caption.items():\n-    lines.append(json.dumps({\"image_path\": image_path, \"captions\": captions}))\n-\n-train_lines = lines[:-8000]\n-valid_line = lines[-8000:]\n-with open(\"coco_dataset/train_dataset.json\", \"w\") as f:\n-    f.write(\"\\n\".join(train_lines))\n-\n-with open(\"coco_dataset/valid_dataset.json\", \"w\") as f:\n-    f.write(\"\\n\".join(valid_line))\n-```\n-\n-> Note: The data loading and processing part of this script can still be improved for maximum performance. In particular one should decode the images beforehand and use those instead decoding them each time. If the dataset is small or if you have huge disk space the you could also pre-process all the dataset beforehand and then use it.\n-\n-## Train the model\n-Next we can run the example script to train the model:\n-\n-```bash\n-python run_hybrid_clip.py \\\n-    --output_dir ${MODEL_DIR} \\\n-    --text_model_name_or_path=\"FacebookAI/roberta-base\" \\\n-    --vision_model_name_or_path=\"openai/clip-vit-base-patch32\" \\\n-    --tokenizer_name=\"FacebookAI/roberta-base\" \\\n-    --train_file=\"coco_dataset/train_dataset.json\" \\\n-    --validation_file=\"coco_dataset/validation_dataset.json\" \\\n-    --do_train --do_eval \\\n-    --num_train_epochs=\"40\" --max_seq_length 96 \\\n-    --per_device_train_batch_size=\"64\" \\\n-    --per_device_eval_batch_size=\"64\" \\\n-    --learning_rate=\"5e-5\" --warmup_steps=\"0\" --weight_decay 0.1 \\\n-    --overwrite_output_dir \\\n-    --preprocessing_num_workers 32 \\\n-    --push_to_hub\n-```\n-\n-This should finish in ~1h50 mins with min validation loss 2.43. Training statistics can be accessed on [tfhub.de](https://tensorboard.dev/experiment/RUNPYd1yRgSD5kZSb9hDig/#scalars)"
        },
        {
            "sha": "5272ac44a1a884eaf9b058c9e29729bfaec29a58",
            "filename": "examples/research_projects/jax-projects/hybrid_clip/configuration_hybrid_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 112,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Fconfiguration_hybrid_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Fconfiguration_hybrid_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Fconfiguration_hybrid_clip.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,112 +0,0 @@\n-import copy\n-\n-from transformers.configuration_utils import PretrainedConfig\n-from transformers.utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class HybridCLIPConfig(PretrainedConfig):\n-    r\"\"\"\n-    :class:`HybridCLIPConfig` is the configuration class to store the configuration of a\n-    :class:`~HybridCLIPModel`. It is used to instantiate HybridCLIPModel model according to the specified arguments,\n-    defining the text model and vision model configs.\n-\n-    Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model\n-    outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.\n-\n-    Args:\n-        text_config_dict (:obj:`dict`):\n-            Dictionary of configuration options that defines text model config.\n-        vision_config_dict (:obj:`dict`):\n-            Dictionary of configuration options that defines vison model config.\n-        projection_dim (:obj:`int`, `optional`, defaults to 512):\n-            Dimentionality of text and vision projection layers.\n-        kwargs (`optional`):\n-            Dictionary of keyword arguments.\n-\n-    Examples::\n-\n-        >>> from transformers import BertConfig, CLIPConfig, HybridCLIPConfig, FlaxHybridCLIP\n-\n-        >>> # Initializing a BERT and CLIP configuration\n-        >>> config_text = BertConfig()\n-        >>> config_vision = CLIPConfig()\n-\n-        >>> config = HybridCLIPConfig.from_text_vision_configs(config_text, config_vision, projection_dim=512)\n-\n-        >>> # Initializing a BERT and CLIPVision model\n-        >>> model = EncoderDecoderModel(config=config)\n-\n-        >>> # Accessing the model configuration\n-        >>> config_text = model.config.text_config\n-        >>> config_vision  = model.config.vision_config\n-\n-        >>> # Saving the model, including its configuration\n-        >>> model.save_pretrained('my-model')\n-\n-        >>> # loading model and config from pretrained folder\n-        >>> encoder_decoder_config = HybridCLIPConfig.from_pretrained('my-model')\n-        >>> model = FlaxHybridCLIP.from_pretrained('my-model', config=encoder_decoder_config)\n-    \"\"\"\n-\n-    model_type = \"hybrid-clip\"\n-    is_composition = True\n-\n-    def __init__(self, projection_dim=512, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        if \"text_config\" not in kwargs:\n-            raise ValueError(\"`text_config` can not be `None`.\")\n-\n-        if \"vision_config\" not in kwargs:\n-            raise ValueError(\"`vision_config` can not be `None`.\")\n-\n-        text_config = kwargs.pop(\"text_config\")\n-        vision_config = kwargs.pop(\"vision_config\")\n-\n-        text_model_type = text_config.pop(\"model_type\")\n-        vision_model_type = vision_config.pop(\"model_type\")\n-\n-        from transformers import AutoConfig\n-\n-        self.text_config = AutoConfig.for_model(text_model_type, **text_config)\n-\n-        if vision_model_type == \"clip\":\n-            self.vision_config = AutoConfig.for_model(vision_model_type, **vision_config).vision_config\n-        elif vision_model_type == \"clip_vision_model\":\n-            from transformers import CLIPVisionConfig\n-\n-            self.vision_config = CLIPVisionConfig(**vision_config)\n-        else:\n-            self.vision_config = AutoConfig.for_model(vision_model_type, **vision_config)\n-\n-        self.projection_dim = projection_dim\n-        self.initializer_factor = 1.0\n-\n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: PretrainedConfig, vision_config: PretrainedConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a :class:`HybridCLIPConfig` (or a derived class) from text model configuration and\n-        vision model configuration.\n-\n-        Returns:\n-            :class:`HybridCLIPConfig`: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default\n-        :meth:`~transformers.PretrainedConfig.to_dict`.\n-\n-        Returns:\n-            :obj:`Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-        output[\"text_config\"] = self.text_config.to_dict()\n-        output[\"vision_config\"] = self.vision_config.to_dict()\n-        output[\"model_type\"] = self.__class__.model_type\n-        return output"
        },
        {
            "sha": "08cb3bd0b3412e535a825b0e6f7a4d09362ff90a",
            "filename": "examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 420,
            "changes": 420,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Fmodeling_hybrid_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Fmodeling_hybrid_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Fmodeling_hybrid_clip.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,420 +0,0 @@\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Optional, Tuple\n-\n-import flax.linen as nn\n-import jax\n-import jax.numpy as jnp\n-from configuration_hybrid_clip import HybridCLIPConfig\n-from flax.core.frozen_dict import FrozenDict\n-\n-from transformers import FLAX_MODEL_MAPPING, FlaxCLIPVisionModel\n-from transformers.modeling_flax_utils import FlaxPreTrainedModel\n-from transformers.models.clip.modeling_flax_clip import FlaxCLIPOutput\n-from transformers.utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class FlaxHybridCLIPModule(nn.Module):\n-    config: HybridCLIPConfig\n-    dtype: jnp.dtype = jnp.float32\n-\n-    def setup(self):\n-        text_config = self.config.text_config\n-        vision_config = self.config.vision_config\n-\n-        self.projection_dim = self.config.projection_dim\n-        self.text_embed_dim = text_config.hidden_size\n-        self.vision_embed_dim = vision_config.hidden_size\n-\n-        text_module = FLAX_MODEL_MAPPING[self.config.text_config.__class__].module_class\n-        vision_module = FLAX_MODEL_MAPPING.get(self.config.vision_config.__class__, FlaxCLIPVisionModel).module_class\n-\n-        self.text_model = text_module(text_config, dtype=self.dtype)\n-        self.vision_model = vision_module(vision_config, dtype=self.dtype)\n-\n-        self.visual_projection = nn.Dense(\n-            self.projection_dim,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(0.02),\n-            use_bias=False,\n-        )\n-        self.text_projection = nn.Dense(\n-            self.projection_dim,\n-            dtype=self.dtype,\n-            kernel_init=jax.nn.initializers.normal(0.02),\n-            use_bias=False,\n-        )\n-        self.logit_scale = self.param(\"logit_scale\", jax.nn.initializers.ones, [])\n-\n-    def __call__(\n-        self,\n-        input_ids=None,\n-        pixel_values=None,\n-        attention_mask=None,\n-        position_ids=None,\n-        token_type_ids=None,\n-        deterministic: bool = True,\n-        output_attentions=None,\n-        output_hidden_states=None,\n-        return_dict=None,\n-    ):\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        vision_outputs = self.vision_model(\n-            pixel_values=pixel_values,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        text_outputs = self.text_model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            position_ids=position_ids,\n-            deterministic=deterministic,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        image_embeds = vision_outputs[1]\n-        image_embeds = self.visual_projection(image_embeds)\n-\n-        text_embeds = text_outputs[1]\n-        text_embeds = self.text_projection(text_embeds)\n-\n-        # normalized features\n-        image_embeds = image_embeds / jnp.linalg.norm(image_embeds, axis=-1, keepdims=True)\n-        text_embeds = text_embeds / jnp.linalg.norm(text_embeds, axis=-1, keepdims=True)\n-\n-        # cosine similarity as logits\n-        logit_scale = jnp.exp(self.logit_scale)\n-        logits_per_text = jnp.matmul(text_embeds, image_embeds.T) * logit_scale\n-        logits_per_image = logits_per_text.T\n-\n-        if not return_dict:\n-            return (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n-\n-        return FlaxCLIPOutput(\n-            logits_per_image=logits_per_image,\n-            logits_per_text=logits_per_text,\n-            text_embeds=text_embeds,\n-            image_embeds=image_embeds,\n-            text_model_output=text_outputs,\n-            vision_model_output=vision_outputs,\n-        )\n-\n-\n-class FlaxHybridCLIP(FlaxPreTrainedModel):\n-    config_class = HybridCLIPConfig\n-    module_class = FlaxHybridCLIPModule\n-\n-    def __init__(\n-        self,\n-        config: HybridCLIPConfig,\n-        input_shape: Optional[Tuple] = None,\n-        seed: int = 0,\n-        dtype: jnp.dtype = jnp.float32,\n-        **kwargs,\n-    ):\n-        if input_shape is None:\n-            input_shape = ((1, 1), (1, config.vision_config.image_size, config.vision_config.image_size, 3))\n-\n-        module = self.module_class(config=config, dtype=dtype, **kwargs)\n-        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype)\n-\n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n-        # init input tensor\n-        input_ids = jnp.zeros(input_shape[0], dtype=\"i4\")\n-        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape[0])\n-        token_type_ids = jnp.ones_like(input_ids)\n-        attention_mask = jnp.ones_like(input_ids)\n-\n-        pixel_values = jax.random.normal(rng, input_shape[1])\n-\n-        params_rng, dropout_rng = jax.random.split(rng)\n-        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n-\n-        return self.module.init(rngs, input_ids, pixel_values, attention_mask, position_ids, token_type_ids)[\"params\"]\n-\n-    def __call__(\n-        self,\n-        input_ids,\n-        pixel_values,\n-        attention_mask=None,\n-        position_ids=None,\n-        token_type_ids=None,\n-        params: dict = None,\n-        dropout_rng: jax.random.PRNGKey = None,\n-        train: bool = False,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ):\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        if position_ids is None:\n-            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n-\n-        if token_type_ids is None:\n-            token_type_ids = jnp.zeros_like(input_ids)\n-\n-        if attention_mask is None:\n-            attention_mask = jnp.ones_like(input_ids)\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        return self.module.apply(\n-            {\"params\": params or self.params},\n-            jnp.array(input_ids, dtype=\"i4\"),\n-            jnp.array(pixel_values, dtype=jnp.float32),\n-            jnp.array(attention_mask, dtype=\"i4\"),\n-            jnp.array(position_ids, dtype=\"i4\"),\n-            jnp.array(token_type_ids, dtype=\"i4\"),\n-            not train,\n-            output_attentions,\n-            output_hidden_states,\n-            return_dict,\n-            rngs=rngs,\n-        )\n-\n-    def get_text_features(\n-        self,\n-        input_ids,\n-        attention_mask=None,\n-        position_ids=None,\n-        token_type_ids=None,\n-        params: dict = None,\n-        dropout_rng: jax.random.PRNGKey = None,\n-        train=False,\n-    ):\n-        r\"\"\"\n-        Args:\n-            input_ids (:obj:`numpy.ndarray` of shape :obj:`(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using :class:`~transformers.PreTrainedTokenizer`. See\n-                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n-                for details.\n-\n-                `What are input IDs? <../glossary.html#input-ids>`__\n-\n-        Returns:\n-            text_features (:obj:`jnp.ndarray` of shape :obj:`(batch_size, output_dim`): The text embeddings\n-            obtained by applying the projection layer to the pooled output of text model.\n-        \"\"\"\n-        if position_ids is None:\n-            position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n-\n-        if token_type_ids is None:\n-            token_type_ids = jnp.zeros_like(input_ids)\n-\n-        if attention_mask is None:\n-            attention_mask = jnp.ones_like(input_ids)\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        def _get_features(module, input_ids, attention_mask, position_ids, token_type_ids, deterministic):\n-            text_outputs = module.text_model(\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                token_type_ids=token_type_ids,\n-                deterministic=deterministic,\n-            )\n-            pooled_output = text_outputs[1]\n-            text_features = module.text_projection(pooled_output)\n-            return text_features\n-\n-        return self.module.apply(\n-            {\"params\": params or self.params},\n-            jnp.array(input_ids, dtype=\"i4\"),\n-            jnp.array(attention_mask, dtype=\"i4\"),\n-            jnp.array(position_ids, dtype=\"i4\"),\n-            jnp.array(token_type_ids, dtype=\"i4\"),\n-            not train,\n-            method=_get_features,\n-            rngs=rngs,\n-        )\n-\n-    def get_image_features(\n-        self, pixel_values, params: dict = None, dropout_rng: jax.random.PRNGKey = None, train=False\n-    ):\n-        r\"\"\"\n-        Args:\n-            pixel_values (:obj:`numpy.ndarray` of shape :obj:`(batch_size, num_channels, height, width)`):\n-                Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained\n-                using :class:`~transformers.ImageFeatureExtractionMixin`. See\n-                :meth:`transformers.ImageFeatureExtractionMixin.__call__` for details.\n-\n-        Returns:\n-            image_features (:obj:`jnp.ndarray` of shape :obj:`(batch_size, output_dim`): The image embeddings\n-            obtained by applying the projection layer to the pooled output of vision model.\n-        \"\"\"\n-\n-        # Handle any PRNG if needed\n-        rngs = {}\n-        if dropout_rng is not None:\n-            rngs[\"dropout\"] = dropout_rng\n-\n-        def _get_features(module, pixel_values, deterministic):\n-            vision_outputs = module.vision_model(pixel_values=pixel_values, deterministic=deterministic)\n-            pooled_output = vision_outputs[1]  # pooled_output\n-            image_features = module.visual_projection(pooled_output)\n-            return image_features\n-\n-        return self.module.apply(\n-            {\"params\": params or self.params},\n-            jnp.array(pixel_values, dtype=jnp.float32),\n-            not train,\n-            method=_get_features,\n-            rngs=rngs,\n-        )\n-\n-    @classmethod\n-    def from_text_vision_pretrained(\n-        cls,\n-        text_model_name_or_path: str = None,\n-        vision_model_name_or_path: str = None,\n-        *model_args,\n-        **kwargs,\n-    ) -> FlaxPreTrainedModel:\n-        \"\"\"\n-        Params:\n-            text_model_name_or_path (:obj: `str`, `optional`):\n-                Information necessary to initiate the text model. Can be either:\n-\n-                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n-                    - A path to a `directory` containing model weights saved using\n-                      :func:`~transformers.FlaxPreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n-                    - A path or url to a `PyTorch checkpoint folder` (e.g, ``./pt_model``). In\n-                      this case, ``from_pt`` should be set to :obj:`True` and a configuration object should be provided\n-                      as ``config`` argument. This loading path is slower than converting the PyTorch checkpoint in\n-                      a Flax model using the provided conversion scripts and loading the Flax model afterwards.\n-\n-            vision_model_name_or_path (:obj: `str`, `optional`, defaults to `None`):\n-                Information necessary to initiate the vision model. Can be either:\n-\n-                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n-                    - A path to a `directory` containing model weights saved using\n-                      :func:`~transformers.FlaxPreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n-                    - A path or url to a `PyTorch checkpoint folder` (e.g, ``./pt_model``). In\n-                      this case, ``from_pt`` should be set to :obj:`True` and a configuration object should be provided\n-                      as ``config`` argument. This loading path is slower than converting the PyTorch checkpoint in\n-                      a Flax model using the provided conversion scripts and loading the Flax model afterwards.\n-\n-            model_args (remaining positional arguments, `optional`):\n-                All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n-\n-            kwargs (remaining dictionary of keyword arguments, `optional`):\n-                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n-                :obj:`output_attentions=True`).\n-\n-                - To update the text configuration, use the prefix `text_` for each configuration parameter.\n-                - To update the vision configuration, use the prefix `vision_` for each configuration parameter.\n-                - To update the parent model configuration, do not use a prefix for each configuration parameter.\n-\n-                Behaves differently depending on whether a :obj:`config` is provided or automatically loaded.\n-\n-        Example::\n-\n-            >>> from transformers import FlaxHybridCLIP\n-            >>> # initialize a model from pretrained BERT and CLIP models. Note that the projection layers will be randomly initialized.\n-            >>> # If using CLIP's vision model the vision projection layer will be initialized using pre-trained weights\n-            >>> model = FlaxHybridCLIP.from_text_vision_pretrained('google-bert/bert-base-uncased', 'openai/clip-vit-base-patch32')\n-            >>> # saving model after fine-tuning\n-            >>> model.save_pretrained(\"./bert-clip\")\n-            >>> # load fine-tuned model\n-            >>> model = FlaxHybridCLIP.from_pretrained(\"./bert-clip\")\n-        \"\"\"\n-\n-        kwargs_text = {\n-            argument[len(\"text_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"text_\")\n-        }\n-\n-        kwargs_vision = {\n-            argument[len(\"vision_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"vision_\")\n-        }\n-\n-        # remove text, vision kwargs from kwargs\n-        for key in kwargs_text.keys():\n-            del kwargs[\"text_\" + key]\n-        for key in kwargs_vision.keys():\n-            del kwargs[\"vision_\" + key]\n-\n-        # Load and initialize the text and vision model\n-        text_model = kwargs_text.pop(\"model\", None)\n-        if text_model is None:\n-            assert (\n-                text_model_name_or_path is not None\n-            ), \"If `model` is not defined as an argument, a `text_model_name_or_path` has to be defined\"\n-            from transformers import FlaxAutoModel\n-\n-            if \"config\" not in kwargs_text:\n-                from transformers import AutoConfig\n-\n-                text_config = AutoConfig.from_pretrained(text_model_name_or_path)\n-                kwargs_text[\"config\"] = text_config\n-\n-            text_model = FlaxAutoModel.from_pretrained(text_model_name_or_path, *model_args, **kwargs_text)\n-\n-        vision_model = kwargs_vision.pop(\"model\", None)\n-        if vision_model is None:\n-            assert (\n-                vision_model_name_or_path is not None\n-            ), \"If `model` is not defined as an argument, a `vision_model_name_or_path` has to be defined\"\n-            from transformers import FlaxAutoModel\n-\n-            if \"config\" not in kwargs_vision:\n-                from transformers import AutoConfig\n-\n-                vision_config = AutoConfig.from_pretrained(vision_model_name_or_path)\n-                kwargs_vision[\"config\"] = vision_config\n-\n-            vision_model = FlaxAutoModel.from_pretrained(vision_model_name_or_path, *model_args, **kwargs_vision)\n-\n-        # instantiate config with corresponding kwargs\n-        dtype = kwargs.pop(\"dtype\", jnp.float32)\n-        config = HybridCLIPConfig.from_text_vision_configs(text_model.config, vision_model.config, **kwargs)\n-\n-        # init model\n-        model = cls(config, *model_args, dtype=dtype, **kwargs)\n-\n-        if vision_config.model_type == \"clip\":\n-            model.params[\"vision_model\"][\"vision_model\"] = vision_model.params[\"vision_model\"]\n-            model.params[\"visual_projection\"][\"kernel\"] = vision_model.params[\"visual_projection\"][\"kernel\"]\n-        else:\n-            model.params[\"vision_model\"] = vision_model.params\n-\n-        model.params[\"text_model\"] = text_model.params\n-\n-        return model"
        },
        {
            "sha": "7b465dde645e6d9baaee1e750a28b45e03cc50d2",
            "filename": "examples/research_projects/jax-projects/hybrid_clip/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,8 +0,0 @@\n-jax>=0.2.8\n-jaxlib>=0.1.59\n-flax>=0.3.5\n-optax>=0.0.8\n--f https://download.pytorch.org/whl/torch_stable.html\n-torch==2.2.0 \n--f https://download.pytorch.org/whl/torch_stable.html\n-torchvision==0.10.0+cpu\n\\ No newline at end of file"
        },
        {
            "sha": "2020f0a35c40a4a40dc366ca1b89d2dfecd1b254",
            "filename": "examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 576,
            "changes": 576,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Frun_hybrid_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Frun_hybrid_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fhybrid_clip%2Frun_hybrid_clip.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,576 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Training a CLIP like dual encoder models using text and vision encoders in the library.\n-\n-The script can be used to train CLIP like models for languages other than english by using\n-a text encoder pre-trained in the desired language. Currently this script support the following vision\n-and text models:\n-Vision models: ViT(https://huggingface.co/models?filter=vit), CLIP (https://huggingface.co/models?filter=clip)\n-Text models: BERT, ROBERTa (https://huggingface.co/models?filter=fill-mask)\n-\"\"\"\n-\n-import json\n-import logging\n-import os\n-import sys\n-import time\n-from dataclasses import dataclass, field\n-from pathlib import Path\n-from typing import Callable, Optional\n-\n-import jax\n-import jax.numpy as jnp\n-import optax\n-import torch\n-from flax import jax_utils\n-from flax.jax_utils import unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, shard, shard_prng_key\n-from modeling_hybrid_clip import FlaxHybridCLIP\n-from torchvision.datasets import VisionDataset\n-from torchvision.io import ImageReadMode, read_image\n-from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n-from torchvision.transforms.functional import InterpolationMode\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import AutoTokenizer, HfArgumentParser, TrainingArguments, is_tensorboard_available, set_seed\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-# Cache the result\n-has_tensorboard = is_tensorboard_available()\n-if has_tensorboard:\n-    try:\n-        from flax.metrics.tensorboard import SummaryWriter\n-    except ImportError as ie:\n-        has_tensorboard = False\n-        print(f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\")\n-\n-else:\n-    print(\n-        \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-        \"Please run pip install tensorboard to enable.\"\n-    )\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    text_model_name_or_path: str = field(\n-        metadata={\n-            \"help\": (\n-                \"The text model checkpoint for weights initialization. \"\n-                \"Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    vision_model_name_or_path: str = field(\n-        metadata={\n-            \"help\": (\n-                \"The vision model checkpoint for weights initialization. \"\n-                \"Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    from_pt: bool = field(\n-        default=True,\n-        metadata={\"help\": \"whether to load the text and vision model using PyTorch checkpoints.\"},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n-    train_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n-    )\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=72,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-\n-    def __post_init__(self):\n-        if self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension == \"json\", \"`train_file` should be a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension == \"json\", \"`validation_file` should be a json file.\"\n-\n-\n-# We use torchvision for faster image pre-processing.\n-# We need to ensure faster processing speed as it can become a bottleneck on TPU\n-class Transform(torch.nn.Module):\n-    def __init__(self, image_size):\n-        super().__init__()\n-        self.transforms = torch.nn.Sequential(\n-            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n-            CenterCrop(image_size),\n-            ConvertImageDtype(torch.float),\n-            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n-        )\n-\n-    def forward(self, x: torch.Tensor) -> torch.Tensor:\n-        with torch.no_grad():\n-            x = self.transforms(x)\n-        return x\n-\n-\n-class ImageTextDataset(VisionDataset):\n-    \"\"\"\n-    Dtaset for loading image-text data for tasks like CLIP training, Image Captioning.\n-\n-    Args:\n-        root: (string): The root path where the dataset is stored\n-        file_path: (string): Path to the file containing the image_paths and associated captions.\n-            The expected format is jsonlines where each line is a json object containing to keys.\n-            `image_path`: The path to the image.\n-            `captions`: An `array` of captions.\n-        transform (callable, optional): A function/transform that  takes in an PIL image\n-            and returns a transformed version. E.g, ``transforms.ToTensor``\n-        target_transform (callable, optional): A function/transform that takes in the\n-            target and transforms it.\n-        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n-            and returns a transformed version.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        root: str,\n-        file_path: str,\n-        captions_per_image=2,\n-        transform: Optional[Callable] = None,\n-        target_transform: Optional[Callable] = None,\n-        transforms: Optional[Callable] = None,\n-    ):\n-        super().__init__(root, transforms, transform, target_transform)\n-\n-        with open(file_path, \"r\") as f:\n-            examples = [json.loads(line) for line in f.readlines()]\n-\n-        self.captions = []\n-        self.image_paths = []\n-\n-        for example in examples:\n-            captions_subset = example[\"captions\"][:captions_per_image]\n-            self.captions.extend(captions_subset)\n-            self.image_paths.extend([example[\"image_path\"]] * len(captions_subset))\n-\n-    def _load_image(self, idx: int):\n-        path = self.image_paths[idx]\n-        return read_image(path, mode=ImageReadMode.RGB)\n-\n-    def _load_target(self, idx):\n-        return self.captions[idx]\n-\n-    def __getitem__(self, index: int):\n-        image = self._load_image(index)\n-        target = self._load_target(index)\n-\n-        if self.transforms is not None:\n-            image, target = self.transforms(image, target)\n-\n-        return image, target\n-\n-    def __len__(self) -> int:\n-        return len(self.captions)\n-\n-\n-class TrainState(train_state.TrainState):\n-    dropout_rng: jnp.ndarray\n-\n-    def replicate(self):\n-        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n-\n-\n-def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n-        )\n-    elif model_args.text_model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.text_model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    model = FlaxHybridCLIP.from_text_vision_pretrained(\n-        model_args.text_model_name_or_path,\n-        model_args.vision_model_name_or_path,\n-        seed=training_args.seed,\n-        dtype=getattr(jnp, model_args.dtype),\n-        text_from_pt=model_args.from_pt,\n-        vision_from_pt=model_args.from_pt,\n-    )\n-    config = model.config\n-    # set seed for torch dataloaders\n-    set_seed(training_args.seed)\n-\n-    # Initialize torchvision transforms and jit them for faster processing\n-    preprocess = Transform(config.vision_config.image_size)\n-    preprocess = torch.jit.script(preprocess)\n-\n-    # Initialize the image-text dataset\n-    train_dataset = ImageTextDataset(\n-        data_args.data_dir,\n-        data_args.train_file,\n-        captions_per_image=2,\n-        transform=preprocess,\n-    )\n-\n-    eval_dataset = ImageTextDataset(\n-        data_args.data_dir,\n-        data_args.validation_file,\n-        captions_per_image=1,\n-        transform=preprocess,\n-    )\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    eval_batch_size = int(training_args.per_device_eval_batch_size) * jax.device_count()\n-    steps_per_epoch = len(train_dataset) // train_batch_size\n-    total_train_steps = steps_per_epoch * num_epochs\n-\n-    # Use collate function to tokenizer the text and convert the processed images to numpy\n-    def collate_fn(examples):\n-        pixel_values = torch.stack([example[0] for example in examples]).permute(0, 2, 3, 1).numpy()\n-        captions = [example[1] for example in examples]\n-        inputs = tokenizer(\n-            captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True, return_tensors=\"np\"\n-        )\n-\n-        batch = {\n-            \"pixel_values\": pixel_values,\n-            \"input_ids\": inputs[\"input_ids\"],\n-            \"attention_mask\": inputs[\"attention_mask\"],\n-        }\n-\n-        return batch\n-\n-    # Create data loaders\n-    train_loader = torch.utils.data.DataLoader(\n-        train_dataset,\n-        batch_size=train_batch_size,\n-        shuffle=True,\n-        num_workers=data_args.preprocessing_num_workers,\n-        persistent_workers=True,\n-        drop_last=True,\n-        collate_fn=collate_fn,\n-    )\n-\n-    eval_loader = torch.utils.data.DataLoader(\n-        eval_dataset,\n-        batch_size=eval_batch_size,\n-        shuffle=False,\n-        num_workers=data_args.preprocessing_num_workers,\n-        persistent_workers=True,\n-        drop_last=True,\n-        collate_fn=collate_fn,\n-    )\n-\n-    # Enable tensorboard only on the master node\n-    if has_tensorboard and jax.process_index() == 0:\n-        summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir).joinpath(\"logs\").as_posix())\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-    )\n-\n-    # Setup train state\n-    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n-\n-    def cross_entropy(logits, axis):\n-        logprobs = jax.nn.log_softmax(logits, axis=axis)\n-        nll = jnp.diag(logprobs)\n-        ce = -jnp.mean(nll)\n-        return ce\n-\n-    def clip_loss(similarity):\n-        loss = (cross_entropy(similarity, axis=0) + cross_entropy(similarity, axis=1)) / 2\n-        return loss\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch):\n-        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n-\n-        def compute_loss(params):\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss = clip_loss(logits)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(compute_loss)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-\n-        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-\n-        return new_state, metrics\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        logits = model(**batch, params=params, train=False)[0]\n-        loss = clip_loss(logits)\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-        return metrics\n-\n-    # Create parallel version of the train and eval step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-    p_eval_step = jax.pmap(eval_step, \"batch\")\n-\n-    # Replicate the train state on each device\n-    state = state.replicate()\n-\n-    logger.info(\"***** Running training *****\")\n-    logger.info(f\"  Num examples = {len(train_dataset)}\")\n-    logger.info(f\"  Num Epochs = {num_epochs}\")\n-    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-\n-    train_time = 0\n-    # Create sampling rng\n-    rng, input_rng = jax.random.split(rng)\n-\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-        train_metrics = []\n-\n-        steps_per_epoch = len(train_dataset) // train_batch_size\n-        train_step_progress_bar = tqdm(total=steps_per_epoch, desc=\"Training...\", position=1, leave=False)\n-        # train\n-        for batch in train_loader:\n-            batch = shard(batch)\n-            state, train_metric = p_train_step(state, batch)\n-            train_metrics.append(train_metric)\n-\n-            train_step_progress_bar.update(1)\n-\n-        train_time += time.time() - train_start\n-\n-        train_metric = unreplicate(train_metric)\n-\n-        train_step_progress_bar.close()\n-        epochs.write(\n-            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n-            f\" {train_metric['learning_rate']})\"\n-        )\n-\n-        # ======================== Evaluating ==============================\n-        eval_metrics = []\n-        eval_steps = len(eval_dataset) // eval_batch_size\n-        eval_step_progress_bar = tqdm(total=eval_steps, desc=\"Evaluating...\", position=2, leave=False)\n-        for batch in eval_loader:\n-            # Model forward\n-            batch = shard(batch)\n-            metrics = p_eval_step(state.params, batch)\n-            eval_metrics.append(metrics)\n-\n-            eval_step_progress_bar.update(1)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-\n-        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-        # Print metrics and update progress bar\n-        eval_step_progress_bar.close()\n-        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {eval_metrics['loss']})\"\n-        epochs.write(desc)\n-        epochs.desc = desc\n-\n-        # Save metrics\n-        if has_tensorboard and jax.process_index() == 0:\n-            cur_step = epoch * (len(train_dataset) // train_batch_size)\n-            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n-\n-        # save checkpoint after each epoch and push checkpoint to the hub\n-        if jax.process_index() == 0:\n-            params = jax.device_get(unreplicate(state.params))\n-            model.save_pretrained(\n-                training_args.output_dir,\n-                params=params,\n-                push_to_hub=training_args.push_to_hub,\n-                commit_message=f\"Saving weights and logs of epoch {epoch+1}\",\n-            )\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "393c9e8937508562b15daeee15029c20617f72a0",
            "filename": "examples/research_projects/jax-projects/model_parallel/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,67 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Model parallel language model training example\n-\n-The following example showcases how to train/fine-tune GPTNeo model with model parallelism using\n-the JAX/Flax backend and the [`pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html) transformation.\n-\n-> Note: The example is experimental and might have bugs. Also currently it only supports single V3-8.\n-\n-The `partition.py` file defines the `PyTree` of `ParitionSpec` for the GPTNeo model which describes how the model will be sharded.\n-The actual sharding is auto-matically handled by `pjit`. The weights are sharded across all local devices.\n-To adapt the script for other models, we need to also change the `ParitionSpec` accordingly.\n-\n-TODO: Add more explantion.\n-\n-Before training, let's prepare our model first. To be able to shard the model, the sharded dimension needs to be a multiple of devices it'll be sharded on. But GPTNeo's vocab size is 50257, so we need to resize the embeddings accordingly. \n-\n-```python\n-from transformers import FlaxGPTNeoForCausalLM, GPTNeoConfig \n-model = FlaxGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n-\n-emb = jnp.zeros((50264, model.config.hidden_size))\n-# update the first 50257 weights using pre-trained weights\n-emb = emb.at[:50257, :].set(model.params[\"transformer\"][\"wte\"][\"embedding\"])\n-params = model.params\n-params[\"transformer\"][\"wte\"][\"embedding\"] = emb\n-\n-# initialize a random model with the right vocab_size\n-config = GPTNeoConfig.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", vocab_size=50264)\n-model = FlaxGPTNeoForCausalLM(config)\n-\n-# assign the pre-trained weights and save the model.\n-model.params = params\n-model.save_pretrained(\"gpt-neo-1.3B\")\n-```\n-\n-\n-### Train Model\n-\n-```bash\n-python run_clm_mp.py \\\n-    --model_name_or_path gpt-neo-1.3B  \\\n-    --tokenizer_name openai-community/gpt2 \\\n-    --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \\\n-    --do_train  --do_eval \\\n-    --block_size 1024 \\\n-    --num_train_epochs 5 \\\n-    --learning_rate 4e-6 \\\n-    --per_device_train_batch_size 3 --per_device_eval_batch_size 3 \\\n-    --overwrite_output_dir --output_dir ~/tmp/flax-clm \\\n-    --cache_dir ~/datasets_cache/wikitext --dtype bfloat16 \\\n-    --logging_steps 96 --eval_steps 96\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "86e54ad670277916345337cdf96f624d8e6cbb65",
            "filename": "examples/research_projects/jax-projects/model_parallel/partitions.py",
            "status": "removed",
            "additions": 0,
            "deletions": 85,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2Fpartitions.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2Fpartitions.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2Fpartitions.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,85 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-# Copyright 2021 The Google Research Authors and The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Utilities for constructing PyTrees of PartitionSpecs.\"\"\"\n-\n-# utils adapted from https://github.com/google-research/google-research/blob/master/flax_models/t5x/partitions.py\n-\n-import re\n-\n-from flax.core.frozen_dict import freeze\n-from flax.traverse_util import flatten_dict, unflatten_dict\n-from jax.experimental import PartitionSpec as P\n-\n-\n-# Sentinels\n-_unmatched = object()\n-\n-# For specifying empty leaf dict `{}`\n-empty_dict = object()\n-\n-\n-def _match(qs, ks):\n-    \"\"\"Return True if regexes in qs match any window of strings in tuple ks.\"\"\"\n-    # compile regexes and force complete match\n-    qts = tuple((re.compile(x + \"$\") for x in qs))\n-    for i in range(len(ks) - len(qs) + 1):\n-        matches = [x.match(y) for x, y in zip(qts, ks[i:])]\n-        if matches and all(matches):\n-            return True\n-    return False\n-\n-\n-def _replacement_rules(rules):\n-    def replace(key, val):\n-        for rule, replacement in rules:\n-            if _match(rule, key):\n-                return replacement\n-        return val\n-\n-    return replace\n-\n-\n-# PartitionSpec for GPTNeo\n-# replicate the hidden dim and shard feed-forward and head dim\n-def _get_partition_rules():\n-    return [\n-        # embeddings\n-        ((\"transformer\", \"wpe\", \"embedding\"), P(\"mp\", None)),\n-        ((\"transformer\", \"wte\", \"embedding\"), P(\"mp\", None)),\n-        # atention\n-        ((\"attention\", \"(q_proj|k_proj|v_proj)\", \"kernel\"), P(None, \"mp\")),\n-        ((\"attention\", \"out_proj\", \"kernel\"), P(\"mp\", None)),\n-        ((\"attention\", \"out_proj\", \"bias\"), None),\n-        # mlp\n-        ((\"mlp\", \"c_fc\", \"kernel\"), P(None, \"mp\")),\n-        ((\"mlp\", \"c_fc\", \"bias\"), P(\"mp\")),\n-        ((\"mlp\", \"c_proj\", \"kernel\"), P(\"mp\", None)),\n-        ((\"mlp\", \"c_proj\", \"bias\"), None),\n-        # layer norms\n-        ((r\"ln_\\d+\", \"bias\"), None),\n-        ((r\"\\d+\", r\"ln_\\d+\", \"scale\"), None),\n-        ((\"ln_f\", \"bias\"), None),\n-        ((\"ln_f\", \"scale\"), None),\n-    ]\n-\n-\n-def set_partitions(in_dict):\n-    rules = _get_partition_rules()\n-    replace = _replacement_rules(rules)\n-    initd = {k: _unmatched for k in flatten_dict(in_dict)}\n-    result = {k: replace(k, v) for k, v in initd.items()}\n-    assert _unmatched not in result.values(), \"Incomplete partition spec.\"\n-    return freeze(unflatten_dict(result))"
        },
        {
            "sha": "067f7cb2b1854c767556fa441d49bc07b9d14bbe",
            "filename": "examples/research_projects/jax-projects/model_parallel/run_clm_mp.py",
            "status": "removed",
            "additions": 0,
            "deletions": 662,
            "changes": 662,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2Frun_clm_mp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2Frun_clm_mp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fmodel_parallel%2Frun_clm_mp.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,662 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Pre-training/Fine-tuning the GPTNeo model for causal language modeling on a text file or a dataset using model parallelism.\n-\"\"\"\n-\n-import logging\n-import math\n-import os\n-import sys\n-import time\n-from dataclasses import dataclass, field\n-from itertools import chain\n-from pathlib import Path\n-from typing import Callable, Optional\n-\n-import datasets\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import Dataset, load_dataset\n-from flax.core.frozen_dict import freeze, unfreeze\n-from flax.training.common_utils import onehot, stack_forest\n-from jax.experimental.maps import mesh\n-from jax.experimental.pjit import pjit\n-from partitions import set_partitions\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForCausalLM,\n-    HfArgumentParser,\n-    TrainingArguments,\n-    is_tensorboard_available,\n-)\n-from transformers.testing_utils import CaptureLogger\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    block_size: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Optional input sequence length after tokenization. \"\n-                \"The training dataset will be truncated in block of this size for training. \"\n-                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n-\n-\n-def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False):\n-    \"\"\"\n-    Returns batches of size `batch_size` from truncated `dataset`, sharded over all local devices.\n-    Shuffle batches if `shuffle` is `True`.\n-    \"\"\"\n-    steps_per_epoch = len(dataset) // batch_size\n-\n-    if shuffle:\n-        batch_idx = jax.random.permutation(rng, len(dataset))\n-    else:\n-        batch_idx = jnp.arange(len(dataset))\n-\n-    batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n-    batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n-\n-    for idx in batch_idx:\n-        batch = dataset[idx]\n-        batch = {k: jnp.array(v) for k, v in batch.items()}\n-        yield batch\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = stack_forest(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        dataset = load_dataset(\n-            data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir, keep_in_memory=False\n-        )\n-\n-        if \"validation\" not in dataset.keys():\n-            dataset[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-            )\n-            dataset[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-            )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if extension == \"txt\":\n-            extension = \"text\"\n-        dataset = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained config and tokenizer\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    if training_args.do_train:\n-        column_names = dataset[\"train\"].column_names\n-    else:\n-        column_names = dataset[\"validation\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n-    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n-\n-    def tokenize_function(examples):\n-        with CaptureLogger(tok_logger) as cl:\n-            output = tokenizer(examples[text_column_name])\n-        # clm input could be much much longer than block_size\n-        if \"Token indices sequence length is longer than the\" in cl.out:\n-            tok_logger.warning(\n-                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n-                \" before being passed to the model.\"\n-            )\n-        return output\n-\n-    tokenized_datasets = dataset.map(\n-        tokenize_function,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        remove_columns=column_names,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    if data_args.block_size is None:\n-        block_size = tokenizer.model_max_length\n-        if block_size > config.max_position_embeddings:\n-            logger.warning(\n-                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n-                f\"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n-            )\n-            block_size = min(1024, config.max_position_embeddings)\n-    else:\n-        if data_args.block_size > tokenizer.model_max_length:\n-            logger.warning(\n-                f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model \"\n-                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n-            )\n-        block_size = min(data_args.block_size, tokenizer.model_max_length)\n-\n-    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n-    def group_texts(examples):\n-        # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n-        total_length = len(concatenated_examples[list(examples.keys())[0]])\n-        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-        # customize this part to your needs.\n-        if total_length >= block_size:\n-            total_length = (total_length // block_size) * block_size\n-        # Split by chunks of max_len.\n-        result = {\n-            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n-            for k, t in concatenated_examples.items()\n-        }\n-        result[\"labels\"] = result[\"input_ids\"].copy()\n-        return result\n-\n-    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n-    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n-    # to preprocess.\n-    #\n-    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-    # https://huggingface.co/docs/datasets/process#map\n-\n-    lm_datasets = tokenized_datasets.map(\n-        group_texts,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    if training_args.do_train:\n-        if \"train\" not in tokenized_datasets:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = lm_datasets[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in tokenized_datasets:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_dataset = lm_datasets[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    eval_batch_size = int(training_args.per_device_eval_batch_size) * jax.device_count()\n-    steps_per_epoch = len(train_dataset) // train_batch_size\n-    total_train_steps = steps_per_epoch * num_epochs\n-\n-    # TODO: weights should be initialized in pjitted fun, this won't work for REALLY large models\n-    # TODO: when loading from pre-trained model we need to make sure the vocab is divisible by num_partitions\n-    # GPT2's vocab is odd, we need to resize it for fine-tuning\n-    model = FlaxAutoModelForCausalLM.from_pretrained(\n-        model_args.model_name_or_path, seed=training_args.seed, dtype=getattr(jnp, model_args.dtype)\n-    )\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    optimizer = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-    )\n-\n-    def get_initial_state(params):\n-        state = optimizer.init(params)\n-        return tuple(state), params\n-\n-    # Get PartitionSpec for model params\n-    param_spec = set_partitions(unfreeze(model.params))\n-\n-    # Get the PyTree for opt_state, we don't actually initialize the opt_state yet.\n-    params_shapes = jax.tree_util.tree_map(lambda x: x.shape, model.params)\n-    state_shapes = jax.eval_shape(get_initial_state, params_shapes)\n-\n-    # get PartitionSpec for opt_state, this is very specific to adamw\n-    # TODO: optax returns different state for different optimizers, how can we handle this generically ?\n-    # or maybe we don't since in our examples we just use adamw or adafactor\n-    def get_opt_spec(x):\n-        if isinstance(x, dict):\n-            return param_spec\n-        return None\n-\n-    opt_state_spec, param_spec = jax.tree_util.tree_map(\n-        get_opt_spec, state_shapes, is_leaf=lambda x: isinstance(x, (dict, optax.EmptyState))\n-    )\n-\n-    # pjit the get_initial_state function to shard params and init\n-    # optimizer state in sharded way\n-    p_get_initial_state = pjit(\n-        get_initial_state,\n-        in_axis_resources=None,\n-        out_axis_resources=(opt_state_spec, param_spec),\n-    )\n-\n-    # hack: move the inital params to CPU to free up device memory\n-    # TODO: allow loading weights on CPU in pre-trained model\n-    model.params = jax.tree_util.tree_map(lambda x: np.asarray(x), model.params)\n-\n-    # mesh defination\n-    mesh_devices = np.array(jax.devices()).reshape(1, jax.local_device_count())\n-\n-    # actually initialize the opt_state\n-    with mesh(mesh_devices, (\"dp\", \"mp\")):\n-        opt_state, params = p_get_initial_state(freeze(model.params))\n-\n-    # cross-entropy with z loss\n-    def loss_fn(logits, labels, z_loss=0):\n-        shift_logits = logits[..., :-1, :]\n-        shift_labels = labels[..., 1:]\n-\n-        shift_labels = onehot(shift_labels, shift_logits.shape[-1])\n-\n-        shift_logits = shift_logits - jax.lax.stop_gradient(shift_logits.max(axis=-1, keepdims=True))\n-        log_z = jnp.log(jnp.sum(jnp.exp(shift_logits), axis=-1, keepdims=True))\n-        log_softmax = shift_logits - log_z\n-        loss = -jnp.sum(shift_labels * log_softmax, axis=-1)\n-\n-        loss += (1e-4 * jnp.square(log_z.squeeze(-1))) * z_loss\n-\n-        return loss.mean()\n-\n-    # Define gradient update step fn\n-    # TODO: try to use TrainState instead of passing params and opt_state individually\n-    def train_step(params, opt_state, dropout_rng, batch, step):\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-\n-        def compute_loss(params):\n-            labels = batch.pop(\"labels\")\n-            logits = model(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss = loss_fn(logits, labels, z_loss=1.0)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(compute_loss)\n-        loss, grads = grad_fn(params)\n-\n-        updates, new_opt_state = optimizer.update(grads, opt_state, params)\n-        new_params = optax.apply_updates(params, updates)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(step)}\n-        return new_params, tuple(new_opt_state), new_dropout_rng, metrics, step + 1\n-\n-    # Define eval fn\n-    def eval_step(input_ids, labels, params):\n-        logits = model(input_ids=input_ids, params=params, train=False)[0]\n-        loss = loss_fn(logits, labels)\n-        # metrics\n-        return {\"loss\": loss}\n-\n-    p_train_step = pjit(\n-        train_step,\n-        in_axis_resources=(param_spec, opt_state_spec, None, None, None),\n-        out_axis_resources=(param_spec, opt_state_spec, None, None, None),\n-        donate_argnums=(0, 1),\n-    )\n-\n-    p_eval_step = pjit(\n-        eval_step,\n-        in_axis_resources=(None, None, param_spec),\n-        out_axis_resources=None,\n-    )\n-\n-    logger.info(\"***** Running training *****\")\n-    logger.info(f\"  Num examples = {len(train_dataset)}\")\n-    logger.info(f\"  Num Epochs = {num_epochs}\")\n-    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-\n-    train_time = 0\n-    train_metrics = []\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    global_step = 0\n-    # we are not doing 2D parallelism (yet!), this just does model parallelism\n-    with mesh(mesh_devices, (\"dp\", \"mp\")):\n-        for _ in epochs:\n-            # ======================== Training ================================\n-            train_start = time.time()\n-\n-            # Create sampling rng\n-            rng, input_rng = jax.random.split(rng)\n-\n-            # Generate an epoch by shuffling sampling indices from the train dataset\n-            train_metrics = []\n-            train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n-            steps_per_epoch = len(train_dataset) // train_batch_size\n-\n-            # train\n-            for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=1, leave=False):\n-                batch = next(train_loader)\n-                params, opt_state, dropout_rng, train_metric, global_step = p_train_step(\n-                    params,\n-                    opt_state,\n-                    dropout_rng,\n-                    batch,\n-                    global_step,\n-                )\n-                train_metrics.append(train_metric)\n-\n-                cur_step = global_step\n-\n-                if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                    # Save metrics\n-                    train_time += time.time() - train_start\n-                    if has_tensorboard and jax.process_index() == 0:\n-                        write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                    epochs.write(\n-                        f\"Step... ({cur_step} | Loss: {train_metric['loss']}, Learning Rate:\"\n-                        f\" {train_metric['learning_rate']})\"\n-                    )\n-\n-                    train_metrics = []\n-\n-                if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n-                    # ======================== Evaluating ==============================\n-                    eval_metrics = []\n-                    eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size)\n-                    eval_steps = len(eval_dataset) // eval_batch_size\n-\n-                    for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=2, leave=False):\n-                        batch = next(eval_loader)\n-                        metrics = p_eval_step(batch[\"input_ids\"], batch[\"labels\"], params)\n-                        eval_metrics.append(metrics)\n-\n-                    # normalize eval metrics\n-                    eval_metrics = stack_forest(eval_metrics)\n-                    eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-                    try:\n-                        eval_metrics[\"perplexity\"] = math.exp(eval_metrics[\"loss\"])\n-                    except OverflowError:\n-                        eval_metrics[\"perplexity\"] = float(\"inf\")\n-\n-                    logger.info(\n-                        f\"Step... ({cur_step} | Eval loss: {eval_metrics['loss']} | Eval Perplexity:\"\n-                        f\" {eval_metrics['perplexity']}\"\n-                    )\n-\n-                if cur_step % training_args.save_steps == 0 and cur_step > 0:\n-                    # save checkpoint after each epoch and push checkpoint to the hub\n-                    if jax.process_index() == 0:\n-                        params = jax.device_get(params)\n-                        model.save_pretrained(\n-                            training_args.output_dir,\n-                            params=params,\n-                            push_to_hub=training_args.push_to_hub,\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                        )\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "5f8e14f47c590cd5f07e66514cbf66dee2e28805",
            "filename": "examples/research_projects/jax-projects/wav2vec2/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 120,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fwav2vec2%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fwav2vec2%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fwav2vec2%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,120 +0,0 @@\n-# Wav2Vec2 Contrastive Loss PreTraining examples\n-\n-The following example showcases how to pretrain a wav2vec2 model using the JAX/Flax backend.\n-Pretraining Wav2Vec2 is rather complex, so it is highly recommended to read the \n-[official paper](https://arxiv.org/abs/2006.11477).\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-`run_wav2vec2_pretrain_flax.py` is a lightweight example of how to download and preprocess a dataset from the ğŸ¤— Datasets library or use your own files (jsonlines or csv), then pretrain the wav2vec2 architectures above on it.\n-\n-For custom datasets in `jsonlines` format please see: [the Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets#json-files) and you also will find examples of these below.\n-\n-Let's start by creating a model repository to save the trained model and logs.\n-Here we call the model `\"wav2vec2-base-robust\"`, but you can change the model name as you like.\n-\n-You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that\n-you are logged in) or via the command line:\n-\n-```bash\n-huggingface-cli repo create wav2vec2-base-robust\n-```\n-\n-Next we clone the model repository to add the tokenizer and model files.\n-\n-```bash\n-git clone https://huggingface.co/<your-username>/wav2vec2-base-robust\n-```\n-\n-To ensure that all tensorboard traces will be uploaded correctly, we need to \n-track them. You can run the following command inside your model repo to do so.\n-\n-```bash\n-cd wav2vec2-base-robust\n-git lfs track \"*tfevents*\"\n-```\n-\n-Great, we have set up our model repository. During training, we will automatically\n-push the training logs and model weights to the repo.\n-\n-Next, let's add a symbolic link to the `run_wav2vec2_pretrain_flax`.\n-\n-```bash\n-export MODEL_DIR=\"./wav2vec2-base-robust\"\n-ln -s ~/transformers/examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py ./\n-```\n-\n-### Create the model configuration\n-\n-Let's first create the model configuration and store it in the model repository. \n-Note that many training parameters can be set in the model configuration including\n-the configuration about the masking distribution (`mask_time_length`, `mask_time_prob`), \n-dropout (`attention_dropout`, ...), the trade-off between the contrastive loss and \n-the diversity loss, etc...\n-Mostly likely you will need to change these parameters depending on your use case.\n-Again, we highly recommend to read the [official paper](https://arxiv.org/abs/2006.11477) \n-to better understand which parameters can be set for pretraining.\n-\n-For this example, we will be using a `\"base\"`-sized model of Wav2Vec2 with robust \n-layer norm and keep most of the default settings.\n-\n-```python\n-model_dir=\"./wav2vec2-base-robust\"\n-\n-from transformers import Wav2Vec2Config\n-config = Wav2Vec2Config.from_pretrained(\n-    \"facebook/wav2vec2-base\", \n-    mask_time_length=10,\n-    mask_time_prob=0.05,\n-    diversity_loss_weight=0.1,\n-    num_negatives=100,\n-    do_stable_layer_norm=True,\n-    feat_extract_norm=\"layer\",\n-)\n-config.save_pretrained(model_dir)\n-```\n-\n-### Create a feature extractor configuration\n-\n-Before we can start the training, we need to define \n-a feature extractor that takes care of normalization, etc...\n-\n-Here we can also re-use the feature extractor of [wav2vec2-base-960h](https://huggingface.co/facebook/wav2vec2-base) while making sure that padding is allowed.\n-\n-\n-```python\n-model_dir=\"./wav2vec2-base-robust\"\n-\n-from transformers import Wav2Vec2FeatureExtractor\n-config = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\", return_attention_mask=True)\n-config.save_pretrained(model_dir)\n-```\n-\n-### Train the model\n-Finally, we can run the example script to train the model:\n-\n-```bash\n-./run_wav2vec2_pretrain_flax.py \\\n-    --output_dir=${MODEL_DIR} \\\n-    --num_train_epochs=\"5\" \\\n-    --per_device_train_batch_size=\"32\" \\\n-    --per_device_eval_batch_size=\"32\" \\\n-    --learning_rate=\"5e-4\" \\\n-    --weight_decay=\"0.01\" \\\n-    --warmup_steps=\"2000\" \\\n-    --model_name_or_path=${MODEL_DIR} \\\n-    --dataset_name=\"librispeech_asr\" \\\n-    --dataset_config_name=\"clean\" \\\n-    --train_split_name=\"train.100\" \\\n-    --preprocessing_num_workers=\"4\" \\\n-    --max_duration_in_seconds=\"10.0\" \\\n-    --adam_beta1=\"0.9\" \\\n-    --adam_beta2=\"0.98\" \\\n-    --pad_to_multiple_of=\"16384\" \\\n-    --push_to_hub\n-```\n-\n-Note that this script is not fully tested yet, so we cannot ensure that \n-the above script leads to satisfying results."
        },
        {
            "sha": "017e910db0a3c64dcc36a61b20f24ea8ca2c7148",
            "filename": "examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 614,
            "changes": 614,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fwav2vec2%2Frun_wav2vec2_pretrain_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fjax-projects%2Fwav2vec2%2Frun_wav2vec2_pretrain_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fjax-projects%2Fwav2vec2%2Frun_wav2vec2_pretrain_flax.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,614 +0,0 @@\n-#!/usr/bin/env python3\n-import logging\n-import sys\n-import time\n-from dataclasses import field\n-from pathlib import Path\n-from typing import Dict, List, Optional, Union\n-\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import librosa\n-import numpy as np\n-import optax\n-from datasets import DatasetDict, load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from tqdm import tqdm\n-\n-from transformers import (\n-    FlaxWav2Vec2ForPreTraining,\n-    HfArgumentParser,\n-    TrainingArguments,\n-    Wav2Vec2Config,\n-    Wav2Vec2FeatureExtractor,\n-    is_tensorboard_available,\n-)\n-from transformers.models.wav2vec2.modeling_flax_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@flax.struct.dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    freeze_feature_extractor: Optional[bool] = field(\n-        default=True, metadata={\"help\": \"Whether to freeze the feature extractor layers of the model.\"}\n-    )\n-    verbose_logging: Optional[bool] = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to log verbose messages or not.\"},\n-    )\n-    max_gumbel_temperature: Optional[float] = field(\n-        default=2.0, metadata={\"help\": \"Maximum temperature for gumbel softmax.\"}\n-    )\n-    min_gumbel_temperature: Optional[float] = field(\n-        default=0.1, metadata={\"help\": \"Minimum temperature for gumbel softmax.\"}\n-    )\n-    gumbel_temperature_decay: Optional[float] = field(\n-        default=0.999995, metadata={\"help\": \"Decay of gumbel temperature during training.\"}\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-\n-\n-@flax.struct.dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-\n-    Using `HfArgumentParser` we can turn this class\n-    into argparse arguments to be able to specify them on\n-    the command line.\n-    \"\"\"\n-\n-    dataset_name: str = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_split_name: Optional[str] = field(\n-        default=\"train\",\n-        metadata={\n-            \"help\": \"The name of the training data set split to use (via the datasets library). Defaults to 'train'\"\n-        },\n-    )\n-    validation_split_name: Optional[str] = field(\n-        default=\"validation\",\n-        metadata={\n-            \"help\": (\n-                \"The name of the validation data set split to use (via the datasets library). Defaults to 'validation'\"\n-            )\n-        },\n-    )\n-    speech_file_column: Optional[str] = field(\n-        default=\"file\",\n-        metadata={\"help\": \"Column in the dataset that contains speech file path. Defaults to 'file'\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_duration_in_seconds: Optional[float] = field(\n-        default=20.0, metadata={\"help\": \"Filter audio files that are longer than `max_duration_in_seconds` seconds\"}\n-    )\n-    pad_to_multiple_of: Optional[int] = field(\n-        default=1024,\n-        metadata={\n-            \"help\": (\n-                \"If set will pad the sequence to a multiple of the provided value. This is important to avoid\"\n-                \" triggering recompilations on TPU\"\n-            )\n-        },\n-    )\n-\n-\n-@flax.struct.dataclass\n-class FlaxDataCollatorForWav2Vec2Pretraining:\n-    \"\"\"\n-    Data collator that will dynamically pad the inputs received and prepare masked indices\n-    for self-supervised pretraining.\n-\n-    Args:\n-        model (:class:`~transformers.FlaxWav2Vec2ForPreTraining`):\n-            The Wav2Vec2 model used for pretraining. The data collator needs to have access\n-            to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n-        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n-            The processor used for processing the data.\n-        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n-            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n-            among:\n-            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-              sequence if provided).\n-            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n-              maximum acceptable input length for the model if that argument is not provided.\n-            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n-              different lengths).\n-        max_length (:obj:`int`, `optional`):\n-            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n-        pad_to_multiple_of (:obj:`int`, `optional`):\n-            If set will pad the sequence to a multiple of the provided value.\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.5 (Volta).\n-    \"\"\"\n-\n-    model: FlaxWav2Vec2ForPreTraining\n-    feature_extractor: Wav2Vec2FeatureExtractor\n-    padding: Union[bool, str] = \"longest\"\n-    pad_to_multiple_of: Optional[int] = None\n-    max_length: Optional[int] = None\n-\n-    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n-        # reformat list to dict and set to pytorch format\n-        batch = self.feature_extractor.pad(\n-            features,\n-            max_length=self.max_length,\n-            padding=self.padding,\n-            pad_to_multiple_of=self.pad_to_multiple_of,\n-            return_tensors=\"np\",\n-        )\n-        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n-\n-        batch_size = batch[\"input_values\"].shape[0]\n-\n-        attention_mask = None\n-        if batch[\"attention_mask\"] is not None:\n-            output_lengths = self.model._get_feat_extract_output_lengths(batch[\"attention_mask\"].sum(-1))\n-            attention_mask = np.zeros((batch_size, mask_indices_seq_length), dtype=np.int8)\n-\n-            # these two operations makes sure that all values\n-            # before the output lengths indices are attended to\n-            attention_mask[(np.arange(attention_mask.shape[0]), output_lengths - 1)] = 1\n-            attention_mask = jnp.flip(jnp.flip(attention_mask, -1).cumsum(-1), -1).astype(\"bool\")\n-\n-        # sample randomly masked indices\n-        batch[\"mask_time_indices\"] = _compute_mask_indices(\n-            (batch_size, mask_indices_seq_length),\n-            self.model.config.mask_time_prob,\n-            self.model.config.mask_time_length,\n-            attention_mask=attention_mask,\n-            min_masks=2,\n-        )\n-\n-        # sample indices to take for negative vectors\n-        batch[\"sampled_negative_indices\"] = _sample_negative_indices(\n-            (batch[\"mask_time_indices\"].shape + (self.model.config.proj_codevector_dim,)),\n-            self.model.config.num_negatives,\n-            attention_mask=attention_mask,\n-        )\n-\n-        return batch\n-\n-\n-def configure_logger(model_args: ModelArguments, training_args: TrainingArguments):\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    logging_level = logging.WARNING\n-    if model_args.verbose_logging:\n-        logging_level = logging.DEBUG\n-    logger.setLevel(logging_level)\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def generate_batch_splits(samples_idx: np.ndarray, batch_size: int) -> np.ndarray:\n-    num_samples = len(samples_idx)\n-    samples_to_remove = num_samples % batch_size\n-\n-    if samples_to_remove != 0:\n-        samples_idx = samples_idx[:-samples_to_remove]\n-    sections_split = num_samples // batch_size\n-    batch_idx = np.split(samples_idx, sections_split)\n-    return batch_idx\n-\n-\n-def compute_contrastive_loss(\n-    quantized_features, transformer_features, negative_indices, mask_time_indices, logits_temp, num_negatives\n-):\n-    batch_size, sequence_length, hidden_size = quantized_features.shape\n-\n-    # take negative vectors from sampled indices\n-    quantized_negatives = quantized_features.reshape(-1, hidden_size)[negative_indices.reshape(-1)]\n-    quantized_negatives = quantized_negatives.reshape(\n-        batch_size, sequence_length, num_negatives, hidden_size\n-    ).transpose(2, 0, 1, 3)\n-\n-    target_features = jnp.concatenate([quantized_features[None, :], quantized_negatives], axis=0)\n-    loss_logits = optax.cosine_similarity(transformer_features, target_features)\n-    loss_logits = loss_logits / logits_temp\n-\n-    neg_is_pos = (quantized_features == quantized_negatives).all(-1)\n-    neg_is_pos = jnp.concatenate([jnp.full((1,) + loss_logits.shape[1:], False), neg_is_pos], axis=0)\n-\n-    # make sure incorrectly sampled vectors don't contribute to loss\n-    loss_logits = jnp.where(neg_is_pos, -1e9, loss_logits)\n-\n-    predictions = loss_logits.transpose(2, 1, 0).reshape(-1, loss_logits.shape[0])\n-    targets = ((1 - mask_time_indices) * -100).transpose(1, 0).flatten()\n-\n-    target_mask = jnp.where(targets >= 0, 1.0, 0.0)\n-    contrastive_loss = optax.softmax_cross_entropy(predictions, onehot(targets, predictions.shape[-1])) * target_mask\n-\n-    contrastive_loss = contrastive_loss.sum()\n-\n-    return contrastive_loss\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-\n-    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-    configure_logger(model_args, training_args)\n-\n-    # Downloading and loading a dataset from the hub.\n-    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\n-\n-    if \"validation\" not in datasets.keys():\n-        # make sure only \"validation\" and \"train\" keys remain\"\n-        datasets = DatasetDict()\n-        datasets[\"validation\"] = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            split=f\"{data_args.train_split_name}[:{data_args.validation_split_percentage}%]\",\n-            cache_dir=model_args.cache_dir,\n-        )\n-        datasets[\"train\"] = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            split=f\"{data_args.train_split_name}[{data_args.validation_split_percentage}%:]\",\n-            cache_dir=model_args.cache_dir,\n-        )\n-    else:\n-        # make sure only \"validation\" and \"train\" keys remain\"\n-        datasets = DatasetDict()\n-        datasets[\"validation\"] = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            split=\"validation\",\n-            cache_dir=model_args.cache_dir,\n-        )\n-        datasets[\"train\"] = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            split=f\"{data_args.train_split_name}\",\n-            cache_dir=model_args.cache_dir,\n-        )\n-\n-    # only normalized-inputs-training is supported\n-    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n-        model_args.model_name_or_path, cache_dir=model_args.cache_dir, do_normalize=True\n-    )\n-\n-    def prepare_dataset(batch):\n-        # check that all files have the correct sampling rate\n-        batch[\"speech\"], _ = librosa.load(batch[data_args.speech_file_column], sr=feature_extractor.sampling_rate)\n-        return batch\n-\n-    # load audio files into numpy arrays\n-    vectorized_datasets = datasets.map(\n-        prepare_dataset, num_proc=data_args.preprocessing_num_workers, remove_columns=datasets[\"train\"].column_names\n-    )\n-\n-    # filter audio files that are too long\n-    vectorized_datasets = vectorized_datasets.filter(\n-        lambda data: len(data[\"speech\"]) < int(data_args.max_duration_in_seconds * feature_extractor.sampling_rate)\n-    )\n-\n-    def normalize(batch):\n-        return feature_extractor(batch[\"speech\"], sampling_rate=feature_extractor.sampling_rate)\n-\n-    # normalize and transform to `BatchFeatures`\n-    vectorized_datasets = vectorized_datasets.map(\n-        normalize,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-        remove_columns=vectorized_datasets[\"train\"].column_names,\n-    )\n-\n-    # pretraining is only supported for \"newer\" stable layer norm architecture\n-    # apply_spec_augment has to be True, mask_feature_prob has to be 0.0\n-    config = Wav2Vec2Config.from_pretrained(\n-        model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    if not config.do_stable_layer_norm or config.feat_extract_norm != \"layer\":\n-        raise ValueError(\n-            \"PreTraining is only supported for ``config.do_stable_layer_norm=True`` and\"\n-            \" ``config.feat_extract_norm='layer'\"\n-        )\n-\n-    model = FlaxWav2Vec2ForPreTraining(config, seed=training_args.seed, dtype=getattr(jnp, model_args.dtype))\n-\n-    # Activate gradient checkpointing if needed\n-    if training_args.gradient_checkpointing:\n-        model.gradient_checkpointing_enable()\n-\n-    data_collator = FlaxDataCollatorForWav2Vec2Pretraining(\n-        model=model, feature_extractor=feature_extractor, pad_to_multiple_of=data_args.pad_to_multiple_of\n-    )\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-    gumbel_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    eval_batch_size = int(training_args.per_device_eval_batch_size) * jax.device_count()\n-\n-    num_train_steps = len(vectorized_datasets[\"train\"]) // train_batch_size * num_epochs\n-\n-    # Create learning rate schedule\n-    warmup_fn = optax.linear_schedule(\n-        init_value=0.0, end_value=training_args.learning_rate, transition_steps=training_args.warmup_steps\n-    )\n-    decay_fn = optax.linear_schedule(\n-        init_value=training_args.learning_rate,\n-        end_value=0,\n-        transition_steps=num_train_steps - training_args.warmup_steps,\n-    )\n-    linear_decay_lr_schedule_fn = optax.join_schedules(\n-        schedules=[warmup_fn, decay_fn], boundaries=[training_args.warmup_steps]\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        flat_mask = {\n-            path: (path[-1] != \"bias\" and path[-2:] not in [(\"layer_norm\", \"scale\"), (\"final_layer_norm\", \"scale\")])\n-            for path in flat_params\n-        }\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    # Setup train state and define training hyper-parameters\n-    state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw)\n-    num_negatives = model.config.num_negatives\n-    contrastive_logits_temperature = model.config.contrastive_logits_temperature\n-    num_codevectors = model.config.num_codevectors_per_group * model.config.num_codevector_groups\n-    diversity_loss_weight = model.config.diversity_loss_weight\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, dropout_rng, gumbel_rng):\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-        gumbel_rng, new_gumbel_rng = jax.random.split(gumbel_rng)\n-\n-        def loss_fn(params):\n-            negative_indices = batch.pop(\"sampled_negative_indices\")\n-\n-            gumbel_temperature = jnp.clip(\n-                model_args.max_gumbel_temperature * model_args.gumbel_temperature_decay**state.step,\n-                a_min=model_args.min_gumbel_temperature,\n-            )\n-\n-            outputs = state.apply_fn(\n-                **batch,\n-                gumbel_temperature=gumbel_temperature,\n-                params=params,\n-                dropout_rng=dropout_rng,\n-                gumbel_rng=gumbel_rng,\n-                train=True,\n-            )\n-\n-            contrastive_loss = compute_contrastive_loss(\n-                outputs.projected_quantized_states,\n-                outputs.projected_states,\n-                negative_indices,\n-                batch[\"mask_time_indices\"],\n-                contrastive_logits_temperature,\n-                num_negatives,\n-            )\n-\n-            diversity_loss = (num_codevectors - outputs.codevector_perplexity) / num_codevectors\n-            loss = contrastive_loss + diversity_loss_weight * diversity_loss\n-\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(loss_fn)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-        new_state = state.apply_gradients(grads=grad)\n-\n-        metrics = jax.lax.pmean(\n-            {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n-        )\n-\n-        return new_state, metrics, new_dropout_rng, new_gumbel_rng\n-\n-    # Create parallel version of the train step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        negative_indices = batch.pop(\"sampled_negative_indices\")\n-\n-        outputs = model(**batch, params=params, train=False)\n-\n-        contrastive_loss = compute_contrastive_loss(\n-            outputs.projected_quantized_states,\n-            outputs.projected_states,\n-            negative_indices,\n-            batch[\"mask_time_indices\"],\n-            contrastive_logits_temperature,\n-            num_negatives,\n-        )\n-\n-        diversity_loss = (num_codevectors - outputs.codevector_perplexity) / num_codevectors\n-        loss = contrastive_loss + diversity_loss_weight * diversity_loss\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss.mean(), \"codevector_perplexity\": outputs.codevector_perplexity}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-\n-        return metrics\n-\n-    p_eval_step = jax.pmap(eval_step, \"batch\", donate_argnums=(0,))\n-\n-    # Replicate the train state on each device\n-    state = jax_utils.replicate(state)\n-\n-    train_time = 0\n-    train_metrics = []\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset\n-        num_train_samples = len(vectorized_datasets[\"train\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        train_samples_idx = np.random.permutation(np.arange(num_train_samples))\n-        train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n-\n-        # Gather the indexes for creating the batch and do a training step\n-        for step, batch_idx in enumerate(tqdm(train_batch_idx, desc=\"Training...\", position=1)):\n-            samples = [vectorized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples)\n-            model_inputs = shard(model_inputs.data)\n-\n-            # Model forward\n-            state, train_metric, dropout_rngs, gumbel_rngs = p_train_step(\n-                state, model_inputs, dropout_rngs, gumbel_rngs\n-            )\n-            train_metrics.append(train_metric)\n-\n-            cur_step = epoch * (num_train_samples // train_batch_size) + step\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = jax_utils.unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step} | Loss: {train_metric['loss'].mean()}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate'].mean()})\"\n-                )\n-\n-                train_metrics = []\n-\n-        # ======================== Evaluating ==============================\n-        num_eval_samples = len(vectorized_datasets[\"validation\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        eval_samples_idx = np.arange(num_eval_samples)\n-        eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size)\n-\n-        eval_metrics = []\n-        for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-            samples = [vectorized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples)\n-\n-            # Model forward\n-            model_inputs = shard(model_inputs.data)\n-            metrics = p_eval_step(state.params, model_inputs)\n-            eval_metrics.append(metrics)\n-\n-        # get eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-        # Update progress bar\n-        epochs.write(\n-            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {eval_metrics['loss']}, Perplexity:\"\n-            f\" {eval_metrics['codevector_perplexity']})\"\n-        )\n-\n-        # Save metrics\n-        if has_tensorboard and jax.process_index() == 0:\n-            cur_step = epoch * (len(vectorized_datasets[\"train\"]) // train_batch_size)\n-            write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-        # save checkpoint after each epoch and push checkpoint to the hub\n-        if jax.process_index() == 0:\n-            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-            model.save_pretrained(training_args.output_dir, params=params, push_to_hub=training_args.push_to_hub)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "2cc0fb75bd2c1650ffeb13d5c41fafe47c53220f",
            "filename": "examples/research_projects/layoutlmv3/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 69,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flayoutlmv3%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flayoutlmv3%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flayoutlmv3%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,69 +0,0 @@\n-<!---\n-Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Token classification with LayoutLMv3 (PyTorch version)\n-\n-This directory contains a script, `run_funsd_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord).\n-\n-The script `run_funsd_cord.py` leverages the ğŸ¤— Datasets library and the Trainer API. You can easily customize it to your needs.\n-\n-## Fine-tuning on FUNSD\n-\n-Fine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\n-\n-```bash\n-python run_funsd_cord.py \\\n-  --model_name_or_path microsoft/layoutlmv3-base \\\n-  --dataset_name funsd \\\n-  --output_dir layoutlmv3-test \\\n-  --do_train \\\n-  --do_eval \\\n-  --max_steps 1000 \\\n-  --eval_strategy steps \\\n-  --eval_steps 100 \\\n-  --learning_rate 1e-5 \\\n-  --load_best_model_at_end \\\n-  --metric_for_best_model \"eval_f1\" \\\n-  --push_to_hub \\\n-  --push_to_hubÂ°model_id layoutlmv3-finetuned-funsd\n-```\n-\n-ğŸ‘€ The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd. By specifying the `push_to_hub` flag, the model gets uploaded automatically to the hub (regularly), together with a model card, which includes metrics such as precision, recall and F1. Note that you can easily update the model card, as it's just a README file of the respective repo on the hub.\n-\n-There's also the \"Training metrics\" [tab](https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd/tensorboard), which shows Tensorboard logs over the course of training. Pretty neat, huh?\n-\n-## Fine-tuning on CORD\n-\n-Fine-tuning LayoutLMv3 for token classification on [CORD](https://github.com/clovaai/cord) can be done as follows:\n-\n-```bash\n-python run_funsd_cord.py \\\n-  --model_name_or_path microsoft/layoutlmv3-base \\\n-  --dataset_name cord \\\n-  --output_dir layoutlmv3-test \\\n-  --do_train \\\n-  --do_eval \\\n-  --max_steps 1000 \\\n-  --eval_strategy steps \\\n-  --eval_steps 100 \\\n-  --learning_rate 5e-5 \\\n-  --load_best_model_at_end \\\n-  --metric_for_best_model \"eval_f1\" \\\n-  --push_to_hub \\\n-  --push_to_hubÂ°model_id layoutlmv3-finetuned-cord\n-```\n-\n-ğŸ‘€ The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord. Note that a model card gets generated automatically in case you specify the `push_to_hub` flag.\n\\ No newline at end of file"
        },
        {
            "sha": "c4fa0075733b1b126cb88d69bd2fe18058aa4d1b",
            "filename": "examples/research_projects/layoutlmv3/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flayoutlmv3%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flayoutlmv3%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flayoutlmv3%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec",
            "patch": "@@ -1,3 +0,0 @@\n-datasets\n-seqeval\n-pillow"
        },
        {
            "sha": "ad83fbdef9dec46c87b67dc55a509c4e897bf4bf",
            "filename": "examples/research_projects/layoutlmv3/run_funsd_cord.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flayoutlmv3%2Frun_funsd_cord.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flayoutlmv3%2Frun_funsd_cord.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flayoutlmv3%2Frun_funsd_cord.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "eaa29d4542260c553c2403f311c2ee45df6c3fd9",
            "filename": "examples/research_projects/longform-qa/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flongform-qa%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6b1b15cc9cbba36391effa64ace63f3cc69ed94a",
            "filename": "examples/research_projects/longform-qa/eli5_app.py",
            "status": "removed",
            "additions": 0,
            "deletions": 349,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2Feli5_app.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2Feli5_app.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flongform-qa%2Feli5_app.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "d4b235fdbaab26218c37f1b60d3142349c11b737",
            "filename": "examples/research_projects/longform-qa/eli5_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 688,
            "changes": 688,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2Feli5_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2Feli5_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flongform-qa%2Feli5_utils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "a21b64d33df8f33f9b008d64cfc08fa2a22445c7",
            "filename": "examples/research_projects/longform-qa/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flongform-qa%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flongform-qa%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "703eb0b4e4235c65ba107108585c1819e0250445",
            "filename": "examples/research_projects/luke/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fluke%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fluke%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fluke%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "aec4133f21b36eee313a5c6371ff48537ccf613c",
            "filename": "examples/research_projects/luke/luke_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 115,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fluke%2Fluke_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fluke%2Fluke_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fluke%2Fluke_utils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "1552acbd42c21dcd686fd4de1b4a166a5b088771",
            "filename": "examples/research_projects/luke/run_luke_ner_no_trainer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 720,
            "changes": 720,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fluke%2Frun_luke_ner_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fluke%2Frun_luke_ner_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fluke%2Frun_luke_ner_no_trainer.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "2ec1aaebbb04fb80c35cb92586846ac434dd8469",
            "filename": "examples/research_projects/lxmert/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "576a4b7631cbabd9c7b058df265beb80fa709925",
            "filename": "examples/research_projects/lxmert/demo.ipynb",
            "status": "removed",
            "additions": 0,
            "deletions": 264,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fdemo.ipynb",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fdemo.ipynb",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Fdemo.ipynb?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6b1342c9b11f93839e3cdda845b9fef1379177b2",
            "filename": "examples/research_projects/lxmert/extracting_data.py",
            "status": "removed",
            "additions": 0,
            "deletions": 149,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fextracting_data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fextracting_data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Fextracting_data.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c7c3bf376ce382bd83df30732dffaaf6c10db57c",
            "filename": "examples/research_projects/lxmert/modeling_frcnn.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1920,
            "changes": 1920,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fmodeling_frcnn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fmodeling_frcnn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Fmodeling_frcnn.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "65f8f6cd377c9f6864362debdd9b294a85dae2cd",
            "filename": "examples/research_projects/lxmert/processing_image.py",
            "status": "removed",
            "additions": 0,
            "deletions": 151,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fprocessing_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fprocessing_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Fprocessing_image.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "e2778663a53c723890e14a16ae1c166c66a31cf6",
            "filename": "examples/research_projects/lxmert/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "995fbd2c19aecfdde4d1b288eac05c074012a4a1",
            "filename": "examples/research_projects/lxmert/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 554,
            "changes": 554,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Futils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "dcfd8426ff4f36a2d5cc4259704f2ac67bf31290",
            "filename": "examples/research_projects/lxmert/visualizing_image.py",
            "status": "removed",
            "additions": 0,
            "deletions": 500,
            "changes": 500,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fvisualizing_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Flxmert%2Fvisualizing_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Fvisualizing_image.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "bf5aa9410826ed96e6acdaefdde8420f8b67b3bc",
            "filename": "examples/research_projects/mlm_wwm/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmlm_wwm%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "2d0f26bd4dc3bb2d65dd149be5d93fbc1cbcb4ae",
            "filename": "examples/research_projects/mlm_wwm/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmlm_wwm%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "eca89df97982da81308abaedc3c01605faf0a4ad",
            "filename": "examples/research_projects/mlm_wwm/run_chinese_ref.py",
            "status": "removed",
            "additions": 0,
            "deletions": 164,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2Frun_chinese_ref.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2Frun_chinese_ref.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmlm_wwm%2Frun_chinese_ref.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "629026bdb20a639bc5022831b337dae31445efc6",
            "filename": "examples/research_projects/mlm_wwm/run_mlm_wwm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 435,
            "changes": 435,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2Frun_mlm_wwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmlm_wwm%2Frun_mlm_wwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmlm_wwm%2Frun_mlm_wwm.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "68b2f15159ec23ad40993471a773f16dee0e292b",
            "filename": "examples/research_projects/mm-imdb/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmm-imdb%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmm-imdb%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmm-imdb%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "686691e0b9ced2da8708565a95c1977e52a481da",
            "filename": "examples/research_projects/mm-imdb/run_mmimdb.py",
            "status": "removed",
            "additions": 0,
            "deletions": 575,
            "changes": 575,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmm-imdb%2Frun_mmimdb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmm-imdb%2Frun_mmimdb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmm-imdb%2Frun_mmimdb.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "df8e38d59749ed736b4d97d6548f89f38b85961f",
            "filename": "examples/research_projects/mm-imdb/utils_mmimdb.py",
            "status": "removed",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmm-imdb%2Futils_mmimdb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmm-imdb%2Futils_mmimdb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmm-imdb%2Futils_mmimdb.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "575ec1a9b492874bd4a5c52403d0a5fefcf28f8d",
            "filename": "examples/research_projects/movement-pruning/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 185,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "0c27bd02a7df9fcffe1bd7de20ca2aafd8f10ec5",
            "filename": "examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb",
            "status": "removed",
            "additions": 0,
            "deletions": 645,
            "changes": 645,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2FSaving_PruneBERT.ipynb",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2FSaving_PruneBERT.ipynb",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2FSaving_PruneBERT.ipynb?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "da7534f4a6f985d64268d4fe0d719a2acefdfbb0",
            "filename": "examples/research_projects/movement-pruning/bertarize.py",
            "status": "removed",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fbertarize.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fbertarize.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Fbertarize.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c0ac53fb7859aae68f009814cefb3247dc2e257c",
            "filename": "examples/research_projects/movement-pruning/counts_parameters.py",
            "status": "removed",
            "additions": 0,
            "deletions": 97,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fcounts_parameters.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fcounts_parameters.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Fcounts_parameters.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6646667ea883781c3bd6b9cff0267b68ee1478e4",
            "filename": "examples/research_projects/movement-pruning/emmental/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9c7459f27a7b66c2adca54b99152d17edc963685",
            "filename": "examples/research_projects/movement-pruning/emmental/configuration_bert_masked.py",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fconfiguration_bert_masked.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fconfiguration_bert_masked.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fconfiguration_bert_masked.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "8c0b091c7de7e7ce40dc419b876de4c73115983e",
            "filename": "examples/research_projects/movement-pruning/emmental/modeling_bert_masked.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1019,
            "changes": 1019,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodeling_bert_masked.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodeling_bert_masked.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodeling_bert_masked.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "761a6343d6b5466bddadbaa927436c2d6351d67f",
            "filename": "examples/research_projects/movement-pruning/emmental/modules/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c96975e3b3750928e4d4adbf241415ddcfb86791",
            "filename": "examples/research_projects/movement-pruning/emmental/modules/binarizer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2Fbinarizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2Fbinarizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2Fbinarizer.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "e3c94836851ec2ef3762c43a57bc164e3222a1be",
            "filename": "examples/research_projects/movement-pruning/emmental/modules/masked_nn.py",
            "status": "removed",
            "additions": 0,
            "deletions": 106,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2Fmasked_nn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2Fmasked_nn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Femmental%2Fmodules%2Fmasked_nn.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "4ddb424835751805a97b9e706f56219c0e5a5201",
            "filename": "examples/research_projects/movement-pruning/masked_run_glue.py",
            "status": "removed",
            "additions": 0,
            "deletions": 962,
            "changes": 962,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fmasked_run_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fmasked_run_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Fmasked_run_glue.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7b1c2b322097a459c3e5645611566fe35ffc9e32",
            "filename": "examples/research_projects/movement-pruning/masked_run_squad.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1147,
            "changes": 1147,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fmasked_run_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Fmasked_run_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Fmasked_run_squad.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "b678a785bc349443817b20aaaa4e527f6d93b96b",
            "filename": "examples/research_projects/movement-pruning/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fmovement-pruning%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c43b0450ea2c4bfacb2e9f5e2af2b6b41d6b340d",
            "filename": "examples/research_projects/onnx/summarization/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "5c1b0da700024bf8b184313b6fe7f3aa3d17f5bb",
            "filename": "examples/research_projects/onnx/summarization/bart_onnx/generation_onnx.py",
            "status": "removed",
            "additions": 0,
            "deletions": 755,
            "changes": 755,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Fbart_onnx%2Fgeneration_onnx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Fbart_onnx%2Fgeneration_onnx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Fbart_onnx%2Fgeneration_onnx.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "1df20e4504da2c201a2d95f48682666385212fbf",
            "filename": "examples/research_projects/onnx/summarization/bart_onnx/reduce_onnx_size.py",
            "status": "removed",
            "additions": 0,
            "deletions": 121,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Fbart_onnx%2Freduce_onnx_size.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Fbart_onnx%2Freduce_onnx_size.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Fbart_onnx%2Freduce_onnx_size.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "215356506121ca417ffa4a08ed02677e80b64a8f",
            "filename": "examples/research_projects/onnx/summarization/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "fa826732701f1ad2c3fa37935319fb8189f85fd7",
            "filename": "examples/research_projects/onnx/summarization/run_onnx_exporter.py",
            "status": "removed",
            "additions": 0,
            "deletions": 206,
            "changes": 206,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Frun_onnx_exporter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Frun_onnx_exporter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fonnx%2Fsummarization%2Frun_onnx_exporter.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "fa847268b0c8b3e3b1208c2dce76ca247c145179",
            "filename": "examples/research_projects/performer/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "8634666f983bb5fd1db46590ea615082ddacd9b3",
            "filename": "examples/research_projects/performer/full_script.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Ffull_script.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Ffull_script.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2Ffull_script.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7c2fde6ddbb5dc188b54d67dccb3625562ff649c",
            "filename": "examples/research_projects/performer/modeling_flax_performer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 551,
            "changes": 551,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c52425093812cdc2964b1cbb86c02d44671c940d",
            "filename": "examples/research_projects/performer/modeling_flax_performer_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 658,
            "changes": 658,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer_utils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "0332fe1575ffe8a099c5afd4f25cabfd46ae77cb",
            "filename": "examples/research_projects/performer/run_mlm_performer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 693,
            "changes": 693,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Frun_mlm_performer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Frun_mlm_performer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2Frun_mlm_performer.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "b96cd7e643ef41b1cf96773aa226ddbe46adaa7f",
            "filename": "examples/research_projects/performer/sanity_script.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Fsanity_script.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fperformer%2Fsanity_script.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2Fsanity_script.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "f37ea8e96f216d1977491779f940c2f9851302da",
            "filename": "examples/research_projects/pplm/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "f4c11ad54d10b300e2051ef6ba2d209447bc92e4",
            "filename": "examples/research_projects/pplm/imgs/headfigure.png",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Fimgs%2Fheadfigure.png",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Fimgs%2Fheadfigure.png",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2Fimgs%2Fheadfigure.png?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "190d3afd49f1795245772a5d8b81a50b821d17b4",
            "filename": "examples/research_projects/pplm/imgs/wooly.png",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Fimgs%2Fwooly.png",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Fimgs%2Fwooly.png",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2Fimgs%2Fwooly.png?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "e26521fe39101f297e24d93e0a73028c803b390b",
            "filename": "examples/research_projects/pplm/pplm_classification_head.py",
            "status": "removed",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Fpplm_classification_head.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Fpplm_classification_head.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2Fpplm_classification_head.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "630d1b0f8f61fbc7e2cc2e89b59773ebefb6ed47",
            "filename": "examples/research_projects/pplm/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "cc49b7fa83c4c3c43501021b48f6bf77c8be4fe2",
            "filename": "examples/research_projects/pplm/run_pplm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 823,
            "changes": 823,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Frun_pplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Frun_pplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2Frun_pplm.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "43ec5823e37764d5b1a5a9820f0a780efad6ad81",
            "filename": "examples/research_projects/pplm/run_pplm_discrim_train.py",
            "status": "removed",
            "additions": 0,
            "deletions": 526,
            "changes": 526,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Frun_pplm_discrim_train.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fpplm%2Frun_pplm_discrim_train.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fpplm%2Frun_pplm_discrim_train.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "e64c9f0e021d4547654192bbfe34f469c76fc6f0",
            "filename": "examples/research_projects/quantization-qdqbert/Dockerfile",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2FDockerfile?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "2cc2d5e5f98c716835a158c43a3cf5ddfce79563",
            "filename": "examples/research_projects/quantization-qdqbert/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 200,
            "changes": 200,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7a8ea2109bc53b6e20159c0ea4d863acb0adb596",
            "filename": "examples/research_projects/quantization-qdqbert/evaluate-hf-trt-qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 457,
            "changes": 457,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fevaluate-hf-trt-qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fevaluate-hf-trt-qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fevaluate-hf-trt-qa.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "bb0436c125800bb4be99d1d3fc63486c7b6e4ea4",
            "filename": "examples/research_projects/quantization-qdqbert/ort-infer-benchmark.py",
            "status": "removed",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fort-infer-benchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fort-infer-benchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fort-infer-benchmark.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "132aa284905875a825e1977c1c8adfd694c39981",
            "filename": "examples/research_projects/quantization-qdqbert/quant_trainer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 305,
            "changes": 305,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fquant_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fquant_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2Fquant_trainer.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "770a36525b5caa0cd36dfdae4e8fdffad5483f6b",
            "filename": "examples/research_projects/quantization-qdqbert/run_quant_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 688,
            "changes": 688,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Frun_quant_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Frun_quant_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2Frun_quant_qa.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "a56d875354ddb0615709c5d0db51ef21b805b6ef",
            "filename": "examples/research_projects/quantization-qdqbert/trainer_quant_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 212,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Ftrainer_quant_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Ftrainer_quant_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2Ftrainer_quant_qa.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "e90d6c4747c97c7932376ef70da1c928722867fe",
            "filename": "examples/research_projects/quantization-qdqbert/utils_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 435,
            "changes": 435,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fquantization-qdqbert%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fquantization-qdqbert%2Futils_qa.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9aa0bc5dbcb1d5f890252e78d2b2a7704da4e565",
            "filename": "examples/research_projects/rag-end2end-retriever/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "09a30ff6d5c43313aea143620978a0ae91e5a8e9",
            "filename": "examples/research_projects/rag-end2end-retriever/callbacks_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fcallbacks_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fcallbacks_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fcallbacks_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "f97467292c25bfac463d12e3a3481ee5b65bf444",
            "filename": "examples/research_projects/rag-end2end-retriever/distributed_ray_retriever.py",
            "status": "removed",
            "additions": 0,
            "deletions": 185,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fdistributed_ray_retriever.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fdistributed_ray_retriever.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fdistributed_ray_retriever.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "55f4da56571d3a3b668a557d59f15191b9836542",
            "filename": "examples/research_projects/rag-end2end-retriever/eval_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 320,
            "changes": 320,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Feval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Feval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Feval_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9bc2e5db6d5d106cd3e66edf8776c30aea0e157a",
            "filename": "examples/research_projects/rag-end2end-retriever/finetune_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 815,
            "changes": 815,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ffinetune_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ffinetune_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ffinetune_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "cef1a264c935ca4d4af4f85907cb7dbda6e4e9f4",
            "filename": "examples/research_projects/rag-end2end-retriever/finetune_rag_ray_end2end.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ffinetune_rag_ray_end2end.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ffinetune_rag_ray_end2end.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ffinetune_rag_ray_end2end.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "444c07b2bab16a66731b312693611b252d7ad310",
            "filename": "examples/research_projects/rag-end2end-retriever/kb_encode_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fkb_encode_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fkb_encode_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fkb_encode_utils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c1a271e88d138cd39fc59fe249eb3746e9be59fd",
            "filename": "examples/research_projects/rag-end2end-retriever/lightning_base.py",
            "status": "removed",
            "additions": 0,
            "deletions": 414,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Flightning_base.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "32025229d0743986aa5c68c51a96da8dca9f89b6",
            "filename": "examples/research_projects/rag-end2end-retriever/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "76da009a2f2310e5780d770bcc15391823cf636b",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-kb/my_knowledge_dataset.csv",
            "status": "removed",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-kb%2Fmy_knowledge_dataset.csv",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-kb%2Fmy_knowledge_dataset.csv",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-kb%2Fmy_knowledge_dataset.csv?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "3d5cbc38039d833e0be32e3692cce8710d96774b",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/test.source",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftest.source",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftest.source",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftest.source?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "a3a6e04372c763167f20ef7dbd857b8cd7ac6ec1",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/test.target",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftest.target",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftest.target",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftest.target?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9f72c3e03a7bb6489fd14950c814f46cef5d1961",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/train.source",
            "status": "removed",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftrain.source",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftrain.source",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftrain.source?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "3bda0caf2e31622c4109d91bda5c994f50d0510c",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/train.target",
            "status": "removed",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftrain.target",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftrain.target",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Ftrain.target?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "a2c628e9ca08c5010016831679d3c0ad11f49e35",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/val.source",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Fval.source",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Fval.source",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Fval.source?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "57bfcf5270a5663e4639c69b710df4c82a0e6fb6",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/dummy-train-data/val.target",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Fval.target",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Fval.target",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Fdummy-train-data%2Fval.target?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c44d110d20046a217e7484365949e41ac21835d7",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/test_finetune.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Ftest_finetune.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Ftest_finetune.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Ftest_finetune.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6c667c0940399233abbe5b5bfc0808a881682316",
            "filename": "examples/research_projects/rag-end2end-retriever/test_run/test_rag_new_features.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Ftest_rag_new_features.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Ftest_rag_new_features.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Ftest_run%2Ftest_rag_new_features.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "20e0ea2d3cc2a2a608601cf41ddcc16aca4e21a3",
            "filename": "examples/research_projects/rag-end2end-retriever/use_own_knowledge_dataset.py",
            "status": "removed",
            "additions": 0,
            "deletions": 175,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fuse_own_knowledge_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fuse_own_knowledge_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Fuse_own_knowledge_dataset.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ec98c1d782e0ea2a00d80420c88702acdd8da98d",
            "filename": "examples/research_projects/rag-end2end-retriever/utils_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 244,
            "changes": 244,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Futils_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag-end2end-retriever%2Futils_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Futils_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "59aa46a89522a1ddbfb756d3b1239d3d86734330",
            "filename": "examples/research_projects/rag/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 203,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "3cee09bb7f51087e92d778c4c9e27d76085d1b30",
            "filename": "examples/research_projects/rag/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2F__init__.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "0906295b3018249fca7b843c7dc2caee913377b2",
            "filename": "examples/research_projects/rag/_test_finetune_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 111,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2F_test_finetune_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2F_test_finetune_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2F_test_finetune_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "d75f97995bd16f75396f1c32392d6b65137b8169",
            "filename": "examples/research_projects/rag/callbacks_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fcallbacks_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fcallbacks_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fcallbacks_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6adae75fea9b12f49d8789688ea5edf90124447e",
            "filename": "examples/research_projects/rag/consolidate_rag_checkpoint.py",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fconsolidate_rag_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fconsolidate_rag_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fconsolidate_rag_checkpoint.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "b8c4b6fc3c50eaca1033f530dedd26df4ea4bc65",
            "filename": "examples/research_projects/rag/distributed_pytorch_retriever.py",
            "status": "removed",
            "additions": 0,
            "deletions": 138,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fdistributed_pytorch_retriever.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fdistributed_pytorch_retriever.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fdistributed_pytorch_retriever.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "dd5baaf726116f8569228af74c221c67b477d1cb",
            "filename": "examples/research_projects/rag/distributed_ray_retriever.py",
            "status": "removed",
            "additions": 0,
            "deletions": 152,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fdistributed_ray_retriever.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fdistributed_ray_retriever.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fdistributed_ray_retriever.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "55f4da56571d3a3b668a557d59f15191b9836542",
            "filename": "examples/research_projects/rag/eval_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 320,
            "changes": 320,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Feval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Feval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Feval_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "af3acd4def67b42ee49847b706bea1f23bce9dce",
            "filename": "examples/research_projects/rag/finetune_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 649,
            "changes": 649,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "8fd1fea3e5467d189d33b44aecfad6a7ce2570ca",
            "filename": "examples/research_projects/rag/finetune_rag.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7c8e7b97e77cd96674cbb6aceaac3c5076fb530c",
            "filename": "examples/research_projects/rag/finetune_rag_ray.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ffinetune_rag_ray.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ffinetune_rag_ray.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Ffinetune_rag_ray.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "12099bc3aa106eb2ae1f2cfdea5a1299ef3a02fb",
            "filename": "examples/research_projects/rag/lightning_base.py",
            "status": "removed",
            "additions": 0,
            "deletions": 404,
            "changes": 404,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Flightning_base.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "4d8a1e5f4674fa197bc1aeb1dfd609404bc478af",
            "filename": "examples/research_projects/rag/parse_dpr_relevance_data.py",
            "status": "removed",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fparse_dpr_relevance_data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fparse_dpr_relevance_data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fparse_dpr_relevance_data.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "5988d38de9e903ad04adec9ebc3baebbc82ec92e",
            "filename": "examples/research_projects/rag/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "76da009a2f2310e5780d770bcc15391823cf636b",
            "filename": "examples/research_projects/rag/test_data/my_knowledge_dataset.csv",
            "status": "removed",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ftest_data%2Fmy_knowledge_dataset.csv",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ftest_data%2Fmy_knowledge_dataset.csv",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Ftest_data%2Fmy_knowledge_dataset.csv?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7e75e0a7a7efcc901815b0fe28e77537c1f8762a",
            "filename": "examples/research_projects/rag/test_distributed_retriever.py",
            "status": "removed",
            "additions": 0,
            "deletions": 338,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ftest_distributed_retriever.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Ftest_distributed_retriever.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Ftest_distributed_retriever.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "d2ab6d07d5cc3653aa93bbb351a93117ad11fcd0",
            "filename": "examples/research_projects/rag/use_own_knowledge_dataset.py",
            "status": "removed",
            "additions": 0,
            "deletions": 208,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fuse_own_knowledge_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Fuse_own_knowledge_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fuse_own_knowledge_dataset.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ec98c1d782e0ea2a00d80420c88702acdd8da98d",
            "filename": "examples/research_projects/rag/utils_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 244,
            "changes": 244,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Futils_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frag%2Futils_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Futils_rag.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ca3c5cdecdecea2c1c04ed7b18034311bc7b827b",
            "filename": "examples/research_projects/robust-speech-event/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 713,
            "changes": 713,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "b6c89a6d49fac9a5086c13477b0f642a1d4f1e96",
            "filename": "examples/research_projects/robust-speech-event/eval.py",
            "status": "removed",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2Feval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2Feval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2Feval.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "cb489ea28d686e50384b57cd00e566e667d64218",
            "filename": "examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py",
            "status": "removed",
            "additions": 0,
            "deletions": 779,
            "changes": 779,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "37f91b9ef6171b9885c81e1d5bb91c7ec369fa09",
            "filename": "examples/research_projects/robust-speech-event/run_speech_recognition_ctc_streaming.py",
            "status": "removed",
            "additions": 0,
            "deletions": 679,
            "changes": 679,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_streaming.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_streaming.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_streaming.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "062d5de7afd0573a0412c0f6bfd85f9fda490546",
            "filename": "examples/research_projects/self-training-text-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 128,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fself-training-text-classification%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "4bf9eb28df2810129040f1fb1bf825ddbba19db0",
            "filename": "examples/research_projects/self-training-text-classification/finetuning.py",
            "status": "removed",
            "additions": 0,
            "deletions": 818,
            "changes": 818,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Ffinetuning.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Ffinetuning.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fself-training-text-classification%2Ffinetuning.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "25d66c8b6a4bf644255fd5fd8b04a31aa80d22ac",
            "filename": "examples/research_projects/self-training-text-classification/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fself-training-text-classification%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "34e91d7c127c895249a24a66171a73ea74c7713b",
            "filename": "examples/research_projects/self-training-text-classification/run.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 81,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Frun.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Frun.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fself-training-text-classification%2Frun.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "d741225b061e887a3870e168b5f0c87a3f5e3d1c",
            "filename": "examples/research_projects/self-training-text-classification/selftraining.py",
            "status": "removed",
            "additions": 0,
            "deletions": 388,
            "changes": 388,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Fselftraining.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fself-training-text-classification%2Fselftraining.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fself-training-text-classification%2Fselftraining.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ab79a652ed38c3fbb954db422b89c18757cc0a11",
            "filename": "examples/research_projects/seq2seq-distillation/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 434,
            "changes": 434,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "fa84a60c0c88e0ac5cc224385c9f7b74ef80d17c",
            "filename": "examples/research_projects/seq2seq-distillation/_test_bash_script.py",
            "status": "removed",
            "additions": 0,
            "deletions": 203,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_bash_script.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_bash_script.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_bash_script.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "73df66315cbd7952e7779108d0dd7adcf738cd3d",
            "filename": "examples/research_projects/seq2seq-distillation/_test_make_student.py",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_make_student.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_make_student.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_make_student.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "0ee4dd8afe1d5e65bf00657098a0ff07202e8404",
            "filename": "examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py",
            "status": "removed",
            "additions": 0,
            "deletions": 444,
            "changes": 444,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_seq2seq_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_seq2seq_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_seq2seq_examples.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9eeb3b30d39986332efb2e78694c7db9c4edc2e6",
            "filename": "examples/research_projects/seq2seq-distillation/_test_seq2seq_examples_multi_gpu.py",
            "status": "removed",
            "additions": 0,
            "deletions": 163,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_seq2seq_examples_multi_gpu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_seq2seq_examples_multi_gpu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2F_test_seq2seq_examples_multi_gpu.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6f6ed5dd58acfd7b053545b6c24c1ff2cb7dbcc8",
            "filename": "examples/research_projects/seq2seq-distillation/callbacks.py",
            "status": "removed",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fcallbacks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fcallbacks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fcallbacks.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "5f3c984f3724c1cb46ffcdc9e57b20a391a423cf",
            "filename": "examples/research_projects/seq2seq-distillation/convert_pl_checkpoint_to_hf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fconvert_pl_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fconvert_pl_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fconvert_pl_checkpoint_to_hf.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "5c938a71604e3d238519203a09636f2b57ebf526",
            "filename": "examples/research_projects/seq2seq-distillation/distil_marian_enro_teacher.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistil_marian_enro_teacher.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistil_marian_enro_teacher.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistil_marian_enro_teacher.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "4f0f53d7960b47402e90f8060fc74e978c54ff8a",
            "filename": "examples/research_projects/seq2seq-distillation/distil_marian_no_teacher.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistil_marian_no_teacher.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistil_marian_no_teacher.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistil_marian_no_teacher.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "323f62bf45812e6a501c327438dd0b05bedae80b",
            "filename": "examples/research_projects/seq2seq-distillation/distillation.py",
            "status": "removed",
            "additions": 0,
            "deletions": 310,
            "changes": 310,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistillation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistillation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdistillation.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "cfe9e21f0f67de9e43510c53003bba11467aec87",
            "filename": "examples/research_projects/seq2seq-distillation/dynamic_bs_example.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdynamic_bs_example.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdynamic_bs_example.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fdynamic_bs_example.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ff889af81e36a6e73040aa006b93dd0c1ec851f7",
            "filename": "examples/research_projects/seq2seq-distillation/finetune.py",
            "status": "removed",
            "additions": 0,
            "deletions": 454,
            "changes": 454,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "683c2d7752df134d3da861dbe438f9fb65543ea4",
            "filename": "examples/research_projects/seq2seq-distillation/finetune.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "f0289b45ab5c90ce273f53929bf898e93f743a2f",
            "filename": "examples/research_projects/seq2seq-distillation/finetune_bart_tiny.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_bart_tiny.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_bart_tiny.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_bart_tiny.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ec7ff98557c180be1dd7f5e18fae1152623acb79",
            "filename": "examples/research_projects/seq2seq-distillation/finetune_pegasus_xsum.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_pegasus_xsum.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_pegasus_xsum.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_pegasus_xsum.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "504e9eb71e3596360bfb575ded4136689854e250",
            "filename": "examples/research_projects/seq2seq-distillation/finetune_t5.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_t5.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_t5.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ffinetune_t5.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "640828bacd3401d94fbe6cd74d816d474719682e",
            "filename": "examples/research_projects/seq2seq-distillation/lightning_base.py",
            "status": "removed",
            "additions": 0,
            "deletions": 393,
            "changes": 393,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Flightning_base.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "83e014bf481e815eb099eab828f5d9f8aa9fa1e8",
            "filename": "examples/research_projects/seq2seq-distillation/make_student.py",
            "status": "removed",
            "additions": 0,
            "deletions": 186,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fmake_student.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fmake_student.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fmake_student.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "fb2713ccde84ba06195ff9c17fb7f2d1eb7b104c",
            "filename": "examples/research_projects/seq2seq-distillation/precomputed_pseudo_labels.md",
            "status": "removed",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fprecomputed_pseudo_labels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fprecomputed_pseudo_labels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fprecomputed_pseudo_labels.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "533f6339ab0898c0e4ec86da5e6fb08f48d9894e",
            "filename": "examples/research_projects/seq2seq-distillation/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "54ad6c6fb6b637c7ce4b6a57a13984809d841a6a",
            "filename": "examples/research_projects/seq2seq-distillation/run_eval.py",
            "status": "removed",
            "additions": 0,
            "deletions": 167,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Frun_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Frun_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Frun_eval.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c5acec73928ccd00dcf049601ebdf37bcdf4cfea",
            "filename": "examples/research_projects/seq2seq-distillation/sentence_splitter.py",
            "status": "removed",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fsentence_splitter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fsentence_splitter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Fsentence_splitter.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6a1bafbdc9c8c944e407bb766a1e5fe6177b0404",
            "filename": "examples/research_projects/seq2seq-distillation/train_distilbart_cnn.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_distilbart_cnn.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_distilbart_cnn.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_distilbart_cnn.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "86a3440fc0c0d48c3a0cbd73906e1c2be3ed1ff7",
            "filename": "examples/research_projects/seq2seq-distillation/train_distilbart_xsum.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_distilbart_xsum.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_distilbart_xsum.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_distilbart_xsum.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "54e7935ff60d96dc814b3cfa61a4ad4cafc7939b",
            "filename": "examples/research_projects/seq2seq-distillation/train_mbart_cc25_enro.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_mbart_cc25_enro.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_mbart_cc25_enro.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Ftrain_mbart_cc25_enro.sh?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "de666e0c24900206999b17002bb90bcfaf4f8cf8",
            "filename": "examples/research_projects/seq2seq-distillation/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 645,
            "changes": 645,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fseq2seq-distillation%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fseq2seq-distillation%2Futils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "30ab999037374d21158a50b42cec3a026c1fbff3",
            "filename": "examples/research_projects/synthid_text/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "35d0ea22f42b23f3463b2a7fec8fc955bbb617a5",
            "filename": "examples/research_projects/synthid_text/detector_training.py",
            "status": "removed",
            "additions": 0,
            "deletions": 502,
            "changes": 502,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2Fdetector_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2Fdetector_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2Fdetector_training.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9e40a93ee08f09b6204da34ba8fd76a555aef1a7",
            "filename": "examples/research_projects/synthid_text/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "abcb6ca2f2825503906d5500a184ce4664f3fbbf",
            "filename": "examples/research_projects/synthid_text/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 408,
            "changes": 408,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fsynthid_text%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2Futils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "b98eb9b428d01cd298a6049e16fadbfbf31e9daa",
            "filename": "examples/research_projects/tapex/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 288,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftapex%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "2379012a9b236945c1fce0b7a0d9991e3a9c054e",
            "filename": "examples/research_projects/tapex/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftapex%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "5dcec10a084c5f2ca9abeaeb49475b73694a41da",
            "filename": "examples/research_projects/tapex/run_tabfact_with_tapex.py",
            "status": "removed",
            "additions": 0,
            "deletions": 471,
            "changes": 471,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frun_tabfact_with_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frun_tabfact_with_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftapex%2Frun_tabfact_with_tapex.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "81e940a77c882c2b80996211c14f7e52337c2515",
            "filename": "examples/research_projects/tapex/run_wikisql_with_tapex.py",
            "status": "removed",
            "additions": 0,
            "deletions": 649,
            "changes": 649,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frun_wikisql_with_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frun_wikisql_with_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftapex%2Frun_wikisql_with_tapex.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "55350025cb3bb4c02139461a3f95526a76e8becb",
            "filename": "examples/research_projects/tapex/run_wikitablequestions_with_tapex.py",
            "status": "removed",
            "additions": 0,
            "deletions": 625,
            "changes": 625,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frun_wikitablequestions_with_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Frun_wikitablequestions_with_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftapex%2Frun_wikitablequestions_with_tapex.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "13d10e091a10c106a78ed16598d2ad459c8f8479",
            "filename": "examples/research_projects/tapex/wikisql_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 257,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Fwikisql_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftapex%2Fwikisql_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftapex%2Fwikisql_utils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "f3594f32dc7ad446c271b7d8ca173ee86f170194",
            "filename": "examples/research_projects/token-healing/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftoken-healing%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftoken-healing%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftoken-healing%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "2dd9148c1bcc58fabe15280fed2005dbac0604d2",
            "filename": "examples/research_projects/token-healing/run_token_healing.py",
            "status": "removed",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftoken-healing%2Frun_token_healing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Ftoken-healing%2Frun_token_healing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Ftoken-healing%2Frun_token_healing.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "ec197ce5f350aaf20b9a1533f3a836053d8d420c",
            "filename": "examples/research_projects/visual_bert/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "9f61beea8e249c7f9766eb87a99beb4be9e10745",
            "filename": "examples/research_projects/visual_bert/demo.ipynb",
            "status": "removed",
            "additions": 0,
            "deletions": 255,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fdemo.ipynb",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fdemo.ipynb",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Fdemo.ipynb?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "6b1342c9b11f93839e3cdda845b9fef1379177b2",
            "filename": "examples/research_projects/visual_bert/extracting_data.py",
            "status": "removed",
            "additions": 0,
            "deletions": 149,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fextracting_data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fextracting_data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Fextracting_data.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "c7c3bf376ce382bd83df30732dffaaf6c10db57c",
            "filename": "examples/research_projects/visual_bert/modeling_frcnn.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1920,
            "changes": 1920,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fmodeling_frcnn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fmodeling_frcnn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Fmodeling_frcnn.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "65f8f6cd377c9f6864362debdd9b294a85dae2cd",
            "filename": "examples/research_projects/visual_bert/processing_image.py",
            "status": "removed",
            "additions": 0,
            "deletions": 151,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fprocessing_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fprocessing_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Fprocessing_image.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "e2778663a53c723890e14a16ae1c166c66a31cf6",
            "filename": "examples/research_projects/visual_bert/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "995fbd2c19aecfdde4d1b288eac05c074012a4a1",
            "filename": "examples/research_projects/visual_bert/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 554,
            "changes": 554,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Futils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "dcfd8426ff4f36a2d5cc4259704f2ac67bf31290",
            "filename": "examples/research_projects/visual_bert/visualizing_image.py",
            "status": "removed",
            "additions": 0,
            "deletions": 500,
            "changes": 500,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fvisualizing_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvisual_bert%2Fvisualizing_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Fvisualizing_image.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "a74bf9209b0a9a7db269c672de85aacb7b76b170",
            "filename": "examples/research_projects/vqgan-clip/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvqgan-clip%2FREADME.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "1bfbc4cd5c36f30b4d6d77d378cb01c08caedafe",
            "filename": "examples/research_projects/vqgan-clip/VQGAN_CLIP.py",
            "status": "removed",
            "additions": 0,
            "deletions": 268,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2FVQGAN_CLIP.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2FVQGAN_CLIP.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvqgan-clip%2FVQGAN_CLIP.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "221ebd86dae785b4059a160b0f3d4c881977976f",
            "filename": "examples/research_projects/vqgan-clip/img_processing.py",
            "status": "removed",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Fimg_processing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Fimg_processing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvqgan-clip%2Fimg_processing.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "88513bcb69180dbb3e20d1ecccff5209f8778a52",
            "filename": "examples/research_projects/vqgan-clip/loaders.py",
            "status": "removed",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Floaders.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Floaders.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvqgan-clip%2Floaders.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "197616324224cf369828f6fb9ceb767df93eb4a5",
            "filename": "examples/research_projects/vqgan-clip/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvqgan-clip%2Frequirements.txt?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7db45fcbb52b0fa3f82226194ff7c824fd873184",
            "filename": "examples/research_projects/vqgan-clip/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fvqgan-clip%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvqgan-clip%2Futils.py?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        },
        {
            "sha": "7a580a36132441f85d15f6f329169c7cdbf667cd",
            "filename": "examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md",
            "status": "removed",
            "additions": 0,
            "deletions": 516,
            "changes": 516,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fwav2vec2%2FFINE_TUNE_XLSR_WAV2VEC2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed1807bab372a524446d9293fd09e06d2b6e79ec/examples%2Fresearch_projects%2Fwav2vec2%2FFINE_TUNE_XLSR_WAV2VEC2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fwav2vec2%2FFINE_TUNE_XLSR_WAV2VEC2.md?ref=ed1807bab372a524446d9293fd09e06d2b6e79ec"
        }
    ],
    "stats": {
        "total": 63688,
        "additions": 39,
        "deletions": 63649
    }
}