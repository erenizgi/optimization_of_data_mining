{
    "author": "faaany",
    "message": "[tests] skip tests for xpu  (#33553)\n\n* enable\r\n\r\n* fix\r\n\r\n* add xpu skip\r\n\r\n* add marker\r\n\r\n* skip for xpu\r\n\r\n* add more\r\n\r\n* add one more",
    "sha": "b87755aa6d06253436b9451b9ea0bc1f448733f3",
    "files": [
        {
            "sha": "b86e3af91ca727b347622cd4f77cd654b333be8a",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b87755aa6d06253436b9451b9ea0bc1f448733f3/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b87755aa6d06253436b9451b9ea0bc1f448733f3/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=b87755aa6d06253436b9451b9ea0bc1f448733f3",
            "patch": "@@ -849,6 +849,13 @@ def require_torch_xpu(test_case):\n     return unittest.skipUnless(is_torch_xpu_available(), \"test requires XPU device\")(test_case)\n \n \n+def require_non_xpu(test_case):\n+    \"\"\"\n+    Decorator marking a test that should be skipped for XPU.\n+    \"\"\"\n+    return unittest.skipUnless(torch_device != \"xpu\", \"test requires a non-XPU\")(test_case)\n+\n+\n def require_torch_multi_xpu(test_case):\n     \"\"\"\n     Decorator marking a test that requires a multi-XPU setup (in PyTorch). These tests are skipped on a machine without"
        },
        {
            "sha": "4f6cf7dffa1ea3c2242871eb2254180a631fafb0",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=b87755aa6d06253436b9451b9ea0bc1f448733f3",
            "patch": "@@ -31,6 +31,7 @@\n     get_torch_dist_unique_port,\n     require_apex,\n     require_bitsandbytes,\n+    require_non_xpu,\n     require_torch,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n@@ -106,6 +107,7 @@ def test_run_seq2seq_dp(self):\n     def test_run_seq2seq_ddp(self):\n         self.run_seq2seq_quick(distributed=True)\n \n+    @require_non_xpu\n     @require_apex\n     @require_torch_gpu\n     def test_run_seq2seq_apex(self):"
        },
        {
            "sha": "94cc4e95432c118760511077d388629201d6ea88",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=b87755aa6d06253436b9451b9ea0bc1f448733f3",
            "patch": "@@ -16,7 +16,14 @@\n \n import unittest\n \n-from transformers.testing_utils import require_detectron2, require_torch, require_torch_multi_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    require_detectron2,\n+    require_non_xpu,\n+    require_torch,\n+    require_torch_multi_gpu,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_detectron2_available, is_torch_available\n \n from ...test_configuration_common import ConfigTester\n@@ -251,6 +258,7 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n+@require_non_xpu\n @require_torch\n @require_detectron2\n class LayoutLMv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "1ad6e93b10ff847d1106a0ef1c264dbd80091ce1",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b87755aa6d06253436b9451b9ea0bc1f448733f3",
            "patch": "@@ -76,6 +76,7 @@\n     require_accelerate,\n     require_bitsandbytes,\n     require_flash_attn,\n+    require_non_xpu,\n     require_read_token,\n     require_safetensors,\n     require_torch,\n@@ -2884,6 +2885,7 @@ def test_inputs_embeds_matches_input_ids_with_generate(self):\n                 )\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    @require_non_xpu\n     @require_torch_multi_gpu\n     def test_multi_gpu_data_parallel_forward(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -4118,6 +4120,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n                     _ = model(**inputs_dict)\n \n+    @require_non_xpu\n     @require_torch_sdpa\n     @require_torch_accelerator\n     @slow"
        },
        {
            "sha": "14014e4a0947cd3814a4efd27e297675a3ed650d",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=b87755aa6d06253436b9451b9ea0bc1f448733f3",
            "patch": "@@ -66,6 +66,7 @@\n     require_intel_extension_for_pytorch,\n     require_liger_kernel,\n     require_lomo,\n+    require_non_xpu,\n     require_optuna,\n     require_peft,\n     require_ray,\n@@ -884,6 +885,7 @@ def test_mixed_bf16(self):\n \n         # will add more specific tests once there are some bugs to fix\n \n+    @require_non_xpu\n     @require_torch_gpu\n     @require_torch_tf32\n     def test_tf32(self):\n@@ -3196,6 +3198,7 @@ def test_fp16_full_eval(self):\n         # perfect world: fp32_init/2 == fp16_eval\n         self.assertAlmostEqual(fp16_eval, fp32_init / 2, delta=5_000)\n \n+    @require_non_xpu\n     @require_torch_non_multi_gpu\n     @require_torchdynamo\n     @require_torch_tensorrt_fx"
        },
        {
            "sha": "3e8c80de2d16768a4a6accc08151dc84d3e78e73",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b87755aa6d06253436b9451b9ea0bc1f448733f3/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=b87755aa6d06253436b9451b9ea0bc1f448733f3",
            "patch": "@@ -22,6 +22,7 @@\n from transformers.testing_utils import (\n     is_torch_available,\n     require_auto_gptq,\n+    require_non_xpu,\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n@@ -317,6 +318,7 @@ def test_hybrid_cache_n_sequences(self):\n         ]\n         self.assertListEqual(decoded, expected_text)\n \n+    @require_non_xpu\n     @require_auto_gptq\n     def test_sink_cache_hard(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMa-7B-GPTQ\")"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 26,
        "deletions": 1
    }
}