{
    "author": "yijun-lee",
    "message": "ğŸŒ [i18n-KO] Translated `generation_utils.md` to Korean (#33818)\n\n* docs: ko: generation_utils.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: manual edits\r\n\r\n* \bfix: resolve suggestions\r\n\r\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\r\n\r\n* Update generation_utils.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Woojun Jung <46880056+jungnerd@users.noreply.github.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "88d01d911934bde8da38c4b60622c6549a7acbb4",
    "files": [
        {
            "sha": "43ff0017c8f842a5f9430e403322b8504a6159d2",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/88d01d911934bde8da38c4b60622c6549a7acbb4/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/88d01d911934bde8da38c4b60622c6549a7acbb4/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=88d01d911934bde8da38c4b60622c6549a7acbb4",
            "patch": "@@ -785,8 +785,8 @@\n       title: í† í¬ë‚˜ì´ì €ë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Utilities for Trainer\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Utilities for Generation\n+    - local: internal/generation_utils\n+      title: ìƒì„±ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°\n     - local: internal/image_processing_utils\n       title: ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ìœ„í•œ ìœ í‹¸ë¦¬í‹°\n     - local: internal/audio_utils"
        },
        {
            "sha": "a66411f71e9ee8ebb8aa293fa540bda77eeb2d2f",
            "filename": "docs/source/ko/internal/generation_utils.md",
            "status": "added",
            "additions": 413,
            "deletions": 0,
            "changes": 413,
            "blob_url": "https://github.com/huggingface/transformers/blob/88d01d911934bde8da38c4b60622c6549a7acbb4/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/88d01d911934bde8da38c4b60622c6549a7acbb4/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fgeneration_utils.md?ref=88d01d911934bde8da38c4b60622c6549a7acbb4",
            "patch": "@@ -0,0 +1,413 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ìƒì„±ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° [[utilities-for-generation]]\n+\n+ì´ í˜ì´ì§€ëŠ” [`~generation.GenerationMixin.generate`]ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëª¨ë“  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ë‚˜ì—´í•©ë‹ˆë‹¤.\n+\n+## ì¶œë ¥ì„ ìƒì„±í•˜ê¸° (Generate Outputs) [[generate-outputs]]\n+\n+[`~generation.GenerationMixin.generate`]ì˜ ì¶œë ¥ì€ [`~utils.ModelOutput`]ì˜ í•˜ìœ„ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì…ë‹ˆë‹¤. ì´ ì¶œë ¥ì€ [`~generation.GenerationMixin.generate`]ì—ì„œ ë°˜í™˜ë˜ëŠ” ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„° êµ¬ì¡°ì²´ì´ë©°, íŠœí”Œ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ë‹¤ìŒì€ ì˜ˆì‹œì…ë‹ˆë‹¤:\n+\n+```python\n+from transformers import GPT2Tokenizer, GPT2LMHeadModel\n+\n+tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n+model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n+\n+inputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\n+generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n+```\n+\n+`generation_output` ê°ì²´ëŠ” [`~generation.GenerateDecoderOnlyOutput`]ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì†ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:\n+\n+- `sequences`: ìƒì„±ëœ í† í° ì‹œí€€ìŠ¤\n+- `scores` (ì˜µì…˜): ê° ìƒì„± ë‹¨ê³„ì—ì„œ ì–¸ì–´ ëª¨ë¸ë§ í—¤ë“œì˜ ì˜ˆì¸¡ ì ìˆ˜\n+- `hidden_states` (ì˜µì…˜): ê° ìƒì„± ë‹¨ê³„ì—ì„œ ëª¨ë¸ì˜ ì€ë‹‰ ìƒíƒœ\n+- `attentions` (ì˜µì…˜): ê° ìƒì„± ë‹¨ê³„ì—ì„œ ëª¨ë¸ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜\n+\n+`output_scores=True`ë¥¼ ì „ë‹¬í–ˆê¸° ë•Œë¬¸ì— `scores`ëŠ” í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, `output_hidden_states=True` ë˜ëŠ” `output_attentions=True`ë¥¼ ì „ë‹¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ `hidden_states`ì™€ `attentions`ëŠ” í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n+\n+ê° ì†ì„±ì€ ì¼ë°˜ì ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì´ í•´ë‹¹ ì†ì„±ì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ë‹¤ë©´ `None`ì´ ë°˜í™˜ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `generation_output.scores`ëŠ” ì–¸ì–´ ëª¨ë¸ë§ í—¤ë“œì—ì„œ ìƒì„±ëœ ëª¨ë“  ì˜ˆì¸¡ ì ìˆ˜ë¥¼ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, `generation_output.attentions`ëŠ” `None`ì…ë‹ˆë‹¤.\n+\n+`generation_output` ê°ì²´ë¥¼ íŠœí”Œë¡œ ì‚¬ìš©í•  ê²½ìš°, `None` ê°’ì´ ì•„ë‹Œ ì†ì„±ë§Œ í¬í•¨ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `loss`ì™€ `logits`ë¼ëŠ” ë‘ ìš”ì†Œê°€ í¬í•¨ëœ ê²½ìš°:\n+\n+```python\n+generation_output[:2]\n+```\n+\n+ìœ„ ì½”ë“œëŠ” `(generation_output.sequences, generation_output.scores)` íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n+\n+`generation_output` ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì‚¬ìš©í•  ê²½ìš°, `None` ê°’ì´ ì•„ë‹Œ ì†ì„±ë§Œ í¬í•¨ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `sequences`ì™€ `scores`ë¼ëŠ” ë‘ ê°œì˜ í‚¤ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì—¬ê¸°ì„œëŠ” ëª¨ë“  ì¶œë ¥ ìœ í˜•ì„ ë¬¸ì„œí™”í•©ë‹ˆë‹¤.\n+\n+\n+### PyTorch [[transformers.generation.GenerateDecoderOnlyOutput]]\n+\n+[[autodoc]] generation.GenerateDecoderOnlyOutput\n+\n+[[autodoc]] generation.GenerateEncoderDecoderOutput\n+\n+[[autodoc]] generation.GenerateBeamDecoderOnlyOutput\n+\n+[[autodoc]] generation.GenerateBeamEncoderDecoderOutput\n+\n+### TensorFlow [[transformers.generation.TFGreedySearchEncoderDecoderOutput]]\n+\n+[[autodoc]] generation.TFGreedySearchEncoderDecoderOutput\n+\n+[[autodoc]] generation.TFGreedySearchDecoderOnlyOutput\n+\n+[[autodoc]] generation.TFSampleEncoderDecoderOutput\n+\n+[[autodoc]] generation.TFSampleDecoderOnlyOutput\n+\n+[[autodoc]] generation.TFBeamSearchEncoderDecoderOutput\n+\n+[[autodoc]] generation.TFBeamSearchDecoderOnlyOutput\n+\n+[[autodoc]] generation.TFBeamSampleEncoderDecoderOutput\n+\n+[[autodoc]] generation.TFBeamSampleDecoderOnlyOutput\n+\n+[[autodoc]] generation.TFContrastiveSearchEncoderDecoderOutput\n+\n+[[autodoc]] generation.TFContrastiveSearchDecoderOnlyOutput\n+\n+### FLAX [[transformers.generation.FlaxSampleOutput]]\n+\n+[[autodoc]] generation.FlaxSampleOutput\n+\n+[[autodoc]] generation.FlaxGreedySearchOutput\n+\n+[[autodoc]] generation.FlaxBeamSearchOutput\n+\n+## LogitsProcessor [[logitsprocessor]]\n+\n+[`LogitsProcessor`]ëŠ” ìƒì„± ì¤‘ ì–¸ì–´ ëª¨ë¸ í—¤ë“œì˜ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n+\n+### PyTorch [[transformers.AlternatingCodebooksLogitsProcessor]]\n+\n+[[autodoc]] AlternatingCodebooksLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] ClassifierFreeGuidanceLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] EncoderNoRepeatNGramLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] EncoderRepetitionPenaltyLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] EpsilonLogitsWarper\n+    - __call__\n+\n+[[autodoc]] EtaLogitsWarper\n+    - __call__\n+\n+[[autodoc]] ExponentialDecayLengthPenalty\n+    - __call__\n+\n+[[autodoc]] ForcedBOSTokenLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] ForcedEOSTokenLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] HammingDiversityLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] InfNanRemoveLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] LogitNormalization\n+    - __call__\n+\n+[[autodoc]] LogitsProcessor\n+    - __call__\n+\n+[[autodoc]] LogitsProcessorList\n+    - __call__\n+\n+[[autodoc]] MinLengthLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] MinNewTokensLengthLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] MinPLogitsWarper\n+    - __call__\n+\n+[[autodoc]] NoBadWordsLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] NoRepeatNGramLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] PrefixConstrainedLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] RepetitionPenaltyLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] SequenceBiasLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] SuppressTokensAtBeginLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] SuppressTokensLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TemperatureLogitsWarper\n+    - __call__\n+\n+[[autodoc]] TopKLogitsWarper\n+    - __call__\n+\n+[[autodoc]] TopPLogitsWarper\n+    - __call__\n+\n+[[autodoc]] TypicalLogitsWarper\n+    - __call__\n+\n+[[autodoc]] UnbatchedClassifierFreeGuidanceLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] WhisperTimeStampLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] WatermarkLogitsProcessor\n+    - __call__\n+\n+\n+### TensorFlow [[transformers.TFForcedBOSTokenLogitsProcessor]]\n+\n+[[autodoc]] TFForcedBOSTokenLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFForcedEOSTokenLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFForceTokensLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFLogitsProcessorList\n+    - __call__\n+\n+[[autodoc]] TFLogitsWarper\n+    - __call__\n+\n+[[autodoc]] TFMinLengthLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFNoBadWordsLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFNoRepeatNGramLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFRepetitionPenaltyLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFSuppressTokensAtBeginLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFSuppressTokensLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] TFTemperatureLogitsWarper\n+    - __call__\n+\n+[[autodoc]] TFTopKLogitsWarper\n+    - __call__\n+\n+[[autodoc]] TFTopPLogitsWarper\n+    - __call__\n+\n+### FLAX [[transformers.FlaxForcedBOSTokenLogitsProcessor]]\n+\n+[[autodoc]] FlaxForcedBOSTokenLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxForcedEOSTokenLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxForceTokensLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxLogitsProcessorList\n+    - __call__\n+\n+[[autodoc]] FlaxLogitsWarper\n+    - __call__\n+\n+[[autodoc]] FlaxMinLengthLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxSuppressTokensAtBeginLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxSuppressTokensLogitsProcessor\n+    - __call__\n+\n+[[autodoc]] FlaxTemperatureLogitsWarper\n+    - __call__\n+\n+[[autodoc]] FlaxTopKLogitsWarper\n+    - __call__\n+\n+[[autodoc]] FlaxTopPLogitsWarper\n+    - __call__\n+\n+[[autodoc]] FlaxWhisperTimeStampLogitsProcessor\n+    - __call__\n+\n+## StoppingCriteria [[transformers.StoppingCriteria]]\n+\n+[`StoppingCriteria`]ëŠ” ìƒì„±ì´ ì–¸ì œ ë©ˆì¶œì§€ë¥¼ ê²°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤ (EOS í† í° ì™¸). ì´ ê¸°ëŠ¥ì€ PyTorch êµ¬í˜„ì—ë§Œ ì œê³µë©ë‹ˆë‹¤.\n+\n+[[autodoc]] StoppingCriteria\n+    - __call__\n+\n+[[autodoc]] StoppingCriteriaList\n+    - __call__\n+\n+[[autodoc]] MaxLengthCriteria\n+    - __call__\n+\n+[[autodoc]] MaxTimeCriteria\n+    - __call__\n+\n+[[autodoc]] StopStringCriteria\n+    - __call__\n+\n+[[autodoc]] EosTokenCriteria\n+    - __call__\n+\n+## Constraint [[transformers.Constraint]]\n+\n+[`Constraint`]ëŠ” ìƒì„± ì¶œë ¥ì— íŠ¹ì • í† í°ì´ë‚˜ ì‹œí€€ìŠ¤ë¥¼ ê°•ì œë¡œ í¬í•¨ì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì€ PyTorch êµ¬í˜„ì—ë§Œ ì œê³µë©ë‹ˆë‹¤.\n+\n+[[autodoc]] Constraint\n+\n+[[autodoc]] PhrasalConstraint\n+\n+[[autodoc]] DisjunctiveConstraint\n+\n+[[autodoc]] ConstraintListState\n+\n+## ë¹” ê²€ìƒ‰ (BeamSearch) [[transformers.BeamScorer]]\n+\n+[[autodoc]] BeamScorer\n+    - process\n+    - finalize\n+\n+[[autodoc]] BeamSearchScorer\n+    - process\n+    - finalize\n+\n+[[autodoc]] ConstrainedBeamSearchScorer\n+    - process\n+    - finalize\n+\n+## ìŠ¤íŠ¸ë¦¬ë¨¸ (Streamers) [[transformers.TextStreamer]]\n+\n+[[autodoc]] TextStreamer\n+\n+[[autodoc]] TextIteratorStreamer\n+\n+## ìºì‹œ (Caches) [[transformers.Cache]]\n+\n+[[autodoc]] Cache\n+    - update\n+\n+[[autodoc]] CacheConfig\n+    - update\n+\n+[[autodoc]] QuantizedCacheConfig\n+    - validate\n+\n+[[autodoc]] DynamicCache\n+    - update\n+    - get_seq_length\n+    - reorder_cache\n+    - to_legacy_cache\n+    - from_legacy_cache\n+\n+[[autodoc]] QuantizedCache\n+    - update\n+    - get_seq_length\n+\n+[[autodoc]] QuantoQuantizedCache\n+\n+[[autodoc]] HQQQuantizedCache\n+\n+[[autodoc]] SinkCache\n+    - update\n+    - get_seq_length\n+    - reorder_cache\n+\n+[[autodoc]] OffloadedCache\n+    - update\n+    - prefetch_layer\n+    - evict_previous_layer\n+\n+[[autodoc]] StaticCache\n+    - update\n+    - get_seq_length\n+    - reset\n+\n+[[autodoc]] OffloadedStaticCache\n+    - update\n+    - get_seq_length\n+    - reset\n+\n+[[autodoc]] HybridCache\n+    - update\n+    - get_seq_length\n+    - reset\n+\n+[[autodoc]] SlidingWindowCache\n+    - update\n+    - reset\n+\n+[[autodoc]] EncoderDecoderCache\n+    - get_seq_length\n+    - to_legacy_cache\n+    - from_legacy_cache\n+    - reset\n+    - reorder_cache\n+\n+[[autodoc]] MambaCache\n+    - update_conv_state\n+    - update_ssm_state\n+    - reset\n+\n+## ì›Œí„°ë§ˆí¬ ìœ í‹¸ë¦¬í‹° (Watermark Utils) [[transformers.WatermarkDetector]]\n+\n+[[autodoc]] WatermarkDetector\n+    - __call__"
        }
    ],
    "stats": {
        "total": 417,
        "additions": 415,
        "deletions": 2
    }
}