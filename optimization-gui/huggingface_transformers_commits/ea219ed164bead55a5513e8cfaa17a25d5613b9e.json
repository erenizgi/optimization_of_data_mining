{
    "author": "yonigozlan",
    "message": "Remove differences between init and preprocess kwargs for fast image processors (#36186)\n\n* Remove differences between init and preprocess kwargs in fast image processors\n\n* make modifs got_ocr2\n\n* update gemma3",
    "sha": "ea219ed164bead55a5513e8cfaa17a25d5613b9e",
    "files": [
        {
            "sha": "a87db33704ce2ee73e7a7edcd859c41aec72e61e",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 30,
            "deletions": 23,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -126,7 +126,7 @@ def divide_to_patches(\n     return patches\n \n \n-class DefaultFastImageProcessorInitKwargs(TypedDict, total=False):\n+class DefaultFastImageProcessorKwargs(TypedDict, total=False):\n     do_resize: Optional[bool]\n     size: Optional[Dict[str, int]]\n     default_to_square: Optional[bool]\n@@ -139,9 +139,6 @@ class DefaultFastImageProcessorInitKwargs(TypedDict, total=False):\n     image_mean: Optional[Union[float, List[float]]]\n     image_std: Optional[Union[float, List[float]]]\n     do_convert_rgb: Optional[bool]\n-\n-\n-class DefaultFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorInitKwargs):\n     return_tensors: Optional[Union[str, TensorType]]\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n@@ -185,8 +182,20 @@ class DefaultFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorInitKwa\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `self.image_std`):\n-            Whether to convert the image to RGB.\"\"\"\n+        do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+            Whether to convert the image to RGB.\n+        return_tensors (`str` or `TensorType`, *optional*, defaults to `self.return_tensors`):\n+            Returns stacked tensors if set to `pt, otherwise returns a list of tensors.\n+        data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.data_format`):\n+            Only `ChannelDimension.FIRST` is supported. Added for compatibility with slow processors.\n+        input_data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.input_data_format`):\n+            The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+            from the input image. Can be one of:\n+            - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+            - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+            - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        device (`torch.device`, *optional*, defaults to `self.device`):\n+            The device to process the images on. If unset, the device is inferred from the input images.\"\"\"\n \n BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS = r\"\"\"\n     Preprocess an image or batch of images.\n@@ -219,20 +228,17 @@ class DefaultFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorInitKwa\n             `True`.\n         do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n             Whether to convert the image to RGB.\n-        return_tensors (`str` or `TensorType`, *optional*):\n+        return_tensors (`str` or `TensorType`, *optional*, defaults to `self.return_tensors`):\n             Returns stacked tensors if set to `pt, otherwise returns a list of tensors.\n-        data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-            The channel dimension format for the output image. Can be one of:\n-            - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-            - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-            - Unset: Use the channel dimension format of the input image.\n-        input_data_format (`ChannelDimension` or `str`, *optional*):\n+        data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.data_format`):\n+            Only `ChannelDimension.FIRST` is supported. Added for compatibility with slow processors.\n+        input_data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.input_data_format`):\n             The channel dimension format for the input image. If unset, the channel dimension format is inferred\n             from the input image. Can be one of:\n             - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n             - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n             - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        device (`torch.device`, *optional*):\n+        device (`torch.device`, *optional*, defaults to `self.device`):\n             The device to process the images on. If unset, the device is inferred from the input images.\"\"\"\n \n \n@@ -253,13 +259,16 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     rescale_factor = 1 / 255\n     do_normalize = None\n     do_convert_rgb = None\n+    return_tensors = None\n+    data_format = ChannelDimension.FIRST\n+    input_data_format = None\n+    device = None\n     model_input_names = [\"pixel_values\"]\n-    valid_init_kwargs = DefaultFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = DefaultFastImageProcessorPreprocessKwargs\n+    valid_kwargs = DefaultFastImageProcessorKwargs\n \n     def __init__(\n         self,\n-        **kwargs: Unpack[DefaultFastImageProcessorInitKwargs],\n+        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n     ) -> None:\n         super().__init__(**kwargs)\n         size = kwargs.pop(\"size\", self.size)\n@@ -270,7 +279,7 @@ def __init__(\n         )\n         crop_size = kwargs.pop(\"crop_size\", self.crop_size)\n         self.crop_size = get_size_dict(crop_size, param_name=\"crop_size\") if crop_size is not None else None\n-        for key in self.valid_init_kwargs.__annotations__.keys():\n+        for key in self.valid_kwargs.__annotations__.keys():\n             kwarg = kwargs.pop(key, None)\n             if kwarg is not None:\n                 setattr(self, key, kwarg)\n@@ -553,14 +562,12 @@ def _prepare_process_arguments(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        **kwargs: Unpack[DefaultFastImageProcessorPreprocessKwargs],\n+        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        validate_kwargs(\n-            captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_preprocess_kwargs.__annotations__.keys()\n-        )\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_preprocess_kwargs.__annotations__:\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n             kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n \n         # Extract parameters that are only used for preparing the input images"
        },
        {
            "sha": "19f959f07d888882561f3f1362d3b561a687d455",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -21,8 +21,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -54,11 +53,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class ConvNextFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    crop_pct: Optional[float]\n-\n-\n-class ConvNextFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class ConvNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     crop_pct: Optional[float]\n \n \n@@ -81,10 +76,9 @@ class ConvNextImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     crop_pct = 224 / 256\n-    valid_init_kwargs = ConvNextFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = ConvNextFastImageProcessorPreprocessKwargs\n+    valid_kwargs = ConvNextFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[ConvNextFastImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[ConvNextFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @add_start_docstrings(\n@@ -95,9 +89,7 @@ def __init__(self, **kwargs: Unpack[ConvNextFastImageProcessorInitKwargs]):\n             overridden by `crop_pct` in the`preprocess` method.\n         \"\"\",\n     )\n-    def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[ConvNextFastImageProcessorPreprocessKwargs]\n-    ) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[ConvNextFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "850370e5933f655b8acb45de09610de260c7ea14",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -12,8 +12,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -58,21 +57,12 @@\n logger = logging.get_logger(__name__)\n \n \n-class DeformableDetrFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n+class DeformableDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n     do_pad: Optional[bool]\n     pad_size: Optional[Dict[str, int]]\n-\n-\n-class DeformableDetrFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n-    format: Optional[AnnotationFormat]\n-    annotations: Optional[Dict]\n-    do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[Dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n \n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n@@ -294,6 +284,8 @@ def prepare_coco_panoptic_annotation(\n             The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n             provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n             height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n     \"\"\",\n )\n class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n@@ -308,10 +300,9 @@ class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_init_kwargs = DeformableDetrFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = DeformableDetrFastImageProcessorPreprocessKwargs\n+    valid_kwargs = DeformableDetrFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[DeformableDetrFastImageProcessorInitKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -605,7 +596,11 @@ def pad(\n         \"\"\",\n     )\n     def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[DeformableDetrFastImageProcessorPreprocessKwargs]\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs],\n     ) -> BatchFeature:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n@@ -621,7 +616,7 @@ def preprocess(\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, **kwargs)\n+        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "8d29a5796f6b3695226da2eb2aec986d369853e1",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -24,8 +24,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     get_image_size_for_max_height_width,\n     get_max_height_width,\n@@ -283,21 +282,12 @@ def prepare_coco_panoptic_annotation(\n     return new_target\n \n \n-class DetrFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n+class DetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n     do_pad: Optional[bool]\n     pad_size: Optional[Dict[str, int]]\n-\n-\n-class DetrFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n-    format: Optional[AnnotationFormat]\n-    annotations: Optional[Dict]\n-    do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[Dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n \n \n @add_start_docstrings(\n@@ -319,6 +309,8 @@ class DetrFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocess\n             The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n             provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n             height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n     \"\"\",\n )\n class DetrImageProcessorFast(BaseImageProcessorFast):\n@@ -333,10 +325,9 @@ class DetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_init_kwargs = DetrFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = DetrFastImageProcessorPreprocessKwargs\n+    valid_kwargs = DetrFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[DetrFastImageProcessorInitKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[DetrFastImageProcessorKwargs]) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n \n@@ -629,7 +620,13 @@ def pad(\n             Path to the directory containing the segmentation masks.\n         \"\"\",\n     )\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[DetrFastImageProcessorPreprocessKwargs]) -> BatchFeature:\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[DetrFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once(\n@@ -644,7 +641,7 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[DetrFastImageProcessor\n             )\n             kwargs[\"size\"] = kwargs.pop(\"max_size\")\n \n-        return super().preprocess(images, **kwargs)\n+        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "50dfcb920fb01a6f62d619af95cea1af7d899b2e",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 18,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -24,8 +24,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n     BatchFeature,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     get_size_dict,\n     group_images_by_shape,\n     reorder_images,\n@@ -67,14 +66,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma3FastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    do_pan_and_scan: Optional[bool]\n-    pan_and_scan_min_crop_size: Optional[int]\n-    pan_and_scan_max_num_crops: Optional[int]\n-    pan_and_scan_min_ratio_to_activate: Optional[float]\n-\n-\n-class Gemma3FastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class Gemma3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     do_pan_and_scan: Optional[bool]\n     pan_and_scan_min_crop_size: Optional[int]\n     pan_and_scan_max_num_crops: Optional[int]\n@@ -108,10 +100,9 @@ class Gemma3ImageProcessorFast(BaseImageProcessorFast):\n     pan_and_scan_min_crop_size = None\n     pan_and_scan_max_num_crops = None\n     pan_and_scan_min_ratio_to_activate = None\n-    valid_init_kwargs = Gemma3FastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = Gemma3FastImageProcessorPreprocessKwargs\n+    valid_kwargs = Gemma3FastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[Gemma3FastImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[Gemma3FastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def _prepare_images_structure(\n@@ -262,14 +253,12 @@ def _process_images_for_pan_and_scan(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        **kwargs: Unpack[Gemma3FastImageProcessorPreprocessKwargs],\n+        **kwargs: Unpack[Gemma3FastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        validate_kwargs(\n-            captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_preprocess_kwargs.__annotations__.keys()\n-        )\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_preprocess_kwargs.__annotations__:\n+        for kwarg_name in self.valid_kwargs.__annotations__:\n             kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n \n         # Extract parameters that are only used for preparing the input images"
        },
        {
            "sha": "8498e378030868a30dd7dac537999d3aa8a39a63",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -21,8 +21,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -54,13 +53,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class GotOcr2ImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n-\n-\n-class GotOcr2ImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class GotOcr2ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     crop_to_patches: Optional[bool]\n     min_patches: Optional[int]\n     max_patches: Optional[int]\n@@ -93,10 +86,9 @@ class GotOcr2ImageProcessorFast(BaseImageProcessorFast):\n     crop_to_patches = False\n     min_patches = 1\n     max_patches = 12\n-    valid_init_kwargs = GotOcr2ImageProcessorInitKwargs\n-    valid_preprocess_kwargs = GotOcr2ImageProcessorPreprocessKwargs\n+    valid_kwargs = GotOcr2ImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[GotOcr2ImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[valid_kwargs]):\n         super().__init__(**kwargs)\n \n     @add_start_docstrings(\n@@ -113,7 +105,7 @@ def __init__(self, **kwargs: Unpack[GotOcr2ImageProcessorInitKwargs]):\n                 set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n         \"\"\",\n     )\n-    def preprocess(self, images: ImageInput, **kwargs: Unpack[GotOcr2ImageProcessorPreprocessKwargs]) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[valid_kwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def crop_image_to_patches("
        },
        {
            "sha": "d85eb89b7c79e25bbefa25f477cf512a44b3f593",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -23,8 +23,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -61,11 +60,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class LlavaFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    do_pad: Optional[bool]\n-\n-\n-class LlavaFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class LlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     do_pad: Optional[bool]\n \n \n@@ -90,10 +85,9 @@ class LlavaImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    valid_init_kwargs = LlavaFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = LlavaFastImageProcessorPreprocessKwargs\n+    valid_kwargs = LlavaFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[LlavaFastImageProcessorInitKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[LlavaFastImageProcessorKwargs]) -> None:\n         super().__init__(**kwargs)\n \n     @add_start_docstrings(\n@@ -103,9 +97,7 @@ def __init__(self, **kwargs: Unpack[LlavaFastImageProcessorInitKwargs]) -> None:\n                 Whether to pad the image to a square based on the longest edge. Can be overridden by the `do_pad` parameter\n         \"\"\",\n     )\n-    def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[LlavaFastImageProcessorPreprocessKwargs]\n-    ) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def pad_to_square("
        },
        {
            "sha": "d4caf2a19a23ed4e454072086e9e12c438da50b0",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -21,8 +21,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     divide_to_patches,\n     group_images_by_shape,\n     reorder_images,\n@@ -57,12 +56,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class LlavaNextFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    image_grid_pinpoints: Optional[List[List[int]]]\n-    do_pad: Optional[bool]\n-\n-\n-class LlavaNextFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class LlavaNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     image_grid_pinpoints: Optional[List[List[int]]]\n     do_pad: Optional[bool]\n \n@@ -96,10 +90,9 @@ class LlavaNextImageProcessorFast(BaseImageProcessorFast):\n     do_convert_rgb = True\n     do_pad = True\n     image_grid_pinpoints = [[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]]\n-    valid_init_kwargs = LlavaNextFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = LlavaNextFastImageProcessorPreprocessKwargs\n+    valid_kwargs = LlavaNextFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[LlavaNextFastImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @add_start_docstrings(\n@@ -113,9 +106,7 @@ def __init__(self, **kwargs: Unpack[LlavaNextFastImageProcessorInitKwargs]):\n                     number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n         \"\"\",\n     )\n-    def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[LlavaNextFastImageProcessorPreprocessKwargs]\n-    ) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _prepare_images_structure("
        },
        {
            "sha": "598ac78f538c70161779edc8b4f81293f8b6725a",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -12,8 +12,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     divide_to_patches,\n     group_images_by_shape,\n     reorder_images,\n@@ -40,12 +39,7 @@\n     from torchvision.transforms import functional as F\n \n \n-class LlavaOnevisionFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    image_grid_pinpoints: Optional[List[List[int]]]\n-    do_pad: Optional[bool]\n-\n-\n-class LlavaOnevisionFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     image_grid_pinpoints: Optional[List[List[int]]]\n     do_pad: Optional[bool]\n \n@@ -77,11 +71,10 @@ class LlavaOnevisionImageProcessorFast(BaseImageProcessorFast):\n     do_convert_rgb = True\n     do_pad = True\n     image_grid_pinpoints = [[384, 384], [384, 768], [384, 1152], [384, 1536], [384, 1920], [384, 2304], [768, 384], [768, 768], [768, 1152], [768, 1536], [768, 1920], [768, 2304], [1152, 384], [1152, 768], [1152, 1152], [1152, 1536], [1152, 1920], [1152, 2304], [1536, 384], [1536, 768], [1536, 1152], [1536, 1536], [1536, 1920], [1536, 2304], [1920, 384], [1920, 768], [1920, 1152], [1920, 1536], [1920, 1920], [1920, 2304], [2304, 384], [2304, 768], [2304, 1152], [2304, 1536], [2304, 1920], [2304, 2304]]  # fmt: skip\n-    valid_init_kwargs = LlavaOnevisionFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = LlavaOnevisionFastImageProcessorPreprocessKwargs\n+    valid_kwargs = LlavaOnevisionFastImageProcessorKwargs\n     model_input_names = [\"pixel_values_videos\"]\n \n-    def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @add_start_docstrings(\n@@ -95,9 +88,7 @@ def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorInitKwargs])\n                     number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n         \"\"\",\n     )\n-    def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorPreprocessKwargs]\n-    ) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def _prepare_images_structure("
        },
        {
            "sha": "0cb4673038536180b062fa2cfb252c32a959c785",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -21,8 +21,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -61,11 +60,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class PixtralFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n-    patch_size: Optional[Dict[str, int]]\n-\n-\n-class PixtralFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+class PixtralFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     patch_size: Optional[Dict[str, int]]\n \n \n@@ -88,10 +83,9 @@ class PixtralImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     do_normalize = True\n     do_convert_rgb = True\n-    valid_init_kwargs = PixtralFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = PixtralFastImageProcessorPreprocessKwargs\n+    valid_kwargs = PixtralFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[PixtralFastImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[PixtralFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @add_start_docstrings(\n@@ -101,9 +95,7 @@ def __init__(self, **kwargs: Unpack[PixtralFastImageProcessorInitKwargs]):\n             Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n         \"\"\",\n     )\n-    def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[PixtralFastImageProcessorPreprocessKwargs]\n-    ) -> BatchFeature:\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[PixtralFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n     def resize("
        },
        {
            "sha": "b54f86bba66f357403281988ea1ebba92717c34d",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -25,7 +25,7 @@\n from ...image_processing_utils_fast import (\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n+    DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -69,7 +69,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Qwen2VLFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n+class Qwen2VLFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     min_pixels: Optional[int]\n     max_pixels: Optional[int]\n     patch_size: Optional[int]\n@@ -107,10 +107,10 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     merge_size = 2\n     min_pixels = 56 * 56\n     max_pixels = 28 * 28 * 1280\n-    valid_init_kwargs = Qwen2VLFastImageProcessorInitKwargs\n+    valid_kwargs = DefaultFastImageProcessorKwargs\n     model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n \n-    def __init__(self, **kwargs: Unpack[Qwen2VLFastImageProcessorInitKwargs]):\n+    def __init__(self, **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "bd34843645ae8b714f00ddd35dad46d2b42ce853",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -12,8 +12,7 @@\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n-    DefaultFastImageProcessorInitKwargs,\n-    DefaultFastImageProcessorPreprocessKwargs,\n+    DefaultFastImageProcessorKwargs,\n     SizeDict,\n     add_start_docstrings,\n     get_image_size_for_max_height_width,\n@@ -53,21 +52,12 @@\n     from torchvision.transforms import functional as F\n \n \n-class RTDetrFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n+class RTDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n     do_pad: Optional[bool]\n     pad_size: Optional[Dict[str, int]]\n-\n-\n-class RTDetrFastImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n-    format: Optional[AnnotationFormat]\n-    annotations: Optional[Dict]\n-    do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[Dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n-    masks_path: Optional[Union[str, pathlib.Path]]\n \n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n@@ -151,6 +141,8 @@ def prepare_coco_detection_annotation(\n             The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n             provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n             height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n     \"\"\",\n )\n class RTDetrImageProcessorFast(BaseImageProcessorFast):\n@@ -165,11 +157,10 @@ class RTDetrImageProcessorFast(BaseImageProcessorFast):\n     size = {\"height\": 640, \"width\": 640}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_init_kwargs = RTDetrFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = RTDetrFastImageProcessorPreprocessKwargs\n+    valid_kwargs = RTDetrFastImageProcessorKwargs\n     do_convert_annotations = True\n \n-    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorInitKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n         # Backwards compatibility\n         do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n         do_normalize = kwargs.get(\"do_normalize\", None)\n@@ -424,9 +415,13 @@ def pad(\n         \"\"\",\n     )\n     def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[RTDetrFastImageProcessorPreprocessKwargs]\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, **kwargs)\n+        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "e1ee97b4dad204bb834a153ae47e01929fca6e44",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -2,8 +2,7 @@\n from typing import Dict, List, Optional, Tuple, Union\n \n from transformers.models.detr.image_processing_detr_fast import (\n-    DetrFastImageProcessorInitKwargs,\n-    DetrFastImageProcessorPreprocessKwargs,\n+    DetrFastImageProcessorKwargs,\n     DetrImageProcessorFast,\n )\n \n@@ -112,11 +111,7 @@ def prepare_coco_detection_annotation(\n     return new_target\n \n \n-class RTDetrFastImageProcessorInitKwargs(DetrFastImageProcessorInitKwargs):\n-    pass\n-\n-\n-class RTDetrFastImageProcessorPreprocessKwargs(DetrFastImageProcessorPreprocessKwargs):\n+class RTDetrFastImageProcessorKwargs(DetrFastImageProcessorKwargs):\n     pass\n \n \n@@ -133,10 +128,9 @@ class RTDetrImageProcessorFast(DetrImageProcessorFast, BaseImageProcessorFast):\n     size = {\"height\": 640, \"width\": 640}\n     default_to_square = False\n     model_input_names = [\"pixel_values\", \"pixel_mask\"]\n-    valid_init_kwargs = RTDetrFastImageProcessorInitKwargs\n-    valid_preprocess_kwargs = RTDetrFastImageProcessorPreprocessKwargs\n+    valid_kwargs = RTDetrFastImageProcessorKwargs\n \n-    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorInitKwargs]) -> None:\n+    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n         # Backwards compatibility\n         do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n         do_normalize = kwargs.get(\"do_normalize\", None)\n@@ -181,9 +175,13 @@ def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorInitKwargs]) -> None\n         \"\"\",\n     )\n     def preprocess(\n-        self, images: ImageInput, **kwargs: Unpack[RTDetrFastImageProcessorPreprocessKwargs]\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return BaseImageProcessorFast().preprocess(images, **kwargs)\n+        return BaseImageProcessorFast().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n \n     def prepare_annotation(\n         self,"
        },
        {
            "sha": "f42cd6847b70c24db7da59782f25bd42ba4b3d7d",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -311,8 +311,10 @@ def test_save_load_fast_slow(self):\n         }\n         dict_slow_0 = {key: dict_slow_0[key] for key in set(dict_slow_0) & set(dict_slow_1)}\n         dict_slow_1 = {key: dict_slow_1[key] for key in set(dict_slow_0) & set(dict_slow_1)}\n-        # check that all additional keys are None, except for `default_to_square` which is only set in fast processors\n-        self.assertTrue(all(value is None for key, value in difference.items() if key not in [\"default_to_square\"]))\n+        # check that all additional keys are None, except for `default_to_square` and `data_format` which are only set in fast processors\n+        self.assertTrue(\n+            all(value is None for key, value in difference.items() if key not in [\"default_to_square\", \"data_format\"])\n+        )\n         # check that the remaining keys are the same\n         self.assertEqual(dict_slow_0, dict_slow_1)\n \n@@ -324,8 +326,10 @@ def test_save_load_fast_slow(self):\n         }\n         dict_fast_0 = {key: dict_fast_0[key] for key in set(dict_fast_0) & set(dict_fast_1)}\n         dict_fast_1 = {key: dict_fast_1[key] for key in set(dict_fast_0) & set(dict_fast_1)}\n-        # check that all additional keys are None, except for `default_to_square` which is only set in fast processors\n-        self.assertTrue(all(value is None for key, value in difference.items() if key not in [\"default_to_square\"]))\n+        # check that all additional keys are None, except for `default_to_square` and `data_format` which are only set in fast processors\n+        self.assertTrue(\n+            all(value is None for key, value in difference.items() if key not in [\"default_to_square\", \"data_format\"])\n+        )\n         # check that the remaining keys are the same\n         self.assertEqual(dict_fast_0, dict_fast_1)\n \n@@ -357,8 +361,10 @@ def test_save_load_fast_slow_auto(self):\n         }\n         dict_slow_0 = {key: dict_slow_0[key] for key in set(dict_slow_0) & set(dict_slow_1)}\n         dict_slow_1 = {key: dict_slow_1[key] for key in set(dict_slow_0) & set(dict_slow_1)}\n-        # check that all additional keys are None, except for `default_to_square` which is only set in fast processors\n-        self.assertTrue(all(value is None for key, value in difference.items() if key not in [\"default_to_square\"]))\n+        # check that all additional keys are None, except for `default_to_square` and `data_format` which are only set in fast processors\n+        self.assertTrue(\n+            all(value is None for key, value in difference.items() if key not in [\"default_to_square\", \"data_format\"])\n+        )\n         # check that the remaining keys are the same\n         self.assertEqual(dict_slow_0, dict_slow_1)\n \n@@ -370,8 +376,10 @@ def test_save_load_fast_slow_auto(self):\n         }\n         dict_fast_0 = {key: dict_fast_0[key] for key in set(dict_fast_0) & set(dict_fast_1)}\n         dict_fast_1 = {key: dict_fast_1[key] for key in set(dict_fast_0) & set(dict_fast_1)}\n-        # check that all additional keys are None, except for `default_to_square` which is only set in fast processors\n-        self.assertTrue(all(value is None for key, value in difference.items() if key not in [\"default_to_square\"]))\n+        # check that all additional keys are None, except for `default_to_square` and `data_format` which are only set in fast processors\n+        self.assertTrue(\n+            all(value is None for key, value in difference.items() if key not in [\"default_to_square\", \"data_format\"])\n+        )\n         # check that the remaining keys are the same\n         self.assertEqual(dict_fast_0, dict_fast_1)\n "
        },
        {
            "sha": "efb8ac00475ef904d77c6cc62b91f78a4986875d",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea219ed164bead55a5513e8cfaa17a25d5613b9e/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea219ed164bead55a5513e8cfaa17a25d5613b9e/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=ea219ed164bead55a5513e8cfaa17a25d5613b9e",
            "patch": "@@ -1087,8 +1087,7 @@ def replace_class_node(\n     \"Processor\": \"processing\",\n     \"ImageProcessor\": \"image_processing\",\n     \"ImageProcessorFast\": \"image_processing*_fast\",  # \"*\" indicates where to insert the model name before the \"_fast\" suffix\n-    \"FastImageProcessorInitKwargs\": \"image_processing*_fast\",\n-    \"FastImageProcessorPreprocessKwargs\": \"image_processing*_fast\",\n+    \"FastImageProcessorKwargs\": \"image_processing*_fast\",\n     \"FeatureExtractor\": \"feature_extractor\",\n     \"ProcessorKwargs\": \"processing\",\n     \"ImagesKwargs\": \"processing\","
        }
    ],
    "stats": {
        "total": 334,
        "additions": 136,
        "deletions": 198
    }
}