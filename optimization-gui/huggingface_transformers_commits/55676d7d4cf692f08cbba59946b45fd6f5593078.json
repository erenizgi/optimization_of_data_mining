{
    "author": "qubvel",
    "message": "Fix warning for output_attentions=True (#40597)\n\n* Fix attn_implementation for output_attentions\n\n* remove setting attention, just raise warning\n\n* improve message\n\n* Update src/transformers/utils/generic.py",
    "sha": "55676d7d4cf692f08cbba59946b45fd6f5593078",
    "files": [
        {
            "sha": "565e5aa1ad859f7a33382bd1abd233dc99ec2405",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/55676d7d4cf692f08cbba59946b45fd6f5593078/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55676d7d4cf692f08cbba59946b45fd6f5593078/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=55676d7d4cf692f08cbba59946b45fd6f5593078",
            "patch": "@@ -1010,6 +1010,21 @@ def wrapper(self, *args, **kwargs):\n         collected_outputs = defaultdict(tuple)\n         monkey_patched_layers = []\n \n+        # Check attention implementation is properly set for capturing attention outputs\n+        if recordable_keys.get(\"output_attentions\", False):\n+            supported_attn = [\"eager\", \"eager_paged\", \"flex_attention\"]\n+            config_attn = getattr(self.config, \"_attn_implementation\", None)\n+            sub_configs = [getattr(self.config, key, None) for key in self.config.sub_configs]\n+            sub_configs_attn = [\n+                getattr(config, \"_attn_implementation\", None) for config in sub_configs if config is not None\n+            ]\n+            if config_attn not in supported_attn or any(attn not in supported_attn for attn in sub_configs_attn):\n+                warnings.warn(\n+                    f\"`output_attentions=True` is not supported with `attn_implementation` other than {supported_attn}. \"\n+                    \"Please use `model.set_attn_implementation('eager')` to enable capturing attention outputs.\",\n+                    UserWarning,\n+                )\n+\n         def make_capture_wrapper(module, orig_forward, key, index):\n             @wraps(orig_forward)\n             def wrapped_forward(*args, **kwargs):"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 15,
        "deletions": 0
    }
}