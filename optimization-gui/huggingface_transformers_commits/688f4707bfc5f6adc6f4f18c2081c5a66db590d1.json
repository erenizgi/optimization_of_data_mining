{
    "author": "Cyrilvallez",
    "message": "All models can be initialized on meta device (#37563)\n\n* Update test_modeling_common.py\n\n* fix all\n\n* more fixes",
    "sha": "688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
    "files": [
        {
            "sha": "94c6f19d4342efae12dd8442bf1fb7e0b0b1815f",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -663,7 +663,7 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n             self.relative_position_bias = BeitRelativePositionBias(config, window_size=window_size)\n \n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers, device=\"cpu\")]\n         self.layer = nn.ModuleList(\n             [\n                 BeitLayer("
        },
        {
            "sha": "a7a51cc86af32e9536ee5ca9f238228c4250548e",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -829,7 +829,7 @@ def __init__(self, config):\n \n         self.num_features = int(config.patch_embeds_hidden_size * 2 ** (self.num_layers - 1))\n \n-        drop_path_rate = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        drop_path_rate = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n \n         grid_size = self.patch_embed.grid_size\n         self.input_resolutions = [(grid_size[0] // (2**i), grid_size[1] // (2**i)) for i in range(self.num_layers)]"
        },
        {
            "sha": "d5f65f18b60cdd4963b72ab42aca8c9597098159",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -225,7 +225,8 @@ def __init__(self, config):\n         super().__init__()\n         self.stages = nn.ModuleList()\n         drop_path_rates = [\n-            x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths)\n+            x.tolist()\n+            for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\").split(config.depths)\n         ]\n         prev_chs = config.hidden_sizes[0]\n         for i in range(config.num_stages):"
        },
        {
            "sha": "a3af36bac35c88b6e8e1621b9fe8780465ef43a9",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -245,7 +245,8 @@ def __init__(self, config):\n         super().__init__()\n         self.stages = nn.ModuleList()\n         drop_path_rates = [\n-            x.tolist() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths)).split(config.depths)\n+            x.tolist()\n+            for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\").split(config.depths)\n         ]\n         prev_chs = config.hidden_sizes[0]\n         for i in range(config.num_stages):"
        },
        {
            "sha": "0088b8b440dcc3e3e82111bf3752e84aa47b413f",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -449,7 +449,9 @@ def __init__(self, config, stage):\n             dropout_rate=config.drop_rate[self.stage],\n         )\n \n-        drop_path_rates = [x.item() for x in torch.linspace(0, config.drop_path_rate[self.stage], config.depth[stage])]\n+        drop_path_rates = [\n+            x.item() for x in torch.linspace(0, config.drop_path_rate[self.stage], config.depth[stage], device=\"cpu\")\n+        ]\n \n         self.layers = nn.Sequential(\n             *["
        },
        {
            "sha": "7ea0ffd39d47f75e3b761db62120af79c121a3a1",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -676,7 +676,7 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n             self.relative_position_bias = Data2VecVisionRelativePositionBias(config, window_size=window_size)\n \n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers, device=\"cpu\")]\n         self.layer = nn.ModuleList(\n             [\n                 Data2VecVisionLayer("
        },
        {
            "sha": "43d8f3f10798be368f95c285d932d937c7e39178",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -790,7 +790,7 @@ def __init__(self, config, grid_size):\n         super().__init__()\n         self.num_layers = len(config.depths)\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         self.layers = nn.ModuleList(\n             [\n                 DonutSwinStage("
        },
        {
            "sha": "41336c1b53acd9cffc2a549d2401c4b3841dabd3",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -486,7 +486,7 @@ def __init__(self, config, index, input_resolution):\n         downsample = FocalNetPatchEmbeddings if (index < self.num_stages - 1) else None\n \n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         drop_path = dpr[sum(config.depths[:index]) : sum(config.depths[: index + 1])]\n \n         self.layers = nn.ModuleList("
        },
        {
            "sha": "0c90746d187e19c8c1a9235d9f386792794f6255",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -331,7 +331,7 @@ def __init__(self, config):\n         self.config = config\n \n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n \n         # patch embeddings\n         embeddings = []"
        },
        {
            "sha": "fa01dd909dca37e295dbe26e792cafe396a0638d",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -639,9 +639,9 @@ def __init__(self, config: HieraConfig) -> None:\n         super().__init__()\n         total_depth = sum(config.depths)\n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, total_depth)]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, total_depth, device=\"cpu\")]\n         # query strides rule\n-        cumulative_depths = torch.tensor(config.depths).cumsum(0).tolist()\n+        cumulative_depths = torch.tensor(config.depths, device=\"cpu\").cumsum(0).tolist()\n         query_pool_layer = cumulative_depths[: config.num_query_pool]\n         query_strides = [math.prod(config.query_stride) if i in query_pool_layer else 1 for i in range(total_depth)]\n "
        },
        {
            "sha": "fd25e84ffe797ae1b704184c9c8b13069b90ca21",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -692,7 +692,7 @@ def __init__(self, config, grid_size):\n         super().__init__()\n         self.num_layers = len(config.depths)\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         self.layers = nn.ModuleList(\n             [\n                 MaskFormerSwinStage("
        },
        {
            "sha": "684cfd2e78616fce77092b001c4727b90af88434",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -246,7 +246,7 @@ class MgpstrEncoder(nn.Module):\n     def __init__(self, config: MgpstrConfig):\n         super().__init__()\n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers, device=\"cpu\")]\n \n         self.blocks = nn.Sequential(\n             *[MgpstrLayer(config=config, drop_path=dpr[i]) for i in range(config.num_hidden_layers)]"
        },
        {
            "sha": "5b2ed8868b19ee5eab5971636be40dd256672572",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -194,7 +194,7 @@ def __init__(self, config):\n         super().__init__()\n         self.config = config\n         # stochastic depth decay rule\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n \n         # patch embeddings\n         embeddings = []"
        },
        {
            "sha": "f9eb1a3c586cee81a0dd3342f5ae8a1fa2ac8ba6",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -369,7 +369,7 @@ def __init__(self, config: PvtConfig):\n         self.config = config\n \n         # stochastic depth decay rule\n-        drop_path_decays = torch.linspace(0, config.drop_path_rate, sum(config.depths)).tolist()\n+        drop_path_decays = torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\").tolist()\n \n         # patch embeddings\n         embeddings = []"
        },
        {
            "sha": "517d2edbe32381f7d811d15dab052ed16d9514c1",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -323,7 +323,7 @@ def __init__(self, config: PvtV2Config, layer_idx: int):\n         )\n         # Transformer block\n         # stochastic depth decay rule\n-        drop_path_decays = torch.linspace(0, config.drop_path_rate, sum(config.depths)).tolist()\n+        drop_path_decays = torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\").tolist()\n         block_layers = []\n         for block_idx in range(config.depths[layer_idx]):\n             block_layers.append("
        },
        {
            "sha": "4f9da2cab7b341ece9337d3f6be32f472142bb6e",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -356,7 +356,9 @@ def __init__(self, config):\n         self.config = config\n \n         # stochastic depth decay rule\n-        drop_path_decays = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        drop_path_decays = [\n+            x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")\n+        ]\n \n         # patch embeddings\n         embeddings = []"
        },
        {
            "sha": "e4058b3346724bb1ec9aec18496ebddeaba3fc2a",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -460,7 +460,7 @@ class SegGptEncoder(nn.Module):\n     def __init__(self, config: SegGptConfig) -> None:\n         super().__init__()\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers, device=\"cpu\")]\n         self.layers = nn.ModuleList([SegGptLayer(config, dpr[i]) for i in range(config.num_hidden_layers)])\n         self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.gradient_checkpointing = False"
        },
        {
            "sha": "ad998f6ff666eaa6a1a0e5eb75e8d735d48e6292",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -823,7 +823,7 @@ def __init__(self, config, grid_size):\n         super().__init__()\n         self.num_layers = len(config.depths)\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         self.layers = nn.ModuleList(\n             [\n                 SwinStage("
        },
        {
            "sha": "6543fe5cdff610ca342be893c3c61ee4dbc513c4",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -682,7 +682,7 @@ def __init__(self, config, grid_size):\n         super().__init__()\n         self.num_stages = len(config.depths)\n         self.config = config\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n         self.stages = nn.ModuleList(\n             [\n                 Swin2SRStage("
        },
        {
            "sha": "52e33cf3d99baba12046a806da7c3b8007c4b1ac",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -877,7 +877,7 @@ def __init__(self, config, grid_size, pretrained_window_sizes=(0, 0, 0, 0)):\n         self.config = config\n         if self.config.pretrained_window_sizes is not None:\n             pretrained_window_sizes = config.pretrained_window_sizes\n-        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n+        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths), device=\"cpu\")]\n \n         layers = []\n         for i_layer in range(self.num_layers):"
        },
        {
            "sha": "0bd79a6cec0425991dba299e1886dd4120a1dd16",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -295,7 +295,7 @@ def __init__(self, config: TimesformerConfig, layer_index: int) -> None:\n         attention_type = config.attention_type\n \n         drop_path_rates = [\n-            x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)\n+            x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers, device=\"cpu\")\n         ]  # stochastic depth decay rule\n         drop_path_rate = drop_path_rates[layer_index]\n "
        },
        {
            "sha": "3d740522884d07b03c182944f6a6644f25c225de",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -535,7 +535,7 @@ def __init__(self, config: VitDetConfig) -> None:\n         depth = config.num_hidden_layers\n \n         # stochastic depth decay rule\n-        drop_path_rate = [x.item() for x in torch.linspace(0, config.drop_path_rate, depth)]\n+        drop_path_rate = [x.item() for x in torch.linspace(0, config.drop_path_rate, depth, device=\"cpu\")]\n \n         layers = []\n         for i in range(depth):"
        },
        {
            "sha": "e546d023f5ead60c127ea89fbf35b0cf8a269b17",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/688f4707bfc5f6adc6f4f18c2081c5a66db590d1/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=688f4707bfc5f6adc6f4f18c2081c5a66db590d1",
            "patch": "@@ -4528,6 +4528,13 @@ def test_generation_tester_mixin_inheritance(self):\n                 ),\n             )\n \n+    def test_can_be_initialized_on_meta(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            # If it does not raise here, the test passes\n+            with torch.device(\"meta\"):\n+                _ = model_class(config)\n+\n     @require_torch_accelerator\n     def test_can_load_with_device_context_manager(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 36,
        "deletions": 23
    }
}