{
    "author": "gante",
    "message": "[kernels] use original forward at compile time (#37604)",
    "sha": "1930e750e4608b63fcbdfed64bbd6490d1946df9",
    "files": [
        {
            "sha": "63e0c381e79809df5597fe22bcabcdb5991a816e",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 39,
            "deletions": 1,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/1930e750e4608b63fcbdfed64bbd6490d1946df9/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1930e750e4608b63fcbdfed64bbd6490d1946df9/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=1930e750e4608b63fcbdfed64bbd6490d1946df9",
            "patch": "@@ -13,14 +13,18 @@\n # limitations under the License.\n from typing import Dict, Union\n \n+from ..utils import is_torchdynamo_compiling\n+\n \n try:\n     from kernels import (\n         Device,\n         LayerRepository,\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n-        use_kernel_forward_from_hub,\n+    )\n+    from kernels import (\n+        use_kernel_forward_from_hub as original_use_kernel_forward_from_hub,\n     )\n \n     _hub_kernels_available = True\n@@ -56,6 +60,40 @@\n \n     register_kernel_mapping(_KERNEL_MAPPING)\n \n+    def use_kernel_forward_from_hub(*args, **kwargs):\n+        \"\"\"\n+        Expands `kernels`' `use_kernel_forward_from_hub` to NOT use a kernel at compile time. This should be removed\n+        when `kernels` supports `torch.compile`.\n+\n+        If the layer has a `config` attribute, we can also set `config.disable_custom_kernels = True` to disable the\n+        kernel.\n+        \"\"\"\n+\n+        def decorator_with_compile_path(cls):\n+            # Keeps a reference to the original forward method\n+            original_forward = cls.forward\n+\n+            # Applies the original decorator\n+            decorator = original_use_kernel_forward_from_hub(*args, **kwargs)\n+            cls = decorator(cls)\n+\n+            # Replaces the kernel forward with a compile-friendly version\n+            kernel_forward = cls.forward\n+\n+            def forward_with_compile_path(*forward_args, **forward_kwargs):\n+                disable_custom_kernels = hasattr(cls, \"config\") and getattr(cls.config, \"disable_custom_kernels\", None)\n+                if is_torchdynamo_compiling() or disable_custom_kernels:\n+                    return original_forward(*forward_args, **forward_kwargs)\n+                else:\n+                    return kernel_forward(*forward_args, **forward_kwargs)\n+\n+            cls.forward = forward_with_compile_path\n+\n+            return cls\n+\n+        return decorator_with_compile_path\n+\n+\n except ImportError:\n     # Stub to make decorators int transformers work when `kernels`\n     # is not installed."
        }
    ],
    "stats": {
        "total": 40,
        "additions": 39,
        "deletions": 1
    }
}