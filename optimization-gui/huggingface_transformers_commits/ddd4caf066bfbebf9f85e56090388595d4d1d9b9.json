{
    "author": "yaswanth19",
    "message": "[Llama4] Remove `image_sizes` arg and deprecate `vision_feature_layer` (#40832)\n\n* Remove unused arg\n\n* deprecate\n\n* revrt one change\n\n* get set go\n\n* version correction\n\n* fix\n\n* make style\n\n* comment",
    "sha": "ddd4caf066bfbebf9f85e56090388595d4d1d9b9",
    "files": [
        {
            "sha": "932f4975dba79d5ed97e34c2017f5ed154d34fd6",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ddd4caf066bfbebf9f85e56090388595d4d1d9b9/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ddd4caf066bfbebf9f85e56090388595d4d1d9b9/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=ddd4caf066bfbebf9f85e56090388595d4d1d9b9",
            "patch": "@@ -14,6 +14,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import warnings\n \n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...utils import logging\n@@ -56,7 +57,6 @@ class Llama4VisionConfig(PretrainedConfig):\n             The size (resolution) of each patch.\n         norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the layer normalization layers.\n-        vision_feature_layer (``, *optional*, defaults to -1): TODO\n         vision_feature_select_strategy (`int`, *optional*, defaults to `\"default\"`): TODO\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n@@ -93,7 +93,6 @@ def __init__(\n         image_size: int = 448,\n         patch_size: int = 14,\n         norm_eps: float = 1e-5,\n-        vision_feature_layer=-1,\n         vision_feature_select_strategy=\"default\",\n         initializer_range: float = 0.02,\n         pixel_shuffle_ratio=0.5,\n@@ -122,9 +121,23 @@ def __init__(\n         self.multi_modal_projector_bias = multi_modal_projector_bias\n         self.projector_dropout = projector_dropout\n         self.attention_dropout = attention_dropout\n-        self.vision_feature_layer = vision_feature_layer\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.rope_theta = rope_theta\n+\n+        self._vision_feature_layer = kwargs.get(\"vision_feature_layer\", -1)\n+\n+        @property\n+        def vision_feature_layer(self):\n+            warnings.warn(\n+                \"The `vision_feature_layer` attribute is deprecated and will be removed in v4.58.0.\",\n+                FutureWarning,\n+            )\n+            return self._vision_feature_layer\n+\n+        @vision_feature_layer.setter\n+        def vision_feature_layer(self, value):\n+            self._vision_feature_layer = value\n+\n         super().__init__(**kwargs)\n \n "
        },
        {
            "sha": "17bd9d59372d5139f37563357ba319833056e2e0",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ddd4caf066bfbebf9f85e56090388595d4d1d9b9/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ddd4caf066bfbebf9f85e56090388595d4d1d9b9/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=ddd4caf066bfbebf9f85e56090388595d4d1d9b9",
            "patch": "@@ -1173,7 +1173,6 @@ def get_decoder(self):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, list[int]],\n         vision_feature_select_strategy: str,\n         **kwargs,\n     ):\n@@ -1183,10 +1182,6 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, list[int]]`):\n-                The index of the layer to select the vision feature. If multiple indices are provided,\n-                the vision feature of the corresponding indices will be concatenated to form the\n-                vision features.\n             vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n@@ -1224,6 +1219,7 @@ def get_placeholder_mask(\n         return special_image_mask\n \n     @auto_docstring\n+    @deprecate_kwarg(\"vision_feature_layer\", version=\"4.58\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1241,7 +1237,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Llama4CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1277,11 +1272,6 @@ def forward(\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        vision_feature_layer = (\n-            vision_feature_layer\n-            if vision_feature_layer is not None\n-            else self.config.vision_config.vision_feature_layer\n-        )\n         vision_feature_select_strategy = (\n             vision_feature_select_strategy\n             if vision_feature_select_strategy is not None\n@@ -1302,9 +1292,7 @@ def forward(\n         if pixel_values is not None:\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n-                vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_sizes=image_sizes,\n             )\n \n             vision_flat = image_features.view(-1, image_features.size(-1))"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 17,
        "deletions": 16
    }
}