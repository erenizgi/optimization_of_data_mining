{
    "author": "mayank31398",
    "message": "fix model name and copyright (#33152)",
    "sha": "386931d9507609c7a6650fe85e7b3239a66fc8c7",
    "files": [
        {
            "sha": "aee62fd249f35010ef2569354db2e2f277930d73",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/386931d9507609c7a6650fe85e7b3239a66fc8c7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/386931d9507609c7a6650fe85e7b3239a66fc8c7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=386931d9507609c7a6650fe85e7b3239a66fc8c7",
            "patch": "@@ -1,10 +1,6 @@\n # coding=utf-8\n-# Copyright 2024 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -1041,8 +1037,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, GraniteForCausalLM\n \n-        >>> model = GraniteForCausalLM.from_pretrained(\"meta-granite/Granite-2-7b-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-granite/Granite-2-7b-hf\")\n+        >>> model = GraniteForCausalLM.from_pretrained(\"ibm/PowerLM-3b\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"ibm/PowerLM-3b\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 3,
        "deletions": 7
    }
}