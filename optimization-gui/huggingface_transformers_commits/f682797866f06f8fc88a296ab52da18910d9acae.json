{
    "author": "cyyever",
    "message": "Fix typing (#40788)\n\n* Fix optional typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix optional typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix schema typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typing\n\n* Fix typing\n\n* Fix typing\n\n* Fix typing\n\n* Use np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Format code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Improve typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix quote string of np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix code\n\n* Format\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "f682797866f06f8fc88a296ab52da18910d9acae",
    "files": [
        {
            "sha": "6f25f892ba7a0699ed02c965b1cc1544aa1d234e",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -23,8 +23,11 @@\n import warnings\n from collections.abc import Sequence\n from io import BytesIO\n-from typing import Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n+\n+if TYPE_CHECKING:\n+    import torch\n import numpy as np\n import requests\n from packaging import version\n@@ -51,7 +54,7 @@\n if is_torchcodec_available():\n     TORCHCODEC_VERSION = version.parse(importlib.metadata.version(\"torchcodec\"))\n \n-AudioInput = Union[np.ndarray, \"torch.Tensor\", Sequence[np.ndarray], Sequence[\"torch.Tensor\"]]  # noqa: F821\n+AudioInput = Union[np.ndarray, \"torch.Tensor\", Sequence[np.ndarray], Sequence[\"torch.Tensor\"]]\n \n \n def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:"
        },
        {
            "sha": "ee9009090f9d439284c5c6c844238cba53ea0d85",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -31,7 +31,7 @@\n from dataclasses import dataclass, field\n from io import BytesIO\n from threading import Thread\n-from typing import Optional, Union\n+from typing import Optional, TypedDict, Union\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n@@ -528,7 +528,7 @@ def __init__(self, args: ServeArguments):\n     def _validate_request(\n         self,\n         request: dict,\n-        schema: \"_TypedDictMeta\",  # noqa: F821\n+        schema: TypedDict,\n         validator: \"TypeAdapter\",\n         unused_fields: set,\n     ):\n@@ -538,7 +538,7 @@ def _validate_request(\n         Args:\n             request (`dict`):\n                 The request to validate.\n-            schema (`_TypedDictMeta`):\n+            schema (`TypedDict`):\n                 The schema of the request to validate. It is a `TypedDict` definition.\n             validator (`TypeAdapter`):\n                 The validator to use to validate the request. Built from `schema`."
        },
        {
            "sha": "ae56eaadf6027668acd042d9a278a3a31efb9890",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -45,13 +45,12 @@\n \n \n if TYPE_CHECKING:\n-    if is_torch_available():\n-        import torch  # noqa\n+    from .feature_extraction_sequence_utils import SequenceFeatureExtractor\n \n \n logger = logging.get_logger(__name__)\n \n-PreTrainedFeatureExtractor = Union[\"SequenceFeatureExtractor\"]  # noqa: F821\n+PreTrainedFeatureExtractor = Union[\"SequenceFeatureExtractor\"]\n \n # type hinting: specifying the type of feature extractor class that inherits from FeatureExtractionMixin\n SpecificFeatureExtractorType = TypeVar(\"SpecificFeatureExtractorType\", bound=\"FeatureExtractionMixin\")\n@@ -110,7 +109,7 @@ def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] =\n         if tensor_type == TensorType.PYTORCH:\n             if not is_torch_available():\n                 raise ImportError(\"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\")\n-            import torch  # noqa\n+            import torch\n \n             def as_tensor(value):\n                 if isinstance(value, (list, tuple)) and len(value) > 0:\n@@ -535,7 +534,9 @@ def get_feature_extractor_dict(\n         return feature_extractor_dict, kwargs\n \n     @classmethod\n-    def from_dict(cls, feature_extractor_dict: dict[str, Any], **kwargs) -> PreTrainedFeatureExtractor:\n+    def from_dict(\n+        cls, feature_extractor_dict: dict[str, Any], **kwargs\n+    ) -> Union[\"FeatureExtractionMixin\", tuple[\"FeatureExtractionMixin\", dict[str, Any]]]:\n         \"\"\"\n         Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a Python dictionary of\n         parameters.\n@@ -585,7 +586,7 @@ def to_dict(self) -> dict[str, Any]:\n         return output\n \n     @classmethod\n-    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> PreTrainedFeatureExtractor:\n+    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> \"FeatureExtractionMixin\":\n         \"\"\"\n         Instantiates a feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`] from the path to\n         a JSON file of parameters."
        },
        {
            "sha": "95cee542b291c5e851f86ca914780535083bc700",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -3432,7 +3432,7 @@ def _assisted_decoding(\n         generation_config: GenerationConfig,\n         synced_gpus: bool = False,\n         streamer: Optional[\"BaseStreamer\"] = None,\n-        inputs_tensor: torch.FloatTensor = None,\n+        inputs_tensor: Optional[torch.FloatTensor] = None,\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         assistant_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,"
        },
        {
            "sha": "c9817022fa9a8e1ed1fc7b09adc525439c86868f",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -43,31 +43,31 @@ class WatermarkDetectorOutput:\n     Outputs of a watermark detector.\n \n     Args:\n-        num_tokens_scored (np.array of shape (batch_size)):\n+        num_tokens_scored (np.ndarray of shape (batch_size)):\n             Array containing the number of tokens scored for each element in the batch.\n-        num_green_tokens (np.array of shape (batch_size)):\n+        num_green_tokens (np.ndarray of shape (batch_size)):\n             Array containing the number of green tokens for each element in the batch.\n-        green_fraction (np.array of shape (batch_size)):\n+        green_fraction (np.ndarray of shape (batch_size)):\n             Array containing the fraction of green tokens for each element in the batch.\n-        z_score (np.array of shape (batch_size)):\n+        z_score (np.ndarray of shape (batch_size)):\n             Array containing the z-score for each element in the batch. Z-score here shows\n             how many standard deviations away is the green token count in the input text\n             from the expected green token count for machine-generated text.\n-        p_value (np.array of shape (batch_size)):\n+        p_value (np.ndarray of shape (batch_size)):\n             Array containing the p-value for each batch obtained from z-scores.\n-        prediction (np.array of shape (batch_size)), *optional*:\n+        prediction (np.ndarray of shape (batch_size)), *optional*:\n             Array containing boolean predictions whether a text is machine-generated for each element in the batch.\n-        confidence (np.array of shape (batch_size)), *optional*:\n+        confidence (np.ndarray of shape (batch_size)), *optional*:\n             Array containing confidence scores of a text being machine-generated for each element in the batch.\n     \"\"\"\n \n-    num_tokens_scored: Optional[np.array] = None\n-    num_green_tokens: Optional[np.array] = None\n-    green_fraction: Optional[np.array] = None\n-    z_score: Optional[np.array] = None\n-    p_value: Optional[np.array] = None\n-    prediction: Optional[np.array] = None\n-    confidence: Optional[np.array] = None\n+    num_tokens_scored: Optional[np.ndarray] = None\n+    num_green_tokens: Optional[np.ndarray] = None\n+    green_fraction: Optional[np.ndarray] = None\n+    z_score: Optional[np.ndarray] = None\n+    p_value: Optional[np.ndarray] = None\n+    prediction: Optional[np.ndarray] = None\n+    confidence: Optional[np.ndarray] = None\n \n \n class WatermarkDetector:\n@@ -179,7 +179,7 @@ def _score_ngrams_in_passage(self, input_ids: torch.LongTensor):\n                 )\n         return num_tokens_scored_batch, green_token_count_batch\n \n-    def _compute_z_score(self, green_token_count: np.ndarray, total_num_tokens: np.ndarray) -> np.array:\n+    def _compute_z_score(self, green_token_count: np.ndarray, total_num_tokens: np.ndarray) -> np.ndarray:\n         expected_count = self.greenlist_ratio\n         numer = green_token_count - expected_count * total_num_tokens\n         denom = np.sqrt(total_num_tokens * expected_count * (1 - expected_count))\n@@ -195,7 +195,7 @@ def __call__(\n         input_ids: torch.LongTensor,\n         z_threshold: float = 3.0,\n         return_dict: bool = False,\n-    ) -> Union[WatermarkDetectorOutput, np.array]:\n+    ) -> Union[WatermarkDetectorOutput, np.ndarray]:\n         \"\"\"\n                 Args:\n                 input_ids (`torch.LongTensor`):\n@@ -207,8 +207,8 @@ def __call__(\n                     Whether to return `~generation.WatermarkDetectorOutput` or not. If not it will return boolean predictions,\n         ma\n                 Return:\n-                    [`~generation.WatermarkDetectorOutput`] or `np.array`: A [`~generation.WatermarkDetectorOutput`]\n-                    if `return_dict=True` otherwise a `np.array`.\n+                    [`~generation.WatermarkDetectorOutput`] or `np.ndarray`: A [`~generation.WatermarkDetectorOutput`]\n+                    if `return_dict=True` otherwise a `np.ndarray`.\n \n         \"\"\"\n "
        },
        {
            "sha": "75210680b57a3aeff29a0384ae860ca61288defe",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -142,8 +142,8 @@ def get_max_height_width(images: list[\"torch.Tensor\"]) -> tuple[int, ...]:\n \n \n def divide_to_patches(\n-    image: Union[np.array, \"torch.Tensor\"], patch_size: int\n-) -> list[Union[np.array, \"torch.Tensor\"]]:\n+    image: Union[np.ndarray, \"torch.Tensor\"], patch_size: int\n+) -> list[Union[np.ndarray, \"torch.Tensor\"]]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n "
        },
        {
            "sha": "1cda7364568a5d63a7abc5a280ae9c10127bac6e",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -313,7 +313,7 @@ def get_resize_output_image_size(\n def resize(\n     image: np.ndarray,\n     size: tuple[int, int],\n-    resample: \"PILImageResampling\" = None,\n+    resample: Optional[\"PILImageResampling\"] = None,\n     reducing_gap: Optional[int] = None,\n     data_format: Optional[ChannelDimension] = None,\n     return_numpy: bool = True,"
        },
        {
            "sha": "fdbc388de337806ee7b3580e302ef614352e4680",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -569,7 +569,7 @@ class ImageFeatureExtractionMixin:\n     def _ensure_format_supported(self, image):\n         if not isinstance(image, (PIL.Image.Image, np.ndarray)) and not is_torch_tensor(image):\n             raise ValueError(\n-                f\"Got type {type(image)} which is not supported, only `PIL.Image.Image`, `np.array` and \"\n+                f\"Got type {type(image)} which is not supported, only `PIL.Image.Image`, `np.ndarray` and \"\n                 \"`torch.Tensor` are.\"\n             )\n "
        },
        {
            "sha": "d68baa0efefc3ba6ba1836302f3049af5111b03b",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -43,20 +43,20 @@\n logger = logging.get_logger(__name__)\n \n \n-def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.array]:\n+def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n     Args:\n-        image (`np.array`):\n+        image (`np.ndarray`):\n             The input image.\n         patch_size (`int`):\n             The size of each patch.\n         input_data_format (`ChannelDimension` or `str`):\n             The channel dimension format of the input image.\n \n     Returns:\n-        list: A list of np.array representing the patches.\n+        list: A list of np.ndarray representing the patches.\n     \"\"\"\n     patches = []\n     height, width = get_image_size(image, channel_dim=input_data_format)\n@@ -339,12 +339,12 @@ def preprocess(\n \n     def _resize_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n \n         Args:\n-            image (np.array):\n+            image (np.ndarray):\n                 The input image.\n             target_resolution (tuple):\n                 The target resolution (height, width) of the image.\n@@ -354,7 +354,7 @@ def _resize_for_patching(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            np.array: The resized and padded image.\n+            np.ndarray: The resized and padded image.\n         \"\"\"\n         new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n@@ -372,7 +372,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n \n     def _pad_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n@@ -457,12 +457,12 @@ def get_image_patches(\n         resample: PILImageResampling,\n         data_format: ChannelDimension,\n         input_data_format: ChannelDimension,\n-    ) -> list[np.array]:\n+    ) -> list[np.ndarray]:\n         \"\"\"\n         Process an image with variable resolutions by dividing it into patches.\n \n         Args:\n-            image (`np.array`):\n+            image (`np.ndarray`):\n                 The input image to be processed.\n             grid_pinpoints (list[tuple[int, int]]):\n                 A list of possible resolutions as tuples.\n@@ -476,7 +476,7 @@ def get_image_patches(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            `list[np.array]`: A list of NumPy arrays containing the processed image patches.\n+            `list[np.ndarray]`: A list of NumPy arrays containing the processed image patches.\n         \"\"\"\n         if not isinstance(grid_pinpoints, list):\n             raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")"
        },
        {
            "sha": "c52a8efde1a0a78d1eb117d2b15a066b660a4a4b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -722,12 +722,12 @@ def preprocess(\n \n     def _resize_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n \n         Args:\n-            image (np.array):\n+            image (np.ndarray):\n                 The input image.\n             target_resolution (tuple):\n                 The target resolution (height, width) of the image.\n@@ -737,7 +737,7 @@ def _resize_for_patching(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            np.array: The resized and padded image.\n+            np.ndarray: The resized and padded image.\n         \"\"\"\n         new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n@@ -755,7 +755,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n \n     def _pad_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n@@ -840,12 +840,12 @@ def get_image_patches(\n         resample: PILImageResampling,\n         data_format: ChannelDimension,\n         input_data_format: ChannelDimension,\n-    ) -> list[np.array]:\n+    ) -> list[np.ndarray]:\n         \"\"\"\n         Process an image with variable resolutions by dividing it into patches.\n \n         Args:\n-            image (`np.array`):\n+            image (`np.ndarray`):\n                 The input image to be processed.\n             grid_pinpoints (list[tuple[int, int]]):\n                 A list of possible resolutions as tuples.\n@@ -859,7 +859,7 @@ def get_image_patches(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            `list[np.array]`: A list of NumPy arrays containing the processed image patches.\n+            `list[np.ndarray]`: A list of NumPy arrays containing the processed image patches.\n         \"\"\"\n         if not isinstance(grid_pinpoints, list):\n             raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")"
        },
        {
            "sha": "d895f95b9fc9a95896f97e6c6e7ea8957a9d8299",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -593,7 +593,7 @@ class BarkSemanticModel(BarkCausalModel):\n     def generate(\n         self,\n         input_ids: torch.Tensor,\n-        semantic_generation_config: BarkSemanticGenerationConfig = None,\n+        semantic_generation_config: Optional[BarkSemanticGenerationConfig] = None,\n         history_prompt: Optional[dict[str, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n@@ -778,8 +778,8 @@ def preprocess_histories(\n     def generate(\n         self,\n         semantic_output: torch.Tensor,\n-        semantic_generation_config: BarkSemanticGenerationConfig = None,\n-        coarse_generation_config: BarkCoarseGenerationConfig = None,\n+        semantic_generation_config: Optional[BarkSemanticGenerationConfig] = None,\n+        coarse_generation_config: Optional[BarkCoarseGenerationConfig] = None,\n         codebook_size: int = 1024,\n         history_prompt: Optional[dict[str, torch.Tensor]] = None,\n         return_output_lengths: Optional[bool] = None,\n@@ -1190,8 +1190,8 @@ def forward(\n     def generate(\n         self,\n         coarse_output: torch.Tensor,\n-        semantic_generation_config: BarkSemanticGenerationConfig = None,\n-        coarse_generation_config: BarkCoarseGenerationConfig = None,\n+        semantic_generation_config: Optional[BarkSemanticGenerationConfig] = None,\n+        coarse_generation_config: Optional[BarkCoarseGenerationConfig] = None,\n         fine_generation_config: BarkFineGenerationConfig = None,\n         codebook_size: int = 1024,\n         history_prompt: Optional[dict[str, torch.Tensor]] = None,"
        },
        {
            "sha": "1555e39a71daf246db2a93428a02175b748d2a6f",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -152,7 +152,7 @@ def to_dict(self) -> dict[str, Any]:\n             del output[\"mel_filters_slaney\"]\n         return output\n \n-    def _np_extract_fbank_features(self, waveform: np.ndarray, mel_filters: Optional[np.array] = None) -> np.ndarray:\n+    def _np_extract_fbank_features(self, waveform: np.ndarray, mel_filters: Optional[np.ndarray] = None) -> np.ndarray:\n         \"\"\"\n         Compute the log-mel spectrogram of the provided `waveform` using the Hann window. In CLAP, two different filter\n         banks are used depending on the truncation pattern:\n@@ -199,7 +199,7 @@ def _random_mel_fusion(self, mel, total_frames, chunk_frames):\n         mel_fusion = np.stack([mel_shrink, mel_chunk_front, mel_chunk_middle, mel_chunk_back], axis=0)\n         return mel_fusion\n \n-    def _get_input_mel(self, waveform: np.ndarray, max_length, truncation, padding) -> np.array:\n+    def _get_input_mel(self, waveform: np.ndarray, max_length, truncation, padding) -> np.ndarray:\n         \"\"\"\n         Extracts the mel spectrogram and prepares it for the mode based on the `truncation` and `padding` arguments.\n         Four different path are possible:\n@@ -289,7 +289,6 @@ def __call__(\n                     - `pad`: the audio is padded.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors instead of list of python integers. Acceptable values are:\n-\n                 - `'pt'`: Return PyTorch `torch.np.array` objects.\n                 - `'np'`: Return Numpy `np.ndarray` objects.\n             sampling_rate (`int`, *optional*):"
        },
        {
            "sha": "4d08c6acd5bb9888ebde3d356c1e82ae93a6d8f9",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -310,7 +310,7 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n \n     def decode(\n         self,\n-        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         truncate_before_pattern: Optional[list[str]] = None,"
        },
        {
            "sha": "72c8d66c829a5ec8f2379c2af092cdefa53b6d8a",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -158,7 +158,7 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n \n     def decode(\n         self,\n-        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         truncate_before_pattern: Optional[list[str]] = None,"
        },
        {
            "sha": "b8c629b745765de613adf4ab7ebd0d938429823c",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -353,7 +353,7 @@ def pad_to_square(\n         background_color: Union[int, tuple[int, int, int]] = 0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n "
        },
        {
            "sha": "93dd31e3aafc2623ba4ae3d457455616da82d4d9",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -426,7 +426,7 @@ def pad_to_square(\n         background_color: Union[int, tuple[int, int, int]] = 0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n "
        },
        {
            "sha": "36f6e6097bc3e55a153b991df9161b01c09c153a",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -1233,7 +1233,7 @@ class XLMProphetNetEncoder(XLMProphetNetPreTrainedModel):\n         embeddings instead of randomly initialized word embeddings.\n     \"\"\"\n \n-    def __init__(self, config: XLMProphetNetConfig, word_embeddings: nn.Embedding = None):\n+    def __init__(self, config: XLMProphetNetConfig, word_embeddings: Optional[nn.Embedding] = None):\n         super().__init__(config)\n \n         self.word_embeddings = ("
        },
        {
            "sha": "85ef20e6fba48d42bfb0175fda01bf6210a23fa0",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -481,7 +481,7 @@ def unnormalize(\n         image_mean: Union[float, Iterable[float]],\n         image_std: Union[float, Iterable[float]],\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n         image = (image * image_std) + image_mean"
        },
        {
            "sha": "b8e0058eee8d10af4bc07acfac53a87d7aaf769f",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -55,7 +55,7 @@\n \n # Adapted from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n-    segmentation_map: \"np.ndarray\",\n+    segmentation_map: np.ndarray,\n     instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n     ignore_index: Optional[int] = None,\n ):"
        },
        {
            "sha": "8478a5fb0c4664bc0f90b706cd0ffaa2410e43a4",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -485,7 +485,7 @@ def __init__(\n         num_heads: int,\n         dropout: float = 0.0,\n         is_cross_attention: bool = False,\n-        config: PretrainedConfig = None,\n+        config: Optional[PretrainedConfig] = None,\n         qk_layer_norms: bool = False,\n         layer_idx: Optional[int] = None,\n     ):"
        },
        {
            "sha": "a56cae7ba975108a9ece517120c5ad4b5f93215f",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -350,7 +350,7 @@ def pad_to_square(\n         background_color: Union[int, tuple[int, int, int]] = 0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n \n@@ -475,7 +475,7 @@ def unnormalize(\n         image_mean: Union[float, Iterable[float]],\n         image_std: Union[float, Iterable[float]],\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n         image = (image * image_std) + image_mean"
        },
        {
            "sha": "a8e24a86b0d307c4ef83f652c811bcd3f428ad2b",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -1359,7 +1359,7 @@ def pad_to_square(\n         background_color: Union[int, tuple[int, int, int]] = 0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n \n@@ -1700,7 +1700,7 @@ def unnormalize(\n         image_mean: Union[float, Iterable[float]],\n         image_std: Union[float, Iterable[float]],\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Unnormalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n         image = (image * image_std) + image_mean"
        },
        {
            "sha": "7c24557e5da503002d15262f6a4c02fc8640d143",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -154,7 +154,7 @@ def pad_to_square(\n         background_color: Union[int, tuple[int, int, int]] = 0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n "
        },
        {
            "sha": "60d36abb748b3dbebaf9b7e41f56c5c283a4d943",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -58,20 +58,20 @@\n     from PIL import Image\n \n \n-def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.array]:\n+def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n     Args:\n-        image (`np.array`):\n+        image (`np.ndarray`):\n             The input image.\n         patch_size (`int`):\n             The size of each patch.\n         input_data_format (`ChannelDimension` or `str`):\n             The channel dimension format of the input image.\n \n     Returns:\n-        list: A list of np.array representing the patches.\n+        list: A list of np.ndarray representing the patches.\n     \"\"\"\n     patches = []\n     height, width = get_image_size(image, channel_dim=input_data_format)\n@@ -86,7 +86,7 @@ def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) ->\n     return patches\n \n \n-def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.array:\n+def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.ndarray:\n     \"\"\"\n     Expands an image to a square by adding a background color.\n     \"\"\"\n@@ -400,12 +400,12 @@ def _preprocess(\n \n     def _resize_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n \n         Args:\n-            image (np.array):\n+            image (np.ndarray):\n                 The input image.\n             target_resolution (tuple):\n                 The target resolution (height, width) of the image.\n@@ -415,7 +415,7 @@ def _resize_for_patching(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            np.array: The resized and padded image.\n+            np.ndarray: The resized and padded image.\n         \"\"\"\n         new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n@@ -433,7 +433,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n \n     def _pad_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n@@ -453,12 +453,12 @@ def get_image_patches(\n         resample: PILImageResampling,\n         data_format: ChannelDimension,\n         input_data_format: ChannelDimension,\n-    ) -> list[np.array]:\n+    ) -> list[np.ndarray]:\n         \"\"\"\n         Process an image with variable resolutions by dividing it into patches.\n \n         Args:\n-            image (np.array):\n+            image (np.ndarray):\n                 The input image to be processed.\n             grid_pinpoints (List):\n                 A string representation of a list of possible resolutions.\n@@ -474,7 +474,7 @@ def get_image_patches(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            list[np.array]: A list of NumPy arrays containing the processed image patches.\n+            list[np.ndarray]: A list of NumPy arrays containing the processed image patches.\n         \"\"\"\n         if not isinstance(grid_pinpoints, list):\n             raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")"
        },
        {
            "sha": "b679ac269747d881f6aa87ffa731fe2673dfd714",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -58,20 +58,20 @@\n \n \n # Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches\n-def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.array]:\n+def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) -> list[np.ndarray]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n     Args:\n-        image (`np.array`):\n+        image (`np.ndarray`):\n             The input image.\n         patch_size (`int`):\n             The size of each patch.\n         input_data_format (`ChannelDimension` or `str`):\n             The channel dimension format of the input image.\n \n     Returns:\n-        list: A list of np.array representing the patches.\n+        list: A list of np.ndarray representing the patches.\n     \"\"\"\n     patches = []\n     height, width = get_image_size(image, channel_dim=input_data_format)\n@@ -87,7 +87,7 @@ def divide_to_patches(image: np.ndarray, patch_size: int, input_data_format) ->\n \n \n # Copied from transformers.models.llava_next.image_processing_llava_next.expand_to_square\n-def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.array:\n+def expand_to_square(image: np.ndarray, background_color, input_data_format) -> np.ndarray:\n     \"\"\"\n     Expands an image to a square by adding a background color.\n     \"\"\"\n@@ -292,12 +292,12 @@ def pad(\n     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._resize_for_patching\n     def _resize_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, resample, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Resizes an image to a target resolution while maintaining aspect ratio.\n \n         Args:\n-            image (np.array):\n+            image (np.ndarray):\n                 The input image.\n             target_resolution (tuple):\n                 The target resolution (height, width) of the image.\n@@ -307,7 +307,7 @@ def _resize_for_patching(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            np.array: The resized and padded image.\n+            np.ndarray: The resized and padded image.\n         \"\"\"\n         new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n \n@@ -327,7 +327,7 @@ def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple\n     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_patching\n     def _pad_for_patching(\n         self, image: np.ndarray, target_resolution: tuple, input_data_format: ChannelDimension\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n@@ -348,12 +348,12 @@ def get_image_patches(\n         resample: PILImageResampling,\n         data_format: ChannelDimension,\n         input_data_format: ChannelDimension,\n-    ) -> list[np.array]:\n+    ) -> list[np.ndarray]:\n         \"\"\"\n         Process an image with variable resolutions by dividing it into patches.\n \n         Args:\n-            image (np.array):\n+            image (np.ndarray):\n                 The input image to be processed.\n             grid_pinpoints (List):\n                 A string representation of a list of possible resolutions.\n@@ -369,7 +369,7 @@ def get_image_patches(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            list[np.array]: A list of NumPy arrays containing the processed image patches.\n+            list[np.ndarray]: A list of NumPy arrays containing the processed image patches.\n         \"\"\"\n         if not isinstance(grid_pinpoints, list):\n             raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")\n@@ -450,7 +450,7 @@ def pad_to_square(\n         background_color: Union[int, tuple[int, int, int]] = 0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Pads an image to a square based on the longest edge.\n "
        },
        {
            "sha": "752a3221d17f167b9919e4cf3dd7ad40559de508",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -303,7 +303,7 @@ def compute_segments(\n # TODO: (Amy) Move to image_transforms\n # Copied from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n-    segmentation_map: \"np.ndarray\",\n+    segmentation_map: np.ndarray,\n     instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n     ignore_index: Optional[int] = None,\n     do_reduce_labels: bool = False,\n@@ -582,7 +582,7 @@ def rescale(\n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.convert_segmentation_map_to_binary_masks\n     def convert_segmentation_map_to_binary_masks(\n         self,\n-        segmentation_map: \"np.ndarray\",\n+        segmentation_map: np.ndarray,\n         instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,"
        },
        {
            "sha": "c306093e116d128cefd76cc2fce502681d040628",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -308,7 +308,7 @@ def compute_segments(\n \n # TODO: (Amy) Move to image_transforms\n def convert_segmentation_map_to_binary_masks(\n-    segmentation_map: \"np.ndarray\",\n+    segmentation_map: np.ndarray,\n     instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n     ignore_index: Optional[int] = None,\n     do_reduce_labels: bool = False,\n@@ -585,7 +585,7 @@ def rescale(\n \n     def convert_segmentation_map_to_binary_masks(\n         self,\n-        segmentation_map: \"np.ndarray\",\n+        segmentation_map: np.ndarray,\n         instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,"
        },
        {
            "sha": "b4ec184773d4141c937ec2b08cba3e0b819f6d3e",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -144,13 +144,13 @@ def crop_margin(\n         gray_threshold: int = 200,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the\n         threshold).\n \n         Args:\n-            image (`np.array`):\n+            image (`np.ndarray`):\n                 The image to be cropped.\n             gray_threshold (`int`, *optional*, defaults to `200`)\n                 Value below which pixels are considered to be gray."
        },
        {
            "sha": "756480d81e5ae403a92d337ad0f9d176202c4d6a",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -265,7 +265,7 @@ def compute_segments(\n \n # Copied from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n def convert_segmentation_map_to_binary_masks(\n-    segmentation_map: \"np.ndarray\",\n+    segmentation_map: np.ndarray,\n     instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n     ignore_index: Optional[int] = None,\n     do_reduce_labels: bool = False,\n@@ -549,7 +549,7 @@ def rescale(\n     # Copied from transformers.models.maskformer.image_processing_maskformer.MaskFormerImageProcessor.convert_segmentation_map_to_binary_masks\n     def convert_segmentation_map_to_binary_masks(\n         self,\n-        segmentation_map: \"np.ndarray\",\n+        segmentation_map: np.ndarray,\n         instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,"
        },
        {
            "sha": "21c55d51af8e7dfa371efa108f0866fda89837f3",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -2617,7 +2617,7 @@ def interpolate_pos_encoding(self, position_embeddings: torch.Tensor, height: in\n         return position_embeddings\n \n     def forward(\n-        self, batch_size: int, interpolate_pos_encoding: bool = False, input_size: torch.Size = None\n+        self, batch_size: int, interpolate_pos_encoding: bool = False, input_size: Optional[torch.Size] = None\n     ) -> torch.Tensor:\n         position_embeddings = self.position_embeddings\n "
        },
        {
            "sha": "901437964896304c0db4145a00526a800c459881",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -1015,7 +1015,7 @@ def forward(\n     \"\"\"\n )\n class ProphetNetEncoder(ProphetNetPreTrainedModel):\n-    def __init__(self, config: ProphetNetConfig, word_embeddings: nn.Embedding = None):\n+    def __init__(self, config: ProphetNetConfig, word_embeddings: Optional[nn.Embedding] = None):\n         r\"\"\"\n         word_embeddings (`torch.nn.Embeddings` of shape `(config.vocab_size, config.hidden_size)`, *optional*):\n             The word embedding parameters. This can be used to initialize [`ProphetNetEncoder`] with pre-defined word"
        },
        {
            "sha": "6b8910d270bb58a723cfe1766bcc94721950f4f1",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -3040,7 +3040,7 @@ def __init__(self, dim, freq_embed_dim=256):\n         self.time_embed = SinusPositionEmbedding(freq_embed_dim)\n         self.time_mlp = nn.ModuleList([nn.Linear(freq_embed_dim, dim), nn.SiLU(), nn.Linear(dim, dim)])\n \n-    def forward(self, timestep):  # noqa: F821\n+    def forward(self, timestep):\n         time_hidden = self.time_embed(timestep)\n         time_hidden = time_hidden.to(timestep.dtype)\n         for layer in self.time_mlp:"
        },
        {
            "sha": "b63c301f36c3f1c9327ed8e73dcc946113a163bb",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -3338,7 +3338,7 @@ def __init__(self, dim, freq_embed_dim=256):\n         self.time_embed = SinusPositionEmbedding(freq_embed_dim)\n         self.time_mlp = nn.ModuleList([nn.Linear(freq_embed_dim, dim), nn.SiLU(), nn.Linear(dim, dim)])\n \n-    def forward(self, timestep):  # noqa: F821\n+    def forward(self, timestep):\n         time_hidden = self.time_embed(timestep)\n         time_hidden = time_hidden.to(timestep.dtype)\n         for layer in self.time_mlp:"
        },
        {
            "sha": "91d3867484add7ce8b3e0478801900adc45e073c",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -691,7 +691,7 @@ def generate_crop_boxes(\n         Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.\n \n         Args:\n-            image (`np.array`):\n+            image (`np.ndarray`):\n                 Input original image\n             target_size (`int`):\n                 Target size of the resized image\n@@ -758,7 +758,7 @@ def filter_masks(\n                 List of IoU scores.\n             original_size (`tuple[int,int]`):\n                 Size of the original image.\n-            cropped_box_image (`np.array`):\n+            cropped_box_image (`np.ndarray`):\n                 The cropped image.\n             pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n                 The threshold for the iou scores.\n@@ -807,7 +807,7 @@ def _filter_masks_pt(\n                 List of IoU scores.\n             original_size (`tuple[int,int]`):\n                 Size of the original image.\n-            cropped_box_image (`np.array`):\n+            cropped_box_image (`np.ndarray`):\n                 The cropped image.\n             pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n                 The threshold for the iou scores."
        },
        {
            "sha": "2c4b066b6225b9f8aefa230c78ace9794c76cc42",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -277,7 +277,7 @@ def _preprocess_image(\n         do_pad: bool = True,\n         pad_size: Optional[dict[str, int]] = None,\n         constant_values: Optional[Union[float, Iterable[float]]] = None,\n-        pad_mode: PaddingMode = None,\n+        pad_mode: Optional[PaddingMode] = None,\n         do_normalize: Optional[bool] = None,\n         do_flip_channel_order: Optional[bool] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,\n@@ -349,7 +349,7 @@ def preprocess(\n         do_pad: Optional[bool] = None,\n         pad_size: Optional[dict[str, int]] = None,\n         constant_values: Optional[Union[float, Iterable[float]]] = None,\n-        pad_mode: PaddingMode = None,\n+        pad_mode: Optional[PaddingMode] = None,\n         do_normalize: Optional[bool] = None,\n         do_flip_channel_order: Optional[bool] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,"
        },
        {
            "sha": "85b0b0f58d89b3e9964baf9a92bbdf684003829d",
            "filename": "src/transformers/models/vitmatte/configuration_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"VitMatte model configuration\"\"\"\n \n import copy\n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -80,7 +81,7 @@ class VitMatteConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        backbone_config: PretrainedConfig = None,\n+        backbone_config: Optional[PretrainedConfig] = None,\n         backbone=None,\n         use_pretrained_backbone=False,\n         use_timm_backbone=False,"
        },
        {
            "sha": "1974d53119b163fc62f356313b3684cf9887db2e",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -91,14 +91,14 @@ def size_divisibility(self, value):\n \n     def _pad_image(\n         self,\n-        images: \"torch.tensor\",\n+        images: torch.Tensor,\n         size_divisibility: int = 32,\n-    ) -> \"torch.tensor\":\n+    ) -> torch.Tensor:\n         \"\"\"\n         Pads an image or batched images constantly so that width and height are divisible by size_divisibility\n \n         Args:\n-            image (`torch,tensor`):\n+            image (`torch.Tensor`):\n                 Image to pad.\n             size_divisibility (`int`, *optional*, defaults to 32):\n                 The width and height of the image will be padded to be divisible by this number."
        },
        {
            "sha": "95f4dd8e5ded7fce8516985b9d6cc7de736ea51a",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -380,12 +380,12 @@ def affine_transform(\n         size: dict[str, int],\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.array:\n+    ) -> np.ndarray:\n         \"\"\"\n         Apply an affine transformation to an image.\n \n         Args:\n-            image (`np.array`):\n+            image (`np.ndarray`):\n                 Image to transform.\n             center (`tuple[float]`):\n                 Center of the bounding box (x, y)."
        },
        {
            "sha": "e9f9ce04b1ba9b2eca4547bda7ec7c2cef05a75e",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -448,7 +448,7 @@ def _decode(\n     # because we need docs for `output_char_offsets` here\n     def batch_decode(\n         self,\n-        sequences: Union[list[int], list[list[int]], \"np.ndarray\", \"torch.Tensor\"],\n+        sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         output_char_offsets: bool = False,\n@@ -518,7 +518,7 @@ def batch_decode(\n     # and `output_word_offsets` here\n     def decode(\n         self,\n-        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         output_char_offsets: bool = False,"
        },
        {
            "sha": "c819e63fd6cf5a550d8e6d57ae4a132110a410af",
            "filename": "src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -445,7 +445,7 @@ def _decode(\n     # overwritten from `tokenization_utils_base.py` because we need docs for `output_char_offsets` here\n     def decode(\n         self,\n-        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         output_char_offsets: bool = False,\n@@ -501,7 +501,7 @@ def decode(\n     # we need docs for `output_char_offsets` here\n     def batch_decode(\n         self,\n-        sequences: Union[list[int], list[list[int]], \"np.ndarray\", \"torch.Tensor\"],\n+        sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         output_char_offsets: bool = False,"
        },
        {
            "sha": "dad6f96945203963d0f1c57614553d6a871e3fd4",
            "filename": "src/transformers/pipelines/audio_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fpipelines%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fpipelines%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Faudio_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -7,7 +7,7 @@\n import numpy as np\n \n \n-def ffmpeg_read(bpayload: bytes, sampling_rate: int) -> np.array:\n+def ffmpeg_read(bpayload: bytes, sampling_rate: int) -> np.ndarray:\n     \"\"\"\n     Helper function to read an audio file through ffmpeg.\n     \"\"\"\n@@ -173,7 +173,7 @@ def ffmpeg_microphone_live(\n     Return:\n         A generator yielding dictionaries of the following form\n \n-        `{\"sampling_rate\": int, \"raw\": np.array(), \"partial\" bool}` With optionally a `\"stride\" (int, int)` key if\n+        `{\"sampling_rate\": int, \"raw\": np.ndarray, \"partial\" bool}` With optionally a `\"stride\" (int, int)` key if\n         `stride_length_s` is defined.\n \n         `stride` and `raw` are all expressed in `samples`, and `partial` is a boolean saying if the current yield item"
        },
        {
            "sha": "5ed381dff2be33f3fae17c31074bd1210c2584dd",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -311,7 +311,7 @@ class AudioKwargs(TypedDict, total=False):\n     \"\"\"\n \n     sampling_rate: Optional[int]\n-    raw_speech: Optional[Union[\"np.ndarray\", list[float], list[\"np.ndarray\"], list[list[float]]]]\n+    raw_speech: Optional[Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]]]\n     padding: Optional[Union[bool, str, PaddingStrategy]]\n     max_length: Optional[int]\n     truncation: Optional[bool]"
        },
        {
            "sha": "d8ea3688efae1c4f76d6553e1bc33c4a11a6a057",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -433,7 +433,7 @@ def encode(\n \n     def decode(\n         self,\n-        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,\n@@ -475,7 +475,7 @@ def decode(\n \n     def batch_decode(\n         self,\n-        sequences: Union[list[int], list[list[int]], \"np.ndarray\", \"torch.Tensor\"],\n+        sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,"
        },
        {
            "sha": "e08d80991dab0e8b32b4374d2faf5282bf06851b",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -138,7 +138,7 @@ def __str__(self):\n EncodedInputPair = tuple[list[int], list[int]]\n \n # Define type aliases for text-related non-text modalities\n-AudioInput = Union[\"np.ndarray\", \"torch.Tensor\", list[\"np.ndarray\"], list[\"torch.Tensor\"]]\n+AudioInput = Union[np.ndarray, \"torch.Tensor\", list[np.ndarray], list[\"torch.Tensor\"]]\n \n # Slow tokenizers used to be saved in three separated files\n SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n@@ -3815,7 +3815,7 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n \n     def batch_decode(\n         self,\n-        sequences: Union[list[int], list[list[int]], \"np.ndarray\", \"torch.Tensor\"],\n+        sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,\n@@ -3849,7 +3849,7 @@ def batch_decode(\n \n     def decode(\n         self,\n-        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,"
        },
        {
            "sha": "5ef1e49ebe65171837f3745990edaeb932d7b083",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -24,7 +24,7 @@\n import re\n import threading\n import time\n-from typing import Any, NamedTuple, Optional, Union\n+from typing import Any, Callable, NamedTuple, Optional, Union\n \n import numpy as np\n \n@@ -778,14 +778,14 @@ def number_of_arguments(func):\n \n \n def find_executable_batch_size(\n-    function: Optional[callable] = None, starting_batch_size: int = 128, auto_find_batch_size: bool = False\n+    function: Optional[Callable] = None, starting_batch_size: int = 128, auto_find_batch_size: bool = False\n ):\n     \"\"\"\n     Args:\n     A basic decorator that will try to execute `function`. If it fails from exceptions related to out-of-memory or\n     CUDNN, the batch size is multiplied by 0.9 and passed to `function`. `function` must take in a `batch_size` parameter as\n     its first argument.\n-        function (`callable`, *optional*)\n+        function (`Callable`, *optional*)\n             A function to wrap\n         starting_batch_size (`int`, *optional*)\n             The batch size to try and fit into memory"
        },
        {
            "sha": "16d690126d37722cbc5cbddbd4c0ce3150cbe099",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -845,7 +845,7 @@ def post_init(self):\n                     \"You current version of `optimum` does not support `modules_in_block_to_quantize` quantization argument, please upgrade `optimum` package to a version superior than 1.15.0 .\"\n                 )\n \n-    def to_dict(self):\n+    def to_dict(self) -> dict[str, Any]:\n         config_dict = super().to_dict()\n         config_dict.pop(\"disable_exllama\", None)\n         return config_dict"
        },
        {
            "sha": "e7001237959939dbe047ba820c76b8fe41da9552",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f682797866f06f8fc88a296ab52da18910d9acae/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=f682797866f06f8fc88a296ab52da18910d9acae",
            "patch": "@@ -100,7 +100,7 @@ def __setitem__(self, key, value):\n         return setattr(self, key, value)\n \n     @property\n-    def timestamps(self) -> float:\n+    def timestamps(self) -> list[float]:\n         \"Timestamps of the sampled frames in seconds.\"\n         if self.fps is None or self.frames_indices is None:\n             raise ValueError(\"Cannot infer video `timestamps` when `fps` or `frames_indices` is None.\")\n@@ -329,7 +329,7 @@ def read_video_opencv(\n     video_path: Union[\"URL\", \"Path\"],\n     sample_indices_fn: Callable,\n     **kwargs,\n-):\n+) -> tuple[np.ndarray, VideoMetadata]:\n     \"\"\"\n     Decode a video using the OpenCV backend.\n \n@@ -345,7 +345,7 @@ def sample_indices_fn(metadata, **kwargs):\n                 return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        tuple[`np.array`, `VideoMetadata`]: A tuple containing:\n+        tuple[`np.ndarray`, `VideoMetadata`]: A tuple containing:\n             - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n             - `VideoMetadata` object.\n     \"\"\"\n@@ -620,7 +620,7 @@ def load_video(\n     backend: str = \"pyav\",\n     sample_indices_fn: Optional[Callable] = None,\n     **kwargs,\n-) -> np.array:\n+) -> np.ndarray:\n     \"\"\"\n     Loads `video` to a numpy array.\n \n@@ -646,7 +646,7 @@ def sample_indices_fn(metadata, **kwargs):\n                 return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        tuple[`np.array`, Dict]: A tuple containing:\n+        tuple[`np.ndarray`, Dict]: A tuple containing:\n             - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n             - Metadata dictionary.\n     \"\"\"\n@@ -721,7 +721,7 @@ def convert_to_rgb(\n     Convert video to RGB by blending the transparency layer if it's in RGBA format, otherwise simply returns it.\n \n     Args:\n-        video (`np.array`):\n+        video (`np.ndarray`):\n             The video to convert.\n         data_format (`ChannelDimension`, *optional*):\n             The channel dimension format of the output video. If unset, will use the inferred format from the input.\n@@ -845,7 +845,7 @@ def _expand_for_data_format(values):\n \n def group_videos_by_shape(\n     videos: list[\"torch.Tensor\"],\n-) -> tuple[dict[tuple[int, int], list[\"torch.Tensor\"]], dict[int, tuple[tuple[int, int], int]]]:\n+) -> tuple[dict[tuple[int, int], \"torch.Tensor\"], dict[int, tuple[tuple[int, int], int]]]:\n     \"\"\"\n     Groups videos by shape.\n     Returns a dictionary with the shape as key and a list of videos with that shape as value,\n@@ -867,7 +867,8 @@ def group_videos_by_shape(\n \n \n def reorder_videos(\n-    processed_videos: dict[tuple[int, int], \"torch.Tensor\"], grouped_videos_index: dict[int, tuple[int, int]]\n+    processed_videos: dict[tuple[int, int], \"torch.Tensor\"],\n+    grouped_videos_index: dict[int, tuple[tuple[int, int], int]],\n ) -> list[\"torch.Tensor\"]:\n     \"\"\"\n     Reconstructs a list of videos in the original order."
        }
    ],
    "stats": {
        "total": 289,
        "additions": 147,
        "deletions": 142
    }
}