{
    "author": "tomaarsen",
    "message": "Fix typo: langauge -> language (#39317)",
    "sha": "5111c8ea2f3eb918fc090f7dd4393d4204940e10",
    "files": [
        {
            "sha": "101bd1b8b4f4286fd03078e23290dfc53ff278ff",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5111c8ea2f3eb918fc090f7dd4393d4204940e10/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5111c8ea2f3eb918fc090f7dd4393d4204940e10/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=5111c8ea2f3eb918fc090f7dd4393d4204940e10",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n # T5Gemma\n \n-T5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://arxiv.org/abs/2504.06225) by Google. It is a family of encoder-decoder large langauge models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\n+T5Gemma (aka encoder-decoder Gemma) was proposed in a [research paper](https://arxiv.org/abs/2504.06225) by Google. It is a family of encoder-decoder large language models, developed by adapting pretrained decoder-only models into encoder-decoder. T5Gemma includes pretrained and instruction-tuned variants. The architecture is based on transformer encoder-decoder design following T5, with improvements from Gemma 2: GQA, RoPE, GeGLU activation, RMSNorm, and interleaved local/global attention.\n \n T5Gemma has two groups of model sizes: 1) [Gemma 2](https://ai.google.dev/gemma/docs/core/model_card_2) sizes (2B-2B, 9B-2B, and 9B-9B), which are based on the offical Gemma 2 models (2B and 9B); and 2) [T5](https://arxiv.org/abs/1910.10683) sizes (Small, Base, Large, and XL), where are pretrained under the Gemma 2 framework following T5 configuration. In addition, we also provide a model at ML size (medium large, ~2B in total), which is in-between T5 Large and T5 XL.\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}