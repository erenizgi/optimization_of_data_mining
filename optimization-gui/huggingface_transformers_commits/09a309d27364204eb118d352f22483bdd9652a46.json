{
    "author": "kmehant",
    "message": "fix: fsdp sharded state dict wont work for save_only_model knob (#36627)\n\nSigned-off-by: Mehant Kammakomati <mehant.kammakomati2@ibm.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "09a309d27364204eb118d352f22483bdd9652a46",
    "files": [
        {
            "sha": "274defeee196b22c81a2dd208a2b5393fbcdcaf4",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/09a309d27364204eb118d352f22483bdd9652a46/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/09a309d27364204eb118d352f22483bdd9652a46/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=09a309d27364204eb118d352f22483bdd9652a46",
            "patch": "@@ -5175,6 +5175,12 @@ def create_accelerator_and_postprocess(self):\n             raise ValueError(\n                 \"`auto_find_batch_size` isn't supported yet with DeepSpeed Zero-3. Please consider using Zero-2, Zero-1, or FSDP\"\n             )\n+        if (\n+            self.args.save_only_model\n+            and self.is_fsdp_enabled\n+            and \"SHARDED_STATE_DICT\" in str(self.accelerator.state.fsdp_plugin.state_dict_type)\n+        ):\n+            raise ValueError(\"save_only_model option is not compatible with FSDP state dict type 'SHARDED_STATE_DICT'\")\n \n     def propagate_args_to_deepspeed(self, auto_find_batch_size=False):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 6,
        "additions": 6,
        "deletions": 0
    }
}