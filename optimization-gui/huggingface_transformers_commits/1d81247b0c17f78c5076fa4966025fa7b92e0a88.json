{
    "author": "liangel-02",
    "message": "[torchao safetensors] integrate torchao safetensors support with transformers  (#40735)\n\n* enable torchao safetensors\n\n* enable torchao safetensors support\n\n* add more version checking",
    "sha": "1d81247b0c17f78c5076fa4966025fa7b92e0a88",
    "files": [
        {
            "sha": "55ab06dcb85aed005426312b4bbf1163294838ee",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d81247b0c17f78c5076fa4966025fa7b92e0a88/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d81247b0c17f78c5076fa4966025fa7b92e0a88/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1d81247b0c17f78c5076fa4966025fa7b92e0a88",
            "patch": "@@ -727,11 +727,12 @@ def _load_state_dict_into_meta_model(\n         device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n \n     is_quantized = hf_quantizer is not None\n-    is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n+    is_hqq_or_bnb_or_ao = is_quantized and hf_quantizer.quantization_config.quant_method in {\n         QuantizationMethod.HQQ,\n         QuantizationMethod.BITS_AND_BYTES,\n+        QuantizationMethod.TORCHAO,\n     }\n-    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb\n+    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb_or_ao\n     file_pointer = None\n     if is_meta_state_dict:\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n@@ -873,7 +874,7 @@ def load_shard_file(args):\n         shard_file,\n         state_dict,\n         disk_only_shard_files,\n-        is_hqq_or_bnb,\n+        is_hqq_or_bnb_or_ao,\n         is_quantized,\n         device_map,\n         hf_quantizer,\n@@ -899,7 +900,7 @@ def load_shard_file(args):\n     map_location = \"cpu\"\n     if (\n         shard_file.endswith(\".safetensors\")\n-        and not is_hqq_or_bnb\n+        and not is_hqq_or_bnb_or_ao\n         and not (is_deepspeed_zero3_enabled() and not is_quantized)\n     ):\n         map_location = \"meta\"\n@@ -922,6 +923,13 @@ def load_shard_file(args):\n \n     # Fix the key names\n     state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n+    metadata = None\n+    if shard_file.endswith(\".safetensors\") and is_safetensors_available():\n+        with safe_open(shard_file, framework=\"pt\") as f:\n+            metadata = f.metadata()\n+\n+    if hf_quantizer:\n+        state_dict = hf_quantizer.update_state_dict_with_metadata(state_dict, metadata)\n \n     error_msgs = []\n \n@@ -5277,9 +5285,10 @@ def _load_pretrained_model(\n             QuantizationMethod.HQQ,\n             QuantizationMethod.QUARK,\n         }\n-        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in {\n+        is_hqq_or_bnb_or_ao = is_quantized and hf_quantizer.quantization_config.quant_method in {\n             QuantizationMethod.HQQ,\n             QuantizationMethod.BITS_AND_BYTES,\n+            QuantizationMethod.TORCHAO,\n         }\n \n         # Get all the keys of the state dicts that we have to initialize the model\n@@ -5451,7 +5460,7 @@ def _load_pretrained_model(\n                 shard_file,\n                 state_dict,\n                 disk_only_shard_files,\n-                is_hqq_or_bnb,\n+                is_hqq_or_bnb_or_ao,\n                 is_quantized,\n                 device_map,\n                 hf_quantizer,"
        },
        {
            "sha": "8710e1426a8e6f2c43d1455d27313c326135c38e",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d81247b0c17f78c5076fa4966025fa7b92e0a88/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d81247b0c17f78c5076fa4966025fa7b92e0a88/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=1d81247b0c17f78c5076fa4966025fa7b92e0a88",
            "patch": "@@ -342,6 +342,10 @@ def get_state_dict_and_metadata(self, model, safe_serialization=False):\n         \"\"\"Get state dict and metadata. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n         return None, {}\n \n+    def update_state_dict_with_metadata(self, state_dict, metadata):\n+        \"\"\"Update state dict with metadata. Default behaviour returns state_dict\"\"\"\n+        return state_dict\n+\n     @abstractmethod\n     def _process_model_before_weight_loading(self, model, **kwargs): ...\n "
        },
        {
            "sha": "344c9e3534ed2b78e7db1d52c3818e41702b504c",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 56,
            "deletions": 4,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d81247b0c17f78c5076fa4966025fa7b92e0a88/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d81247b0c17f78c5076fa4966025fa7b92e0a88/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=1d81247b0c17f78c5076fa4966025fa7b92e0a88",
            "patch": "@@ -35,6 +35,17 @@\n     import torch\n     import torch.nn as nn\n \n+if is_torchao_available():\n+    import torchao\n+\n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.14.0\"):\n+        from torchao.prototype.safetensors.safetensors_support import (\n+            flatten_tensor_state_dict,\n+            unflatten_tensor_state_dict,\n+        )\n+        from torchao.prototype.safetensors.safetensors_utils import is_metadata_torchao\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -81,6 +92,15 @@ def _linear_extra_repr(self):\n         return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight={weight}\"\n \n \n+if is_torchao_available():\n+    SUPPORTED_SAFE_SERIALIZATION_CONFIGS = [\n+        torchao.quantization.Float8WeightOnlyConfig,\n+        torchao.quantization.Float8DynamicActivationFloat8WeightConfig,\n+    ]\n+\n+    TORCHAO_VERSION = version.parse(importlib.metadata.version(\"torchao\"))\n+\n+\n class TorchAoHfQuantizer(HfQuantizer):\n     \"\"\"\n     Quantizer for torchao: https://github.com/pytorch/ao/\n@@ -137,6 +157,21 @@ def update_dtype(self, dtype):\n                 dtype = torch.float32\n         return dtype\n \n+    def get_state_dict_and_metadata(self, model, safe_serialization: Optional[bool] = False):\n+        \"\"\"\n+        If the model is safe serializable, we flatten the state dict of tensor subclasses so that it is compatible with\n+        the safetensors format.\n+        \"\"\"\n+        if type(self.quantization_config.quant_type) in SUPPORTED_SAFE_SERIALIZATION_CONFIGS and safe_serialization:\n+            if TORCHAO_VERSION >= version.parse(\"0.14.0\"):\n+                return flatten_tensor_state_dict(model.state_dict())\n+            else:\n+                raise RuntimeError(\n+                    f\"In order to use safetensors with torchao, please use torchao version >= 0.14.0. Current version: {TORCHAO_VERSION}\"\n+                )\n+        else:\n+            return super().get_state_dict_and_metadata(model)\n+\n     def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.19.0\"):\n             from accelerate.utils import CustomDtype\n@@ -279,6 +314,16 @@ def create_quantized_param(\n \n             quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n+    def update_state_dict_with_metadata(self, state_dict, metadata):\n+        \"\"\"\n+        If the metadata contains torchao tensor subclass information, we reconstruct the tensor subclass state dict\n+        from the provided state_dict and metadata.\n+        \"\"\"\n+        if TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(metadata):\n+            return unflatten_tensor_state_dict(state_dict, metadata)\n+        else:\n+            return super().update_state_dict_with_metadata(state_dict, metadata)\n+\n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n         if self.quantization_config.quant_type == \"autoquant\":\n@@ -297,10 +342,17 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n \n     def is_serializable(self, safe_serialization=None) -> bool:\n         if safe_serialization:\n-            logger.warning(\n-                \"torchao quantized model does not support safe serialization, please set `safe_serialization` to False\"\n-            )\n-            return False\n+            _is_torchao_serializable = type(\n+                self.quantization_config.quant_type\n+            ) in SUPPORTED_SAFE_SERIALIZATION_CONFIGS and TORCHAO_VERSION >= version.parse(\"0.14.0\")\n+            if not _is_torchao_serializable:\n+                logger.warning(\n+                    f\"torchao quantized model only supports safe serialization for {SUPPORTED_SAFE_SERIALIZATION_CONFIGS}, \\\n+                    and torchao version >= 0.14.0, please set `safe_serialization` to False for \\\n+                    {type(self.quantization_config.quant_type)} and {TORCHAO_VERSION}.\"\n+                )\n+            return _is_torchao_serializable\n+\n         _is_torchao_serializable = version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse(\n             \"0.25.0\"\n         )"
        },
        {
            "sha": "1ddc2de0801f4d085ac582b57540572c59397c0c",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 54,
            "deletions": 8,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d81247b0c17f78c5076fa4966025fa7b92e0a88/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d81247b0c17f78c5076fa4966025fa7b92e0a88/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=1d81247b0c17f78c5076fa4966025fa7b92e0a88",
            "patch": "@@ -18,6 +18,7 @@\n import unittest\n \n from packaging import version\n+from parameterized import parameterized\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n from transformers.testing_utils import (\n@@ -37,6 +38,8 @@\n     import torch\n \n if is_torchao_available():\n+    import torchao\n+\n     # renamed in torchao 0.7.0, please install the latest torchao\n     from torchao.dtypes import (\n         AffineQuantizedTensor,\n@@ -135,7 +138,7 @@ class TorchAoTest(unittest.TestCase):\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     device = \"cpu\"\n     quant_scheme_kwargs = (\n-        {\"group_size\": 32, \"layout\": Int4CPULayout()}\n+        {\"group_size\": 32, \"layout\": Int4CPULayout(), \"version\": 1}\n         if is_torchao_available() and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n         else {\"group_size\": 32}\n     )\n@@ -225,6 +228,7 @@ def test_include_input_output_embeddings(self):\n             weight_dtype=weight_dtype,\n             granularity=granularity,\n             mapping_type=mapping_type,\n+            version=1,\n         )\n         config = ModuleFqnToConfig(\n             {\"_default\": None, \"model.embed_tokens\": embedding_config, \"lm_head\": embedding_config}\n@@ -277,7 +281,7 @@ def test_per_module_config_skip(self):\n @require_torch_accelerator\n class TorchAoAcceleratorTest(TorchAoTest):\n     device = torch_device\n-    quant_scheme_kwargs = {\"group_size\": 32}\n+    quant_scheme_kwargs = {\"group_size\": 32, \"version\": 1}\n \n     # called only once for all test in this class\n     @classmethod\n@@ -327,7 +331,7 @@ def test_int4wo_offload(self):\n             \"lm_head\": 0,\n         }\n \n-        quant_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n+        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n@@ -399,7 +403,7 @@ def test_autoquant(self):\n \n         check_autoquantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n \n-        EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJane: (sighs)\"\n+        EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n         output = quantized_model.generate(\n             **input_ids, max_new_tokens=self.max_new_tokens, cache_implementation=\"static\"\n         )\n@@ -414,7 +418,7 @@ class TorchAoSerializationTest(unittest.TestCase):\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     quant_scheme = \"int4_weight_only\"\n     quant_scheme_kwargs = (\n-        {\"group_size\": 32, \"layout\": Int4CPULayout()}\n+        {\"group_size\": 32, \"layout\": Int4CPULayout(), \"version\": 1}\n         if is_torchao_available() and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n         else {\"group_size\": 32}\n     )\n@@ -447,13 +451,13 @@ def test_original_model_expected_output(self):\n \n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    def check_serialization_expected_output(self, device, expected_output):\n+    def check_serialization_expected_output(self, device, expected_output, safe_serialization=False):\n         \"\"\"\n         Test if we can serialize and load/infer the model again on the same device\n         \"\"\"\n         dtype = torch.bfloat16 if self.quant_scheme == \"int4_weight_only\" else \"auto\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n-            self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n+            self.quantized_model.save_pretrained(tmpdirname, safe_serialization=safe_serialization)\n             loaded_quantized_model = AutoModelForCausalLM.from_pretrained(tmpdirname, dtype=dtype, device_map=device)\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(device)\n \n@@ -464,6 +468,48 @@ def test_serialization_expected_output(self):\n         self.check_serialization_expected_output(self.device, self.EXPECTED_OUTPUT)\n \n \n+@require_torchao\n+@require_torchao_version_greater_or_equal(\"0.14.0\")\n+class TorchAoSafeSerializationTest(TorchAoSerializationTest):\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n+\n+    def tearDown(self):\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+        if hasattr(self, \"quantized_model\"):\n+            del self.quantized_model\n+        gc.collect()\n+\n+    test_params = (\n+        [\n+            (\n+                torchao.quantization.Float8DynamicActivationFloat8WeightConfig(),\n+                \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+            ),\n+            (torchao.quantization.Float8WeightOnlyConfig(), \"What are we having for dinner?\\n\\nJessica: (smiling)\"),\n+        ]\n+        if is_torchao_available()\n+        else []\n+    )\n+\n+    @parameterized.expand(test_params, skip_on_empty=True)\n+    def test_serialization_expected_output(self, config, expected_output):\n+        device = \"cuda\"\n+        self.quant_config = TorchAoConfig(config)\n+        self.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            dtype=torch.bfloat16,\n+            device_map=device,\n+            quantization_config=self.quant_config,\n+        )\n+        self.check_serialization_expected_output(device, expected_output, safe_serialization=True)\n+\n+\n class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n \n@@ -500,7 +546,7 @@ def test_serialization_expected_output_on_accelerator(self):\n \n @require_torch_accelerator\n class TorchAoSerializationAcceleratorTest(TorchAoSerializationTest):\n-    quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32}\n+    quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32, \"version\": 1}\n     device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class"
        }
    ],
    "stats": {
        "total": 147,
        "additions": 129,
        "deletions": 18
    }
}