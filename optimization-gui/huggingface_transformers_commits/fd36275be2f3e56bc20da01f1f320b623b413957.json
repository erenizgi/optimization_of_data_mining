{
    "author": "molbap",
    "message": "handle inputs from Siglip/Siglip2 non-automapped encoder layers (#41930)\n\n* handle inputs from non-automapped encoder layers\n\n* correct inheritance + protect executorch\n\n* fixup\n\n* fix tests\n\n* missing docstring\n\n* attn support\n\n* fix initialization\n\n* reorder/simplify\n\n* flag test as broken\n\n* minor changes\n\n* modulaaar",
    "sha": "fd36275be2f3e56bc20da01f1f320b623b413957",
    "files": [
        {
            "sha": "2379f95cc8e72e2782112e2731be00cb1960d3a9",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd36275be2f3e56bc20da01f1f320b623b413957/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd36275be2f3e56bc20da01f1f320b623b413957/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=fd36275be2f3e56bc20da01f1f320b623b413957",
            "patch": "@@ -678,9 +678,14 @@ def forward(\n         )\n \n \n-class SiglipVisionTransformer(nn.Module):\n+class SiglipVisionTransformer(SiglipPreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": SiglipEncoderLayer,\n+        \"attentions\": SiglipAttention,\n+    }\n+\n     def __init__(self, config: SiglipVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         embed_dim = config.hidden_size\n \n@@ -691,6 +696,7 @@ def __init__(self, config: SiglipVisionConfig):\n         if self.use_head:\n             self.head = SiglipMultiheadAttentionPoolingHead(config)\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ee1e0620b02c6c3ad97a90fee226de61cf4e22e1",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 99,
            "deletions": 93,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd36275be2f3e56bc20da01f1f320b623b413957/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd36275be2f3e56bc20da01f1f320b623b413957/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=fd36275be2f3e56bc20da01f1f320b623b413957",
            "patch": "@@ -349,99 +349,6 @@ def forward(\n         return hidden_states\n \n \n-class Siglip2Encoder(nn.Module):\n-    \"\"\"\n-    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n-    [`Siglip2EncoderLayer`].\n-\n-    Args:\n-        config: Siglip2Config\n-    \"\"\"\n-\n-    def __init__(self, config: Siglip2Config):\n-        super().__init__()\n-        self.config = config\n-        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n-\n-    # Ignore copy\n-    @auto_docstring\n-    def forward(\n-        self,\n-        inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> BaseModelOutput:\n-        hidden_states = inputs_embeds\n-        for encoder_layer in self.layers:\n-            hidden_states = encoder_layer(\n-                hidden_states,\n-                attention_mask,\n-                **kwargs,\n-            )\n-\n-        return BaseModelOutput(last_hidden_state=hidden_states)\n-\n-\n-class Siglip2VisionTransformer(nn.Module):\n-    def __init__(self, config: Siglip2VisionConfig):\n-        super().__init__()\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = Siglip2VisionEmbeddings(config)\n-        self.encoder = Siglip2Encoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n-        if self.use_head:\n-            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        attention_mask: torch.Tensor,\n-        spatial_shapes: torch.LongTensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-    ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n-            Tensor containing the spatial dimensions (height, width) of the input images.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n-\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-        else:\n-            encoder_attention_mask = attention_mask\n-\n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            inputs_embeds=hidden_states,\n-            attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-\n-        last_hidden_state = encoder_outputs.last_hidden_state\n-        last_hidden_state = self.post_layernorm(last_hidden_state)\n-\n-        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n def _trunc_normal_(tensor, mean, std, a, b):\n     # Cut & paste from PyTorch official master until it's in a few official releases - RW\n     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n@@ -607,6 +514,105 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n+class Siglip2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Siglip2EncoderLayer`].\n+\n+    Args:\n+        config: Siglip2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @auto_docstring\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                **kwargs,\n+            )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class Siglip2VisionTransformer(Siglip2PreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": Siglip2EncoderLayer,\n+        \"attentions\": Siglip2Attention,\n+    }\n+\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = Siglip2VisionEmbeddings(config)\n+        self.encoder = Siglip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n+        if self.use_head:\n+            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+            Tensor containing the spatial dimensions (height, width) of the input images.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n+\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+        else:\n+            encoder_attention_mask = attention_mask\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n class Siglip2TextEmbeddings(nn.Module):\n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__()"
        },
        {
            "sha": "1b0a06f666c249cd46e777ef9997e7fd6cc2f1a6",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd36275be2f3e56bc20da01f1f320b623b413957/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd36275be2f3e56bc20da01f1f320b623b413957/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=fd36275be2f3e56bc20da01f1f320b623b413957",
            "patch": "@@ -37,6 +37,7 @@\n \n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...utils import auto_docstring, filter_out_non_signature_kwargs\n+from ...utils.generic import check_model_inputs\n \n \n class Siglip2TextConfig(SiglipTextConfig):\n@@ -230,6 +231,10 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n         return embeddings\n \n \n+class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n+    pass\n+\n+\n class Siglip2VisionTransformer(SiglipVisionTransformer):\n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n@@ -280,10 +285,6 @@ def forward(\n         )\n \n \n-class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n-    pass\n-\n-\n class Siglip2TextModel(SiglipTextModel):\n     pass\n \n@@ -314,6 +315,8 @@ def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Ten\n \n class Siglip2VisionModel(SiglipVisionModel):\n     # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
        },
        {
            "sha": "762b50e12ceb79f6af30df13e5e361a67c26bb7e",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd36275be2f3e56bc20da01f1f320b623b413957/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd36275be2f3e56bc20da01f1f320b623b413957/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=fd36275be2f3e56bc20da01f1f320b623b413957",
            "patch": "@@ -90,6 +90,8 @@\n     \"Kosmos2_5TextForCausalLM\",\n     \"Kosmos2_5VisionModel\",\n     \"SmolVLMVisionTransformer\",\n+    \"SiglipVisionTransformer\",\n+    \"Siglip2VisionTransformer\",\n     \"AriaTextForCausalLM\",\n     \"AriaTextModel\",\n     \"Phi4MultimodalAudioModel\",\n@@ -358,7 +360,9 @@\n     \"SegGptForImageSegmentation\",\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n+    \"SiglipVisionTransformer\",\n     \"Siglip2VisionModel\",\n+    \"Siglip2VisionTransformer\",\n     \"Siglip2TextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n     \"VitPoseForPoseEstimation\","
        }
    ],
    "stats": {
        "total": 217,
        "additions": 118,
        "deletions": 99
    }
}