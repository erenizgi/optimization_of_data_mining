{
    "author": "gante",
    "message": "[test] update `test_past_key_values_format` (#37614)\n\nallow custom shapes",
    "sha": "362fa37da239d4d4dea456850cabc79c8a2ffa16",
    "files": [
        {
            "sha": "caae8738e3f648e43fbed82f5ead00299dba2590",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 98,
            "deletions": 57,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -1539,92 +1539,133 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @pytest.mark.generate\n-    def test_past_key_values_format(self):\n-        # Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a\n-        # standard KV cache format is important for a consistent API (and for advanced generation methods).\n+    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n+        \"\"\"\n+        Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test, or pass the\n+        expected cache shapes.\n+        Having a standard KV cache format is important for a consistent API (and for advanced generation methods).\n+        \"\"\"\n         for model_class in self.all_generative_model_classes:\n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n \n-            # If it doesn't support cache, pass the test\n+            # 1. If it doesn't support cache, skip the test\n             if not hasattr(config.get_text_config(), \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n             model = model_class(config).to(torch_device)\n+            model = model.eval()\n             if \"use_cache\" not in inputs:\n                 inputs[\"use_cache\"] = True\n             outputs = model(**inputs)\n \n-            # If \"past_key_values\" is not returned, pass the test (e.g. RWKV uses a different cache name and format)\n             if \"past_key_values\" not in outputs:\n                 self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n \n+            # 2. retrieve the KV cache and compute its default expected shapes (if no custom shapes are provided)\n+            past_kv = outputs[\"past_key_values\"]\n+            is_legacy_cache = not isinstance(past_kv, Cache)\n+\n             text_config = config.get_text_config()\n-            num_hidden_layers = (\n+            num_decoder_layers = (\n                 getattr(text_config, \"decoder_layers\", None)\n                 or getattr(text_config, \"num_decoder_layers\", None)\n                 or text_config.num_hidden_layers\n             )\n-            num_attention_heads = getattr(text_config, \"decoder_attention_heads\", text_config.num_attention_heads)\n-            embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n-            per_head_embed_dim = embed_dim // num_attention_heads\n-\n-            # some models have different num-head for query vs key/value so we need to assign correct value\n-            # BUT only after `per_head_embed_dim` is set\n-            num_attention_heads = (\n-                text_config.num_key_value_heads\n-                if getattr(text_config, \"num_key_value_heads\", None) is not None\n-                else num_attention_heads\n-            )\n \n-            past_kv = outputs[\"past_key_values\"]\n-            self.assertEqual(len(past_kv), num_hidden_layers)\n+            if custom_all_cache_shapes is None:\n+                num_query_attention_heads = getattr(\n+                    text_config, \"decoder_attention_heads\", text_config.num_attention_heads\n+                )\n+                embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n+                per_head_embed_dim = embed_dim // num_query_attention_heads\n+                num_key_value_heads = (\n+                    text_config.num_key_value_heads\n+                    if getattr(text_config, \"num_key_value_heads\", None) is not None\n+                    else num_query_attention_heads\n+                )\n+                if config.is_encoder_decoder:\n+                    encoder_num_attention_heads = (\n+                        text_config.encoder_attention_heads\n+                        if hasattr(text_config, \"encoder_attention_heads\")\n+                        else text_config.num_attention_heads\n+                    )\n+                    encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n+                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape\n+                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n+                    # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n+                    default_cross_attention_shape = (\n+                        batch_size,\n+                        encoder_num_attention_heads,\n+                        encoder_per_head_embed_dim,\n+                    )\n+                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+                    all_cache_shapes = [\n+                        [\n+                            default_self_attention_shape,\n+                            default_self_attention_shape,\n+                            default_cross_attention_shape,\n+                            default_cross_attention_shape,\n+                        ]\n+                        for _ in range(num_decoder_layers)\n+                    ]\n+                else:\n+                    batch_size, seq_length = inputs[\"input_ids\"].shape\n+                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+                    all_cache_shapes = [\n+                        [default_self_attention_shape, default_self_attention_shape] for _ in range(num_decoder_layers)\n+                    ]\n \n-            # Encoder-Decoder checks\n+            else:\n+                all_cache_shapes = custom_all_cache_shapes\n+\n+            # 3. Check cache shapes\n+            # 3.1. Encoder-Decoder checks\n             if config.is_encoder_decoder:\n-                # encoder-decoder models usually don't have text config\n-                # below is needed only for Pix2Struct which we cannot modify now due to BC\n-                config = config.get_text_config()\n-                encoder_num_attention_heads = (\n-                    config.encoder_attention_heads\n-                    if hasattr(config, \"encoder_attention_heads\")\n-                    else config.num_attention_heads\n+                num_cache_decoder_layers = (\n+                    len(past_kv) if is_legacy_cache else len(past_kv.self_attention_cache.key_cache)\n                 )\n-                encoder_per_head_embed_dim = embed_dim // encoder_num_attention_heads\n-                batch_size, seq_length = inputs[\"decoder_input_ids\"].shape\n-                for i in range(num_hidden_layers):\n-                    self.assertEqual(len(past_kv[i]), 4)  # K V for the decoder + K V for the encoder = 4\n-                    self.assertEqual(\n-                        past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n+                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n+\n+                for i in range(num_decoder_layers):\n+                    if is_legacy_cache:\n+                        self.assertEqual(len(past_kv[0]), 4)  # legacy check: confirm number of elements in tuple\n+\n+                    # Self attention\n+                    self_attention_layer_key_cache = (\n+                        past_kv[i][0] if is_legacy_cache else past_kv.self_attention_cache.key_cache[i]\n                     )\n-                    self.assertEqual(\n-                        past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n+                    self_attention_layer_value_cache = (\n+                        past_kv[i][1] if is_legacy_cache else past_kv.self_attention_cache.value_cache[i]\n                     )\n-                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n-                    # autoregressive generation, I'm keeping the test general and not checking the 3rd dim\n-                    self.assertEqual(\n-                        (past_kv[i][2].shape[0], past_kv[i][2].shape[1], past_kv[i][2].shape[3]),\n-                        (batch_size, encoder_num_attention_heads, encoder_per_head_embed_dim),\n+                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n+\n+                    # Cross attention (ignore 3rd dim, see default shape preparation)\n+                    cross_attention_layer_key_cache = (\n+                        past_kv[i][2] if is_legacy_cache else past_kv.cross_attention_cache.key_cache[i]\n                     )\n-                    self.assertEqual(\n-                        (past_kv[i][3].shape[0], past_kv[i][3].shape[1], past_kv[i][3].shape[3]),\n-                        (batch_size, encoder_num_attention_heads, encoder_per_head_embed_dim),\n+                    cross_attention_layer_value_cache = (\n+                        past_kv[i][3] if is_legacy_cache else past_kv.cross_attention_cache.value_cache[i]\n                     )\n+                    cross_attention_layer_key_cache = cross_attention_layer_key_cache[:, :, 0, :]\n+                    cross_attention_layer_value_cache = cross_attention_layer_value_cache[:, :, 0, :]\n+                    self.assertEqual(cross_attention_layer_key_cache.shape, all_cache_shapes[i][2])\n+                    self.assertEqual(cross_attention_layer_value_cache.shape, all_cache_shapes[i][3])\n \n-            # Decoder-only checks\n+            # 3.2. Decoder-only checks\n             else:\n-                # TODO: this line is only needed because of imagegpt, where \"pixel_values\" = \"input_ids\". Fix the\n-                # tests in imagegpt such that `prepare_config_and_inputs_for_common` returns the later (and the other\n-                # tests use it)\n-                key = \"input_ids\" if \"input_ids\" in inputs else \"pixel_values\"\n-                batch_size, seq_length = inputs[key].shape\n-                for i in range(num_hidden_layers):\n-                    self.assertEqual(len(past_kv[0]), 2)  # K V for the decoder = 2\n-                    self.assertEqual(\n-                        past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n-                    )\n-                    self.assertEqual(\n-                        past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n-                    )\n+                num_cache_decoder_layers = len(past_kv) if is_legacy_cache else len(past_kv.key_cache)\n+                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n+\n+                for i in range(num_decoder_layers):\n+                    if is_legacy_cache:\n+                        self.assertEqual(len(past_kv[0]), 2)  # legacy check: confirm number of elements in tuple\n+\n+                    # Self attention\n+                    self_attention_layer_key_cache = past_kv[i][0] if is_legacy_cache else past_kv.key_cache[i]\n+                    self_attention_layer_value_cache = past_kv[i][1] if is_legacy_cache else past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n+                    self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n \n     @pytest.mark.generate\n     @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])"
        },
        {
            "sha": "1c2690b54edf0033b14b998d1584beb35e9af711",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -429,9 +429,23 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    @unittest.skip(reason=\"Deepseek-V3 uses MLA on all models so the KV cache is a non standard format\")\n     def test_past_key_values_format(self):\n-        pass\n+        \"\"\"\n+        Overwritting to pass the expected cache shapes (Deepseek-V3 uses MLA so the cache shapes are non-standard)\n+        \"\"\"\n+        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+        batch_size, seq_length = inputs[\"input_ids\"].shape\n+        # difference: last dim\n+        k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n+        v_embed_dim = config.v_head_dim\n+        self_attention_key_cache_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n+        self_attention_value_cache_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n+        # build the full cache shapes\n+        num_hidden_layers = config.num_hidden_layers\n+        all_cache_shapes = [\n+            [self_attention_key_cache_shape, self_attention_value_cache_shape] for _ in range(num_hidden_layers)\n+        ]\n+        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n \n     @require_torch_sdpa\n     @slow"
        },
        {
            "sha": "6a63177476bc60e1090fb01ae01748b892bf9b07",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -264,51 +264,6 @@ def test_falcon_sequence_classification_model_for_multi_label(self):\n         result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n         self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n \n-    def test_past_key_values_format(self):\n-        # Falcon can have different numbers of KV-heads than the number of query heads, so we need\n-        # to override this test to use the right head counts.\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            # If it doesn't support cache, pass the test\n-            if not hasattr(config, \"use_cache\"):\n-                self.skipTest(reason=\"Model does not support cache\")\n-\n-            model = model_class(config).to(torch_device)\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            # If \"past_key_values\" is not returned, pass the test (e.g. RWKV uses a different cache name and format)\n-            if \"past_key_values\" not in outputs:\n-                self.skipTest(reason=\"Model does not return past_key_values\")\n-\n-            num_hidden_layers = (\n-                getattr(config, \"decoder_layers\", None)\n-                or getattr(config, \"num_decoder_layers\", None)\n-                or config.num_hidden_layers\n-            )\n-            num_attention_heads = getattr(config, \"num_kv_heads\", config.num_attention_heads)\n-            embed_dim = getattr(config, \"d_model\", config.hidden_size)\n-            per_head_embed_dim = embed_dim // num_attention_heads\n-\n-            past_kv = outputs[\"past_key_values\"]\n-            self.assertEqual(len(past_kv), num_hidden_layers)\n-\n-            batch_size, seq_length = inputs[\"input_ids\"].shape\n-            for i in range(num_hidden_layers):\n-                if config.new_decoder_architecture:\n-                    num_attention_heads = config.num_attention_heads\n-                elif config.multi_query:\n-                    num_attention_heads = 1\n-                self.assertEqual(len(past_kv[0]), 2)  # K V for the decoder = 2\n-                self.assertEqual(\n-                    past_kv[i][0].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n-                )\n-                self.assertEqual(\n-                    past_kv[i][1].shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n-                )\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_model_rope_scaling_from_config with Llama->Falcon\n     def test_model_rope_scaling_from_config(self, scaling_type):"
        },
        {
            "sha": "ce0aadd16379fd852b1c0dc44cfb8e2831e6a770",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -296,10 +296,6 @@ def test_Gemma_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Gemma uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "9e8eda5cb239649ab853de1980e380686294fb02",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -264,10 +264,6 @@ def test_Glm_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Glm uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @is_flaky()\n     def test_custom_4d_attention_mask(self):\n         \"\"\"Overwrite the common test to use atol=1e-3 instead of 1e-4. Can still rarely fail, thus flaky.\"\"\""
        },
        {
            "sha": "f604dbf0365da7c8b0e0cfe89a57785764dec0e4",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -222,12 +222,6 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"GotOcr2's language backbone is Qwen2 which uses GQA so the KV cache is a non standard format\"\n-    )\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "c20d00e7337253296566e408733d523a68012761",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -319,6 +319,10 @@ def test_forward_signature(self):\n     def test_left_padding_compatibility(self):\n         pass\n \n+    @unittest.skip(reason=\"Model inputs don't fit test pattern\")  # and it's not used enough to be worth fixing :)\n+    def test_past_key_values_format(self):\n+        pass\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "0dfc7e2cef9705d68910f02826dd7c9d1fb38fce",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -251,10 +251,6 @@ def test_jetmoe_sequence_classification_model_for_multi_label(self):\n         result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n         self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n \n-    @unittest.skip(reason=\"JetMoe uses MoA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "7eee96f2ef94774a0d9668a755570712238b2022",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -292,10 +292,6 @@ def test_Mistral_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Mistral uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "aec7c6f23f8512b7294187e18156786424114e24",
            "filename": "tests/models/mistral/test_modeling_tf_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -324,10 +324,6 @@ def test_Mistral_sequence_classification_model_for_multi_label(self):\n         result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n         self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n \n-    @unittest.skip(\"Mistral uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @unittest.skip(\"Vocab resizing is not supported\")\n     def test_save_load_after_resize_token_embeddings(self):\n         pass"
        },
        {
            "sha": "2d7c95529be2e62bf15c7838d11e453938e204ce",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -291,10 +291,6 @@ def test_Mixtral_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Mixtral uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "6308f6d4c0e993ad6d06a33e5674598ce1478624",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -409,7 +409,7 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n         pass\n \n     @pytest.mark.generate\n-    # overridden because mllama has special cache for self and cross attentions\n+    # overridden because mllama is not an encoder-decoder model, but has encoder-decoder-like cache\n     def test_past_key_values_format(self):\n         # Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a\n         # standard KV cache format is important for a consistent API (and for advanced generation methods)."
        },
        {
            "sha": "1339a09b64fc4f2bf34742173a968a78c3539622",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -303,10 +303,6 @@ def test_Qwen2_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Qwen2 uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "ecea6a34977736f58582dff1d1fd2472a87603d4",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -331,10 +331,6 @@ def test_Qwen2Moe_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Qwen2Moe uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "0a5660ecd2c8858ccbca37f8d9b74def4c362d01",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -306,10 +306,6 @@ def test_Qwen3_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    # Ignore copy\n-    def test_past_key_values_format(self):\n-        super().test_past_key_values_format()\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "c14f71407d987c41e99fc46b78a5799e0b7a894b",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -325,10 +325,6 @@ def test_Qwen3Moe_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    # Ignore copy\n-    def test_past_key_values_format(self):\n-        super().test_past_key_values_format()\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "4a41cecc0ad7205b3a795a11f456a51d6a4d1452",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -223,10 +223,6 @@ def test_model_various_embeddings(self):\n             config_and_inputs[0].position_embedding_type = type\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skip(reason=\"RecurrentGemma does not return pkv\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @unittest.skip(reason=\"RecurrentGemma only supports sdpa\")\n     def test_eager_matches_sdpa_generate(self):\n         pass"
        },
        {
            "sha": "dbc3c0dc8079387b2a8e34e5776e4389437ed889",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -281,10 +281,6 @@ def test_Starcoder2_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n-    @unittest.skip(reason=\"Starcoder2 uses GQA on all models so the KV cache is a non standard format\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "78079293f3a91aa609e8bfa3889bff65c9b81dd3",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/362fa37da239d4d4dea456850cabc79c8a2ffa16/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=362fa37da239d4d4dea456850cabc79c8a2ffa16",
            "patch": "@@ -322,14 +322,22 @@ def setUp(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @unittest.skip(\"Zamba2 has a hybrid cache\")\n     def test_past_key_values_format(self):\n-        r\"\"\"\n-        Zamba2's cache shape depends on whether a given layer is mamba or attention.\n-        For mamba layers, the KV cache has shape is empty and has shape [batch_size, 0].\n-        The shape checks of this test assume instead that every layer has an attention cache, so we skip it.\n         \"\"\"\n-        pass\n+        Overwritting to pass the expected cache shapes (Zamba2 has cache shape = [batch_size, 0] for mamba layers)\n+        \"\"\"\n+        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+        batch_size, seq_length = inputs[\"input_ids\"].shape\n+        per_head_embed_dim = config.attention_head_dim  # note: this one is not a common attribute name\n+        self_attention_cache_shape = (batch_size, config.num_key_value_heads, seq_length, per_head_embed_dim)\n+        # build the full cache shapes, including mamba layers\n+        all_cache_shapes = []\n+        for i in range(config.num_hidden_layers):\n+            if config.layers_block_type[i] == \"mamba\":\n+                all_cache_shapes.append([torch.Size([batch_size, 0]), torch.Size([batch_size, 0])])\n+            else:\n+                all_cache_shapes.append([self_attention_cache_shape, self_attention_cache_shape])\n+        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n \n     @unittest.skip(reason=\"Zamba2 has hybrid cache.\")\n     def test_generate_continue_from_inputs_embeds(self):"
        }
    ],
    "stats": {
        "total": 298,
        "additions": 133,
        "deletions": 165
    }
}