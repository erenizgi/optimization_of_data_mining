{
    "author": "stevhliu",
    "message": "[docs] Update README (#36265)\n\n* update\n\n* feedback\n\n* feedback\n\n* update versions",
    "sha": "ac1a1b66b99be5b2fbec055b794259e75e19565d",
    "files": [
        {
            "sha": "c03ad89c3367cb8afd78681edf9aa1e60b31d00f",
            "filename": "README.md",
            "status": "modified",
            "additions": 169,
            "deletions": 189,
            "changes": 358,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac1a1b66b99be5b2fbec055b794259e75e19565d/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac1a1b66b99be5b2fbec055b794259e75e19565d/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=ac1a1b66b99be5b2fbec055b794259e75e19565d",
            "patch": "@@ -25,6 +25,7 @@ limitations under the License.\n </p>\n \n <p align=\"center\">\n+    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n     <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n     <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n     <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n@@ -54,275 +55,254 @@ limitations under the License.\n </h4>\n \n <h3 align=\"center\">\n-    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>\n+    <p>State-of-the-art pretrained models for inference and training</p>\n </h3>\n \n <h3 align=\"center\">\n     <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n </h3>\n \n-ü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.\n+Transformers is a library of pretrained text, computer vision, audio, video, and multimodal models for inference and training. Use Transformers to fine-tune models on your data, build inference applications, and for generative AI use cases across multiple modalities.\n \n-These models can be applied on:\n+There are over 500K+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n \n-* üìù Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.\n-* üñºÔ∏è Images, for tasks like image classification, object detection, and segmentation.\n-* üó£Ô∏è Audio, for tasks like speech recognition and audio classification.\n+Explore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n \n-Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n+## Installation\n \n-ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.\n+Transformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.0+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.\n \n-ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.\n+Create and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n \n-## Online demos\n+```py\n+# venv\n+python -m venv my-env\n+source ./my-env/bin/activate\n \n-You can test most of our models directly on their pages from the [model hub](https://huggingface.co/models). We also offer [private model hosting, versioning, & an inference API](https://huggingface.co/pricing) for public and private models.\n+# uv\n+uv venv my-env\n+source ./my-env/bin/activate\n+```\n \n-Here are a few examples:\n+Install Transformers in your virtual environment.\n \n-In Natural Language Processing:\n-- [Masked word completion with BERT](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n-- [Named Entity Recognition with Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n-- [Text generation with Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n-- [Natural Language Inference with RoBERTa](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n-- [Summarization with BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n-- [Question answering with DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n-- [Translation with T5](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n+```py\n+# pip\n+pip install transformers\n \n-In Computer Vision:\n-- [Image classification with ViT](https://huggingface.co/google/vit-base-patch16-224)\n-- [Object Detection with DETR](https://huggingface.co/facebook/detr-resnet-50)\n-- [Semantic Segmentation with SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n-- [Panoptic Segmentation with Mask2Former](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)\n-- [Depth Estimation with Depth Anything](https://huggingface.co/docs/transformers/main/model_doc/depth_anything)\n-- [Video Classification with VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\n-- [Universal Segmentation with OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)\n+# uv\n+uv pip install transformers\n+```\n \n-In Audio:\n-- [Automatic Speech Recognition with Whisper](https://huggingface.co/openai/whisper-large-v3)\n-- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n-- [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\n+Install Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n \n-In Multimodal tasks:\n-- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\n-- [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\n-- [Image captioning with LLaVa](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n-- [Zero-shot Image Classification with SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384)\n-- [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\n-- [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\n-- [Zero-shot Object Detection with OWLv2](https://huggingface.co/docs/transformers/en/model_doc/owlv2)\n-- [Zero-shot Image Segmentation with CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)\n-- [Automatic Mask Generation with SAM](https://huggingface.co/docs/transformers/model_doc/sam)\n+```shell\n+git clone https://github.com/huggingface/transformers.git\n+cd transformers\n+pip install .\n+```\n \n+## Quickstart\n \n-## 100 projects using Transformers\n+Get started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n \n-Transformers is more than a toolkit to use pretrained models: it's a community of projects built around it and the\n-Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\n-else to build their dream projects.\n+Instantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n \n-In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on the\n-community, and we have created the [awesome-transformers](./awesome-transformers.md) page which lists 100\n-incredible projects built in the vicinity of transformers.\n+```py\n+from transformers import pipeline\n \n-If you own or use a project that you believe should be part of the list, please open a PR to add it!\n+pipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\n+pipeline(\"the secret to baking a really good cake is \")\n+[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n+```\n \n-## Serious about AI in your organisation? Build faster with the Hugging Face Enterprise Hub.\n+To chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n \n-<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n-    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n-</a><br>\n-\n-## Quick tour\n+> [!TIP]\n+> You can also chat with a model directly from the command line.\n+> ```shell\n+> transformers-cli chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n+> ```\n \n-To immediately use a model on a given input (text, image, audio, ...), we provide the `pipeline` API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:\n+```py\n+import torch\n+from transformers import pipeline\n \n-```python\n->>> from transformers import pipeline\n+chat = [\n+    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n+    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n+]\n \n-# Allocate a pipeline for sentiment-analysis\n->>> classifier = pipeline('sentiment-analysis')\n->>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n-[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+response = pipeline(chat, max_new_tokens=512)\n+print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n-The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here, the answer is \"positive\" with a confidence of 99.97%.\n-\n-Many tasks have a pre-trained `pipeline` ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:\n-\n-``` python\n->>> import requests\n->>> from PIL import Image\n->>> from transformers import pipeline\n-\n-# Download an image with cute cats\n->>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n->>> image_data = requests.get(url, stream=True).raw\n->>> image = Image.open(image_data)\n-\n-# Allocate a pipeline for object detection\n->>> object_detector = pipeline('object-detection')\n->>> object_detector(image)\n-[{'score': 0.9982201457023621,\n-  'label': 'remote',\n-  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n- {'score': 0.9960021376609802,\n-  'label': 'remote',\n-  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n- {'score': 0.9954745173454285,\n-  'label': 'couch',\n-  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n- {'score': 0.9988006353378296,\n-  'label': 'cat',\n-  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n- {'score': 0.9986783862113953,\n-  'label': 'cat',\n-  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n+Expand the examples below to see how `Pipeline` works for different modalities and tasks.\n+\n+<details>\n+<summary>Automatic speech recognition</summary>\n+\n+```py\n+from transformers import pipeline\n+\n+pipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n+pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n+{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n ```\n \n-Here, we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:\n+</details>\n+\n+<details>\n+<summary>Image classification</summary>\n \n <h3 align=\"center\">\n-    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\n-    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\n+    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n </h3>\n \n-You can learn more about the tasks supported by the `pipeline` API in [this tutorial](https://huggingface.co/docs/transformers/task_summary).\n+```py\n+from transformers import pipeline\n+\n+pipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\n+pipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n+[{'label': 'macaw', 'score': 0.997848391532898},\n+ {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n+  'score': 0.0016551691805943847},\n+ {'label': 'lorikeet', 'score': 0.00018523589824326336},\n+ {'label': 'African grey, African gray, Psittacus erithacus',\n+  'score': 7.85409429227002e-05},\n+ {'label': 'quail', 'score': 5.502637941390276e-05}]\n+```\n \n-In addition to `pipeline`, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:\n-```python\n->>> from transformers import AutoTokenizer, AutoModel\n+</details>\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n+<details>\n+<summary>Visual question answering</summary>\n \n->>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n->>> outputs = model(**inputs)\n-```\n \n-And here is the equivalent code for TensorFlow:\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModel\n+<h3 align=\"center\">\n+    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n+</h3>\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n+```py\n+from transformers import pipeline\n \n->>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n->>> outputs = model(**inputs)\n+pipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\n+pipeline(\n+    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n+    question=\"What is in the image?\",\n+)\n+[{'answer': 'statue of liberty'}]\n ```\n \n-The tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.\n-\n-The model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (depending on your backend) which you can use as usual. [This tutorial](https://huggingface.co/docs/transformers/training) explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our `Trainer` API to quickly fine-tune on a new dataset.\n+</details>\n \n-## Why should I use transformers?\n+## Why should I use Transformers?\n \n 1. Easy-to-use state-of-the-art models:\n-    - High performance on natural language understanding & generation, computer vision, and audio tasks.\n-    - Low barrier to entry for educators and practitioners.\n+    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n+    - Low barrier to entry for researchers, engineers, and developers.\n     - Few user-facing abstractions with just three classes to learn.\n     - A unified API for using all our pretrained models.\n \n 1. Lower compute costs, smaller carbon footprint:\n-    - Researchers can share trained models instead of always retraining.\n-    - Practitioners can reduce compute time and production costs.\n-    - Dozens of architectures with over 400,000 pretrained models across all modalities.\n+    - Share trained models instead of training from scratch.\n+    - Reduce compute time and production costs.\n+    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n \n-1. Choose the right framework for every part of a model's lifetime:\n+1. Choose the right framework for every part of a models lifetime:\n     - Train state-of-the-art models in 3 lines of code.\n-    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.\n-    - Seamlessly pick the right framework for training, evaluation, and production.\n+    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n+    - Pick the right framework for training, evaluation, and production.\n \n 1. Easily customize a model or an example to your needs:\n     - We provide examples for each architecture to reproduce the results published by its original authors.\n     - Model internals are exposed as consistently as possible.\n     - Model files can be used independently of the library for quick experiments.\n \n-## Why shouldn't I use transformers?\n-\n-- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n-- The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, [Accelerate](https://huggingface.co/docs/accelerate)).\n-- While we strive to present as many use cases as possible, the scripts in our [examples folder](https://github.com/huggingface/transformers/tree/main/examples) are just that: examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.\n-\n-## Installation\n-\n-### With pip\n-\n-This repository is tested on Python 3.9+, Flax 0.4.1+, PyTorch 2.0+, and TensorFlow 2.6+.\n-\n-You should install ü§ó Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n-\n-First, create a virtual environment with the version of Python you're going to use and activate it.\n-\n-**macOS/Linux**\n-\n-```python -m venv env\n-source env/bin/activate\n-```\n-\n-**Windows**\n-\n-``` python -m venv env\n-env\\Scripts\\activate\n-```\n+<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n+    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n+</a><br>\n \n-To use ü§ó Transformers, you must install at least one of Flax, PyTorch, or TensorFlow. Refer to the official installation guides for platform-specific commands:\n+## Why shouldn't I use Transformers?\n \n-[TensorFlow installation page](https://www.tensorflow.org/install/), \n-[PyTorch installation page](https://pytorch.org/get-started/locally/#start-locally) and/or [Flax](https://github.com/google/flax#quick-install) and [Jax](https://github.com/google/jax#installation) \n+- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n+- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n+- The [example scripts]((https://github.com/huggingface/transformers/tree/main/examples)) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n \n-When one of those backends has been installed, ü§ó Transformers can be installed using pip as follows:\n+## 100 projects using Transformers\n \n-```\n-pip install transformers\n-```\n+Transformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\n+Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\n+else to build their dream projects.\n \n-If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must [install the library from source](https://huggingface.co/docs/transformers/installation#installing-from-source).\n+In order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\n+community with the [awesome-transformers](./awesome-transformers.md) page which lists 100\n+incredible projects built with Transformers.\n \n-```\n-git clone https://github.com/huggingface/transformers.git\n-cd transformers\n-pip install .\n-```\n+If you own or use a project that you believe should be part of the list, please open a PR to add it!\n \n-### With conda\n+## Example models\n \n-ü§ó Transformers can be installed using conda as follows:\n+You can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n \n-```shell script\n-conda install conda-forge::transformers\n-```\n+Expand each modality below to see a few example models for various use cases.\n \n-> **_NOTE:_** Installing `transformers` from the `huggingface` channel is deprecated.\n+<details>\n+<summary>Audio</summary>\n \n-Follow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.\n+- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n+- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n+- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n+- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n+- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n+- Text to speech with [Bark](https://huggingface.co/suno/bark)\n \n-> **_NOTE:_**  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in [this issue](https://github.com/huggingface/huggingface_hub/issues/1062).\n+</details>\n \n-## Model architectures\n+<details>\n+<summary>Computer vision</summary>\n \n-**[All the model checkpoints](https://huggingface.co/models)** provided by ü§ó Transformers are seamlessly integrated from the huggingface.co [model hub](https://huggingface.co/models), where they are uploaded directly by [users](https://huggingface.co/users) and [organizations](https://huggingface.co/organizations).\n+- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n+- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n+- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n+- Keypoint detection with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n+- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue)\n+- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n+- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n+- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n+- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n \n-Current number of checkpoints: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n+</details>\n \n-ü§ó Transformers currently provides the following architectures: see [here](https://huggingface.co/docs/transformers/model_summary) for a high-level summary of each them.\n+<details>\n+<summary>Multimodal</summary>\n \n-To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the ü§ó Tokenizers library, refer to [this table](https://huggingface.co/docs/transformers/index#supported-frameworks).\n+- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n+- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n+- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n+- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n+- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n+- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n+- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n+- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n+- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n+- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n \n-These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the [documentation](https://github.com/huggingface/transformers/tree/main/examples).\n+</details>\n \n+<details>\n+<summary>NLP</summary>\n \n-## Learn more\n+- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n+- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n+- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n+- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n+- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n+- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n+- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n \n-| Section | Description |\n-|-|-|\n-| [Documentation](https://huggingface.co/docs/transformers/) | Full API documentation and tutorials |\n-| [Task summary](https://huggingface.co/docs/transformers/task_summary) | Tasks supported by ü§ó Transformers |\n-| [Preprocessing tutorial](https://huggingface.co/docs/transformers/preprocessing) | Using the `Tokenizer` class to prepare data for the models |\n-| [Training and fine-tuning](https://huggingface.co/docs/transformers/training) | Using the models provided by ü§ó Transformers in a PyTorch/TensorFlow training loop and the `Trainer` API |\n-| [Quick tour: Fine-tuning/usage scripts](https://github.com/huggingface/transformers/tree/main/examples) | Example scripts for fine-tuning models on a wide range of tasks |\n-| [Model sharing and uploading](https://huggingface.co/docs/transformers/model_sharing) | Upload and share your fine-tuned models with the community |\n+</details>\n \n ## Citation\n "
        },
        {
            "sha": "f98982d6ac8c583af001c12fa5f1770836f3854a",
            "filename": "docs/source/en/installation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac1a1b66b99be5b2fbec055b794259e75e19565d/docs%2Fsource%2Fen%2Finstallation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac1a1b66b99be5b2fbec055b794259e75e19565d/docs%2Fsource%2Fen%2Finstallation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finstallation.md?ref=ac1a1b66b99be5b2fbec055b794259e75e19565d",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n # Installation\n \n-Transformers works with [PyTorch](https://pytorch.org/get-started/locally/), [TensorFlow 2.0](https://www.tensorflow.org/install/pip), and [Flax](https://flax.readthedocs.io/en/latest/). It has been tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax.\n+Transformers works with [PyTorch](https://pytorch.org/get-started/locally/), [TensorFlow 2.0](https://www.tensorflow.org/install/pip), and [Flax](https://flax.readthedocs.io/en/latest/). It has been tested on Python 3.9+, PyTorch 2.0+, TensorFlow 2.6+, and Flax 0.4.1+.\n \n ## Virtual environment\n "
        }
    ],
    "stats": {
        "total": 360,
        "additions": 170,
        "deletions": 190
    }
}