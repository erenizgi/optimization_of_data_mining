{
    "author": "jiqing-feng",
    "message": "Enable XPU doc (#38929)\n\n* fix example with dataset\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update torchao doc\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update torchao doc\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix device type\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert torchao change\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix torchao doc\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert torchao change\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update xpu torchao doc\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update chat_templating_multimodal.md\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* use full name for int8\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert int8 title\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "03db2700abf84971351c7374a548a9d4fc156916",
    "files": [
        {
            "sha": "190f731772832477679bf2ae278a8478bad1c8ec",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=03db2700abf84971351c7374a548a9d4fc156916",
            "patch": "@@ -56,7 +56,7 @@ Create a [`ImageTextToTextPipeline`] and pass the chat to it. For large models,\n import torch\n from transformers import pipeline\n \n-pipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device=\"cuda\", torch_dtype=torch.float16)\n+pipeline = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\", device_map=\"auto\", torch_dtype=torch.float16)\n pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n [{'input_text': [{'role': 'system',\n     'content': [{'type': 'text',\n@@ -175,7 +175,7 @@ processed_chat = processor.apply_chat_template(\n     add_generation_prompt=True,\n     tokenize=True,\n     return_dict=True,\n-    video_fps=32,\n+    video_fps=16,\n     video_load_backend=\"decord\",\n )\n print(processed_chat.keys())"
        },
        {
            "sha": "38c1247909edc2bf1a5e6fda2c93b5f0ceb4fd39",
            "filename": "docs/source/en/feature_extractors.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Ffeature_extractors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Ffeature_extractors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ffeature_extractors.md?ref=03db2700abf84971351c7374a548a9d4fc156916",
            "patch": "@@ -26,6 +26,7 @@ Pass the audio signal, typically stored in `array`, to the feature extractor and\n from transformers import AutoFeatureExtractor\n \n feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n+dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n processed_sample = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=16000)\n processed_sample\n {'input_values': [array([ 9.4472744e-05,  3.0777880e-03, -2.8888427e-03, ...,"
        },
        {
            "sha": "c09835483633c082eedc825e0e43099205036f2e",
            "filename": "docs/source/en/quantization/finegrained_fp8.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md?ref=03db2700abf84971351c7374a548a9d4fc156916",
            "patch": "@@ -47,7 +47,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"\n \n tokenizer = AutoTokenizer.from_pretrained(model_name)\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device.type)\n \n output = quantized_model.generate(**input_ids, max_new_tokens=10)\n print(tokenizer.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "c86807d57aa576d6ef108817ab117016f3158c35",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 71,
            "deletions": 5,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/03db2700abf84971351c7374a548a9d4fc156916/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=03db2700abf84971351c7374a548a9d4fc156916",
            "patch": "@@ -49,6 +49,7 @@ Check the table below to see if your hardware is compatible.\n | Component | Compatibility |\n |----------|----------------|\n | CUDA Versions | ✅ cu118, cu126, cu128 |\n+| XPU Versions | ✅ pytorch2.8 |\n | CPU | ✅ change `device_map=\"cpu\"` (see examples below) |\n \n \n@@ -278,6 +279,71 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoption>\n </hfoptions>\n \n+### Intel XPU\n+<hfoptions id=\"examples-Intel-XPU\">\n+<hfoption id=\"int8-dynamic-and-weight-only\">\n+    \n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig\n+\n+quant_config = Int8DynamicActivationInt8WeightConfig()\n+# or int8 weight only quantization\n+# quant_config = Int8WeightOnlyConfig()\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"xpu\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+\n+<hfoption id=\"int4-weight-only\">\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.dtypes import Int4XPULayout\n+from torchao.quantization.quant_primitives import ZeroPointDomain\n+\n+\n+quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4XPULayout(), zero_point_domain=ZeroPointDomain.INT)\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"xpu\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+</hfoptions>\n+\n+\n ### CPU\n <hfoptions id=\"examples-CPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n@@ -363,7 +429,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n # Manual Testing\n prompt = \"Hey, are you conscious? Can you talk to me?\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(quantized_model.device.type)\n generated_ids = quantized_model.generate(**inputs, max_new_tokens=128)\n output_text = tokenizer.batch_decode(\n     generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n@@ -434,7 +500,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device.type)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -474,7 +540,7 @@ tokenizer.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n \n ## Loading quantized models\n \n-Loading a quantized model depends on the quantization scheme. For quantization schemes, like int8 and float8, you can quantize the model on any device and also load it on any device. The example below demonstrates quantizing a model on the CPU and then loading it on CUDA.\n+Loading a quantized model depends on the quantization scheme. For quantization schemes, like int8 and float8, you can quantize the model on any device and also load it on any device. The example below demonstrates quantizing a model on the CPU and then loading it on CUDA or XPU.\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n@@ -491,7 +557,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n     quantization_config=quantization_config\n )\n # save the quantized model\n-output_dir = \"llama-3.1-8b-torchao-int8-cuda\"\n+output_dir = \"llama-3.1-8b-torchao-int8\"\n quantized_model.save_pretrained(output_dir, safe_serialization=False)\n \n # reload the quantized model\n@@ -502,7 +568,7 @@ reloaded_model = AutoModelForCausalLM.from_pretrained(\n )\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(reloaded_model.device.type)\n \n output = reloaded_model.generate(**input_ids, max_new_tokens=10)\n print(tokenizer.decode(output[0], skip_special_tokens=True))"
        }
    ],
    "stats": {
        "total": 83,
        "additions": 75,
        "deletions": 8
    }
}