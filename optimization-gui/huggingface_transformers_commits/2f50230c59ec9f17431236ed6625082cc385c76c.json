{
    "author": "ydshieh",
    "message": "fix `t5gemma` tests (#39052)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "2f50230c59ec9f17431236ed6625082cc385c76c",
    "files": [
        {
            "sha": "a7d60d2fa78687ec89ab4aa1c4227fdf3ac4ec2a",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f50230c59ec9f17431236ed6625082cc385c76c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f50230c59ec9f17431236ed6625082cc385c76c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=2f50230c59ec9f17431236ed6625082cc385c76c",
            "patch": "@@ -41,7 +41,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n \n \n@@ -1112,7 +1112,7 @@ def __init__(self, config: T5GemmaConfig):\n         self.model = T5GemmaModel(config)\n         self.vocab_size = config.decoder.vocab_size\n         self.lm_head = T5GemmaLMHead(config.decoder.hidden_size, self.vocab_size)\n-        self.loss_type = \"ForMaskedLMLoss\"\n+        self.loss_type = \"ForMaskedLM\"\n \n         self.post_init()\n \n@@ -1169,10 +1169,14 @@ def forward(\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n         if self.training and self.config._attn_implementation != \"eager\":\n-            logger.warning_once(\n+            msg = (\n                 \"It is strongly recommended to train T5Gemma models with the `eager` attention implementation \"\n                 f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n             )\n+            if is_torchdynamo_compiling():\n+                raise ValueError(msg)\n+            else:\n+                logger.warning_once(msg)\n \n         if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n             # get decoder inputs from shifting lm labels to the right"
        },
        {
            "sha": "b3dbe761a22432afb710d974bf655ebf2dfd6e36",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f50230c59ec9f17431236ed6625082cc385c76c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f50230c59ec9f17431236ed6625082cc385c76c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=2f50230c59ec9f17431236ed6625082cc385c76c",
            "patch": "@@ -37,6 +37,7 @@\n     auto_docstring,\n     can_return_tuple,\n     is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n     logging,\n )\n from ..gemma2.configuration_gemma2 import Gemma2Config\n@@ -1058,7 +1059,7 @@ def __init__(self, config: T5GemmaConfig):\n         self.model = T5GemmaModel(config)\n         self.vocab_size = config.decoder.vocab_size\n         self.lm_head = T5GemmaLMHead(config.decoder.hidden_size, self.vocab_size)\n-        self.loss_type = \"ForMaskedLMLoss\"\n+        self.loss_type = \"ForMaskedLM\"\n \n         self.post_init()\n \n@@ -1115,10 +1116,14 @@ def forward(\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n         if self.training and self.config._attn_implementation != \"eager\":\n-            logger.warning_once(\n+            msg = (\n                 \"It is strongly recommended to train T5Gemma models with the `eager` attention implementation \"\n                 f\"instead of `{self.config._attn_implementation}`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\"\n             )\n+            if is_torchdynamo_compiling():\n+                raise ValueError(msg)\n+            else:\n+                logger.warning_once(msg)\n \n         if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n             # get decoder inputs from shifting lm labels to the right"
        },
        {
            "sha": "fd61e5e5c5dbbad9a896278a6eb8c17534a241a7",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f50230c59ec9f17431236ed6625082cc385c76c/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f50230c59ec9f17431236ed6625082cc385c76c/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=2f50230c59ec9f17431236ed6625082cc385c76c",
            "patch": "@@ -595,6 +595,11 @@ class T5GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n \n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = T5GemmaForConditionalGeneration if is_torch_available() else None\n+    # `t5gemma` will give warning or raise error if it is not `eager` during training.\n+    _torch_compile_train_attn_implementation = \"eager\"\n+\n+    # won't fix\n+    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = T5GemmaModelTester(self)\n@@ -1584,6 +1589,9 @@ class T5GemmaEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = False\n     model_split_percents = [0.4, 0.5]\n \n+    # won't fix\n+    test_torchscript = False\n+\n     def setUp(self):\n         self.model_tester = T5GemmaEncoderOnlyModelTester(self)\n         self.config_tester = ConfigTester("
        },
        {
            "sha": "b3625255553cb10117e4f0b1775adbc027956869",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f50230c59ec9f17431236ed6625082cc385c76c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f50230c59ec9f17431236ed6625082cc385c76c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2f50230c59ec9f17431236ed6625082cc385c76c",
            "patch": "@@ -3748,7 +3748,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n                 )\n-            if config.model_type in [\"modernbert\", \"gemma3\"]:\n+            if config.model_type in [\"modernbert\", \"gemma3\", \"t5gemma\"]:\n                 self.skipTest(\n                     reason=f\"{config.model_type} currently (transformers==4.52.0) automatically adds an attention_mask input\"\n                 )\n@@ -4414,6 +4414,10 @@ def test_torch_compile_for_training(self):\n \n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         cls = self._torch_compile_train_cls\n+        attn_implementation = getattr(self, \"_torch_compile_train_attn_implementation\", None)\n+        if attn_implementation is not None:\n+            config._attn_implementation = attn_implementation\n+\n         model = cls(config).to(torch_device)\n \n         inputs = {"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 27,
        "deletions": 6
    }
}