{
    "author": "gante",
    "message": "Tests: move `generate` tests to the right mixin and delete redundant tests (#34464)\n\n* tmp commit\r\n\r\n* tmp commit\r\n\r\n* cull overwrites of deleted tests\r\n\r\n* typo\r\n\r\n* more specific docstring\r\n\r\n* make fixup\r\n\r\n* parameterize at the top?\r\n\r\n* correction\r\n\r\n* more deletions :D\r\n\r\n* tmp commit\r\n\r\n* for VLMs too\r\n\r\n* fix _check_outputs\r\n\r\n* test nit\r\n\r\n* make fixup\r\n\r\n* fix another flaky\r\n\r\n* test_generate_from_inputs_embeds -- handle missing attention mask",
    "sha": "8a734ea2c340beee23e665601919814918bf4c43",
    "files": [
        {
            "sha": "6e6d5b8bdce71d128ce074a990bf2d26e06aa4f6",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 11,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -378,10 +378,14 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n             model_inputs[\"past_key_values\"] = past_key_values\n-            if inputs_embeds is not None or cache_position[-1] >= input_ids.shape[1]:  # Exception 1 or Exception 3\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]\n@@ -414,7 +418,7 @@ def prepare_inputs_for_generation(\n         for model_input_name in [\"position_ids\", \"token_type_ids\"]:\n             model_input = kwargs.get(model_input_name)\n             if model_input is not None:\n-                if past_key_values:\n+                if past_key_values is not None:\n                     model_input = model_input[:, -input_ids.shape[1] :]\n                     model_input = model_input.clone(memory_format=torch.contiguous_format)\n                 model_inputs[model_input_name] = model_input\n@@ -568,27 +572,34 @@ def _maybe_initialize_input_ids_for_generation(\n \n     def _prepare_attention_mask_for_generation(\n         self,\n-        inputs: torch.Tensor,\n-        pad_token_id: Optional[torch.Tensor],\n-        eos_token_id: Optional[torch.Tensor],\n+        inputs_tensor: torch.Tensor,\n+        generation_config: GenerationConfig,\n+        model_kwargs: Dict[str, Any],\n     ) -> torch.LongTensor:\n+        pad_token_id = generation_config._pad_token_tensor\n+        eos_token_id = generation_config._eos_token_tensor\n+\n+        # `input_ids` may be present in the model kwargs, instead of being the main input (e.g. multimodal model)\n+        if \"input_ids\" in model_kwargs and model_kwargs[\"input_ids\"].shape[1] > 0:\n+            inputs_tensor = model_kwargs[\"input_ids\"]\n+\n         # No information for attention mask inference -> return default attention mask\n-        default_attention_mask = torch.ones(inputs.shape[:2], dtype=torch.long, device=inputs.device)\n+        default_attention_mask = torch.ones(inputs_tensor.shape[:2], dtype=torch.long, device=inputs_tensor.device)\n         if pad_token_id is None:\n             return default_attention_mask\n \n-        is_input_ids = len(inputs.shape) == 2 and inputs.dtype in [torch.int, torch.long]\n+        is_input_ids = len(inputs_tensor.shape) == 2 and inputs_tensor.dtype in [torch.int, torch.long]\n         if not is_input_ids:\n             return default_attention_mask\n \n         is_pad_token_in_inputs = (pad_token_id is not None) and (\n-            isin_mps_friendly(elements=inputs, test_elements=pad_token_id).any()\n+            isin_mps_friendly(elements=inputs_tensor, test_elements=pad_token_id).any()\n         )\n         is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or ~(\n             isin_mps_friendly(elements=eos_token_id, test_elements=pad_token_id).any()\n         )\n         can_infer_attention_mask = is_pad_token_in_inputs * is_pad_token_not_equal_to_eos_token_id\n-        attention_mask_from_padding = inputs.ne(pad_token_id).long()\n+        attention_mask_from_padding = inputs_tensor.ne(pad_token_id).long()\n \n         attention_mask = (\n             attention_mask_from_padding * can_infer_attention_mask + default_attention_mask * ~can_infer_attention_mask\n@@ -2020,7 +2031,7 @@ def generate(\n \n         if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n-                inputs_tensor, generation_config._pad_token_tensor, generation_config._eos_token_tensor\n+                inputs_tensor, generation_config, model_kwargs\n             )\n         elif kwargs_has_attention_mask:\n             # TODO (joao): generalize this check with other types of inputs"
        },
        {
            "sha": "85c109919da736097a3f4bb85b17907a99c86e61",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -911,7 +911,8 @@ def forward(\n \n         if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n             )\n \n         legacy_processing = False"
        },
        {
            "sha": "2025140bb6e36ad854423570a1e45f1d2d991104",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -424,7 +424,8 @@ def forward(\n \n         if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n             )\n \n         legacy_processing = False"
        },
        {
            "sha": "2aa6b2fa1d6fa56bb4290885903b6ca2095eaf79",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -657,7 +657,8 @@ def forward(\n \n         if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError(\n-                \"You cannot specify both pixel_values/pixel_values_videos and inputs_embeds at the same time, and must specify either one\"\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n             )\n \n         if inputs_embeds is None:"
        },
        {
            "sha": "109ddfb626d26bf1cb9c0d6512b72ca5e4f26acf",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -1562,7 +1562,7 @@ def generate(\n \n         if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask:\n             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n-                input_ids, generation_config._pad_token_tensor, generation_config._eos_token_tensor\n+                input_ids, generation_config, model_kwargs\n             )\n \n         # 5. Prepare `max_length` depending on other stopping criteria.\n@@ -2578,7 +2578,7 @@ def generate(\n \n         if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask:\n             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n-                inputs_tensor, generation_config._pad_token_tensor, generation_config._eos_token_tensor\n+                inputs_tensor, generation_config, model_kwargs\n             )\n \n         if \"encoder_outputs\" not in model_kwargs:"
        },
        {
            "sha": "61f2ce414e1ddfa9442fa179b9032506a5779e75",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -1484,7 +1484,7 @@ def generate(\n \n         if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask:\n             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n-                input_ids, generation_config._pad_token_tensor, generation_config._eos_token_tensor\n+                input_ids, generation_config, model_kwargs\n             )\n \n         # 5. Prepare `max_length` depending on other stopping criteria.\n@@ -2425,7 +2425,7 @@ def generate(\n \n         if model_kwargs.get(\"attention_mask\", None) is None and requires_attention_mask:\n             model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n-                inputs_tensor, generation_config._pad_token_tensor, generation_config._eos_token_tensor\n+                inputs_tensor, generation_config, model_kwargs\n             )\n \n         if \"encoder_hidden_states\" not in model_kwargs:"
        },
        {
            "sha": "a3b3de33fa66ee60115b0910a094e06c0857baf7",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -534,7 +534,8 @@ def forward(\n \n         if (pixel_values_images is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n             raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+                \"You cannot specify both `pixel_values_images`/`pixel_values_videos` and `inputs_embeds` at the same \"\n+                \"time, and must specify either one\"\n             )\n \n         legacy_processing = False"
        },
        {
            "sha": "545b696d67370a89d9b52316a7a6e6eb4c72f1aa",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 220,
            "deletions": 157,
            "changes": 377,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -29,6 +29,7 @@\n from transformers.testing_utils import (\n     is_flaky,\n     require_accelerate,\n+    require_flash_attn,\n     require_optimum_quanto,\n     require_torch,\n     require_torch_gpu,\n@@ -136,6 +137,34 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n \n         return config, filtered_inputs_dict\n \n+    def _check_similar_generate_outputs(self, output_1, output_2, atol=1e-5, rtol=1e-5):\n+        \"\"\"\n+        Checks whether a pair of generate outputs are similar. Two `generate` call outputs are considered similar in\n+        the following siturations:\n+        1. The sequences are the same\n+        2. The sequences are different, but the scores up to (and including) the first mismatch are nearly identical\n+        \"\"\"\n+        # scores doesn't include data regarding decoder input tokens\n+        decoder_input_length = output_1.sequences.shape[1] - len(output_1.scores)\n+        output_matches = output_1.sequences == output_2.sequences\n+        has_matching_outputs = output_matches.all()\n+        has_matching_scores = None\n+        if not has_matching_outputs:\n+            for batch_idx in range(output_1.sequences.shape[0]):\n+                batch_matches = output_matches[batch_idx]\n+                if batch_matches.all():\n+                    continue\n+                first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n+                first_mismatch_idx -= decoder_input_length\n+                output_1_first_mismatch_scores = output_1.scores[first_mismatch_idx][batch_idx]\n+                output_2_first_mismatch_scores = output_2.scores[first_mismatch_idx][batch_idx]\n+                has_matching_scores = torch.allclose(\n+                    output_1_first_mismatch_scores, output_2_first_mismatch_scores, rtol=atol, atol=rtol\n+                )\n+                if not has_matching_scores:\n+                    break\n+        self.assertTrue(has_matching_outputs or has_matching_scores)\n+\n     def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {\n             \"bad_words_ids\": [[1, 0]],\n@@ -426,7 +455,6 @@ def test_greedy_generate(self):\n     def test_greedy_generate_dict_outputs(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n@@ -453,13 +481,12 @@ def test_greedy_generate_dict_outputs(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchDecoderOnlyOutput)\n \n-            self._check_outputs(output_generate, main_input, model.config)\n+            self._check_outputs(output_generate, model.config)\n \n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -486,7 +513,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                     output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                 )\n \n-            self._check_outputs(output_generate, main_input, model.config, use_cache=True)\n+            self._check_outputs(output_generate, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_sample_generate(self):\n@@ -505,7 +532,6 @@ def test_sample_generate(self):\n     def test_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(\n@@ -533,7 +559,7 @@ def test_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleDecoderOnlyOutput)\n \n-            self._check_outputs(output_generate, main_input, model.config, num_return_sequences=2)\n+            self._check_outputs(output_generate, model.config, num_return_sequences=2)\n \n     @pytest.mark.generate\n     def test_beam_search_generate(self):\n@@ -554,7 +580,6 @@ def test_beam_search_generate(self):\n     def test_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n@@ -583,14 +608,16 @@ def test_beam_search_generate_dict_output(self):\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate,\n+                model.config,\n+                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n+                num_beams=beam_kwargs[\"num_beams\"],\n             )\n \n     @pytest.mark.generate\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -623,10 +650,10 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n \n             self._check_outputs(\n                 output_generate,\n-                main_input,\n                 model.config,\n                 use_cache=True,\n-                num_return_sequences=beam_kwargs[\"num_beams\"],\n+                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n+                num_beams=beam_kwargs[\"num_beams\"],\n             )\n \n     @require_accelerate\n@@ -675,7 +702,6 @@ def test_beam_sample_generate(self):\n     def test_beam_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n@@ -706,7 +732,10 @@ def test_beam_sample_generate_dict_output(self):\n                 self.assertIsInstance(output_generate, BeamSampleDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate,\n+                model.config,\n+                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n+                num_beams=beam_kwargs[\"num_beams\"],\n             )\n \n     @pytest.mark.generate\n@@ -765,7 +794,6 @@ def test_group_beam_search_generate(self):\n     def test_group_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_diverse_beam_kwargs()\n@@ -794,7 +822,10 @@ def test_group_beam_search_generate_dict_output(self):\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate,\n+                model.config,\n+                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n+                num_beams=beam_kwargs[\"num_beams\"],\n             )\n \n     # TODO: @gante check why it is flaky\n@@ -859,7 +890,6 @@ def test_constrained_beam_search_generate(self):\n     def test_constrained_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -899,7 +929,10 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n             self._check_outputs(\n-                output_generate, main_input, model.config, num_return_sequences=beam_kwargs[\"num_beams\"]\n+                output_generate,\n+                model.config,\n+                num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n+                num_beams=beam_kwargs[\"num_beams\"],\n             )\n \n     @pytest.mark.generate\n@@ -942,7 +975,6 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                 self.skipTest(reason=\"Won't fix: old model with different cache format\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -968,7 +1000,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                     output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                 )\n \n-            self._check_outputs(output_generate, main_input, model.config, use_cache=True)\n+            self._check_outputs(output_generate, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_contrastive_generate_low_memory(self):\n@@ -1064,14 +1096,10 @@ def test_beam_search_low_memory(self):\n \n     @pytest.mark.generate\n     @parameterized.expand([(\"random\",), (\"same\",)])\n-    @is_flaky()  # Read NOTE (1) below. If there are API issues, all attempts will fail.\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n         # This test ensures that the assisted generation does not introduce output changes over greedy search.\n-        # NOTE (1): The sentence above is true most of the time, there is a tiny difference in the logits due to matmul\n-        # shape differences -- and it may result in a different output. The input shape difference happens in the\n-        # main model, that runs the forward pass with several candidates at once (as opposed to generating one token at\n-        # a time). See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535 for more info.\n-        # NOTE (2): It breaks the pattern in the tests above, for multiple reasons:\n+        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535 for more info.\n+        # NOTE: It breaks the pattern in the tests above, for multiple reasons:\n         # - assisted_decoding, contrarily to the other methods, can't be called on its own (e.g. needs to\n         # prepare the assistant encoder outputs in the main generate body);\n         # - assisted_decoding does not support `use_cache = False`\n@@ -1100,7 +1128,6 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n \n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1141,12 +1168,10 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             output_assisted = model.generate(**generation_kwargs, **inputs_dict)\n \n             # The two outputs must match and their shape must be as expected\n-\n-            self.assertListEqual(output_greedy.sequences.tolist(), output_assisted.sequences.tolist())\n+            self._check_similar_generate_outputs(output_greedy, output_assisted)\n             for output in (output_greedy, output_assisted):\n-                self._check_outputs(output, main_input, model.config, use_cache=True)\n+                self._check_outputs(output, model.config, use_cache=True)\n \n-    @is_flaky()\n     @pytest.mark.generate\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         # This test ensures that the prompt lookup generation does not introduce output changes over greedy search.\n@@ -1175,7 +1200,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n \n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1208,10 +1232,9 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             output_prompt_lookup = model.generate(**generation_kwargs, **inputs_dict)\n \n             # The two outputs must match and their shape must be as expected\n-\n-            self.assertListEqual(output_greedy.sequences.tolist(), output_prompt_lookup.sequences.tolist())\n+            self._check_similar_generate_outputs(output_greedy, output_prompt_lookup)\n             for output in (output_greedy, output_prompt_lookup):\n-                self._check_outputs(output, main_input, model.config, use_cache=True)\n+                self._check_outputs(output, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_dola_decoding_sample(self):\n@@ -1231,7 +1254,6 @@ def test_dola_decoding_sample(self):\n \n             # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             # Encoder-decoder models are not supported\n             if config.is_encoder_decoder:\n@@ -1259,7 +1281,7 @@ def test_dola_decoding_sample(self):\n                 \"dola_layers\": \"low\",\n             }\n             output_dola = model.generate(**generation_kwargs, **inputs_dict)\n-            self._check_outputs(output_dola, main_input, model.config, use_cache=getattr(config, \"use_cache\", False))\n+            self._check_outputs(output_dola, model.config, use_cache=getattr(config, \"use_cache\", False))\n \n     @pytest.mark.generate\n     def test_assisted_decoding_sample(self):\n@@ -1289,7 +1311,6 @@ def test_assisted_decoding_sample(self):\n \n             # enable cache\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n-            main_input = inputs_dict[model_class.main_input_name]\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1321,7 +1342,7 @@ def test_assisted_decoding_sample(self):\n             }\n             output_assisted = model.generate(**generation_kwargs, **inputs_dict)\n \n-            self._check_outputs(output_assisted, main_input, config, use_cache=True)\n+            self._check_outputs(output_assisted, config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_prompt_lookup_decoding_stops_at_eos(self):\n@@ -1547,75 +1568,93 @@ def test_past_key_values_format(self):\n                     )\n \n     @pytest.mark.generate\n-    @parameterized.expand([(1,), (2,)])\n-    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n+        \"\"\"Tests that we can generate from `inputs_embeds` instead of `input_ids` in LLMs, VLMs, etc\"\"\"\n         # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n         # if fails, you should probably update the `prepare_inputs_for_generation` function\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n-            # Ignore:\n-            # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n-            #   which would cause a mismatch),\n-            config.pad_token_id = config.eos_token_id = -1\n-            # b) embedding scaling, the scaling factor applied after embeding from input_ids (requires knowledge of the\n-            #   variable that holds the scaling factor, which is model-dependent)\n-            if hasattr(config, \"scale_embedding\"):\n-                config.scale_embedding = False\n-\n             # This test is for decoder-only models (encoder-decoder models have native input embeddings support in the\n             # decoder)\n             if config.is_encoder_decoder:\n                 continue\n+            config.is_decoder = True\n \n             # Skip models without explicit support\n-            config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n             if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                 continue\n \n+            # There are a few exception patterns in this test:\n+            # 1 - Some models can't generate without `input_ids`, when `inputs_embeds` are passed\n+            requires_inputs_ids = any(\n+                model_name in model_class.__name__.lower() for model_name in [\"idefics\", \"qwen2vl\"]\n+            )\n+            # 2 - Complex `inputs_embeds` computation, i.e. the correct computation of inputs embeds is more complex\n+            # than calling the embedding layer with `input_ids`. Subcases of this exception:\n+            #   2.A - Ignore `scale_embedding`, if the model supports it (it is controlled by a model-dependent flag)\n+            if hasattr(config, \"scale_embedding\"):\n+                config.scale_embedding = False\n+            #   2.B - Some VLMs assume `inputs_embeds` and `pixel_values` are mutually exclusive AND fall in the\n+            #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n+            #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n+            pixel_values_is_mutually_exclusive = any(\n+                model_name in model_class.__name__.lower()\n+                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\"]\n+            )\n+            if pixel_values_is_mutually_exclusive:\n+                inputs_dict.pop(\"pixel_values\", None)\n+                inputs_dict.pop(\"pixel_values_videos\", None)\n+                inputs_dict.pop(\"pixel_values_images\", None)\n+            #   2.C - No easy fix, let's skip the check that compares the outputs from `input_ids` and `inputs_embeds`\n+            has_complex_embeds_computation = any(\n+                model_name in model_class.__name__.lower() for model_name in [\"moshi\"]\n+            )\n+            # 3 - `inputs_dict` doesn't contain `attention_mask`. When `attention_mask` is not passed to generate,\n+            # we infer it from `input_ids`. The last test case will fail if there is a pad token in the original input.\n+            missing_attention_mask = \"attention_mask\" not in inputs_dict\n+\n+            # Traditional way of generating text\n             input_ids = inputs_dict.pop(\"input_ids\")\n             generation_kwargs = {\n                 \"return_dict_in_generate\": True,\n                 \"output_scores\": True,\n                 \"num_beams\": num_beams,\n                 \"do_sample\": False,\n+                \"max_new_tokens\": 5,\n+                \"min_new_tokens\": 5,  # generate exactly 5 tokens\n             }\n-\n-            # Traditional way of generating text\n-            outputs_from_ids = model.generate(input_ids, max_new_tokens=5, **generation_kwargs)\n+            outputs_from_ids = model.generate(input_ids, **generation_kwargs, **inputs_dict)\n             self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n \n-            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n+            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output).\n+            # The output of the two calls should be the same.\n             inputs_embeds = model.get_input_embeddings()(input_ids)\n             outputs_from_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=inputs_embeds,\n-                max_new_tokens=5,\n-                **generation_kwargs,\n+                input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n             )\n-            self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n+            if not has_complex_embeds_computation:\n+                self._check_similar_generate_outputs(outputs_from_ids, outputs_from_embeds)\n \n-            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n+            # If we pass different inputs_embeds, we should get different outputs (the output text may be the\n             # same, but the logits will almost surely be different)\n             random_embeds = torch.rand_like(inputs_embeds)\n             outputs_from_rand_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=random_embeds,\n-                max_new_tokens=5,\n-                **generation_kwargs,\n+                input_ids, inputs_embeds=random_embeds, **generation_kwargs, **inputs_dict\n             )\n             for i in range(len(outputs_from_rand_embeds.scores)):\n                 self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n \n-            # input_ids is not a required input -- if we don't pass it, the newly generated tokens will be the same\n-            outputs_from_embeds_wo_ids = model.generate(\n-                inputs_embeds=inputs_embeds, max_new_tokens=5, **generation_kwargs\n-            )\n-            self.assertListEqual(\n-                outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :].tolist(),\n-                outputs_from_embeds_wo_ids.sequences.tolist(),\n-            )\n+            # input_ids is not a required input on most models -- if we don't pass it, the newly generated tokens will\n+            # be the same\n+            if not (requires_inputs_ids or missing_attention_mask):\n+                outputs_from_embeds_wo_ids = model.generate(\n+                    inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n+                )\n+                outputs_from_embeds.sequences = outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :]\n+                self._check_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds)\n \n     @pytest.mark.generate\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n@@ -1829,10 +1868,8 @@ def test_new_cache_format(self, num_beams, do_sample):\n     @pytest.mark.generate\n     def test_generate_with_static_cache(self):\n         \"\"\"\n-        Tests if StaticCache works if we set attn_implementation=static when generation.\n-        This doesn't test if generation quality is good, but tests that models with\n-        self._supports_static_cache don't throw an error when generating and return\n-        a StaticCache object at the end.\n+        Tests that generating with static cache give almost same results as with dynamic cache, and the output cache\n+        has the expected shapes\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n@@ -1851,13 +1888,15 @@ def test_generate_with_static_cache(self):\n \n             model = model_class(config).to(torch_device).eval()\n             generation_kwargs = {\n-                \"max_length\": None,\n                 \"max_new_tokens\": max_new_tokens,\n-                \"cache_implementation\": \"static\",\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+                \"output_scores\": True,\n                 \"use_cache\": True,\n             }\n \n+            static_cache_generation = model.generate(**generation_kwargs, **inputs_dict, cache_implementation=\"static\")\n+\n+            # Check 1: The cache shapes must match the expected shapes\n             max_cache_len = seq_length + max_new_tokens\n             config = config.text_config if hasattr(config, \"text_config\") else config\n             head_dim = (\n@@ -1869,12 +1908,14 @@ def test_generate_with_static_cache(self):\n                 else config.num_key_value_heads\n             )\n             num_hidden_layers = config.num_hidden_layers\n-            results = model.generate(**generation_kwargs, **inputs_dict)\n-\n             cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n-            self.assertTrue(isinstance(results.past_key_values, StaticCache))\n-            self.assertTrue(len(results.past_key_values.key_cache) == num_hidden_layers)\n-            self.assertTrue(results.past_key_values.key_cache[0].shape == cache_shape)\n+            self.assertTrue(isinstance(static_cache_generation.past_key_values, StaticCache))\n+            self.assertTrue(len(static_cache_generation.past_key_values.key_cache) == num_hidden_layers)\n+            self.assertTrue(static_cache_generation.past_key_values.key_cache[0].shape == cache_shape)\n+\n+            # Check 2: The outputs must be similar to the case with dynamic cache\n+            dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n+            self._check_similar_generate_outputs(dynamic_cache_generation, static_cache_generation)\n \n     @require_optimum_quanto\n     @pytest.mark.generate\n@@ -1908,25 +1949,32 @@ def test_generate_with_quant_cache(self):\n             with self.assertRaises(ValueError):\n                 model.generate(**generation_kwargs, **inputs_dict)\n \n+    @parameterized.expand(\n+        [\n+            (\"forward_only\", False),  # TODO (@joao): a few models failing. After fixed, this should not be \"@slow\"\n+            (\"end_to_end\", True),  # TODO (@joao): end-to-end compilation is broken with torch 2.5+, explore and fix\n+        ]\n+    )\n     @pytest.mark.generate\n     @require_torch_gpu\n     @slow\n-    @is_flaky()  # compilation may result in equivalent (!= same) FP ops, causing the argmax in `generate` to be flaky\n-    def test_generate_compile_fullgraph(self):\n+    def test_generate_compile(self, _, end_to_end):\n         \"\"\"\n-        Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results.\n+        Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results. Tests\n+        end-to-end compilation and forward pass compilation only.\n         ⚠️ Runs two sequential generations to ensure the cache doesn't get stuck after the first compiled run! ⚠️\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n                 self.skipTest(\"This model doesn't support static cache\")\n+\n             # TODO (joao) -- fix and enable me :)\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"whisper\"]):\n+            if end_to_end and any(model_name in model_class.__name__.lower() for model_name in [\"whisper\"]):\n                 self.skipTest(\"whisper model end-to-end generate compile not yet supported\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             # TODO (joao) -- fix and enable me :)\n-            if config.is_encoder_decoder:\n+            if end_to_end and config.is_encoder_decoder:\n                 self.skipTest(\"Encoder-decoder model end-to-end generate compile not yet supported\")\n \n             model = model_class(config).to(torch_device)\n@@ -1941,27 +1989,33 @@ def test_generate_compile_fullgraph(self):\n             generation_kwargs = {\n                 \"do_sample\": False,\n                 \"max_new_tokens\": 10,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n             }\n+            # end-to-end works best with dynamic cache, forward compilation works best with static cache\n+            if not end_to_end:\n+                generation_kwargs[\"cache_implementation\"] = \"static\"\n \n-            max_cache_len = input_ids.shape[1] + generation_kwargs[\"max_new_tokens\"]\n-            config = config.get_text_config()\n-            past_key_values = StaticCache(\n-                config, batch_size=half_batch_size, max_cache_len=max_cache_len, device=torch_device\n-            )\n+            # get eager + dynamic cache results for future comparison\n+            dynamic_outputs = []\n+            for model_inputs in input_ids_sets:\n+                dynamic_outputs.append(model.generate(model_inputs, **generation_kwargs))\n+\n+            # get compiled results\n+            generation_config = copy.deepcopy(model.generation_config)\n+            generation_config.update(**generation_kwargs)\n+            torch.compiler.reset()\n+            if end_to_end:\n+                model.generate = torch.compile(model.generate, fullgraph=True, mode=\"reduce-overhead\")\n+            else:\n+                model.forward = torch.compile(model.forward, fullgraph=True, mode=\"reduce-overhead\")\n \n+            compiled_outputs = []\n             for model_inputs in input_ids_sets:\n-                # eager dynamic cache\n-                output_dynamic = model.generate(model_inputs, **generation_kwargs)\n-\n-                # end-to-end compiled dynamic cache\n-                torch.compiler.reset()\n-                compiled_generate = torch.compile(model.generate, fullgraph=True, mode=\"reduce-overhead\")\n-                generation_config = copy.deepcopy(model.generation_config)\n-                generation_config.update(**generation_kwargs)\n-                output_compiled = compiled_generate(\n-                    model_inputs, generation_config=generation_config, past_key_values=past_key_values\n-                )\n-                self.assertListEqual(output_dynamic.tolist(), output_compiled.tolist())\n+                compiled_outputs.append(model.generate(model_inputs, generation_config=generation_config))\n+\n+            for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n+                self._check_similar_generate_outputs(dynamic_result, compiled_result)\n \n     @pytest.mark.generate\n     def test_generate_methods_with_num_logits_to_keep(self):\n@@ -1989,7 +2043,6 @@ def test_generate_methods_with_num_logits_to_keep(self):\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     @pytest.mark.generate\n-    @is_flaky()  # assisted generation tests are flaky (minor fp ops differences)\n     def test_assisted_decoding_with_num_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n             if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n@@ -1998,6 +2051,9 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=1)\n+            # NOTE: assisted generation only works with cache on at the moment.\n+            if not hasattr(config, \"use_cache\"):\n+                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             config.use_cache = True\n             config.is_decoder = True\n \n@@ -2010,14 +2066,16 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n                 \"max_new_tokens\": 10,\n                 \"do_sample\": False,\n                 \"assistant_model\": assistant_model,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n             }\n \n-            assistant_model.generation_config.assistant_confidence_threshold = None\n             # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n             with_all_logits = model.generate(**generation_kwargs, **inputs_dict, num_logits_to_keep=0)\n             # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n             without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n-            self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n+\n+            self._check_similar_generate_outputs(with_all_logits, without_all_logits)\n \n     @pytest.mark.generate\n     def test_inherits_generation_mixin(self):\n@@ -2028,14 +2086,21 @@ def test_inherits_generation_mixin(self):\n         for model_class in self.all_generative_model_classes:\n             self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n \n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n+    def _test_attention_implementation(self, attn_implementation):\n+        \"\"\"\n+        Compares the output of generate with the eager attention implementation against other implementations.\n+        NOTE: despite the test logic being the same, different implementations actually need diferent decorators, hence\n+        this separate function.\n+        \"\"\"\n         max_new_tokens = 30\n+        support_flag = {\n+            \"sdpa\": \"_supports_sdpa\",\n+            \"flash_attention_2\": \"_supports_flash_attn_2\",\n+        }\n \n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n+            if not getattr(model_class, support_flag[attn_implementation]):\n+                self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n \n             config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()\n             inputs_dict = {}\n@@ -2062,63 +2127,59 @@ def test_eager_matches_sdpa_generate(self):\n                     \"do_sample\": False,\n                     \"return_dict_in_generate\": True,\n                     \"output_scores\": True,\n+                    \"use_cache\": True,\n                 }\n \n-                model_sdpa = model_class.from_pretrained(\n+                model_eager = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     low_cpu_mem_usage=True,\n+                    attn_implementation=\"eager\",\n                 ).to(torch_device)\n-                res_sdpa = model_sdpa.generate(**inputs_dict, **generate_kwargs)\n-                del model_sdpa\n+                res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n+                del model_eager\n                 gc.collect()\n \n-                model_eager = model_class.from_pretrained(\n+                model_attn = model_class.from_pretrained(\n                     tmpdirname,\n                     torch_dtype=torch.float16,\n                     low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n+                    attn_implementation=attn_implementation,\n                 ).to(torch_device)\n-                res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n-                del model_eager\n+                res_attn = model_attn.generate(**inputs_dict, **generate_kwargs)\n+                del model_attn\n                 gc.collect()\n \n-                # Eager and SDPA are very similar, but not exactly the same. Because we are using random models, this\n-                # test would be flaky if we only checked the sequences. Two situations in which this test passes:\n-                # 1. The sequences are the same\n-                # 2. The sequences are different, but the scores up until the first mismatch are nearly identical\n-                output_matches = res_eager.sequences == res_sdpa.sequences\n-                has_matching_outputs = output_matches.all()\n-                has_matching_scores = None\n-                if not has_matching_outputs:\n-                    input_length = main_input.shape[1]\n-                    for batch_idx in range(res_eager.sequences.shape[0]):\n-                        batch_matches = output_matches[batch_idx]\n-                        if batch_matches.all():\n-                            continue\n-                        first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n-                        first_mismatch_idx -= input_length  # scores doesn't include data regarding input tokens\n-                        sdpa_first_mismatch_scores = res_sdpa.scores[first_mismatch_idx][batch_idx]\n-                        eager_first_mismatch_scores = res_eager.scores[first_mismatch_idx][batch_idx]\n-                        has_matching_scores = torch.allclose(\n-                            sdpa_first_mismatch_scores, eager_first_mismatch_scores, rtol=1e-3, atol=1e-3\n-                        )\n-                        if not has_matching_scores:\n-                            break\n+                self._check_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3)\n \n-                self.assertTrue(has_matching_outputs or has_matching_scores)\n+    @pytest.mark.generate\n+    @require_torch_sdpa\n+    @slow\n+    def test_eager_matches_sdpa_generate(self):\n+        \"\"\"Tests that generate has equivalent outputs with SDPA and eager attention implementations.\"\"\"\n+        self._test_attention_implementation(\"sdpa\")\n \n-    def _check_outputs(self, output, main_input, config, use_cache=False, num_return_sequences=1):\n-        # we can be sure what is batch size from main input but seq length depends on model type and whether input is text/audio/image\n-        # so we infer actual text seq length from model_tester, same was as it is done in `test_modeling_common.py` tests`\n-        batch_size = main_input.shape[0]\n+    @pytest.mark.flash_attn_test\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @slow\n+    def test_eager_matches_fa2_generate(self):\n+        \"\"\"Tests that generate has equivalent outputs with FA2 and eager attention implementations.\"\"\"\n+        # TODO (@joao @raushan) -- this test is failing the output checks on most models, investigate. After fixing,\n+        # check whether we still need the overwrites\n+        self._test_attention_implementation(\"flash_attention_2\")\n+\n+    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n+        internal_batch_size = (\n+            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n+        )\n \n         seq_length = getattr(self.model_tester, \"seq_length\", None)\n         seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n         seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n \n         config = config.text_config if hasattr(config, \"text_config\") else config\n-        num_sequences_in_output = batch_size * num_return_sequences\n \n         gen_len = (\n             output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n@@ -2129,19 +2190,21 @@ def _check_outputs(self, output, main_input, config, use_cache=False, num_return\n             seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n \n         # scores\n-        self._check_scores(num_sequences_in_output, output.scores, length=gen_len, config=config)\n+        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n \n         # unprocessed logits\n-        self._check_logits(num_sequences_in_output, output.logits, config=config)\n+        self._check_logits(internal_batch_size, output.logits, config=config)\n \n         # Attentions\n         if self.has_attentions:\n             if config.is_encoder_decoder:\n                 # encoder\n-                self._check_encoder_attention_for_generate(output.encoder_attentions, batch_size, config, seq_length)\n+                self._check_encoder_attention_for_generate(\n+                    output.encoder_attentions, input_batch_size, config, seq_length\n+                )\n                 # decoder\n                 self._check_attentions_for_generate(\n-                    num_sequences_in_output,\n+                    internal_batch_size,\n                     output.decoder_attentions,\n                     min_length=1,\n                     max_length=output.sequences.shape[-1],\n@@ -2153,7 +2216,7 @@ def _check_outputs(self, output, main_input, config, use_cache=False, num_return\n                 attentions = output.attentions if not use_cache else output.attentions[1:]\n                 min_length = seq_length if not use_cache else seq_length + 1\n                 self._check_attentions_for_generate(\n-                    num_sequences_in_output,\n+                    internal_batch_size,\n                     attentions=attentions,\n                     min_length=min_length,\n                     max_length=output.sequences.shape[-1],\n@@ -2165,12 +2228,12 @@ def _check_outputs(self, output, main_input, config, use_cache=False, num_return\n         if config.is_encoder_decoder:\n             # encoder\n             self._check_encoder_hidden_states_for_generate(\n-                output.encoder_hidden_states, batch_size, config, seq_length\n+                output.encoder_hidden_states, input_batch_size, config, seq_length\n             )\n \n             # decoder\n             self._check_hidden_states_for_generate(\n-                num_sequences_in_output,\n+                internal_batch_size,\n                 output.decoder_hidden_states,\n                 min_length=1,\n                 max_length=output.sequences.shape[-1],\n@@ -2182,7 +2245,7 @@ def _check_outputs(self, output, main_input, config, use_cache=False, num_return\n             hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n             min_length = seq_length if not use_cache else seq_length + 1\n             self._check_hidden_states_for_generate(\n-                num_sequences_in_output,\n+                internal_batch_size,\n                 hidden_states,\n                 min_length=min_length,\n                 max_length=output.sequences.shape[-1],\n@@ -2213,7 +2276,7 @@ def _check_outputs(self, output, main_input, config, use_cache=False, num_return\n                 past_key_values = output.past_key_values\n                 past_sequence_length = output.sequences.shape[-1] - 1\n                 self._check_past_key_values_for_generate(\n-                    num_sequences_in_output,\n+                    internal_batch_size,\n                     past_key_values,\n                     seq_length=past_sequence_length,\n                     config=config,"
        },
        {
            "sha": "e4d0df141be2b9cde0dbc2b7da93421997ae67a6",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -1532,8 +1532,3 @@ def test_retain_grad_hidden_states_attentions(self):\n     @unittest.skip\n     def test_save_load_fast_init_from_base(self):\n         pass\n-\n-    @unittest.skip(reason=\"Generate needs input ids\")\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        # generate only works with input ids for bartforcausalLM\n-        pass"
        },
        {
            "sha": "25566027742507c457c3b42fe71a5571a44ab04f",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -511,11 +511,6 @@ def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n \n-    @unittest.skip(reason=\"Generate needs input ids\")\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        # generate only works with input ids for bertforcausalLM\n-        pass\n-\n     def test_model_as_decoder_with_default_input_mask(self):\n         # This regression test was failing with PyTorch < 1.3\n         ("
        },
        {
            "sha": "2a8e7633ba40c53fdc3c740aa9f22cebfa9e6979",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -16,17 +16,14 @@\n \n import unittest\n \n-import pytest\n import requests\n from parameterized import parameterized\n \n from transformers import ChameleonConfig, is_torch_available, is_vision_available, set_seed\n from transformers.testing_utils import (\n     require_bitsandbytes,\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -329,43 +326,6 @@ def test_model_rope_scaling(self, scaling_type):\n         # The output should be different for long inputs\n         self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n \n-    @require_flash_attn\n-    @require_read_token\n-    @require_torch_gpu\n-    @require_bitsandbytes\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        model = ChameleonForConditionalGeneration.from_pretrained(\n-            \"facebook/chameleon-7b\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-        )\n-\n-        processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-\n-        processor.tokenizer.padding_side = \"right\"\n-\n-        inputs = processor(text=texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_native = processor.tokenizer.batch_decode(output_native)\n-\n-        model = ChameleonForConditionalGeneration.from_pretrained(\n-            \"facebook/chameleon-7b\",\n-            load_in_4bit=True,\n-            attn_implementation=\"flash_attention_2\",\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_fa_2 = processor.tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(output_native, output_fa_2)\n-\n     @unittest.skip(\"Chameleon forces some token ids to be -inf!\")\n     def test_batching_equivalence(self):\n         pass"
        },
        {
            "sha": "e8483f8c7c7d32126fcc64f09f3eab15cd8ba069",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -319,9 +319,6 @@ class GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.6]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"google/gemma-2b\"\n-\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = GemmaForCausalLM if is_torch_available() else None\n \n@@ -419,51 +416,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Gemma apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "7bca83f96d73ab9534cedc4b20edaf8abb8a56a7",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -78,7 +78,6 @@ class Gemma2ModelTest(GemmaModelTest, unittest.TestCase):\n     test_pruning = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]\n-    _torch_compile_test_ckpt = \"google/gemma-2-9b\"\n \n     def setUp(self):\n         self.model_tester = Gemma2ModelTester(self)"
        },
        {
            "sha": "b92c5db815b77ab662de2e37424a5260d5828d14",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -28,7 +28,6 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     require_torch_sdpa,\n     slow,\n     torch_device,\n@@ -306,10 +305,6 @@ class GlmModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     test_headmasking = False\n     test_pruning = False\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"THUDM/glm-4-9b\"\n-    _torch_compile_test_revision = \"refs/pr/15\"\n-\n     def setUp(self):\n         self.model_tester = GlmModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=GlmConfig, hidden_size=37)\n@@ -426,41 +421,6 @@ def test_custom_4d_attention_mask(self):\n \n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"Overwrite the common test as the test is flaky on tiny models.\"\"\"\n-        model = GlmForCausalLM.from_pretrained(\n-            \"THUDM/glm-4-9b\",\n-            device_map={\"\": 0},\n-            torch_dtype=torch.bfloat16,\n-            revision=\"refs/pr/15\",\n-        )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b\", revision=\"refs/pr/15\")\n-        tokenizer.padding_side = \"right\"\n-\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        output_native = model.generate(**inputs, max_new_tokens=15, do_sample=False)\n-        output_native = tokenizer.batch_decode(output_native)\n-\n-        model = GlmForCausalLM.from_pretrained(\n-            \"THUDM/glm-4-9b\",\n-            device_map={\"\": 0},\n-            attn_implementation=\"flash_attention_2\",\n-            torch_dtype=torch.bfloat16,\n-            revision=\"refs/pr/15\",\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=15, do_sample=False)\n-        output_fa_2 = tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(output_native, output_fa_2)\n-\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n     @slow"
        },
        {
            "sha": "afc741cd502dec287e3d4f7deaddc85ea06b9975",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 44,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -17,14 +17,9 @@\n import datetime\n import unittest\n \n-import pytest\n-\n-from transformers import BitsAndBytesConfig, GPTJConfig, is_torch_available\n+from transformers import GPTJConfig, is_torch_available\n from transformers.testing_utils import (\n-    require_bitsandbytes,\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     tooslow,\n     torch_device,\n@@ -505,44 +500,6 @@ def test_model_from_pretrained(self):\n         model = GPTJModel.from_pretrained(model_name, revision=\"float16\", torch_dtype=torch.float16)\n         self.assertIsNotNone(model)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @require_bitsandbytes\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\")\n-\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-        expected_outputs = [\n-            \"hi<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Q: I have a question about the new version of the game. I have a question about the\",\n-            \"Hello this is a very long sentence.\\n\\nA:\\n\\nI think the best way to understand this is to think of it\",\n-        ]\n-\n-        tokenizer.padding_side = \"right\"\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n-\n-        model = GPTJForCausalLM.from_pretrained(\n-            \"EleutherAI/gpt-j-6b\",\n-            device_map={\"\": 0},\n-            attn_implementation=\"flash_attention_2\",\n-            revision=\"float16\",\n-            torch_dtype=torch.float16,\n-            quantization_config=quantization_config,\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_fa_2 = tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(expected_outputs, output_fa_2)\n-\n \n @require_torch\n class GPTJModelLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "97b59f5aa5062127ee831b86a49eaefb32adaf07",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 46,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -17,12 +17,10 @@\n import tempfile\n import unittest\n \n-import pytest\n from parameterized import parameterized\n \n-from transformers import AutoTokenizer, GraniteConfig, is_torch_available, set_seed\n+from transformers import GraniteConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n@@ -303,9 +301,6 @@ class GraniteModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"ibm/PowerLM-3b\"\n-\n     def setUp(self):\n         self.model_tester = GraniteModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=GraniteConfig, hidden_size=37)\n@@ -423,46 +418,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @require_bitsandbytes\n-    @pytest.mark.flash_attn_test\n-    @require_read_token\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        model = GraniteForCausalLM.from_pretrained(\n-            \"ibm/PowerLM-3b\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-        )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"ibm/PowerLM-3b\")\n-\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-\n-        tokenizer.padding_side = \"right\"\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_native = tokenizer.batch_decode(output_native)\n-\n-        model = GraniteForCausalLM.from_pretrained(\n-            \"ibm/PowerLM-3b\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-            attn_implementation=\"flash_attention_2\",\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_fa_2 = tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(output_native, output_fa_2)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @slow"
        },
        {
            "sha": "f2f76b9fa75bf33c35c643378bce8b7cabe2e21a",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -17,12 +17,10 @@\n import tempfile\n import unittest\n \n-import pytest\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, GraniteMoeConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n@@ -302,9 +300,6 @@ class GraniteMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"ibm/PowerMoE-3b\"\n-\n     def setUp(self):\n         self.model_tester = GraniteMoeModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=GraniteMoeConfig, hidden_size=37)\n@@ -422,46 +417,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @require_bitsandbytes\n-    @pytest.mark.flash_attn_test\n-    @require_read_token\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        model = GraniteMoeForCausalLM.from_pretrained(\n-            \"ibm-granite/granitemoe-3b\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-        )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granitemoe-3b\")\n-\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-\n-        tokenizer.padding_side = \"right\"\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_native = tokenizer.batch_decode(output_native)\n-\n-        model = GraniteMoeForCausalLM.from_pretrained(\n-            \"ibm-granite/granitemoe-3b\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-            attn_implementation=\"flash_attention_2\",\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_fa_2 = tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(output_native, output_fa_2)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @slow"
        },
        {
            "sha": "d19d10932bfcdc10e0e34f31ef919c70452cb8ce",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -770,13 +770,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_custom_4d_attention_mask(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"IDEFICS has specific requirements for working with inputs embeds like passing also the ids and pixels\"\n-    )\n-    @parameterized.expand([(1,), (2,)])\n-    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n-        pass\n-\n     @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n     def test_generate_compile_fullgraph(self):\n         pass"
        },
        {
            "sha": "042fecf4bd25f7718b69778a6c454d21ac90aded",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -20,7 +20,6 @@\n import unittest\n from io import BytesIO\n \n-import pytest\n import requests\n \n from transformers import (\n@@ -420,50 +419,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n     def test_flash_attn_2_fp32_ln(self):\n         pass\n \n-    @pytest.mark.generate\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n-        # overwrite because IDEFICS needs ids and embeds at the input to be not None\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            # Ignore:\n-            # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n-            #   which would cause a mismatch),\n-            config.pad_token_id = config.eos_token_id = -1\n-            config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n-            input_ids = inputs_dict.pop(\"input_ids\")\n-\n-            # Traditional way of generating text\n-            outputs_from_ids = model.generate(\n-                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n-            )\n-            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n-\n-            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs_from_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=inputs_embeds,\n-                max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n-            )\n-            self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n-\n-            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n-            # same, but the logits will almost surely be different)\n-            random_embeds = torch.rand_like(inputs_embeds)\n-            outputs_from_rand_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=random_embeds,\n-                max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n-            )\n-            for i in range(len(outputs_from_rand_embeds.scores)):\n-                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n-\n     # We need to override as we need to prepare such that the image token is the last token\n     def test_resize_tokens_embeddings(self):\n         (original_config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "5dc352d22fe0c09f2ba9904e9490d44a07662c0f",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 78,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -19,7 +19,6 @@\n import unittest\n from io import BytesIO\n \n-import pytest\n import requests\n \n from transformers import (\n@@ -180,10 +179,6 @@ def test_inputs_embeds():\n     def test_inputs_embeds_matches_input_ids(self):\n         pass\n \n-    @unittest.skip(reason=\"Model does not support padding right\")\n-    def test_flash_attn_2_generate_padding_right(self):\n-        pass\n-\n     @unittest.skip(reason=\"Model does not support padding right\")\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n@@ -337,10 +332,6 @@ def setUp(self):\n     def test_inputs_embeds():\n         pass\n \n-    @unittest.skip(reason=\"Model does not support padding right\")\n-    def test_flash_attn_2_generate_padding_right(self):\n-        pass\n-\n     @unittest.skip(reason=\"Model does not support padding right\")\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n@@ -367,50 +358,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n     def test_flash_attn_2_fp32_ln(self):\n         pass\n \n-    @pytest.mark.generate\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n-        # overwrite because IDEFICS needs ids and embeds at the input to be not None\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            # Ignore:\n-            # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n-            #   which would cause a mismatch),\n-            config.pad_token_id = config.eos_token_id = -1\n-            config.is_decoder = True\n-            model = model_class(config).to(torch_device).eval()\n-            input_ids = inputs_dict.pop(\"input_ids\")\n-\n-            # Traditional way of generating text\n-            outputs_from_ids = model.generate(\n-                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n-            )\n-            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n-\n-            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs_from_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=inputs_embeds,\n-                max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n-            )\n-            self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n-\n-            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n-            # same, but the logits will almost surely be different)\n-            random_embeds = torch.rand_like(inputs_embeds)\n-            outputs_from_rand_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=random_embeds,\n-                max_new_tokens=5,\n-                return_dict_in_generate=True,\n-                output_scores=True,\n-            )\n-            for i in range(len(outputs_from_rand_embeds.scores)):\n-                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n-\n     # We need to override as we need to prepare such that the image token is the last token\n     def test_resize_tokens_embeddings(self):\n         (original_config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -526,31 +473,6 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        # overwrite because IDEFICS needs ids and embeds at the input to be not None\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            pad_token_id = config.pad_token_id if config.pad_token_id is not None else 1\n-\n-            wte = model.get_input_embeddings()\n-\n-            input_ids = inputs[\"input_ids\"]\n-            # some models infer position ids/attn mask differently when input ids\n-            # by check if pad_token let's make sure no padding is in input ids\n-            not_pad_token_id = pad_token_id + 1 if max(0, pad_token_id - 1) == 0 else pad_token_id - 1\n-            input_ids[input_ids == pad_token_id] = not_pad_token_id\n-            del inputs[\"input_ids\"]\n-            inputs_embeds = wte(input_ids)\n-            out_ids = model.generate(input_ids=input_ids, **inputs, max_new_tokens=2)\n-            out_embeds = model.generate(input_ids=input_ids, inputs_embeds=inputs_embeds, **inputs, max_new_tokens=2)\n-\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n-\n \n @require_torch\n class Idefics3ForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "ef0b5831587be10ac92ca03cf286a41bd735e4ae",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -539,93 +539,6 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_generate_padding_right test as the Jamba model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_generate_use_cache test as the Jamba model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Jamba does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "dc510f0ff040bb1c37d800925b33bfc5522b3df2",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch JetMoe model.\"\"\"\n \n import gc\n-import tempfile\n import unittest\n \n import pytest\n@@ -377,85 +376,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: JetMoe apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "0f0b595d3d230612c752c8dea28506ee3eb03d2c",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -438,12 +438,6 @@ def check_same_values(layer_1, layer_2):\n             # self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)\n             # self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))\n \n-    @unittest.skip(\n-        \"KOSMOS-2 doesn't support inputs embeds. The test isn't skipped by checking ipnut args because KOSMOS-2 has `generate()` overwritten\"\n-    )\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        pass\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"microsoft/kosmos-2-patch14-224\""
        },
        {
            "sha": "375ec1dd3e6f3aa5eeda995132ece7343f61469e",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -26,7 +26,6 @@\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     backend_empty_cache,\n-    require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n@@ -316,9 +315,6 @@ class LlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"meta-llama/Llama-2-7b-hf\"\n-\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = LlamaForCausalLM if is_torch_available() else None\n \n@@ -585,43 +581,6 @@ def _reinitialize_config(base_config, new_kwargs):\n         with self.assertRaises(KeyError):\n             config = _reinitialize_config(base_config, {\"rope_scaling\": {\"rope_type\": \"linear\"}})  # missing \"factor\"\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @require_bitsandbytes\n-    @pytest.mark.flash_attn_test\n-    @require_read_token\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        model = LlamaForCausalLM.from_pretrained(\n-            \"meta-llama/Llama-2-7b-hf\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-        )\n-\n-        tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n-\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-\n-        tokenizer.padding_side = \"right\"\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_native = tokenizer.batch_decode(output_native)\n-\n-        model = LlamaForCausalLM.from_pretrained(\n-            \"meta-llama/Llama-2-7b-hf\", load_in_4bit=True, device_map={\"\": 0}, attn_implementation=\"flash_attention_2\"\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_fa_2 = tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(output_native, output_fa_2)\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @slow"
        },
        {
            "sha": "9b3a9563b58ddceba63b744984dcbc661b69d15d",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -204,8 +204,8 @@ def test_generate_without_input_ids(self):\n         pass\n \n     @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    @parameterized.expand([(1,), (2,)])\n-    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n         pass\n \n     @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n@@ -276,12 +276,6 @@ def recursive_check(tuple_object, dict_object):\n             dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n             check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n \n-    @unittest.skip(\n-        reason=\"Mamba2 does not support generating with input embeddings (custom cache_position computation)\"\n-    )\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        pass\n-\n \n @require_torch\n @slow"
        },
        {
            "sha": "df0007d666a077b28ee420953c29e92690aff45f",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -21,7 +21,6 @@\n \n import numpy as np\n from datasets import Audio, load_dataset\n-from packaging import version\n from parameterized import parameterized\n from pytest import mark\n \n@@ -745,22 +744,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n-    # For now, Let's focus only on GPU for `torch.compile`\n-    @slow\n-    @require_torch_gpu\n-    def test_torch_compile(self):\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        n_iter = 3\n-        for model_class in self.all_model_classes:\n-            model = model_class(config).to(torch_device)\n-            model.forward = torch.compile(model.forward)\n-            for i in range(n_iter):\n-                _ = model(inputs_dict[\"input_values\"].to(torch_device))\n-\n     @is_flaky()\n     def test_batching_equivalence(self):\n         super().test_batching_equivalence()"
        },
        {
            "sha": "1538735ad78bd7c6f3772d982ec0ebc84a28c1de",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch Mistral model.\"\"\"\n \n import gc\n-import tempfile\n import unittest\n \n import pytest\n@@ -416,85 +415,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Mistral apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "931bb1f17beccf5e907ad7a319f7f98c8e8544f7",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Mixtral model.\"\"\"\n \n-import tempfile\n import unittest\n \n import pytest\n@@ -415,85 +414,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Mixtral apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "5174247b895eead7b222564078d7f878c2019e8e",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -126,7 +126,6 @@ class MllamaForCausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, unitte\n     all_generative_model_classes = (MllamaForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n-    _torch_compile_test_ckpt = \"nltpt/Llama-3.2-11B-Vision\"\n \n     def setUp(self):\n         self.model_tester = MllamaText2TextModelTester(self)"
        },
        {
            "sha": "7d4b855c10d8bf1ed87de82788f4af99397d0edb",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 5,
            "deletions": 56,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -560,7 +560,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n         return config, input_ids, attention_mask, inputs_dict\n \n     def prepare_config_and_inputs_for_generate(self, batch_size=2):\n-        config, filtered_inputs_dict = super().prepare_config_and_inputs_for_generate()\n+        config, filtered_inputs_dict = super().prepare_config_and_inputs_for_generate(batch_size=batch_size)\n \n         # Make sure we only return `input_ids`.\n         # Note that audio_codes will still be generated internally, so the ability to test audio codes is still there.\n@@ -591,9 +591,11 @@ def _check_hidden_states_for_generate(\n                 [expected_shape] * len(iter_hidden_states),\n             )\n \n-    def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n+    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         # Overwrite because the generate method actually alway uses `inputs_embeds` so `use_cache` is always `True`\n-        super()._check_outputs(output, input_ids, config, use_cache=True, num_return_sequences=num_return_sequences)\n+        super()._check_outputs(\n+            output, config, use_cache=True, num_return_sequences=num_return_sequences, num_beams=num_beams\n+        )\n \n     def _check_hidden_states_for_generate(\n         self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n@@ -655,59 +657,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    @pytest.mark.generate\n-    @parameterized.expand([(1,), (2,)])\n-    def test_generate_from_inputs_embeds_decoder_only(self, num_beams):\n-        for model_class in self.all_generative_model_classes:\n-            config, input_ids, _, inputs_dict = self._get_input_ids_and_config()\n-\n-            model = model_class(config).to(torch_device).eval()\n-            generation_kwargs = {\n-                \"return_dict_in_generate\": True,\n-                \"output_scores\": True,\n-                \"num_beams\": num_beams,\n-                \"do_sample\": False,\n-            }\n-\n-            # Traditional way of generating text\n-            outputs_from_ids = model.generate(input_ids, max_new_tokens=5, **generation_kwargs, **inputs_dict)\n-            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n-\n-            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs_from_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=inputs_embeds,\n-                max_new_tokens=5,\n-                **generation_kwargs,\n-                **inputs_dict,\n-            )\n-\n-            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n-            # same, but the logits will almost surely be different)\n-            random_embeds = torch.rand_like(inputs_embeds)\n-            outputs_from_rand_embeds = model.generate(\n-                input_ids,\n-                inputs_embeds=random_embeds,\n-                max_new_tokens=5,\n-                **generation_kwargs,\n-                **inputs_dict,\n-            )\n-            for i in range(len(outputs_from_rand_embeds.scores)):\n-                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n-\n-            # input_ids is not a required input -- if we don't pass it, the newly generated tokens will be the same\n-            outputs_from_embeds_wo_ids = model.generate(\n-                inputs_embeds=inputs_embeds,\n-                max_new_tokens=5,\n-                **generation_kwargs,\n-                **inputs_dict,\n-            )\n-            self.assertListEqual(\n-                outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :].tolist(),\n-                outputs_from_embeds_wo_ids.sequences.tolist(),\n-            )\n-\n     @unittest.skip(reason=\"Continuing from past key values is not straightforward as we're dealing with 3 inputs\")\n     def test_generate_continue_from_past_key_values(self):\n         pass"
        },
        {
            "sha": "1628d3a5893eaa6cff238e0cdf8c9fc5b421b2d6",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -576,9 +576,6 @@ class MT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     # The small MT5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"google/mt5-small\"\n-\n     def setUp(self):\n         self.model_tester = MT5ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MT5Config, d_model=37)"
        },
        {
            "sha": "963cace28d6e4141731dffcc5c8416347fc2e8c3",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 281,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -450,144 +450,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n-    def test_flash_attn_2_generate_left_padding(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # make sure we do left padding\n-                dummy_attention_mask[:, :-1] = 0\n-                dummy_attention_mask[:, -1:] = 1\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n-    def test_flash_attn_2_generate_padding_right(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # make sure we do right padding\n-                dummy_attention_mask[:, :-1] = 1\n-                dummy_attention_mask[:, -1:] = 0\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n-    def test_flash_attn_2_generate_use_cache(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n     @require_torch_sdpa\n     @slow\n@@ -1585,149 +1447,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n-    def test_flash_attn_2_generate_left_padding(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\")\n-                if dummy_attention_mask is None:\n-                    dummy_attention_mask = torch.ones_like(dummy_input)\n-\n-                # make sure we do left padding\n-                dummy_attention_mask[:, :-1] = 0\n-                dummy_attention_mask[:, -1:] = 1\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n-    def test_flash_attn_2_generate_padding_right(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\")\n-                if dummy_attention_mask is None:\n-                    dummy_attention_mask = torch.ones_like(dummy_input)\n-                # make sure we do right padding\n-                dummy_attention_mask[:, :-1] = 1\n-                dummy_attention_mask[:, -1:] = 0\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n-    def test_flash_attn_2_generate_use_cache(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.has_attentions:"
        },
        {
            "sha": "957db9f23b0f21bcf5ee2186b6a9bb4faf242783",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 143,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -1437,149 +1437,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_left_padding\n-    def test_flash_attn_2_generate_left_padding(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\")\n-                if dummy_attention_mask is None:\n-                    dummy_attention_mask = torch.ones_like(dummy_input)\n-\n-                # make sure we do left padding\n-                dummy_attention_mask[:, :-1] = 0\n-                dummy_attention_mask[:, -1:] = 1\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_padding_right\n-    def test_flash_attn_2_generate_padding_right(self):\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\")\n-                if dummy_attention_mask is None:\n-                    dummy_attention_mask = torch.ones_like(dummy_input)\n-                # make sure we do right padding\n-                dummy_attention_mask[:, :-1] = 1\n-                dummy_attention_mask[:, -1:] = 0\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=8, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_generate_use_cache\n-    def test_flash_attn_2_generate_use_cache(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation={\"decoder\": \"flash_attention_2\", \"audio_encoder\": None, \"text_encoder\": None},\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         if not self.has_attentions:"
        },
        {
            "sha": "37a581a33866ce8b52e3f997f3bba4d9b9dc529c",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -92,8 +92,6 @@ class NemotronModelTest(GemmaModelTest):\n     test_pruning = False\n     fx_compatible = False\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"nvidia/nemotron-3-8b-base-4k-hf\"\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = NemotronForCausalLM if is_torch_available() else None\n "
        },
        {
            "sha": "1d96b9c338fef00216331fbdd0341e7a9f652f79",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -346,10 +346,6 @@ def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"TODO (@joao): fix me -- failing to produce similar results\")\n-    def test_static_cache_matches_dynamic(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "eae6789bef252e1b56bfc107e329bd3ca5840fcd",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -17,15 +17,11 @@\n \n import unittest\n \n-import pytest\n from parameterized import parameterized\n \n from transformers import PhiConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    require_bitsandbytes,\n-    require_flash_attn,\n     require_torch,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -468,43 +464,6 @@ def test_model_rope_scaling(self):\n             torch.testing.assert_close(ntk_sin_long, original_sin_long)\n         self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @require_bitsandbytes\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    # Copied from tests.models.llama.test_modeling_llama.LlamaModelTest.test_flash_attn_2_generate_padding_right with LlamaForCausalLM->PhiForCausalLM,LlamaTokenizer->AutoTokenizer,meta-llama/Llama-2-7b-hf->microsoft/phi-1\n-    def test_flash_attn_2_generate_padding_right(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        model = PhiForCausalLM.from_pretrained(\n-            \"microsoft/phi-1\",\n-            load_in_4bit=True,\n-            device_map={\"\": 0},\n-        )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\")\n-\n-        texts = [\"hi\", \"Hello this is a very long sentence\"]\n-\n-        tokenizer.padding_side = \"right\"\n-        tokenizer.pad_token = tokenizer.eos_token\n-\n-        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(0)\n-\n-        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_native = tokenizer.batch_decode(output_native)\n-\n-        model = PhiForCausalLM.from_pretrained(\n-            \"microsoft/phi-1\", load_in_4bit=True, device_map={\"\": 0}, attn_implementation=\"flash_attention_2\"\n-        )\n-\n-        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_fa_2 = tokenizer.batch_decode(output_fa_2)\n-\n-        self.assertListEqual(output_native, output_fa_2)\n-\n \n @slow\n @require_torch"
        },
        {
            "sha": "f51dc2e0a5e26fe71ee3f05ad0184b2ef3805a22",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch Qwen2 model.\"\"\"\n \n import gc\n-import tempfile\n import unittest\n \n import pytest\n@@ -428,85 +427,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Qwen2 apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "abc7b57919b083e967cd0763bd726a3d9e83a231",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch Qwen2MoE model.\"\"\"\n \n import gc\n-import tempfile\n import unittest\n \n import pytest\n@@ -453,85 +452,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Qwen2Moe apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "a3272853a78427da082814cc53f83eeccd0b071d",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -301,10 +301,6 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(reason=\"Generate needs input ids\")\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        pass\n-\n     @unittest.skip(reason=\"CPU offload is not yet supported\")\n     def test_cpu_offload(self):\n         pass"
        },
        {
            "sha": "985115d7707b6e7e574ad085ac2c0dd2c9dd201e",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -420,10 +420,6 @@ def _check_hidden_states_for_generate(\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(reason=\"RecurrentGemma does not support generating with input embeddings (missing position_ids)\")\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        pass\n-\n \n @require_torch_accelerator\n @slow"
        },
        {
            "sha": "df743f132c114078c072236cb44275e0ea911587",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Starcoder2 model.\"\"\"\n \n-import tempfile\n import unittest\n \n import pytest\n@@ -404,85 +403,6 @@ def test_save_load_fast_init_from_base(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Starcoder2 apparently does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "b03416390766d0192213c0770763b8bef6b908ee",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -580,9 +580,6 @@ class T5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     # The small T5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"google-t5/t5-small\"\n-\n     def setUp(self):\n         self.model_tester = T5ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=T5Config, d_model=37)"
        },
        {
            "sha": "377668851c581530b3544b4e667d89a4fb2e2862",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -317,9 +317,6 @@ class UMT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     # The small UMT5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n \n-    # used in `test_torch_compile`\n-    _torch_compile_test_ckpt = \"google/umt5-small\"\n-\n     def setUp(self):\n         self.model_tester = UMT5ModelTester(self)\n "
        },
        {
            "sha": "12aedaca8cf986e4a38edac8396bd869e5b6915d",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -1574,59 +1574,6 @@ def test_generate_output_type(self, return_dict_in_generate):\n             )\n             assert isinstance(pred_ids, expected_output_type)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_reuse_cache(self):\n-        max_new_tokens = 2\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name][..., :10]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = dummy_input.shape[1] * 2 + max_new_tokens * 2 + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # run generate once to get filled cache\n-                output = model.generate(\n-                    dummy_input,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                    return_dict_in_generate=True,\n-                )\n-                past_key_values = output.past_key_values\n-\n-                # Try to continue generation from where we left, given that we have more than 1 new token to process\n-                # e.g. this can happen in speculative decoding when feeding candidate tokens back to target model\n-                _ = model.generate(\n-                    dummy_input,\n-                    decoder_input_ids=output.sequences,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                    past_key_values=past_key_values,\n-                )\n-\n     def test_labels_sequence_max_length_correct(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -3961,11 +3908,6 @@ def test_generate_without_input_ids(self):\n         # generate only works with input ids for whisper\n         pass\n \n-    @unittest.skip(reason=\"Generate needs input ids\")\n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        # generate only works with input ids for whisper\n-        pass\n-\n     @unittest.skip(reason=\"Decoder can't keep attention grads\")\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n@@ -3974,18 +3916,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"FA2 testing suite needs to be refactored to be compatible with WhisperDecoder for that test\"\n-    )\n-    def test_flash_attn_2_generate_reuse_cache(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"Duplicated test with WhisperModelTest + the FA2 testing suite needs to be refactored to be compatible with WhisperDecoder for that test\"\n-    )\n-    def test_flash_attn_2_generate_padding_right(self):\n-        pass\n-\n     @unittest.skip(\n         \"Duplicated test with WhisperModelTest + the FA2 testing suite needs to be refactored to be compatible with WhisperDecoder for that test\"\n     )"
        },
        {
            "sha": "a6dd516f98a412fc9036654708cc45515d931c41",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -542,93 +542,6 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_generate_padding_right test as the Zamba model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        import torch\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = torch.LongTensor([[0, 2, 3, 4], [0, 2, 3, 4]]).to(torch_device)\n-                dummy_attention_mask = torch.LongTensor([[1, 1, 1, 1], [1, 1, 1, 0]]).to(torch_device)\n-\n-                model.generate(dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                with self.assertRaises(ValueError):\n-                    _ = model.generate(\n-                        dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                    )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @pytest.mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        r\"\"\"\n-        Overriding the test_flash_attn_2_generate_use_cache test as the Zamba model, like Mixtral, doesn't support\n-        right padding + use cache with FA2\n-        \"\"\"\n-        import torch\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # NOTE: Zamba does not support right padding + use_cache with FA2.\n-                dummy_attention_mask[:, -1] = 1\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "e2719d8cf1b6005cacb513732985f278ecb88b8d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 425,
            "changes": 425,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a734ea2c340beee23e665601919814918bf4c43/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a734ea2c340beee23e665601919814918bf4c43/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8a734ea2c340beee23e665601919814918bf4c43",
            "patch": "@@ -22,7 +22,6 @@\n import random\n import re\n import tempfile\n-import time\n import warnings\n from collections import defaultdict\n from contextlib import contextmanager\n@@ -37,10 +36,7 @@\n from transformers import (\n     AutoModel,\n     AutoModelForCausalLM,\n-    AutoModelForSeq2SeqLM,\n     AutoModelForSequenceClassification,\n-    AutoTokenizer,\n-    GenerationConfig,\n     PretrainedConfig,\n     PreTrainedModel,\n     is_torch_available,\n@@ -86,7 +82,6 @@\n     require_deepspeed,\n     require_flash_attn,\n     require_non_xpu,\n-    require_read_token,\n     require_safetensors,\n     require_torch,\n     require_torch_accelerator,\n@@ -3000,71 +2995,6 @@ def test_inputs_embeds_matches_input_ids(self):\n                     )[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_generative_model_classes:\n-            if model_class.__name__ not in [\n-                *get_values(MODEL_FOR_CAUSAL_LM_MAPPING_NAMES),\n-                *get_values(MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES),\n-            ]:\n-                continue\n-\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            model_forward_args = inspect.signature(model.forward).parameters\n-            if any(argument not in model_forward_args for argument in [\"inputs_embeds\", \"position_ids\"]):\n-                self.skipTest(reason=\"This model doesn't use `inputs_embeds` or `position_ids`.\")\n-            has_inputs_embeds_forwarding = \"inputs_embeds\" in set(\n-                inspect.signature(model.prepare_inputs_for_generation).parameters.keys()\n-            )\n-            if not has_inputs_embeds_forwarding:\n-                self.skipTest(reason=\"This model doesn't support `inputs_embeds` passed to `generate`.\")\n-            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            pad_token_id = config.pad_token_id if config.pad_token_id is not None else 1\n-\n-            # VLMs can't generate with embeds and pixels at the same time. We expect the user to pass merged\n-            # embeds already\n-            if model_class.__name__ in get_values(MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES):\n-                inputs.pop(\"pixel_values\", None)\n-                inputs.pop(\"pixel_values_videos\", None)\n-                inputs.pop(\"pixel_values_images\", None)\n-\n-            wte = model.get_input_embeddings()\n-            if not self.is_encoder_decoder:\n-                input_ids = inputs[\"input_ids\"]\n-                # some models infer position ids/attn mask differently when input ids\n-                # by check if pad_token let's make sure no padding is in input ids\n-                not_pad_token_id = pad_token_id + 1 if max(0, pad_token_id - 1) == 0 else pad_token_id - 1\n-                input_ids[input_ids == pad_token_id] = not_pad_token_id\n-                del inputs[\"input_ids\"]\n-                inputs_embeds = wte(input_ids)\n-                out_ids = model.generate(input_ids=input_ids, **inputs, max_new_tokens=2)[:, -2:]\n-                out_embeds = model.generate(inputs_embeds=inputs_embeds, **inputs, max_new_tokens=2)\n-            else:\n-                encoder_input_ids = inputs[\"input_ids\"]\n-                decoder_input_ids = inputs.get(\"decoder_input_ids\", encoder_input_ids)\n-                encoder_input_ids[encoder_input_ids == pad_token_id] = max(0, pad_token_id + 1)\n-                decoder_input_ids[decoder_input_ids == pad_token_id] = max(0, pad_token_id + 1)\n-                del inputs[\"input_ids\"]\n-                inputs.pop(\"decoder_input_ids\", None)\n-                inputs_embeds = wte(encoder_input_ids)\n-                decoder_inputs_embeds = wte(decoder_input_ids)\n-                out_ids = model.generate(\n-                    input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids, **inputs, max_new_tokens=2\n-                )[:, -2:]\n-                out_embeds = model.generate(\n-                    inputs_embeds=inputs_embeds,\n-                    decoder_inputs_embeds=decoder_inputs_embeds,\n-                    **inputs,\n-                    max_new_tokens=2,\n-                )\n-            # NOTE: this test changes the order of FP ops, there may be tiny differences in the output\n-            number_of_different_tokens = (out_ids != out_embeds).sum()\n-            max_differences = int(out_ids.shape[0] * out_ids.shape[1] * 0.1)\n-            self.assertTrue(number_of_different_tokens <= max_differences)  # accept up to 10% mismatch\n-\n     @require_non_xpu\n     @require_torch_multi_gpu\n     def test_multi_gpu_data_parallel_forward(self):\n@@ -3857,102 +3787,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    @is_flaky()\n-    def test_flash_attn_2_generate_left_padding(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # make sure we do left padding\n-                dummy_attention_mask[:, :-1] = 0\n-                dummy_attention_mask[:, -1:] = 1\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @is_flaky()\n-    @slow\n-    def test_flash_attn_2_generate_padding_right(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\n-                    torch_device\n-                )\n-\n-                dummy_input = inputs_dict[model.main_input_name]\n-                if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                    dummy_input = dummy_input.to(torch.float16)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-                # make sure we do right padding\n-                dummy_attention_mask[:, :-1] = 1\n-                dummy_attention_mask[:, -1:] = 0\n-\n-                out = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                )\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                out_fa = model.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=1, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(out, out_fa))\n-\n     def test_attn_implementation_composite_models(self):\n         \"\"\"\n         Tests if composite models can receive a dict object as attn_implementation, where each key should be\n@@ -4525,65 +4359,6 @@ def test_sdpa_matches_eager_sliding_window(self):\n                     torch.allclose(res_eager[attention_mask == 1], res_sdpa[attention_mask == 1], rtol=1e-4, atol=1e-4)\n                 )\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_use_cache(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n-                # Generate with one batch only to test generation when attention mask will be None\n-                # when real inputs are used, because there is no padding. See issue #32237 for more\n-                dummy_input = dummy_input[:1, ...]\n-                dummy_attention_mask = torch.ones_like(dummy_attention_mask[:1, ...])\n-                _ = model.generate(\n-                    dummy_input,\n-                    attention_mask=dummy_attention_mask,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n@@ -4640,62 +4415,6 @@ def test_flash_attn_2_can_dispatch_composite_models(self):\n                     if not has_fa2:\n                         raise ValueError(\"The FA2 model should have FA2 layers\")\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_generate_reuse_cache(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        max_new_tokens = 2\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = dummy_input.shape[1] * 2 + max_new_tokens * 2 + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                model = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    attn_implementation=\"flash_attention_2\",\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                # run generate once to get filled cache\n-                output = model.generate(\n-                    dummy_input,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                    return_dict_in_generate=True,\n-                )\n-                past_key_values = output.past_key_values\n-\n-                # Try to continue generation from where we left, given that we have more than 1 new token to process\n-                # e.g. this can happen in speculative decoding when feeding candidate tokens back to target model\n-                dummy_input_updated = torch.cat([dummy_input, output.sequences], dim=-1)\n-                _ = model.generate(\n-                    dummy_input_updated,\n-                    max_new_tokens=max_new_tokens,\n-                    do_sample=False,\n-                    use_cache=True,\n-                    past_key_values=past_key_values,\n-                )\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @require_bitsandbytes\n@@ -4999,82 +4718,6 @@ def test_custom_4d_attention_mask(self):\n             normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n-    def test_static_cache_matches_dynamic(self):\n-        \"\"\"\n-        Tests that generating with static cache give almost same results as with dynamic cache.\n-        This test does not compile the model and check only logits similarity for numerical precision\n-        errors.\n-        \"\"\"\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(\n-                reason=\"Model architecture has no generative classes, and thus not necessarily supporting 4D masks\"\n-            )\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_static_cache:\n-                self.skipTest(f\"{model_class.__name__} does not support static cache\")\n-\n-            if not model_class._supports_cache_class:\n-                self.skipTest(f\"{model_class.__name__} does not support cache class\")\n-\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n-                self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n-\n-            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n-            model.eval()\n-\n-            dynamic_out = model.generate(\n-                **inputs, do_sample=False, max_new_tokens=10, output_logits=True, return_dict_in_generate=True\n-            )\n-            static_out = model.generate(\n-                **inputs,\n-                do_sample=False,\n-                max_new_tokens=10,\n-                cache_implementation=\"static\",\n-                output_logits=True,\n-                return_dict_in_generate=True,\n-            )\n-            self.assertTrue(torch.allclose(dynamic_out.logits[0], static_out.logits[0], rtol=1e-3, atol=1e-4))\n-\n-    # For now, Let's focus only on GPU for `torch.compile`\n-    @slow\n-    @require_torch_accelerator\n-    @require_read_token\n-    def test_torch_compile(self):\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-        torch.compiler.reset()\n-        if not hasattr(self, \"_torch_compile_test_ckpt\"):\n-            self.skipTest(f\"{self.__class__.__name__} doesn't have the attribute `_torch_compile_test_ckpt`.\")\n-        ckpt = self._torch_compile_test_ckpt\n-        revision = \"main\" if not hasattr(self, \"_torch_compile_test_revision\") else self._torch_compile_test_revision\n-\n-        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n-\n-        batch_size = 1\n-        n_iter = 3\n-\n-        tokenizer = AutoTokenizer.from_pretrained(ckpt)\n-        if self.is_encoder_decoder:\n-            model = AutoModelForSeq2SeqLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n-                torch_device\n-            )\n-        else:\n-            model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n-                torch_device\n-            )\n-\n-        model.generation_config.max_new_tokens = 4\n-\n-        model.generation_config.cache_implementation = \"static\"\n-        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\")\n-\n-        input_text = \"Why dogs are cute?\"\n-        input_ids = tokenizer([input_text] * batch_size, return_tensors=\"pt\").to(torch_device)\n-\n-        for i in range(n_iter):\n-            _ = model.generate(**input_ids, do_sample=False)\n-\n     @slow\n     @require_torch_gpu\n     def test_torch_compile_for_training(self):\n@@ -5118,74 +4761,6 @@ def test_torch_compile_for_training(self):\n         for name, param in model._orig_mod.named_parameters():\n             torch.testing.assert_close(param.grad.detach().cpu(), params[name], rtol=1e-4, atol=1e-4)\n \n-    @slow\n-    @require_torch_gpu  # Testing cuda graphs.\n-    @require_read_token\n-    def test_compile_cuda_graph_time(self):\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        # TODO felix: All models supporting `StaticCache` or `torch.compile` should be tested.\n-        # At the moment, only llama, gemma and gemma2 are tested here!\n-        if not hasattr(self, \"_torch_compile_test_ckpt\"):\n-            self.skipTest(f\"{self.__class__.__name__} doesn't have the attribute `_torch_compile_test_ckpt`.\")\n-        ckpt = self._torch_compile_test_ckpt\n-        revision = \"main\" if not hasattr(self, \"_torch_compile_test_revision\") else self._torch_compile_test_revision\n-\n-        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n-\n-        tokenizer = AutoTokenizer.from_pretrained(ckpt)\n-        if self.is_encoder_decoder:\n-            model = AutoModelForSeq2SeqLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n-                torch_device\n-            )\n-        else:\n-            model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n-                torch_device\n-            )\n-\n-        cache_implementation = \"static\"\n-        if model.config.model_type == \"gemma2\":\n-            cache_implementation = \"hybrid\"\n-\n-        new_tokens = 50\n-        gen_config = GenerationConfig(\n-            max_new_tokens=new_tokens,\n-            min_new_tokens=new_tokens,\n-            use_cache=True,\n-            pad_token_id=tokenizer.pad_token_id,\n-            num_beams=1,\n-            do_sample=False,\n-            eos_token_id=None,  # This is required for min_new_tokens to actually have an effect.\n-        )\n-        model.generation_config.eos_token_id = None  # greedy_search falls back on this eos_token_id that we need to set to None as well for min_new_tokens to have an effect.\n-\n-        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n-\n-        inp = tokenizer(\"Why cats are cute?\", return_tensors=\"pt\").to(torch_device)\n-\n-        # First run: the first run warms up each graph, which does things like CuBlas or Triton benchmarking\n-        start = time.perf_counter()\n-        _ = model.generate(**inp, generation_config=gen_config, cache_implementation=cache_implementation)\n-        end = time.perf_counter()\n-        graph_warmup_time = end - start\n-\n-        # Second run: CUDA Graph recording, and replays it\n-        start = time.perf_counter()\n-        _ = model.generate(**inp, generation_config=gen_config, cache_implementation=cache_implementation)\n-        end = time.perf_counter()\n-        record_time = end - start\n-\n-        # Finally: we hit the optimized, CUDA Graph replay path\n-        start = time.perf_counter()\n-        _ = model.generate(**inp, generation_config=gen_config, cache_implementation=cache_implementation)\n-        end = time.perf_counter()\n-        opt_time = end - start\n-\n-        # For the recording step, we expect only two cuda graphs and this step should be much faster than the first.\n-        self.assertTrue(record_time < 0.15 * graph_warmup_time)\n-        self.assertTrue(opt_time < record_time)\n-\n     def test_forward_with_num_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n             if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):"
        }
    ],
    "stats": {
        "total": 2609,
        "additions": 263,
        "deletions": 2346
    }
}