{
    "author": "Cyrilvallez",
    "message": "[style] Rework ruff rules and update all files (#43144)\n\n* improve ruff setup\n\n* fix\n\n* ruff was already bumped, no need to remove\n\n* remove F402\n\n* remove one more\n\n* one more\n\n* latest ruff\n\n* fix\n\n* more removed rules\n\n* last removed exceptions\n\n* fix copies again\n\n* fix\n\n* fix\n\n* all typing\n\n* fix copies\n\n* consistency\n\n* fix\n\n* oupsi\n\n* more precise modular converter\n\n* doc\n\n* last fix",
    "sha": "3a275d3581c0ecf962f7412aa764c2047331fd6b",
    "files": [
        {
            "sha": "fb35c995aed5f720db76abb5180fe7d7b1ea3f49",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -392,7 +392,7 @@ def save_results(self, model_name: str, results: dict, timestamp: str = \"\", summ\n         os.makedirs(model_dir, exist_ok=True)\n \n         # Create filename with timestamp\n-        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+        timestamp = timestamp if timestamp else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         filename = f\"{model_name}_benchmark_{timestamp}.json\"\n         filepath = os.path.join(model_dir, filename)\n \n@@ -443,7 +443,7 @@ def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestam\n                     f.write(\"\\n\".join(json_lines))\n \n                 # NOTE: we expect the repository to already exist\n-                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+                timestamp = timestamp if timestamp else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                 file_name = file_name + \"/\" + f\"benchmark_run_{timestamp}.jsonl\"\n                 api.upload_file(\n                     path_or_fileobj=jsonl_path,"
        },
        {
            "sha": "fff95cf57cfbee1c8a5c30ad84885a80d19e4295",
            "filename": "examples/modular-transformers/configuration_duplicated_method.py",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_duplicated_method.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n-from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n@@ -113,27 +112,27 @@ class DuplicatedMethodConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 32000,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 11008,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        max_position_embeddings: Optional[int] = 2048,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-6,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        pretraining_tp: Optional[int] = 1,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n-        mlp_bias: Optional[bool] = False,\n-        head_dim: Optional[int] = None,\n+        vocab_size: int | None = 32000,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 11008,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"silu\",\n+        max_position_embeddings: int | None = 2048,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-6,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        pretraining_tp: int | None = 1,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n+        mlp_bias: bool | None = False,\n+        head_dim: int | None = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "4ff064ebcdaaeaa53100d48b8cb46e3390ecf9bb",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_my_new_model.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n-from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n@@ -149,27 +148,27 @@ class MyNewModelConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 32000,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 11008,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        max_position_embeddings: Optional[int] = 2048,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-6,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        pretraining_tp: Optional[int] = 1,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n+        vocab_size: int | None = 32000,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 11008,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"silu\",\n+        max_position_embeddings: int | None = 2048,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-6,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        pretraining_tp: int | None = 1,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n         mlp_bias=True,\n-        head_dim: Optional[int] = None,\n+        head_dim: int | None = None,\n         new_param=0,\n         **kwargs,\n     ):"
        },
        {
            "sha": "85875a92484d93bde532718c12f0b50967cda3c2",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n@@ -52,27 +51,27 @@ class MyNewModel2Config(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 32000,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 11008,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        max_position_embeddings: Optional[int] = 2048,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-6,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        pretraining_tp: Optional[int] = 1,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n-        mlp_bias: Optional[bool] = False,\n-        head_dim: Optional[int] = None,\n+        vocab_size: int | None = 32000,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 11008,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"silu\",\n+        max_position_embeddings: int | None = 2048,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-6,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        pretraining_tp: int | None = 1,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n+        mlp_bias: bool | None = False,\n+        head_dim: int | None = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "7a9d9463ae18399fabe1a2c2747a0bf203546972",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_new_imgproc_model.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -74,13 +73,13 @@ class ImgprocModelImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n+        size: dict[str, int] | None = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n+        rescale_factor: int | float = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -103,8 +102,8 @@ def resize(\n         image: np.ndarray,\n         size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -150,18 +149,18 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n+        do_resize: bool | None = None,\n+        size: dict[str, int] | None = None,\n+        resample: PILImageResampling | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        do_convert_rgb: bool | None = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "8f49bc27ae75f64a2358dd94a3e76227280de50e",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_add_function.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # Note that zamba does not have the `apply_rotary_pos_emb` function!\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -65,5 +64,5 @@ class TestAttention(nn.Module):\n     def __init__(self):\n         pass\n \n-    def forward(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    def forward(self) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "85ebc3121e8fe6917e0b4b173c4e8e4fe2ad5533",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 43,
            "deletions": 44,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_dummy_bert.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -45,10 +44,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n@@ -91,8 +90,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -143,9 +142,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -215,9 +214,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[EncoderDecoderCache] = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: EncoderDecoderCache | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -290,11 +289,11 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n@@ -362,11 +361,11 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n@@ -415,14 +414,14 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         for i, layer_module in enumerate(self.layer):\n             hidden_states = layer_module(\n                 hidden_states,\n@@ -553,21 +552,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: list[torch.FloatTensor] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:"
        },
        {
            "sha": "dc4e41f149512fe04c955ad900a206d57f1b15eb",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -6,7 +6,6 @@\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -24,7 +23,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -43,7 +42,7 @@ def eager_attention_forward(\n class FromUppercaseModelAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Union[FromUppercaseModelVisionConfig, FromUppercaseModelTextConfig]):\n+    def __init__(self, config: FromUppercaseModelVisionConfig | FromUppercaseModelTextConfig):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -66,9 +65,9 @@ def __init__(self, config: Union[FromUppercaseModelVisionConfig, FromUppercaseMo\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -118,7 +117,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class FromUppercaseModelEncoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: Union[FromUppercaseModelVisionConfig, FromUppercaseModelTextConfig]):\n+    def __init__(self, config: FromUppercaseModelVisionConfig | FromUppercaseModelTextConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n         self.self_attn = FromUppercaseModelAttention(config)"
        },
        {
            "sha": "9f4eb5ad84ddb5b91af37485f29e6984e199b412",
            "filename": "examples/modular-transformers/modeling_global_indexing.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_global_indexing.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -71,7 +70,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -132,10 +131,10 @@ def __init__(self, config: GlobalIndexingConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "343488309d91045be8c1f165e8a79794bcff7e2f",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -6,7 +6,6 @@\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -28,7 +27,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -47,7 +46,7 @@ def eager_attention_forward(\n class Multimodal2VisionAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Union[Multimodal2VisionConfig, Multimodal2TextConfig]):\n+    def __init__(self, config: Multimodal2VisionConfig | Multimodal2TextConfig):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -70,9 +69,9 @@ def __init__(self, config: Union[Multimodal2VisionConfig, Multimodal2TextConfig]\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -172,7 +171,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -299,8 +298,8 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n+        pixel_values: torch.FloatTensor | None = None,\n+        interpolate_pos_encoding: bool | None = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         if pixel_values is None:\n@@ -369,7 +368,7 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:"
        },
        {
            "sha": "cba02153d6221641e15106abf138f3dae49f1f0c",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -109,7 +108,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -160,10 +159,10 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -215,12 +214,12 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "33b37906742299ca5286991a95c88d9f22d14348",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 47,
            "deletions": 47,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -6,7 +6,7 @@\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import ClassVar, Optional, Union\n+from typing import ClassVar\n \n import torch\n from torch import nn\n@@ -40,7 +40,7 @@ class NewTaskModelModelOutputWithPast(BaseModelOutputWithPast):\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+    image_hidden_states: torch.FloatTensor | None = None\n \n \n @dataclass\n@@ -65,12 +65,12 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n         image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n+    image_hidden_states: torch.FloatTensor | None = None\n \n \n class NewTaskModelMultiModalProjector(nn.Module):\n@@ -100,9 +100,9 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n \n \n def token_type_ids_mask_function(\n-    token_type_ids: Optional[torch.Tensor],\n-    image_group_ids: Optional[torch.Tensor],\n-) -> Optional[Callable]:\n+    token_type_ids: torch.Tensor | None,\n+    image_group_ids: torch.Tensor | None,\n+) -> Callable | None:\n     \"\"\"\n     This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n     not start and end indices.\n@@ -142,12 +142,12 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n def create_causal_mask_mapping(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     cache_position: torch.Tensor,\n-    past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor],\n-    token_type_ids: Optional[torch.Tensor] = None,\n-    pixel_values: Optional[torch.FloatTensor] = None,\n+    past_key_values: Cache | None,\n+    position_ids: torch.Tensor | None,\n+    token_type_ids: torch.Tensor | None = None,\n+    pixel_values: torch.FloatTensor | None = None,\n     is_training: bool = False,\n     **kwargs,\n ) -> dict:\n@@ -283,21 +283,21 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, NewTaskModelModelOutputWithPast]:\n+    ) -> tuple | NewTaskModelModelOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -450,19 +450,19 @@ def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         pixel_values: torch.FloatTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         num_logits_to_keep: int = 0,\n-    ) -> Union[tuple, NewTaskModelCausalLMOutputWithPast]:\n+    ) -> tuple | NewTaskModelCausalLMOutputWithPast:\n         r\"\"\"\n         Returns:\n         \"\"\"\n@@ -537,11 +537,11 @@ def prepare_inputs_for_generation(\n     def create_masks_for_generate(\n         config: PreTrainedConfig,\n         input_embeds: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor],\n+        attention_mask: torch.Tensor | None,\n         cache_position: torch.Tensor,\n-        past_key_values: Optional[Cache],\n-        position_ids: Optional[torch.Tensor],\n-        token_type_ids: Optional[torch.Tensor] = None,\n+        past_key_values: Cache | None,\n+        position_ids: torch.Tensor | None,\n+        token_type_ids: torch.Tensor | None = None,\n         **kwargs,\n     ) -> dict:\n         # Uses the overwritten `create_masks_for_generate` with `token_type_ids` masking\n@@ -558,7 +558,7 @@ def create_masks_for_generate(\n         )\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None, mean_resizing=True\n+        self, new_num_tokens: int | None = None, pad_to_multiple_of=None, mean_resizing=True\n     ) -> nn.Embedding:\n         model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n "
        },
        {
            "sha": "78c0049b478acd7eec21c32415f97a966cfa7fef",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 40,
            "deletions": 41,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_roberta.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -48,10 +47,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n@@ -94,8 +93,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -146,9 +145,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -218,9 +217,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[EncoderDecoderCache] = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: EncoderDecoderCache | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -293,11 +292,11 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n@@ -365,11 +364,11 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n@@ -418,14 +417,14 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         for i, layer_module in enumerate(self.layer):\n             hidden_states = layer_module(\n                 hidden_states,\n@@ -556,18 +555,18 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:"
        },
        {
            "sha": "623cfc5a6dc25fed86bd51d6cb30f46d13d5e4aa",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_super.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -66,9 +66,9 @@ def __init__(self, config: SuperConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[SuperConfig] = None,\n+        config: SuperConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -178,7 +178,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -229,10 +229,10 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -284,12 +284,12 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -357,16 +357,16 @@ def __init__(self, config: SuperConfig):\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+    ) -> tuple | CausalLMOutputWithPast:\n         out = super().forward(\n             input_ids,\n             attention_mask,"
        },
        {
            "sha": "195c68eb398b320b25a052e893df22f2c36bf33b",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -6,7 +6,6 @@\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # Note that llama and cohere have different definitions for rotate_half\n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -72,7 +71,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -123,10 +122,10 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "47732f1b289962111a0fd5193e654e2c1f690032",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 39,
            "deletions": 40,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -8,7 +8,6 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n@@ -107,12 +106,12 @@ class TestDetrDecoderOutput(ModelOutput):\n         used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n-    intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    intermediate_hidden_states: torch.FloatTensor | None = None\n+    intermediate_reference_points: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n+    cross_attentions: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -139,18 +138,18 @@ class TestDetrModelOutput(ModelOutput):\n         Logits of predicted bounding boxes coordinates in the first stage.\n     \"\"\"\n \n-    init_reference_points: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n-    intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    enc_outputs_class: Optional[torch.FloatTensor] = None\n-    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    init_reference_points: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    intermediate_hidden_states: torch.FloatTensor | None = None\n+    intermediate_reference_points: torch.FloatTensor | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor] | None = None\n+    cross_attentions: tuple[torch.FloatTensor] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor] | None = None\n+    enc_outputs_class: torch.FloatTensor | None = None\n+    enc_outputs_coord_logits: torch.FloatTensor | None = None\n \n \n class TestDetrFrozenBatchNorm2d(nn.Module):\n@@ -407,16 +406,16 @@ def __init__(self, config: TestDetrConfig, num_heads: int, n_points: int):\n \n         self.disable_custom_kernels = config.disable_custom_kernels\n \n-    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n+    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Tensor | None):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        position_embeddings: torch.Tensor | None = None,\n         reference_points=None,\n         spatial_shapes=None,\n         spatial_shapes_list=None,\n@@ -514,16 +513,16 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, batch_size: int):\n         return tensor.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n+    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Tensor | None):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_embeddings: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, target_len, embed_dim = hidden_states.size()\n@@ -626,7 +625,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        position_embeddings: torch.Tensor | None = None,\n         reference_points=None,\n         spatial_shapes=None,\n         spatial_shapes_list=None,\n@@ -725,14 +724,14 @@ def __init__(self, config: TestDetrConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        position_embeddings: torch.Tensor | None = None,\n         reference_points=None,\n         spatial_shapes=None,\n         spatial_shapes_list=None,\n         level_start_index=None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        output_attentions: bool | None = False,\n     ):\n         \"\"\"\n         Args:\n@@ -1375,16 +1374,16 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_outputs: Optional[torch.FloatTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.FloatTensor | None = None,\n+        encoder_outputs: torch.FloatTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.FloatTensor], TestDetrModelOutput]:\n+    ) -> tuple[torch.FloatTensor] | TestDetrModelOutput:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n             Not used by default. Can be used to mask object queries."
        },
        {
            "sha": "a0b2b4f852bebf27d0220187c50020c4a4227544",
            "filename": "examples/modular-transformers/modeling_test_suffix.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_test_suffix.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodeling_test_suffix.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_suffix.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_test_suffix.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n import torch.nn as nn\n@@ -113,7 +112,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -164,10 +163,10 @@ def __init__(self, config: TestSuffixLlamaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -219,12 +218,12 @@ def __init__(self, config: TestSuffixLlamaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states"
        },
        {
            "sha": "8a38ef13fdab582d17a168f7678217d9227ac7b2",
            "filename": "examples/modular-transformers/modular_dummy_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_dummy_bert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional, Union\n-\n import torch\n \n from transformers.models.bert.modeling_bert import BertModel\n@@ -12,19 +10,19 @@\n class DummyBertModel(BertModel):\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: list[torch.FloatTensor] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         return super().forward(input_ids, **kwargs)"
        },
        {
            "sha": "61099c2690dc7b4adc43a115d60e5d015f6d4d14",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import ClassVar, Optional\n+from typing import ClassVar\n \n import torch\n import torch.utils.checkpoint\n@@ -35,17 +35,17 @@ def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         pixel_values: torch.FloatTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         num_logits_to_keep: int = 0,\n     ):\n         r\"\"\"\n@@ -79,7 +79,7 @@ def forward(\n         return (embeddings,) + vlm_outputs\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None, mean_resizing=True\n+        self, new_num_tokens: int | None = None, pad_to_multiple_of=None, mean_resizing=True\n     ) -> nn.Embedding:\n         model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n "
        },
        {
            "sha": "a63b8ee43e284ec9f9204994c2822ff274b41525",
            "filename": "examples/modular-transformers/modular_super.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodular_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fmodular-transformers%2Fmodular_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_super.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional, Union\n-\n import torch\n \n from transformers.modeling_outputs import CausalLMOutputWithPast\n@@ -13,16 +11,16 @@ class SuperModel(LlamaModel):\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+    ) -> tuple | CausalLMOutputWithPast:\n         out = super().forward(\n             input_ids,\n             attention_mask,"
        },
        {
            "sha": "a99988d9824750e2e64343e028a03f805c1ffb02",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -33,7 +33,6 @@\n import os\n from collections.abc import Iterable\n from contextlib import nullcontext\n-from typing import Optional\n \n import torch\n import torch.distributed as dist\n@@ -586,7 +585,7 @@ def all_reduce_grads(model, world_mesh, use_ddp):\n class ContextParallelCollator:\n     \"\"\"Collator for context parallel training that splits sequences into chunks.\"\"\"\n \n-    def __init__(self, cp_mesh: Optional[DeviceMesh] = None):\n+    def __init__(self, cp_mesh: DeviceMesh | None = None):\n         self.cp_mesh = cp_mesh\n \n     def __call__(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n@@ -760,8 +759,7 @@ def get_parameters(model: nn.Module) -> Iterable[torch.Tensor]:\n             if isinstance(attr, torch.Tensor) and attr.requires_grad:\n                 yield attr\n         # Recursively get parameters from submodules\n-        for param in get_parameters(module):\n-            yield param\n+        yield from get_parameters(module)\n \n \n def update_model_parameters(model: nn.Module) -> None:"
        },
        {
            "sha": "6f0ec4865803b0f2f812ea58bb278976e54e624f",
            "filename": "examples/pytorch/audio-classification/run_audio_classification.py",
            "status": "modified",
            "additions": 9,
            "deletions": 12,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -29,7 +29,6 @@\n import sys\n from dataclasses import dataclass, field\n from random import randint\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -76,14 +75,14 @@ class DataTrainingArguments:\n     the command line.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(default=None, metadata={\"help\": \"Name of a dataset from the datasets package\"})\n-    dataset_config_name: Optional[str] = field(\n+    dataset_name: str | None = field(default=None, metadata={\"help\": \"Name of a dataset from the datasets package\"})\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(\n+    train_file: str | None = field(\n         default=None, metadata={\"help\": \"A file containing the training audio paths and labels.\"}\n     )\n-    eval_file: Optional[str] = field(\n+    eval_file: str | None = field(\n         default=None, metadata={\"help\": \"A file containing the validation audio paths and labels.\"}\n     )\n     train_split_name: str = field(\n@@ -107,7 +106,7 @@ class DataTrainingArguments:\n     label_column_name: str = field(\n         default=\"label\", metadata={\"help\": \"The name of the dataset column containing the labels. Defaults to 'label'\"}\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -116,7 +115,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -141,19 +140,17 @@ class ModelArguments:\n         default=\"facebook/wav2vec2-base\",\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from the Hub\"}\n     )\n     model_revision: str = field(\n         default=\"main\",\n         metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n     )\n-    feature_extractor_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Name or path of preprocessor config.\"}\n-    )\n+    feature_extractor_name: str | None = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n     freeze_feature_encoder: bool = field(\n         default=True, metadata={\"help\": \"Whether to freeze the feature encoder layers of the model.\"}\n     )"
        },
        {
            "sha": "1a1d73dce331a664094cc6b2ad61deb40dbd21f4",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,6 @@\n import os\n import time\n from itertools import cycle\n-from typing import Optional\n \n import datasets\n import torch\n@@ -89,8 +87,8 @@ def batch_generate(\n     generation_config: GenerationConfig,\n     tokenizer: AutoTokenizer,\n     displayed_samples: int = 0,  # -1: no display, 0: display stats, >0: display inputs and some outputs\n-    output_file: Optional[str] = None,\n-    expected_outputs: Optional[list[str]] = None,\n+    output_file: str | None = None,\n+    expected_outputs: list[str] | None = None,\n ) -> tuple[float, float]:\n     # Actual batch generation\n     if displayed_samples >= 0:"
        },
        {
            "sha": "3016902e969d877f7a20a18a9af8c7a5700ba284",
            "filename": "examples/pytorch/continuous_batching_simple.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching_simple.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "db70ce378572e086e4913f0413e27d437160a4ca",
            "filename": "examples/pytorch/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -36,7 +36,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import torch\n from datasets import load_dataset\n@@ -76,14 +75,14 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n     image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n     )\n     model_revision: str = field(\n@@ -127,29 +126,27 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n-    image_column: Optional[str] = field(\n+    data_dir: str | None = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n+    image_column: str | None = field(\n         default=\"image_path\",\n         metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n     )\n-    caption_column: Optional[str] = field(\n+    caption_column: str | None = field(\n         default=\"caption\",\n         metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n     )\n-    train_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n-    )\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n     )\n-    max_seq_length: Optional[int] = field(\n+    max_seq_length: int | None = field(\n         default=128,\n         metadata={\n             \"help\": (\n@@ -158,7 +155,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -167,7 +164,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -179,7 +176,7 @@ class DataTrainingArguments:\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )"
        },
        {
            "sha": "b05b62876a73579ab7f5e6ee3322a7b580956f45",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -28,7 +28,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import evaluate\n import numpy as np\n@@ -89,21 +88,21 @@ class DataTrainingArguments:\n     them on the command line.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Name of a dataset from the hub (could be your own, possibly private dataset hosted on the hub).\"\n         },\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n-    validation_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n-    train_val_split: Optional[float] = field(\n+    train_dir: str | None = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n+    validation_dir: str | None = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n+    train_val_split: float | None = field(\n         default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -112,7 +111,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -147,14 +146,14 @@ class ModelArguments:\n         default=\"google/vit-base-patch16-224-in21k\",\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n     )\n-    model_type: Optional[str] = field(\n+    model_type: str | None = field(\n         default=None,\n         metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n     )\n     model_revision: str = field("
        },
        {
            "sha": "4adc55063a027091836adbeda636099c5b8760ff",
            "filename": "examples/pytorch/image-pretraining/run_mae.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -25,7 +25,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import torch\n from datasets import load_dataset\n@@ -64,10 +63,10 @@ class DataTrainingArguments:\n     the command line.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=\"cifar10\", metadata={\"help\": \"Name of a dataset from the datasets package\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n     trust_remote_code: bool = field(\n@@ -80,15 +79,15 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    image_column_name: Optional[str] = field(\n+    image_column_name: str | None = field(\n         default=None, metadata={\"help\": \"The column name of the images in the files.\"}\n     )\n-    train_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n-    validation_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n-    train_val_split: Optional[float] = field(\n+    train_dir: str | None = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n+    validation_dir: str | None = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n+    train_val_split: float | None = field(\n         default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -97,7 +96,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -130,10 +129,10 @@ class ModelArguments:\n             )\n         },\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name_or_path\"}\n     )\n-    config_overrides: Optional[str] = field(\n+    config_overrides: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -142,7 +141,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n     )\n     model_revision: str = field("
        },
        {
            "sha": "47e73ee2bca5176166618b3a1cdffda7516b0482",
            "filename": "examples/pytorch/image-pretraining/run_mim.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -25,7 +25,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import numpy as np\n import torch\n@@ -71,27 +70,27 @@ class DataTrainingArguments:\n     specify them on the command line.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=\"cifar10\", metadata={\"help\": \"Name of a dataset from the datasets package\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    image_column_name: Optional[str] = field(\n+    image_column_name: str | None = field(\n         default=None,\n         metadata={\"help\": \"The column name of the images in the files. If not set, will try to use 'image' or 'img'.\"},\n     )\n-    train_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n-    validation_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n-    train_val_split: Optional[float] = field(\n+    train_dir: str | None = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n+    validation_dir: str | None = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n+    train_val_split: float | None = field(\n         default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n     )\n     mask_patch_size: int = field(default=32, metadata={\"help\": \"The size of the square patches to use for masking.\"})\n     mask_ratio: float = field(\n         default=0.6,\n         metadata={\"help\": \"Percentage of patches to mask.\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -100,7 +99,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -135,14 +134,14 @@ class ModelArguments:\n             )\n         },\n     )\n-    model_type: Optional[str] = field(\n+    model_type: str | None = field(\n         default=None,\n         metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n     )\n-    config_name_or_path: Optional[str] = field(\n+    config_name_or_path: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    config_overrides: Optional[str] = field(\n+    config_overrides: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -151,7 +150,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store (cache) the pretrained models/datasets downloaded from the hub\"},\n     )\n@@ -179,23 +178,23 @@ class ModelArguments:\n             )\n         },\n     )\n-    image_size: Optional[int] = field(\n+    image_size: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"The size (resolution) of each image. If not specified, will use `image_size` of the configuration.\"\n             )\n         },\n     )\n-    patch_size: Optional[int] = field(\n+    patch_size: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"The size (resolution) of each patch. If not specified, will use `patch_size` of the configuration.\"\n             )\n         },\n     )\n-    encoder_stride: Optional[int] = field(\n+    encoder_stride: int | None = field(\n         default=None,\n         metadata={\"help\": \"Stride to use for the encoder.\"},\n     )"
        },
        {
            "sha": "db470d4fba3b604d3767b2eefdd09a4724032e32",
            "filename": "examples/pytorch/instance-segmentation/run_instance_segmentation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -31,7 +31,7 @@\n from collections.abc import Mapping\n from dataclasses import dataclass, field\n from functools import partial\n-from typing import Any, Optional\n+from typing import Any\n \n import albumentations as A\n import numpy as np\n@@ -89,8 +89,8 @@ class Arguments:\n             )\n         },\n     )\n-    image_height: Optional[int] = field(default=512, metadata={\"help\": \"Image height after resizing.\"})\n-    image_width: Optional[int] = field(default=512, metadata={\"help\": \"Image width after resizing.\"})\n+    image_height: int | None = field(default=512, metadata={\"help\": \"Image height after resizing.\"})\n+    image_width: int | None = field(default=512, metadata={\"help\": \"Image width after resizing.\"})\n     token: str = field(\n         default=None,\n         metadata={\n@@ -327,7 +327,7 @@ def setup_logging(training_args: TrainingArguments) -> None:\n     transformers.utils.logging.enable_explicit_format()\n \n \n-def find_last_checkpoint(training_args: TrainingArguments) -> Optional[str]:\n+def find_last_checkpoint(training_args: TrainingArguments) -> str | None:\n     \"\"\"Find the last checkpoint in the output directory according to parameters specified in `training_args`.\"\"\"\n \n     checkpoint = None"
        },
        {
            "sha": "00a65dd2c16d0bd96acfc8de9998a5864f8e06d7",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 17,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -41,7 +41,6 @@\n import sys\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -85,19 +84,19 @@ class ModelArguments:\n     Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n     \"\"\"\n \n-    model_name_or_path: Optional[str] = field(\n+    model_name_or_path: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n             )\n         },\n     )\n-    model_type: Optional[str] = field(\n+    model_type: str | None = field(\n         default=None,\n         metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n     )\n-    config_overrides: Optional[str] = field(\n+    config_overrides: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -106,13 +105,13 @@ class ModelArguments:\n             )\n         },\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -143,7 +142,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    dtype: Optional[str] = field(\n+    dtype: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -167,18 +166,18 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -187,7 +186,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -197,7 +196,7 @@ class DataTrainingArguments:\n         },\n     )\n     streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n-    block_size: Optional[int] = field(\n+    block_size: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -210,13 +209,13 @@ class DataTrainingArguments:\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    validation_split_percentage: Optional[int] = field(\n+    validation_split_percentage: int | None = field(\n         default=5,\n         metadata={\n             \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n         },\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )"
        },
        {
            "sha": "f78ecdeca9c58e1c6ca0e7d299b7d2f58f98552c",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 24,
            "deletions": 25,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -42,7 +42,6 @@\n import sys\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -88,19 +87,19 @@ class ModelArguments:\n     Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n     \"\"\"\n \n-    model_name_or_path: Optional[str] = field(\n+    model_name_or_path: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n             )\n         },\n     )\n-    model_type: Optional[str] = field(\n+    model_type: str | None = field(\n         default=None,\n         metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n     )\n-    config_overrides: Optional[str] = field(\n+    config_overrides: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -109,13 +108,13 @@ class ModelArguments:\n             )\n         },\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -146,7 +145,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    dtype: Optional[str] = field(\n+    dtype: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -165,7 +164,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    attn_implementation: Optional[str] = field(\n+    attn_implementation: str | None = field(\n         default=\"sdpa\", metadata={\"help\": (\"The attention implementation to use. \")}\n     )\n \n@@ -182,18 +181,18 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -202,7 +201,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -212,7 +211,7 @@ class DataTrainingArguments:\n         },\n     )\n     streaming: bool = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n-    block_size: Optional[int] = field(\n+    block_size: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -222,7 +221,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    fim_rate: Optional[float] = field(\n+    fim_rate: float | None = field(\n         default=0.5,\n         metadata={\n             \"help\": (\n@@ -232,7 +231,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    fim_spm_rate: Optional[float] = field(\n+    fim_spm_rate: float | None = field(\n         default=0.5,\n         metadata={\n             \"help\": (\n@@ -243,7 +242,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    truncate_or_pad: Optional[bool] = field(\n+    truncate_or_pad: bool | None = field(\n         default=True,\n         metadata={\n             \"help\": (\n@@ -253,19 +252,19 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    fim_prefix_token: Optional[str] = field(\n+    fim_prefix_token: str | None = field(\n         default=\"<fim_prefix>\",\n         metadata={\"help\": (\"Fill-in-Middle Prefix token. Defaults to '<fim_prefix>'.\")},\n     )\n-    fim_middle_token: Optional[str] = field(\n+    fim_middle_token: str | None = field(\n         default=\"<fim_middle>\",\n         metadata={\"help\": (\"Fill-in-Middle Middle token. Defaults to '<fim_middle>'.\")},\n     )\n-    fim_suffix_token: Optional[str] = field(\n+    fim_suffix_token: str | None = field(\n         default=\"<fim_suffix>\",\n         metadata={\"help\": (\"Fill-in-Middle Suffix token. Defaults to '<fim_suffix>'.\")},\n     )\n-    pad_token: Optional[str] = field(\n+    pad_token: str | None = field(\n         default=\"<fim_pad>\",\n         metadata={\n             \"help\": (\n@@ -276,13 +275,13 @@ class DataTrainingArguments:\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    validation_split_percentage: Optional[int] = field(\n+    validation_split_percentage: int | None = field(\n         default=5,\n         metadata={\n             \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n         },\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )"
        },
        {
            "sha": "c96f968210b9bc3cffea310480f28c5b70147171",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 17,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -41,7 +41,6 @@\n import sys\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -82,19 +81,19 @@ class ModelArguments:\n     Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n     \"\"\"\n \n-    model_name_or_path: Optional[str] = field(\n+    model_name_or_path: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n             )\n         },\n     )\n-    model_type: Optional[str] = field(\n+    model_type: str | None = field(\n         default=None,\n         metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n     )\n-    config_overrides: Optional[str] = field(\n+    config_overrides: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -103,13 +102,13 @@ class ModelArguments:\n             )\n         },\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -140,7 +139,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    dtype: Optional[str] = field(\n+    dtype: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -164,27 +163,27 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    validation_split_percentage: Optional[int] = field(\n+    validation_split_percentage: int | None = field(\n         default=5,\n         metadata={\n             \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n         },\n     )\n-    max_seq_length: Optional[int] = field(\n+    max_seq_length: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -193,7 +192,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -213,7 +212,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -222,7 +221,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "88afac706dd122b08d8883189c481b3f4b50feb0",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -38,7 +38,6 @@\n import sys\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import Optional\n \n import datasets\n from datasets import load_dataset\n@@ -73,18 +72,18 @@ class ModelArguments:\n     Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n     \"\"\"\n \n-    model_name_or_path: Optional[str] = field(\n+    model_name_or_path: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n             )\n         },\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    config_overrides: Optional[str] = field(\n+    config_overrides: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -93,10 +92,10 @@ class ModelArguments:\n             )\n         },\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -131,10 +130,10 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n     trust_remote_code: bool = field(\n@@ -147,15 +146,15 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    validation_split_percentage: Optional[int] = field(\n+    validation_split_percentage: int | None = field(\n         default=5,\n         metadata={\n             \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n@@ -170,7 +169,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -199,7 +198,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -208,7 +207,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "b1fc16e613a78117dc462c3725793410d9d1b923",
            "filename": "examples/pytorch/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -34,7 +34,6 @@\n import sys\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import Optional\n \n import datasets\n import numpy as np\n@@ -70,13 +69,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -115,19 +114,19 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n-    max_seq_length: Optional[int] = field(\n+    max_seq_length: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -146,7 +145,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -155,7 +154,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "ae4996fd7022869c5fc01c986ec77463984b100a",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -31,7 +31,7 @@\n from collections.abc import Mapping\n from dataclasses import dataclass, field\n from functools import partial\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import albumentations as A\n import numpy as np\n@@ -158,7 +158,7 @@ def augment_and_transform_batch(\n     return result\n \n \n-def collate_fn(batch: list[BatchFeature]) -> Mapping[str, Union[torch.Tensor, list[Any]]]:\n+def collate_fn(batch: list[BatchFeature]) -> Mapping[str, torch.Tensor | list[Any]]:\n     data = {}\n     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n     data[\"labels\"] = [x[\"labels\"] for x in batch]\n@@ -172,7 +172,7 @@ def compute_metrics(\n     evaluation_results: EvalPrediction,\n     image_processor: AutoImageProcessor,\n     threshold: float = 0.0,\n-    id2label: Optional[Mapping[int, str]] = None,\n+    id2label: Mapping[int, str] | None = None,\n ) -> Mapping[str, float]:\n     \"\"\"\n     Compute mean average mAP, mAR and their variants for the object detection task.\n@@ -253,17 +253,17 @@ class DataTrainingArguments:\n             \"help\": \"Name of a dataset from the hub (could be your own, possibly private dataset hosted on the hub).\"\n         },\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_val_split: Optional[float] = field(\n+    train_val_split: float | None = field(\n         default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n     )\n-    image_square_size: Optional[int] = field(\n+    image_square_size: int | None = field(\n         default=600,\n         metadata={\"help\": \"Image longest size will be resized to this value, then image will be padded to square.\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -272,7 +272,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -281,7 +281,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    use_fast: Optional[bool] = field(\n+    use_fast: bool | None = field(\n         default=True,\n         metadata={\"help\": \"Use a fast torchvision-base image processor if it is supported for a given model.\"},\n     )\n@@ -297,10 +297,10 @@ class ModelArguments:\n         default=\"facebook/detr-resnet-50\",\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n     )\n     model_revision: str = field("
        },
        {
            "sha": "1c70ec24fee3cb8356119573faf31882f8122f02",
            "filename": "examples/pytorch/object-detection/run_object_detection_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -33,7 +33,7 @@\n from collections.abc import Mapping\n from functools import partial\n from pathlib import Path\n-from typing import Any, Union\n+from typing import Any\n \n import albumentations as A\n import datasets\n@@ -164,7 +164,7 @@ def augment_and_transform_batch(\n \n \n # Copied from examples/pytorch/object-detection/run_object_detection.collate_fn\n-def collate_fn(batch: list[BatchFeature]) -> Mapping[str, Union[torch.Tensor, list[Any]]]:\n+def collate_fn(batch: list[BatchFeature]) -> Mapping[str, torch.Tensor | list[Any]]:\n     data = {}\n     data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n     data[\"labels\"] = [x[\"labels\"] for x in batch]"
        },
        {
            "sha": "bebed230b63d1b1e0fb9a5fc3ac42ac6212ab784",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -22,7 +22,6 @@\n import sys\n import warnings\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -63,13 +62,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -104,25 +103,25 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n-    test_file: Optional[str] = field(\n+    test_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -144,7 +143,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -153,7 +152,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -162,7 +161,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "92a2b89b92732198f77cf56b94322ef24a7f1cc0",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -21,7 +21,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -62,13 +61,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -93,10 +92,10 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n     trust_remote_code: bool = field(\n@@ -109,19 +108,19 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n-    test_file: Optional[str] = field(\n+    test_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input test data file to test the perplexity on (a text file).\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -143,7 +142,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -152,7 +151,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -161,7 +160,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "cf81398d8d88b77459168c87b580897029b1745b",
            "filename": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -21,7 +21,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -61,13 +60,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -106,37 +105,37 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    context_column: Optional[str] = field(\n+    context_column: str | None = field(\n         default=\"context\",\n         metadata={\"help\": \"The name of the column in the datasets containing the contexts (for question answering).\"},\n     )\n-    question_column: Optional[str] = field(\n+    question_column: str | None = field(\n         default=\"question\",\n         metadata={\"help\": \"The name of the column in the datasets containing the questions (for question answering).\"},\n     )\n-    answer_column: Optional[str] = field(\n+    answer_column: str | None = field(\n         default=\"answers\",\n         metadata={\"help\": \"The name of the column in the datasets containing the answers (for question answering).\"},\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n     )\n-    test_file: Optional[str] = field(\n+    test_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -158,7 +157,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    val_max_answer_length: Optional[int] = field(\n+    val_max_answer_length: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -178,7 +177,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -187,7 +186,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -196,7 +195,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -226,7 +225,7 @@ class DataTrainingArguments:\n         default=20,\n         metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n     )\n-    num_beams: Optional[int] = field(\n+    num_beams: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "8de1fa00947995348d970ba5886df6e8f72c4035",
            "filename": "examples/pytorch/question-answering/trainer_seq2seq_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Ftrainer_seq2seq_qa.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -17,7 +17,6 @@\n \n import math\n import time\n-from typing import Optional\n \n from torch.utils.data import Dataset\n \n@@ -39,9 +38,9 @@ def __init__(self, *args, eval_examples=None, post_process_function=None, **kwar\n     # def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n     def evaluate(\n         self,\n-        eval_dataset: Optional[Dataset] = None,\n+        eval_dataset: Dataset | None = None,\n         eval_examples=None,\n-        ignore_keys: Optional[list[str]] = None,\n+        ignore_keys: list[str] | None = None,\n         metric_key_prefix: str = \"eval\",\n         **gen_kwargs,\n     ) -> dict[str, float]:"
        },
        {
            "sha": "6c7606894597528e53707b6cce1d6e8b6d43fda4",
            "filename": "examples/pytorch/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,6 @@\n import json\n import logging\n import os\n-from typing import Optional\n \n import numpy as np\n from tqdm.auto import tqdm\n@@ -36,9 +35,9 @@ def postprocess_qa_predictions(\n     n_best_size: int = 20,\n     max_answer_length: int = 30,\n     null_score_diff_threshold: float = 0.0,\n-    output_dir: Optional[str] = None,\n-    prefix: Optional[str] = None,\n-    log_level: Optional[int] = logging.WARNING,\n+    output_dir: str | None = None,\n+    prefix: str | None = None,\n+    log_level: int | None = logging.WARNING,\n ):\n     \"\"\"\n     Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n@@ -258,9 +257,9 @@ def postprocess_qa_predictions_with_beam_search(\n     max_answer_length: int = 30,\n     start_n_top: int = 5,\n     end_n_top: int = 5,\n-    output_dir: Optional[str] = None,\n-    prefix: Optional[str] = None,\n-    log_level: Optional[int] = logging.WARNING,\n+    output_dir: str | None = None,\n+    prefix: str | None = None,\n+    log_level: int | None = logging.WARNING,\n ):\n     \"\"\"\n     Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the"
        },
        {
            "sha": "f82cda1c96fa796cc91e490e8ec7ff526dc99118",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -31,7 +31,6 @@\n import warnings\n from dataclasses import dataclass, field\n from functools import partial\n-from typing import Optional\n \n import albumentations as A\n import evaluate\n@@ -89,19 +88,19 @@ class DataTrainingArguments:\n     them on the command line.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=\"segments/sidewalk-semantic\",\n         metadata={\n             \"help\": \"Name of a dataset from the hub (could be your own, possibly private dataset hosted on the hub).\"\n         },\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_val_split: Optional[float] = field(\n+    train_val_split: float | None = field(\n         default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -110,7 +109,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -119,11 +118,11 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    do_reduce_labels: Optional[bool] = field(\n+    do_reduce_labels: bool | None = field(\n         default=False,\n         metadata={\"help\": \"Whether or not to reduce all labels by 1 and replace background by 255.\"},\n     )\n-    reduce_labels: Optional[bool] = field(\n+    reduce_labels: bool | None = field(\n         default=False,\n         metadata={\"help\": \"Whether or not to reduce all labels by 1 and replace background by 255.\"},\n     )\n@@ -151,10 +150,10 @@ class ModelArguments:\n         default=\"nvidia/mit-b0\",\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n     )\n     model_revision: str = field("
        },
        {
            "sha": "475f0c345dacfaf2bbf6fcd5d50674f6e5a30f89",
            "filename": "examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -30,7 +30,6 @@\n import os\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Optional, Union\n \n import aiohttp\n import datasets\n@@ -333,12 +332,12 @@ class DataCollatorForWav2Vec2Pretraining:\n \n     model: Wav2Vec2ForPreTraining\n     feature_extractor: Wav2Vec2FeatureExtractor\n-    padding: Union[bool, str] = \"longest\"\n-    pad_to_multiple_of: Optional[int] = None\n-    mask_time_prob: Optional[float] = 0.65\n-    mask_time_length: Optional[int] = 10\n+    padding: bool | str = \"longest\"\n+    pad_to_multiple_of: int | None = None\n+    mask_time_prob: float | None = 0.65\n+    mask_time_length: int | None = 10\n \n-    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, list[int] | torch.Tensor]]) -> dict[str, torch.Tensor]:\n         # reformat list to dict and set to pytorch format\n         batch = self.feature_extractor.pad(\n             features,"
        },
        {
            "sha": "a9059f1febac4b8d6c6a80a06cdbc0c2750d8a08",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -35,7 +35,6 @@\n import sys\n import warnings\n from dataclasses import dataclass, field\n-from typing import Optional, Union\n \n import datasets\n import evaluate\n@@ -85,11 +84,11 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    tokenizer_name_or_path: Optional[str] = field(\n+    tokenizer_name_or_path: str | None = field(\n         default=None,\n         metadata={\"help\": \"Path to pretrained tokenizer or tokenizer identifier from huggingface.co/models\"},\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -145,18 +144,18 @@ class ModelArguments:\n         metadata={\"help\": \"Length of vector span to mask along the feature axis.\"},\n     )\n     layerdrop: float = field(default=0.0, metadata={\"help\": \"The LayerDrop probability.\"})\n-    ctc_loss_reduction: Optional[str] = field(\n+    ctc_loss_reduction: str | None = field(\n         default=\"mean\",\n         metadata={\"help\": \"The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'.\"},\n     )\n-    ctc_zero_infinity: Optional[bool] = field(\n+    ctc_zero_infinity: bool | None = field(\n         default=False,\n         metadata={\n             \"help\": \"Whether to zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite losses mainly\"\n             \" occur when the inputs are too short to be aligned to the targets.\"\n         },\n     )\n-    add_adapter: Optional[bool] = field(\n+    add_adapter: bool | None = field(\n         default=False,\n         metadata={\n             \"help\": \"Whether a convolutional attention network should be stacked on top of the Wav2Vec2Bert Encoder. Can be very\"\n@@ -211,11 +210,11 @@ class DataTrainingArguments:\n         default=False,\n         metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"},\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -224,7 +223,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -233,7 +232,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    chars_to_ignore: Optional[list[str]] = list_field(\n+    chars_to_ignore: list[str] | None = list_field(\n         default=None,\n         metadata={\"help\": \"A list of characters to remove from the transcripts.\"},\n     )\n@@ -296,7 +295,7 @@ class DataTrainingArguments:\n         default=\"|\",\n         metadata={\"help\": \"The word delimiter token for the tokenizer\"},\n     )\n-    phoneme_language: Optional[str] = field(\n+    phoneme_language: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -336,12 +335,12 @@ class DataCollatorCTCWithPadding:\n     \"\"\"\n \n     processor: AutoProcessor\n-    padding: Union[bool, str] = \"longest\"\n-    pad_to_multiple_of: Optional[int] = None\n-    pad_to_multiple_of_labels: Optional[int] = None\n-    feature_extractor_input_name: Optional[str] = \"input_values\"\n+    padding: bool | str = \"longest\"\n+    pad_to_multiple_of: int | None = None\n+    pad_to_multiple_of_labels: int | None = None\n+    feature_extractor_input_name: str | None = \"input_values\"\n \n-    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, list[int] | torch.Tensor]]) -> dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         input_features = [\n@@ -375,9 +374,9 @@ def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) ->\n \n def create_vocabulary_from_data(\n     datasets: DatasetDict,\n-    word_delimiter_token: Optional[str] = None,\n-    unk_token: Optional[str] = None,\n-    pad_token: Optional[str] = None,\n+    word_delimiter_token: str | None = None,\n+    unk_token: str | None = None,\n+    pad_token: str | None = None,\n ):\n     # Given training and test labels create vocabulary\n     def extract_all_chars(batch):"
        },
        {
            "sha": "2c6c515ece7997799dd60c4f8380c393b58a58ea",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -35,7 +35,6 @@\n import sys\n import warnings\n from dataclasses import dataclass, field\n-from typing import Optional, Union\n \n import datasets\n import evaluate\n@@ -88,11 +87,11 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    tokenizer_name_or_path: Optional[str] = field(\n+    tokenizer_name_or_path: str | None = field(\n         default=None,\n         metadata={\"help\": \"Path to pretrained tokenizer or tokenizer identifier from huggingface.co/models\"},\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -129,7 +128,7 @@ class ModelArguments:\n         metadata={\"help\": \"Length of vector span to mask along the feature axis.\"},\n     )\n     layerdrop: float = field(default=0.0, metadata={\"help\": \"The LayerDrop probability.\"})\n-    ctc_loss_reduction: Optional[str] = field(\n+    ctc_loss_reduction: str | None = field(\n         default=\"mean\",\n         metadata={\"help\": \"The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'.\"},\n     )\n@@ -198,11 +197,11 @@ class DataTrainingArguments:\n         default=False,\n         metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"},\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -211,7 +210,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -220,7 +219,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    chars_to_ignore: Optional[list[str]] = list_field(\n+    chars_to_ignore: list[str] | None = list_field(\n         default=None,\n         metadata={\"help\": \"A list of characters to remove from the transcripts.\"},\n     )\n@@ -316,11 +315,11 @@ class DataCollatorCTCWithPadding:\n     \"\"\"\n \n     processor: AutoProcessor\n-    padding: Union[bool, str] = \"longest\"\n-    pad_to_multiple_of: Optional[int] = None\n-    pad_to_multiple_of_labels: Optional[int] = None\n+    padding: bool | str = \"longest\"\n+    pad_to_multiple_of: int | None = None\n+    pad_to_multiple_of_labels: int | None = None\n \n-    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, list[int] | torch.Tensor]]) -> dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n@@ -352,9 +351,9 @@ def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) ->\n \n def create_vocabulary_from_data(\n     datasets: DatasetDict,\n-    word_delimiter_token: Optional[str] = None,\n-    unk_token: Optional[str] = None,\n-    pad_token: Optional[str] = None,\n+    word_delimiter_token: str | None = None,\n+    unk_token: str | None = None,\n+    pad_token: str | None = None,\n ):\n     # Given training and test labels create vocabulary\n     def extract_all_chars(batch):"
        },
        {
            "sha": "e6ad45089a368512341bdef9d6e55df81e0c40dc",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -35,7 +35,7 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import datasets\n import evaluate\n@@ -77,19 +77,19 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None,\n         metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"},\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None,\n         metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"},\n     )\n-    feature_extractor_name: Optional[str] = field(\n+    feature_extractor_name: str | None = field(\n         default=None,\n         metadata={\"help\": \"feature extractor name or path if not the same as model_name\"},\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -168,11 +168,11 @@ class DataTrainingArguments:\n         default=False,\n         metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -181,7 +181,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -270,7 +270,7 @@ class DataCollatorSpeechSeq2SeqWithPadding:\n     decoder_start_token_id: int\n     forward_attention_mask: bool\n \n-    def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n+    def __call__(self, features: list[dict[str, list[int] | torch.Tensor]]) -> dict[str, torch.Tensor]:\n         # split inputs and labels since they have to be of different lengths and need\n         # different padding methods\n         model_input_name = self.processor.model_input_names[0]"
        },
        {
            "sha": "b0044fbbf33731d21218526ec6f1f6f69c424f3b",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -37,7 +37,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -96,13 +95,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -133,7 +132,7 @@ class ModelArguments:\n             )\n         },\n     )\n-    resize_position_embeddings: Optional[bool] = field(\n+    resize_position_embeddings: bool | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -150,34 +149,34 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    lang: Optional[str] = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n+    lang: str | None = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    text_column: Optional[str] = field(\n+    text_column: str | None = field(\n         default=None,\n         metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n     )\n-    summary_column: Optional[str] = field(\n+    summary_column: str | None = field(\n         default=None,\n         metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n     )\n-    train_file: Optional[str] = field(\n+    train_file: str | None = field(\n         default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n     )\n-    validation_file: Optional[str] = field(\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n             )\n         },\n     )\n-    test_file: Optional[str] = field(\n+    test_file: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n@@ -186,11 +185,11 @@ class DataTrainingArguments:\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n-    max_source_length: Optional[int] = field(\n+    max_source_length: int | None = field(\n         default=1024,\n         metadata={\n             \"help\": (\n@@ -199,7 +198,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_target_length: Optional[int] = field(\n+    max_target_length: int | None = field(\n         default=128,\n         metadata={\n             \"help\": (\n@@ -208,7 +207,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    val_max_target_length: Optional[int] = field(\n+    val_max_target_length: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -229,7 +228,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -238,7 +237,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -247,7 +246,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -256,7 +255,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    num_beams: Optional[int] = field(\n+    num_beams: int | None = field(\n         default=1,\n         metadata={\n             \"help\": (\n@@ -271,11 +270,11 @@ class DataTrainingArguments:\n             \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n         },\n     )\n-    source_prefix: Optional[str] = field(\n+    source_prefix: str | None = field(\n         default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n     )\n \n-    forced_bos_token: Optional[str] = field(\n+    forced_bos_token: str | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "457ccc9001bfbb79fab7fddefbcff7a331aab106",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -35,7 +35,6 @@\n import random\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -78,10 +77,10 @@ class DataTrainingArguments:\n     the command line.\n     \"\"\"\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n     do_regression: bool = field(\n@@ -90,7 +89,7 @@ class DataTrainingArguments:\n             \"help\": \"Whether to do regression instead of classification. If None, will be inferred from the dataset.\"\n         },\n     )\n-    text_column_names: Optional[str] = field(\n+    text_column_names: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -99,36 +98,36 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    text_column_delimiter: Optional[str] = field(\n+    text_column_delimiter: str | None = field(\n         default=\" \", metadata={\"help\": \"The delimiter to use to join text columns into a single sentence.\"}\n     )\n-    train_split_name: Optional[str] = field(\n+    train_split_name: str | None = field(\n         default=None,\n         metadata={\n             \"help\": 'The name of the train split in the input dataset. If not specified, will use the \"train\" split when do_train is enabled'\n         },\n     )\n-    validation_split_name: Optional[str] = field(\n+    validation_split_name: str | None = field(\n         default=None,\n         metadata={\n             \"help\": 'The name of the validation split in the input dataset. If not specified, will use the \"validation\" split when do_eval is enabled'\n         },\n     )\n-    test_split_name: Optional[str] = field(\n+    test_split_name: str | None = field(\n         default=None,\n         metadata={\n             \"help\": 'The name of the test split in the input dataset. If not specified, will use the \"test\" split when do_predict is enabled'\n         },\n     )\n-    remove_splits: Optional[str] = field(\n+    remove_splits: str | None = field(\n         default=None,\n         metadata={\"help\": \"The splits to remove from the dataset. Multiple splits should be separated by commas.\"},\n     )\n-    remove_columns: Optional[str] = field(\n+    remove_columns: str | None = field(\n         default=None,\n         metadata={\"help\": \"The columns to remove from the dataset. Multiple columns should be separated by commas.\"},\n     )\n-    label_column_name: Optional[str] = field(\n+    label_column_name: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -146,7 +145,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -168,7 +167,7 @@ class DataTrainingArguments:\n     shuffle_seed: int = field(\n         default=42, metadata={\"help\": \"Random seed that will be used to shuffle the train dataset.\"}\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -177,7 +176,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -186,7 +185,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -195,14 +194,14 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    metric_name: Optional[str] = field(default=None, metadata={\"help\": \"The metric to use for evaluation.\"})\n-    train_file: Optional[str] = field(\n+    metric_name: str | None = field(default=None, metadata={\"help\": \"The metric to use for evaluation.\"})\n+    train_file: str | None = field(\n         default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n     )\n-    validation_file: Optional[str] = field(\n+    validation_file: str | None = field(\n         default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n     )\n-    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n+    test_file: str | None = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n \n     def __post_init__(self):\n         if self.dataset_name is None:\n@@ -226,13 +225,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )"
        },
        {
            "sha": "77e4193e7a3c6d5499d85491126047d62462e98f",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -36,7 +36,6 @@\n import sys\n from collections import Counter\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -91,14 +90,14 @@ class DataTrainingArguments:\n     the command line.\n     \"\"\"\n \n-    task_name: Optional[str] = field(\n+    task_name: str | None = field(\n         default=None,\n         metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n     )\n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n     max_seq_length: int = field(\n@@ -122,7 +121,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -131,7 +130,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -140,7 +139,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -149,13 +148,13 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    train_file: Optional[str] = field(\n+    train_file: str | None = field(\n         default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n     )\n-    validation_file: Optional[str] = field(\n+    validation_file: str | None = field(\n         default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n     )\n-    test_file: Optional[str] = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n+    test_file: str | None = field(default=None, metadata={\"help\": \"A csv or a json file containing the test data.\"})\n \n     def __post_init__(self):\n         if self.task_name is not None:\n@@ -184,13 +183,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )"
        },
        {
            "sha": "3015b3537428f0523534a7f24347975918b7b90d",
            "filename": "examples/pytorch/text-classification/run_xnli.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -36,7 +36,6 @@\n import random\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -78,7 +77,7 @@ class DataTrainingArguments:\n     the command line.\n     \"\"\"\n \n-    max_seq_length: Optional[int] = field(\n+    max_seq_length: int | None = field(\n         default=128,\n         metadata={\n             \"help\": (\n@@ -99,7 +98,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -108,7 +107,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -117,7 +116,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -140,20 +139,20 @@ class ModelArguments:\n     language: str = field(\n         default=None, metadata={\"help\": \"Evaluation language. Also train language if `train_language` is set to None.\"}\n     )\n-    train_language: Optional[str] = field(\n+    train_language: str | None = field(\n         default=None, metadata={\"help\": \"Train language if it is different from the evaluation language.\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n-    do_lower_case: Optional[bool] = field(\n+    do_lower_case: bool | None = field(\n         default=False,\n         metadata={\"help\": \"arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()\"},\n     )"
        },
        {
            "sha": "23ad008e09f485a4e588496b18737aa3bb62622f",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 15,
            "deletions": 16,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -34,7 +34,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -74,13 +73,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -119,34 +118,34 @@ class DataTrainingArguments:\n     Arguments pertaining to what data we are going to input our model for training and eval.\n     \"\"\"\n \n-    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n-    dataset_name: Optional[str] = field(\n+    task_name: str | None = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(\n+    train_file: str | None = field(\n         default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n     )\n-    validation_file: Optional[str] = field(\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n     )\n-    test_file: Optional[str] = field(\n+    test_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n     )\n-    text_column_name: Optional[str] = field(\n+    text_column_name: str | None = field(\n         default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n     )\n-    label_column_name: Optional[str] = field(\n+    label_column_name: str | None = field(\n         default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n@@ -169,7 +168,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -178,7 +177,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -187,7 +186,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "a705bc94a7f31ea2994a5d28edc987e558076255",
            "filename": "examples/pytorch/translation/run_translation.py",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -36,7 +36,6 @@\n import os\n import sys\n from dataclasses import dataclass, field\n-from typing import Optional\n \n import datasets\n import evaluate\n@@ -85,13 +84,13 @@ class ModelArguments:\n     model_name_or_path: str = field(\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n-    config_name: Optional[str] = field(\n+    config_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n     )\n-    tokenizer_name: Optional[str] = field(\n+    tokenizer_name: str | None = field(\n         default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n     )\n-    cache_dir: Optional[str] = field(\n+    cache_dir: str | None = field(\n         default=None,\n         metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n     )\n@@ -133,31 +132,31 @@ class DataTrainingArguments:\n     source_lang: str = field(default=None, metadata={\"help\": \"Source language id for translation.\"})\n     target_lang: str = field(default=None, metadata={\"help\": \"Target language id for translation.\"})\n \n-    dataset_name: Optional[str] = field(\n+    dataset_name: str | None = field(\n         default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n+    dataset_config_name: str | None = field(\n         default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n     )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a jsonlines).\"})\n-    validation_file: Optional[str] = field(\n+    train_file: str | None = field(default=None, metadata={\"help\": \"The input training data file (a jsonlines).\"})\n+    validation_file: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"An optional input evaluation data file to evaluate the metrics (sacrebleu) on a jsonlines file.\"\n         },\n     )\n-    test_file: Optional[str] = field(\n+    test_file: str | None = field(\n         default=None,\n         metadata={\"help\": \"An optional input test data file to evaluate the metrics (sacrebleu) on a jsonlines file.\"},\n     )\n     overwrite_cache: bool = field(\n         default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n     )\n-    preprocessing_num_workers: Optional[int] = field(\n+    preprocessing_num_workers: int | None = field(\n         default=None,\n         metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n     )\n-    max_source_length: Optional[int] = field(\n+    max_source_length: int | None = field(\n         default=1024,\n         metadata={\n             \"help\": (\n@@ -166,7 +165,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_target_length: Optional[int] = field(\n+    max_target_length: int | None = field(\n         default=128,\n         metadata={\n             \"help\": (\n@@ -175,7 +174,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    val_max_target_length: Optional[int] = field(\n+    val_max_target_length: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -196,7 +195,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_train_samples: Optional[int] = field(\n+    max_train_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -205,7 +204,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_eval_samples: Optional[int] = field(\n+    max_eval_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -214,7 +213,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    max_predict_samples: Optional[int] = field(\n+    max_predict_samples: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -223,7 +222,7 @@ class DataTrainingArguments:\n             )\n         },\n     )\n-    num_beams: Optional[int] = field(\n+    num_beams: int | None = field(\n         default=1,\n         metadata={\n             \"help\": (\n@@ -238,10 +237,10 @@ class DataTrainingArguments:\n             \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n         },\n     )\n-    source_prefix: Optional[str] = field(\n+    source_prefix: str | None = field(\n         default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n     )\n-    forced_bos_token: Optional[str] = field(\n+    forced_bos_token: str | None = field(\n         default=None,\n         metadata={\n             \"help\": ("
        },
        {
            "sha": "02244fde49471812c3880fdea595c34d35a7e862",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,5 +1,5 @@\n import json\n-from typing import Any, Optional\n+from typing import Any\n \n import torch\n import torch.nn as nn\n@@ -112,7 +112,7 @@ class Int8SymmetricConfig(QuantizationConfigMixin):\n     Configuration for INT8 symmetric quantization.\n     \"\"\"\n \n-    def __init__(self, modules_to_not_convert: Optional[list[str]] = None, **kwargs):\n+    def __init__(self, modules_to_not_convert: list[str] | None = None, **kwargs):\n         self.quant_method = \"int8_symmetric\"\n         self.modules_to_not_convert = modules_to_not_convert\n "
        },
        {
            "sha": "e872759540abc7f73d36708695f64f54b2c5c7bf",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 36,
            "deletions": 20,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -18,29 +18,48 @@ target-version = \"py310\"\n line-length = 119\n \n [tool.ruff.lint]\n-# Never enforce `E501` (line length violations).\n-# SIM300: Yoda condition detected\n-# SIM212: Checks for if expressions that check against a negated condition.\n-# SIM905: Consider using a list literal instead of `str.split`\n-# UP009: UTF-8 encoding declaration is unnecessary\n-# UP015: Unnecessary mode argument\n-# UP031: Use format specifiers instead of percent format\n-# UP004: Class `XXX` inherits from `object`\n-# UP028: Checks for for loops that can be replaced with yield from expressions\n-# UP045: Use `X | None` for type annotations\n-# UP007: Use `X | Y` for type annotations\n-ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\", \"SIM1\", \"SIM300\", \"SIM212\", \"SIM905\", \"UP009\", \"UP015\", \"UP031\", \"UP028\", \"UP004\", \"UP045\", \"UP007\"]\n-# RUF013: Checks for the use of implicit Optional\n-#  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"PERF102\", \"PLC1802\", \"PLC0208\", \"SIM\", \"UP\", \"PIE794\", \"FURB\"]\n-extend-safe-fixes = [\"UP006\"]\n+select = [\n+    \"E\",  # pycodestyle errors\n+    \"F\",  # Pyflakes\n+    \"I\",  # isort\n+    \"W\",  # pycodestyle warnings\n+    \"UP\",  # pyupgrade\n+    \"FURB\",  # refurb\n+    \"SIM\",  # flake8-simplify\n+    \"C4\",  # flake8-comprehensions\n+    \"RUF013\",  # Checks for the use of implicit Optional in type annotations when the default parameter value is None.\n+    \"PERF102\",  # Checks for uses of dict.items() that discard either the key or the value when iterating over the dictionary.\n+    \"PLC1802\",  # Checks for len calls on sequences in a boolean test context.\n+    \"PLC0208\",  # Checks for iteration over a set literal where each element in the set is itself a literal value.\n+    \"PIE794\",  # Checks for duplicate field definitions in classes.\n+]\n+ignore = [\n+    \"E501\",  # Checks for lines that exceed the specified maximum character length.\n+    \"E741\",  # Checks for the use of the characters 'l', 'O', or 'I' as variable names.\n+    \"SIM1\",  # All SIM1XX rules\n+    \"SIM905\",  # Checks for static str.split calls that can be replaced with list literals.\n+    \"UP015\",  # Checks for redundant open mode arguments.\n+    \"UP031\",  # Checks for printf-style string formatting, and offers to replace it with str.format calls.\n+]\n+extend-safe-fixes = [\n+    \"UP006\",  # Checks for the use of generics that can be replaced with standard library variants based on PEP 585.\n+]\n \n # Ignore import violations in all `__init__.py` files.\n [tool.ruff.lint.per-file-ignores]\n \"__init__.py\" = [\"E402\", \"F401\", \"F403\", \"F811\"]\n \"src/transformers/file_utils.py\" = [\"F401\"]\n \"src/transformers/utils/dummy_*.py\" = [\"F401\"]\n-\"examples/legacy/**/*.py\" = [\"UP\"]\n+# type validation does not work with `x | y` syntax, so we have to exclude all these files....\n+# TODO: really needs to be upstreamed in huggingface_hub\n+\"src/transformers/processing_utils.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/image_processing_utils_fast.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/video_processing_utils.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/image_utils.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/video_utils.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/models/**/image_processing_*.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/models/**/processing_*.py\" = [\"UP045\", \"UP007\"]\n+\"src/transformers/models/**/video_processing_*.py\" = [\"UP045\", \"UP007\"]\n \n [tool.ruff.lint.isort]\n lines-after-imports = 2\n@@ -49,13 +68,10 @@ known-first-party = [\"transformers\"]\n [tool.ruff.format]\n # Like Black, use double quotes for strings.\n quote-style = \"double\"\n-\n # Like Black, indent with spaces, rather than tabs.\n indent-style = \"space\"\n-\n # Like Black, respect magic trailing commas.\n skip-magic-trailing-comma = false\n-\n # Like Black, automatically detect the appropriate line ending.\n line-ending = \"auto\"\n "
        },
        {
            "sha": "756cbd1637b9cea7efca191fad46b36e94fb6d00",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -122,7 +122,7 @@\n     \"rhoknp>=1.1.0,<1.3.1\",\n     \"rjieba\",\n     \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n-    \"ruff==0.13.1\",\n+    \"ruff==0.14.10\",\n     # `sacrebleu` not used in `transformers`. However, it is needed in several tests, when a test calls\n     # `evaluate.load(\"sacrebleu\")`. This metric is used in the examples that we use to test the `Trainer` with, in the\n     # `Trainer` tests (see references to `run_translation.py`)."
        },
        {
            "sha": "5a1d02800558d97b5a946db89c67cd380969bb03",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -23,7 +23,7 @@\n import warnings\n from collections.abc import Sequence\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Union\n \n import httpx\n import numpy as np\n@@ -57,7 +57,7 @@\n AudioInput = Union[np.ndarray, \"torch.Tensor\", Sequence[np.ndarray], Sequence[\"torch.Tensor\"]]\n \n \n-def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:\n+def load_audio(audio: str | np.ndarray, sampling_rate=16000, timeout=None) -> np.ndarray:\n     \"\"\"\n     Loads `audio` to an np.ndarray object.\n \n@@ -77,7 +77,7 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n         # Try to load with `torchcodec` but do not enforce users to install it. If not found\n         # fallback to `librosa`. If using an audio-only model, most probably `torchcodec` won't be\n         # needed. Do not raise any errors if not installed or versions do not match\n-        if is_torchcodec_available() and TORCHCODEC_VERSION >= version.parse(\"0.3.0\"):\n+        if is_torchcodec_available() and version.parse(\"0.3.0\") <= TORCHCODEC_VERSION:\n             audio = load_audio_torchcodec(audio, sampling_rate=sampling_rate)\n         else:\n             audio = load_audio_librosa(audio, sampling_rate=sampling_rate, timeout=timeout)\n@@ -88,7 +88,7 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n     return audio\n \n \n-def load_audio_torchcodec(audio: Union[str, np.ndarray], sampling_rate=16000) -> np.ndarray:\n+def load_audio_torchcodec(audio: str | np.ndarray, sampling_rate=16000) -> np.ndarray:\n     \"\"\"\n     Loads `audio` to an np.ndarray object using `torchcodec`.\n \n@@ -112,7 +112,7 @@ def load_audio_torchcodec(audio: Union[str, np.ndarray], sampling_rate=16000) ->\n     return audio\n \n \n-def load_audio_librosa(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:\n+def load_audio_librosa(audio: str | np.ndarray, sampling_rate=16000, timeout=None) -> np.ndarray:\n     \"\"\"\n     Loads `audio` to an np.ndarray object using `librosa`.\n \n@@ -143,10 +143,10 @@ def load_audio_librosa(audio: Union[str, np.ndarray], sampling_rate=16000, timeo\n def load_audio_as(\n     audio: str,\n     return_format: str,\n-    timeout: Optional[int] = None,\n+    timeout: int | None = None,\n     force_mono: bool = False,\n-    sampling_rate: Optional[int] = None,\n-) -> Union[str, dict[str, Any], io.BytesIO, None]:\n+    sampling_rate: int | None = None,\n+) -> str | dict[str, Any] | io.BytesIO | None:\n     \"\"\"\n     Load audio from either a local file path or URL and return in specified format.\n \n@@ -239,7 +239,7 @@ def is_valid_list_of_audio(audio):\n \n \n def make_list_of_audio(\n-    audio: Union[list[AudioInput], AudioInput],\n+    audio: list[AudioInput] | AudioInput,\n ) -> AudioInput:\n     \"\"\"\n     Ensure that the output is a list of audio.\n@@ -260,7 +260,7 @@ def make_list_of_audio(\n     raise ValueError(\"Invalid input type. Must be a single audio or a list of audio\")\n \n \n-def hertz_to_mel(freq: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n+def hertz_to_mel(freq: float | np.ndarray, mel_scale: str = \"htk\") -> float | np.ndarray:\n     \"\"\"\n     Convert frequency from hertz to mels.\n \n@@ -296,7 +296,7 @@ def hertz_to_mel(freq: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Unio\n     return mels\n \n \n-def mel_to_hertz(mels: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Union[float, np.ndarray]:\n+def mel_to_hertz(mels: float | np.ndarray, mel_scale: str = \"htk\") -> float | np.ndarray:\n     \"\"\"\n     Convert frequency from mels to hertz.\n \n@@ -332,7 +332,7 @@ def mel_to_hertz(mels: Union[float, np.ndarray], mel_scale: str = \"htk\") -> Unio\n     return freq\n \n \n-def hertz_to_octave(freq: Union[float, np.ndarray], tuning: float = 0.0, bins_per_octave: int = 12):\n+def hertz_to_octave(freq: float | np.ndarray, tuning: float = 0.0, bins_per_octave: int = 12):\n     \"\"\"\n     Convert frequency from hertz to fractional octave numbers.\n     Adapted from *librosa*.\n@@ -380,8 +380,8 @@ def chroma_filter_bank(\n     num_chroma: int,\n     sampling_rate: int,\n     tuning: float = 0.0,\n-    power: Optional[float] = 2.0,\n-    weighting_parameters: Optional[tuple[float, float]] = (5.0, 2.0),\n+    power: float | None = 2.0,\n+    weighting_parameters: tuple[float, float] | None = (5.0, 2.0),\n     start_at_c_chroma: bool = True,\n ):\n     \"\"\"\n@@ -456,7 +456,7 @@ def mel_filter_bank(\n     min_frequency: float,\n     max_frequency: float,\n     sampling_rate: int,\n-    norm: Optional[str] = None,\n+    norm: str | None = None,\n     mel_scale: str = \"htk\",\n     triangularize_in_mel_space: bool = False,\n ) -> np.ndarray:\n@@ -561,7 +561,7 @@ def window_function(\n     window_length: int,\n     name: str = \"hann\",\n     periodic: bool = True,\n-    frame_length: Optional[int] = None,\n+    frame_length: int | None = None,\n     center: bool = True,\n ) -> np.ndarray:\n     \"\"\"\n@@ -626,19 +626,19 @@ def spectrogram(\n     window: np.ndarray,\n     frame_length: int,\n     hop_length: int,\n-    fft_length: Optional[int] = None,\n-    power: Optional[float] = 1.0,\n+    fft_length: int | None = None,\n+    power: float | None = 1.0,\n     center: bool = True,\n     pad_mode: str = \"reflect\",\n     onesided: bool = True,\n     dither: float = 0.0,\n-    preemphasis: Optional[float] = None,\n-    mel_filters: Optional[np.ndarray] = None,\n+    preemphasis: float | None = None,\n+    mel_filters: np.ndarray | None = None,\n     mel_floor: float = 1e-10,\n-    log_mel: Optional[str] = None,\n+    log_mel: str | None = None,\n     reference: float = 1.0,\n     min_value: float = 1e-10,\n-    db_range: Optional[float] = None,\n+    db_range: float | None = None,\n     remove_dc_offset: bool = False,\n     dtype: np.dtype = np.float32,\n ) -> np.ndarray:\n@@ -837,19 +837,19 @@ def spectrogram_batch(\n     window: np.ndarray,\n     frame_length: int,\n     hop_length: int,\n-    fft_length: Optional[int] = None,\n-    power: Optional[float] = 1.0,\n+    fft_length: int | None = None,\n+    power: float | None = 1.0,\n     center: bool = True,\n     pad_mode: str = \"reflect\",\n     onesided: bool = True,\n     dither: float = 0.0,\n-    preemphasis: Optional[float] = None,\n-    mel_filters: Optional[np.ndarray] = None,\n+    preemphasis: float | None = None,\n+    mel_filters: np.ndarray | None = None,\n     mel_floor: float = 1e-10,\n-    log_mel: Optional[str] = None,\n+    log_mel: str | None = None,\n     reference: float = 1.0,\n     min_value: float = 1e-10,\n-    db_range: Optional[float] = None,\n+    db_range: float | None = None,\n     remove_dc_offset: bool = False,\n     dtype: np.dtype = np.float32,\n ) -> list[np.ndarray]:\n@@ -1047,7 +1047,7 @@ def power_to_db(\n     spectrogram: np.ndarray,\n     reference: float = 1.0,\n     min_value: float = 1e-10,\n-    db_range: Optional[float] = None,\n+    db_range: float | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Converts a power spectrogram to the decibel scale. This computes `10 * log10(spectrogram / reference)`, using basic\n@@ -1098,7 +1098,7 @@ def power_to_db_batch(\n     spectrogram: np.ndarray,\n     reference: float = 1.0,\n     min_value: float = 1e-10,\n-    db_range: Optional[float] = None,\n+    db_range: float | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Converts a batch of power spectrograms to the decibel scale. This computes `10 * log10(spectrogram / reference)`,\n@@ -1147,7 +1147,7 @@ def amplitude_to_db(\n     spectrogram: np.ndarray,\n     reference: float = 1.0,\n     min_value: float = 1e-5,\n-    db_range: Optional[float] = None,\n+    db_range: float | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Converts an amplitude spectrogram to the decibel scale. This computes `20 * log10(spectrogram / reference)`, using\n@@ -1193,7 +1193,7 @@ def amplitude_to_db(\n \n \n def amplitude_to_db_batch(\n-    spectrogram: np.ndarray, reference: float = 1.0, min_value: float = 1e-5, db_range: Optional[float] = None\n+    spectrogram: np.ndarray, reference: float = 1.0, min_value: float = 1e-5, db_range: float | None = None\n ) -> np.ndarray:\n     \"\"\"\n     Converts a batch of amplitude spectrograms to the decibel scale. This computes `20 * log10(spectrogram / reference)`,"
        },
        {
            "sha": "351d6c772b8d06da340e5f8d957c8631efd8c151",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,6 +1,6 @@\n from abc import ABC, abstractmethod\n from collections.abc import Iterable\n-from typing import Any, Optional\n+from typing import Any\n \n import torch\n \n@@ -29,8 +29,8 @@ class CacheLayerMixin(ABC):\n     is_compileable = False\n \n     def __init__(self):\n-        self.keys: Optional[torch.Tensor] = None\n-        self.values: Optional[torch.Tensor] = None\n+        self.keys: torch.Tensor | None = None\n+        self.values: torch.Tensor | None = None\n         self.is_initialized = False\n \n     def __repr__(self):\n@@ -41,7 +41,7 @@ def lazy_initialization(self, key_states: torch.Tensor, value_states: torch.Tens\n \n     @abstractmethod\n     def update(\n-        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = None\n+        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: dict[str, Any] | None = None\n     ) -> tuple[torch.Tensor, torch.Tensor]: ...\n \n     @abstractmethod\n@@ -99,7 +99,7 @@ def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Update the key and value caches in-place, and return the necessary keys and value states.\n@@ -186,7 +186,7 @@ def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Update the key and value caches in-place, and return the necessary keys and value states.\n@@ -310,7 +310,7 @@ def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Update the key and value caches in-place, and return the necessary keys and value states.\n@@ -385,7 +385,7 @@ def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Update the key and value caches in-place, and return the necessary keys and value states.\n@@ -518,7 +518,7 @@ def update(\n         self,\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Update the key and value caches in-place, and return the necessary keys and value states.\n@@ -692,8 +692,8 @@ class Cache:\n \n     def __init__(\n         self,\n-        layers: Optional[list[CacheLayerMixin]] = None,\n-        layer_class_to_replicate: Optional[type[CacheLayerMixin]] = None,\n+        layers: list[CacheLayerMixin] | None = None,\n+        layer_class_to_replicate: type[CacheLayerMixin] | None = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = True,\n     ):\n@@ -751,7 +751,7 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n@@ -939,8 +939,8 @@ class DynamicCache(Cache):\n \n     def __init__(\n         self,\n-        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], ...]]] = None,\n-        config: Optional[PreTrainedConfig] = None,\n+        ddp_cache_data: Iterable[tuple[torch.Tensor | None, ...]] | None = None,\n+        config: PreTrainedConfig | None = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = False,\n     ):"
        },
        {
            "sha": "4b13a5afab5130756a4b4868f17e33884c554136",
            "filename": "src/transformers/cli/chat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcli%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcli%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fchat.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,7 @@\n import string\n import time\n from collections.abc import AsyncIterator\n-from typing import Annotated, Any, Optional\n+from typing import Annotated, Any\n from urllib.parse import urljoin, urlparse\n \n import httpx\n@@ -206,7 +206,7 @@ def __init__(\n         self,\n         model_id: Annotated[str, typer.Argument(help=\"ID of the model to use (e.g. 'HuggingFaceTB/SmolLM3-3B').\")],\n         base_url: Annotated[\n-            Optional[str], typer.Argument(help=\"Base url to connect to (e.g. http://localhost:8000/v1).\")\n+            str | None, typer.Argument(help=\"Base url to connect to (e.g. http://localhost:8000/v1).\")\n         ] = f\"http://{DEFAULT_HTTP_ENDPOINT['hostname']}:{DEFAULT_HTTP_ENDPOINT['port']}\",\n         generate_flags: Annotated[\n             list[str] | None,\n@@ -374,7 +374,7 @@ async def _inner_run(self):\n         config = self.config\n \n         async with AsyncInferenceClient(base_url=self.base_url) as client:\n-            pending_user_input: Optional[str] = None\n+            pending_user_input: str | None = None\n             while True:\n                 try:\n                     if pending_user_input is not None:"
        },
        {
            "sha": "981cf6d04ea16ff94cc9caaeb9596431c0f8ba91",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -387,7 +387,7 @@ def __init__(\n             ),\n         ] = None,\n         quantization: Annotated[\n-            Optional[str],\n+            str | None,\n             typer.Option(help=\"Which quantization method to use. choices: 'bnb-4bit', 'bnb-8bit'\"),\n         ] = None,\n         host: Annotated[str, typer.Option(help=\"Interface the server will listen to.\")] = \"localhost\",\n@@ -1728,7 +1728,7 @@ def is_continuation(self, req: dict) -> bool:\n         self.last_messages = messages\n         return req_continues_last_messages\n \n-    def get_quantization_config(self) -> Optional[BitsAndBytesConfig]:\n+    def get_quantization_config(self) -> BitsAndBytesConfig | None:\n         \"\"\"\n         Returns the quantization config for the given CLI arguments.\n "
        },
        {
            "sha": "4212fe5cf80d8c557d0a2ae4f4f370d0ef6e5c8e",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,7 @@\n import math\n import os\n import warnings\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n+from typing import TYPE_CHECKING, Any, TypeVar, Union\n \n from huggingface_hub import create_repo\n from packaging import version\n@@ -184,10 +184,10 @@ class PreTrainedConfig(PushToHubMixin, RotaryEmbeddingConfigMixin):\n     sub_configs: dict[str, type[\"PreTrainedConfig\"]] = {}\n     has_no_defaults_at_init: bool = False\n     attribute_map: dict[str, str] = {}\n-    base_model_tp_plan: Optional[dict[str, Any]] = None\n-    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n-    base_model_ep_plan: Optional[dict[str, tuple[list[str]]]] = None\n-    _auto_class: Optional[str] = None\n+    base_model_tp_plan: dict[str, Any] | None = None\n+    base_model_pp_plan: dict[str, tuple[list[str]]] | None = None\n+    base_model_ep_plan: dict[str, tuple[list[str]]] | None = None\n+    _auto_class: str | None = None\n \n     def __setattr__(self, key, value):\n         if key in super().__getattribute__(\"attribute_map\"):\n@@ -206,30 +206,30 @@ def __init__(\n         output_hidden_states: bool = False,\n         output_attentions: bool = False,\n         return_dict: bool = True,\n-        dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n+        dtype: Union[str, \"torch.dtype\"] | None = None,\n         # Common arguments\n         tie_word_embeddings: bool = True,\n         chunk_size_feed_forward: int = 0,\n         is_encoder_decoder: bool = False,\n         is_decoder: bool = False,\n-        cross_attention_hidden_size: Optional[int] = None,\n+        cross_attention_hidden_size: int | None = None,\n         add_cross_attention: bool = False,\n         # Fine-tuning task arguments\n-        architectures: Optional[list[str]] = None,\n-        finetuning_task: Optional[str] = None,\n-        id2label: Optional[dict[int, str]] = None,\n-        label2id: Optional[dict[str, int]] = None,\n-        num_labels: Optional[int] = None,\n-        task_specific_params: Optional[dict[str, Any]] = None,\n-        problem_type: Optional[str] = None,\n+        architectures: list[str] | None = None,\n+        finetuning_task: str | None = None,\n+        id2label: dict[int, str] | None = None,\n+        label2id: dict[str, int] | None = None,\n+        num_labels: int | None = None,\n+        task_specific_params: dict[str, Any] | None = None,\n+        problem_type: str | None = None,\n         # Tokenizer kwargs\n-        tokenizer_class: Optional[str] = None,\n-        prefix: Optional[str] = None,\n-        bos_token_id: Optional[int] = None,\n-        pad_token_id: Optional[int] = None,\n-        eos_token_id: Optional[int] = None,\n-        sep_token_id: Optional[int] = None,\n-        decoder_start_token_id: Optional[int] = None,\n+        tokenizer_class: str | None = None,\n+        prefix: str | None = None,\n+        bos_token_id: int | None = None,\n+        pad_token_id: int | None = None,\n+        eos_token_id: int | None = None,\n+        sep_token_id: int | None = None,\n+        decoder_start_token_id: int | None = None,\n         **kwargs,\n     ):\n         # Validation for some arguments\n@@ -351,7 +351,7 @@ def _create_id_label_maps(self, num_labels: int):\n         self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n \n     @property\n-    def name_or_path(self) -> Optional[str]:\n+    def name_or_path(self) -> str | None:\n         return getattr(self, \"_name_or_path\", None)\n \n     @name_or_path.setter\n@@ -1306,7 +1306,7 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n )\n \n \n-def layer_type_validation(layer_types: list[str], num_hidden_layers: Optional[int] = None, attention: bool = True):\n+def layer_type_validation(layer_types: list[str], num_hidden_layers: int | None = None, attention: bool = True):\n     \"\"\"Check that `layer_types` is correctly defined.\"\"\"\n     allowed_layer_types = ALLOWED_ATTENTION_LAYER_TYPES if attention else ALLOWED_MLP_LAYER_TYPES\n     if not all(layer_type in allowed_layer_types for layer_type in layer_types):"
        },
        {
            "sha": "931ad169c99a740cf7fb98dc0c7b3c1af337f39b",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright (C) 2025 the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0e4201f6553be46279ef6538912cdebe8f839631",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -21,7 +21,6 @@\n import warnings\n from collections.abc import Collection\n from functools import lru_cache\n-from typing import Optional\n \n from packaging import version\n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n@@ -120,7 +119,7 @@ def _get_prepend_scheme(add_prefix_space: bool, original_tokenizer) -> str:\n     return prepend_scheme\n \n \n-def generate_merges(vocab, vocab_scores, skip_tokens: Optional[Collection[str]] = None):\n+def generate_merges(vocab, vocab_scores, skip_tokens: Collection[str] | None = None):\n     skip_tokens = set(skip_tokens) if skip_tokens is not None else set()\n     reverse = vocab_scores is not None\n     vocab_scores = dict(vocab_scores) if reverse else vocab\n@@ -420,9 +419,7 @@ def converted(self) -> Tokenizer:\n \n \n class GPT2Converter(Converter):\n-    def converted(\n-        self, vocab: Optional[dict[str, int]] = None, merges: Optional[list[tuple[str, str]]] = None\n-    ) -> Tokenizer:\n+    def converted(self, vocab: dict[str, int] | None = None, merges: list[tuple[str, str]] | None = None) -> Tokenizer:\n         if not vocab:\n             vocab = self.original_tokenizer.encoder\n         if not merges:\n@@ -491,9 +488,7 @@ def converted(self) -> Tokenizer:\n \n \n class Qwen2Converter(Converter):\n-    def converted(\n-        self, vocab: Optional[dict[str, int]] = None, merges: Optional[list[tuple[str, str]]] = None\n-    ) -> Tokenizer:\n+    def converted(self, vocab: dict[str, int] | None = None, merges: list[tuple[str, str]] | None = None) -> Tokenizer:\n         if not vocab:\n             vocab = self.original_tokenizer.encoder\n         if not merges:"
        },
        {
            "sha": "4f51f23793ace33e766f0c5f383ed496b7c3f62e",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -27,7 +26,7 @@\n from copy import deepcopy\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n import torch\n \n@@ -51,7 +50,7 @@\n \n \n def build_glob_alternation(\n-    globs: list[Union[WeightRenaming, WeightConverter, str]],\n+    globs: list[WeightRenaming | WeightConverter | str],\n ) -> tuple[re.Pattern, dict[str, str], dict[str, str]]:\n     \"\"\"\n     Build a single alternation regex with one named group per glob.\n@@ -442,12 +441,12 @@ def reverse_op(self) -> ConversionOps:\n \n @dataclass(slots=True)\n class WeightTransform:\n-    source_patterns: Union[str, list[str]] = field(init=True)\n-    target_patterns: Union[str, list[str]] = field(init=True)\n+    source_patterns: str | list[str] = field(init=True)\n+    target_patterns: str | list[str] = field(init=True)\n     compiled_sources: re.Pattern = field(init=False)\n \n-    distributed_operation: Optional[TensorParallelLayer] = None\n-    quantization_operation: Optional[ConversionOps] = None\n+    distributed_operation: TensorParallelLayer | None = None\n+    quantization_operation: ConversionOps | None = None\n \n     collected_tensors: dict[str, list[Future]] = field(default_factory=lambda: defaultdict(list), init=False)\n     layer_targets: dict[str, set[str]] = field(default_factory=lambda: defaultdict(set), init=False)\n@@ -573,8 +572,8 @@ def convert(\n         model=None,\n         config=None,\n         hf_quantizer=None,\n-        missing_keys: Optional[MutableSet[str]] = None,\n-        conversion_errors: Optional[MutableMapping[str, str]] = None,\n+        missing_keys: MutableSet[str] | None = None,\n+        conversion_errors: MutableMapping[str, str] | None = None,\n     ):\n         # Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\n         # attribute during the whole process\n@@ -630,8 +629,8 @@ def convert(\n         model=None,\n         config=None,\n         hf_quantizer=None,\n-        missing_keys: Optional[MutableSet[str]] = None,\n-        conversion_errors: Optional[MutableMapping[str, str]] = None,\n+        missing_keys: MutableSet[str] | None = None,\n+        conversion_errors: MutableMapping[str, str] | None = None,\n     ):\n         # Collect the tensors here - we use a new dictionary to avoid keeping them in memory in the internal\n         # attribute during the whole process\n@@ -742,15 +741,15 @@ def log_conversion_errors(\n     first_target_key: str,\n     conversion_errors: MutableMapping[str, str],\n     extras: Any = None,\n-    op: Union[list[ConversionOps], ConversionOps, None] = None,\n+    op: list[ConversionOps] | ConversionOps | None = None,\n ):\n     \"\"\"Catch all exceptions during `convert` calls, and log the errors for later. Re-raise a `SkipParameters` exception\n     that will be catched later to skip the parameters that raised the original Exception.\"\"\"\n     try:\n         yield\n     except Exception as e:\n \n-        def _format_op_name(curr_op: Union[list[ConversionOps], ConversionOps, None]) -> Optional[str]:\n+        def _format_op_name(curr_op: list[ConversionOps] | ConversionOps | None) -> str | None:\n             if curr_op is None:\n                 return None\n             if isinstance(curr_op, (list, tuple, set)):\n@@ -786,7 +785,7 @@ def set_param_for_module(\n     mismatch_keys: MutableSet[tuple[str, torch.Size, torch.Size]],\n     missing_keys: MutableSet[str],\n     unexpected_keys: MutableSet[str],\n-    distributed_operation: Optional[TensorParallelLayer],\n+    distributed_operation: TensorParallelLayer | None,\n     hf_quantizer: HfQuantizer,\n ):\n     module_path, _, param_name = target_name.rpartition(\".\")"
        },
        {
            "sha": "9e792369d41ca762bdb93bb38092dc439928bf9b",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "cf40aeff7500d08bb35c42ca8afa199a4db5d0a7",
            "filename": "src/transformers/data/processors/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "d3667ea2697c4115061f0e5f5d26ebb2c1842f44",
            "filename": "src/transformers/data/processors/xnli.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdata%2Fprocessors%2Fxnli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdata%2Fprocessors%2Fxnli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fxnli.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "29b6a0509bb6100fed6754f544c9e9ddd37395c7",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -60,7 +60,7 @@\n     \"rhoknp\": \"rhoknp>=1.1.0,<1.3.1\",\n     \"rjieba\": \"rjieba\",\n     \"rouge-score\": \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n-    \"ruff\": \"ruff==0.13.1\",\n+    \"ruff\": \"ruff==0.14.10\",\n     \"sacrebleu\": \"sacrebleu>=1.4.12,<2.0.0\",\n     \"sacremoses\": \"sacremoses\",\n     \"safetensors\": \"safetensors>=0.4.3\","
        },
        {
            "sha": "7726d9f3290dba97fa288f5ebe54eb107f6af192",
            "filename": "src/transformers/distributed/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdistributed%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdistributed%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdistributed%2Fconfiguration_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -75,8 +75,7 @@ def to_dict(self) -> dict[str, Any]:\n     # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__iter__\n     def __iter__(self):\n         \"\"\"allows `dict(obj)` for situations where obj may be a dict or QuantizationConfigMixin\"\"\"\n-        for attr, value in copy.deepcopy(self.__dict__).items():\n-            yield attr, value\n+        yield from copy.deepcopy(self.__dict__).items()\n \n     # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.__repr__\n     def __repr__(self):"
        },
        {
            "sha": "e94617d2f51e4c7c51da63526a15c9aab98bea93",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -28,7 +28,7 @@\n import threading\n from pathlib import Path\n from types import ModuleType\n-from typing import Any, Optional, Union\n+from typing import Any\n \n from huggingface_hub import is_offline_mode, try_to_load_from_cache\n from packaging import version\n@@ -98,7 +98,7 @@ def init_hf_modules():\n         importlib.invalidate_caches()\n \n \n-def create_dynamic_module(name: Union[str, os.PathLike]) -> None:\n+def create_dynamic_module(name: str | os.PathLike) -> None:\n     \"\"\"\n     Creates a dynamic module in the cache directory for modules.\n \n@@ -120,7 +120,7 @@ def create_dynamic_module(name: Union[str, os.PathLike]) -> None:\n         importlib.invalidate_caches()\n \n \n-def get_relative_imports(module_file: Union[str, os.PathLike]) -> list[str]:\n+def get_relative_imports(module_file: str | os.PathLike) -> list[str]:\n     \"\"\"\n     Get the list of modules that are relatively imported in a module file.\n \n@@ -141,7 +141,7 @@ def get_relative_imports(module_file: Union[str, os.PathLike]) -> list[str]:\n     return list(set(relative_imports))\n \n \n-def get_relative_import_files(module_file: Union[str, os.PathLike]) -> list[str]:\n+def get_relative_import_files(module_file: str | os.PathLike) -> list[str]:\n     \"\"\"\n     Get the list of all files that are needed for a given module. Note that this function recurses through the relative\n     imports (if a imports b and b imports c, it will return module files for b and c).\n@@ -173,7 +173,7 @@ def get_relative_import_files(module_file: Union[str, os.PathLike]) -> list[str]\n     return all_relative_imports\n \n \n-def get_imports(filename: Union[str, os.PathLike]) -> list[str]:\n+def get_imports(filename: str | os.PathLike) -> list[str]:\n     \"\"\"\n     Extracts all the libraries (not relative imports this time) that are imported in a file.\n \n@@ -228,7 +228,7 @@ def recursive_look_for_imports(node):\n     return sorted(imported_modules)\n \n \n-def check_imports(filename: Union[str, os.PathLike]) -> list[str]:\n+def check_imports(filename: str | os.PathLike) -> list[str]:\n     \"\"\"\n     Check if the current Python environment contains all the libraries that are imported in a file. Will raise if a\n     library is missing.\n@@ -265,7 +265,7 @@ def check_imports(filename: Union[str, os.PathLike]) -> list[str]:\n \n def get_class_in_module(\n     class_name: str,\n-    module_path: Union[str, os.PathLike],\n+    module_path: str | os.PathLike,\n     *,\n     force_reload: bool = False,\n ) -> type:\n@@ -290,7 +290,7 @@ def get_class_in_module(\n         if force_reload:\n             sys.modules.pop(name, None)\n             importlib.invalidate_caches()\n-        cached_module: Optional[ModuleType] = sys.modules.get(name)\n+        cached_module: ModuleType | None = sys.modules.get(name)\n         module_spec = importlib.util.spec_from_file_location(name, location=module_file)\n \n         # Hash the module file and all its relative imports to check if we need to reload it\n@@ -312,16 +312,16 @@ def get_class_in_module(\n \n \n def get_cached_module_file(\n-    pretrained_model_name_or_path: Union[str, os.PathLike],\n+    pretrained_model_name_or_path: str | os.PathLike,\n     module_file: str,\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n+    cache_dir: str | os.PathLike | None = None,\n     force_download: bool = False,\n-    proxies: Optional[dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n+    proxies: dict[str, str] | None = None,\n+    token: bool | str | None = None,\n+    revision: str | None = None,\n     local_files_only: bool = False,\n-    repo_type: Optional[str] = None,\n-    _commit_hash: Optional[str] = None,\n+    repo_type: str | None = None,\n+    _commit_hash: str | None = None,\n     **deprecated_kwargs,\n ) -> str:\n     \"\"\"\n@@ -475,15 +475,15 @@ def get_cached_module_file(\n \n def get_class_from_dynamic_module(\n     class_reference: str,\n-    pretrained_model_name_or_path: Union[str, os.PathLike],\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n+    pretrained_model_name_or_path: str | os.PathLike,\n+    cache_dir: str | os.PathLike | None = None,\n     force_download: bool = False,\n-    proxies: Optional[dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n+    proxies: dict[str, str] | None = None,\n+    token: bool | str | None = None,\n+    revision: str | None = None,\n     local_files_only: bool = False,\n-    repo_type: Optional[str] = None,\n-    code_revision: Optional[str] = None,\n+    repo_type: str | None = None,\n+    code_revision: str | None = None,\n     **kwargs,\n ) -> type:\n     \"\"\"\n@@ -583,7 +583,7 @@ def get_class_from_dynamic_module(\n     return get_class_in_module(class_name, final_module, force_reload=force_download)\n \n \n-def custom_object_save(obj: Any, folder: Union[str, os.PathLike], config: Optional[dict] = None) -> list[str]:\n+def custom_object_save(obj: Any, folder: str | os.PathLike, config: dict | None = None) -> list[str]:\n     \"\"\"\n     Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally\n     adds the proper fields in a config."
        },
        {
            "sha": "5e346d3e15e5a956785df989e20f624590be249c",
            "filename": "src/transformers/feature_extraction_sequence_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 23,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -15,8 +15,6 @@\n Sequence feature extraction class for common feature extractors to preprocess sequences.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from .audio_utils import is_valid_audio, load_audio\n@@ -52,19 +50,17 @@ def __init__(self, feature_size: int, sampling_rate: int, padding_value: float,\n \n     def pad(\n         self,\n-        processed_features: Union[\n-            BatchFeature,\n-            list[BatchFeature],\n-            dict[str, BatchFeature],\n-            dict[str, list[BatchFeature]],\n-            list[dict[str, BatchFeature]],\n-        ],\n-        padding: Union[bool, str, PaddingStrategy] = True,\n-        max_length: Optional[int] = None,\n+        processed_features: BatchFeature\n+        | list[BatchFeature]\n+        | dict[str, BatchFeature]\n+        | dict[str, list[BatchFeature]]\n+        | list[dict[str, BatchFeature]],\n+        padding: bool | str | PaddingStrategy = True,\n+        max_length: int | None = None,\n         truncation: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_attention_mask: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the\n@@ -224,11 +220,11 @@ def pad(\n \n     def _pad(\n         self,\n-        processed_features: Union[dict[str, np.ndarray], BatchFeature],\n-        max_length: Optional[int] = None,\n+        processed_features: dict[str, np.ndarray] | BatchFeature,\n+        max_length: int | None = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        return_attention_mask: bool | None = None,\n     ) -> dict:\n         \"\"\"\n         Pad inputs (on left/right and up to predefined length or max length in the batch)\n@@ -296,10 +292,10 @@ def _pad(\n \n     def _truncate(\n         self,\n-        processed_features: Union[dict[str, np.ndarray], BatchFeature],\n-        max_length: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n-        truncation: Optional[bool] = None,\n+        processed_features: dict[str, np.ndarray] | BatchFeature,\n+        max_length: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n+        truncation: bool | None = None,\n     ):\n         \"\"\"\n         Truncate inputs to predefined length or max length in the batch\n@@ -369,7 +365,7 @@ def _get_padding_strategies(self, padding=False, max_length=None):\n \n         return padding_strategy\n \n-    def fetch_audio(self, audio_url_or_urls: Union[str, list[str], list[list[str]]]):\n+    def fetch_audio(self, audio_url_or_urls: str | list[str] | list[list[str]]):\n         \"\"\"\n         Convert a single or a list of urls into the corresponding `np.ndarray` objects.\n "
        },
        {
            "sha": "781a4d4603a9b9adc98b46d727208383a68e993a",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,7 @@\n import json\n import os\n from collections import UserDict\n-from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n+from typing import TYPE_CHECKING, Any, TypeVar, Union\n \n import numpy as np\n from huggingface_hub import create_repo, is_offline_mode\n@@ -74,9 +74,9 @@ class BatchFeature(UserDict):\n \n     def __init__(\n         self,\n-        data: Optional[dict[str, Any]] = None,\n-        tensor_type: Union[None, str, TensorType] = None,\n-        skip_tensor_conversion: Optional[Union[list[str], set[str]]] = None,\n+        data: dict[str, Any] | None = None,\n+        tensor_type: None | str | TensorType = None,\n+        skip_tensor_conversion: list[str] | set[str] | None = None,\n     ):\n         super().__init__(data)\n         self.convert_to_tensors(tensor_type=tensor_type, skip_tensor_conversion=skip_tensor_conversion)\n@@ -104,7 +104,7 @@ def __setstate__(self, state):\n         if \"data\" in state:\n             self.data = state[\"data\"]\n \n-    def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] = None):\n+    def _get_is_as_tensor_fns(self, tensor_type: str | TensorType | None = None):\n         if tensor_type is None:\n             return None, None\n \n@@ -156,8 +156,8 @@ def as_tensor(value, dtype=None):\n \n     def convert_to_tensors(\n         self,\n-        tensor_type: Optional[Union[str, TensorType]] = None,\n-        skip_tensor_conversion: Optional[Union[list[str], set[str]]] = None,\n+        tensor_type: str | TensorType | None = None,\n+        skip_tensor_conversion: list[str] | set[str] | None = None,\n     ):\n         \"\"\"\n         Convert the inner content to tensors.\n@@ -282,11 +282,11 @@ def __init__(self, **kwargs):\n     @classmethod\n     def from_pretrained(\n         cls: type[SpecificFeatureExtractorType],\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        pretrained_model_name_or_path: str | os.PathLike,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         **kwargs,\n     ) -> SpecificFeatureExtractorType:\n@@ -376,7 +376,7 @@ def from_pretrained(\n \n         return cls.from_dict(feature_extractor_dict, **kwargs)\n \n-    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n+    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save a feature_extractor object to the directory `save_directory`, so that it can be re-loaded using the\n         [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] class method.\n@@ -426,7 +426,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n \n     @classmethod\n     def get_feature_extractor_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n@@ -591,7 +591,7 @@ def to_dict(self) -> dict[str, Any]:\n         return output\n \n     @classmethod\n-    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> \"FeatureExtractionMixin\":\n+    def from_json_file(cls, json_file: str | os.PathLike) -> \"FeatureExtractionMixin\":\n         \"\"\"\n         Instantiates a feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`] from the path to\n         a JSON file of parameters.\n@@ -624,7 +624,7 @@ def to_json_string(self) -> str:\n \n         return json.dumps(dictionary, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+    def to_json_file(self, json_file_path: str | os.PathLike):\n         \"\"\"\n         Save this instance to a JSON file.\n "
        },
        {
            "sha": "bc0f5ca1392e57e0b779493d8e126857749db9bb",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -489,7 +488,7 @@ def _get_tokens_diag(prompt, prompt_plus_new_tokens):\n             new_tokens_only: 2D array of shape (batch_size, new_token_length), represents the new tokens that are not in prompt\n             discrepancy_only: 2D array of shape (batch_size, discrepancy_length), represents the new tokens that are in prompt but not in prompt_plus_new_tokens\n         \"\"\"\n-        compare_mat = prompt_plus_new_tokens.T == prompt\n+        compare_mat = prompt == prompt_plus_new_tokens.T\n         if not torch.is_tensor(compare_mat):\n             compare_mat = torch.tensor(compare_mat)\n "
        },
        {
            "sha": "8f5e32fd2d6a338823b541c739c7dc0f39db4741",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -1261,8 +1260,7 @@ def to_dict(self) -> dict[str, Any]:\n         return output\n \n     def __iter__(self):\n-        for attr, value in copy.deepcopy(self.__dict__).items():\n-            yield attr, value\n+        yield from copy.deepcopy(self.__dict__).items()\n \n     def __repr__(self):\n         return f\"{self.__class__.__name__} {self.to_json_string()}\""
        },
        {
            "sha": "2364d08c38a8f8d624d9b1aef54d20242c56d3a3",
            "filename": "src/transformers/generation/continuous_batching/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2F__init__.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "21567986b846c928e629fa3a36c3c42d28476e00",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "34f2567c53a7881393ad5b7aef7b91bb9bded0a2",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c238a6aaa96f8a3d2ed42f0e40b246b899f725c0",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team.\n # Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "3bc3c66db5e8c17699172f2d6f4c0f6e2da1d624",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b26dba2d0ccdd8a32f3b89a247f3fe361b2e819b",
            "filename": "src/transformers/generation/continuous_batching/scheduler.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fscheduler.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "557378c7fca49bdbe4b9b073f48a57cd50819754",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team and Google DeepMind.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "cc9f709f5116f8c5adc02f4dda6b551cff61ecbd",
            "filename": "src/transformers/generation/streamers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstreamers.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "e662a450704a6e3565d4fe6004b9edca5e59ff4d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.\n # Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -21,7 +20,7 @@\n from collections.abc import Callable\n from contextlib import contextmanager\n from dataclasses import dataclass\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional\n \n import torch\n import torch.distributed as dist\n@@ -332,9 +331,9 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n \n \n # Typing shortcuts\n-GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]\n-GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]\n-GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]\n+GenerateNonBeamOutput = GenerateDecoderOnlyOutput | GenerateEncoderDecoderOutput\n+GenerateBeamOutput = GenerateBeamDecoderOnlyOutput | GenerateBeamEncoderDecoderOutput\n+GenerateOutput = GenerateNonBeamOutput | GenerateBeamOutput\n \n \n class GenerationMixin(ContinuousMixin):"
        },
        {
            "sha": "c6e88407fddafcd6482678d64c2fc1327f1d7f5b",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team and Google DeepMind.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,7 @@\n import collections\n from dataclasses import dataclass\n from functools import lru_cache\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Union\n \n import numpy as np\n import torch\n@@ -125,7 +124,7 @@ def __init__(\n         self,\n         model_config: \"PreTrainedConfig\",\n         device: str,\n-        watermarking_config: Optional[Union[\"WatermarkingConfig\", dict]],\n+        watermarking_config: Union[\"WatermarkingConfig\", dict] | None,\n         ignore_repeated_ngrams: bool = False,\n         max_cache_size: int = 128,\n     ):"
        },
        {
            "sha": "b9e6f99b041d6fdb7b9986f06bbb9db5383d5073",
            "filename": "src/transformers/hf_argparser.py",
            "status": "modified",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fhf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fhf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhf_argparser.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -23,7 +23,7 @@\n from enum import Enum\n from inspect import isclass\n from pathlib import Path\n-from typing import Any, Literal, NewType, Optional, Union, get_type_hints\n+from typing import Any, Literal, NewType, Union, get_type_hints\n \n import yaml\n \n@@ -63,11 +63,11 @@ def make_choice_type_function(choices: list) -> Callable[[str], Any]:\n \n def HfArg(\n     *,\n-    aliases: Optional[Union[str, list[str]]] = None,\n-    help: Optional[str] = None,\n+    aliases: str | list[str] | None = None,\n+    help: str | None = None,\n     default: Any = dataclasses.MISSING,\n     default_factory: Callable[[], Any] = dataclasses.MISSING,\n-    metadata: Optional[dict] = None,\n+    metadata: dict | None = None,\n     **kwargs,\n ) -> dataclasses.Field:\n     \"\"\"Argument helper enabling a concise syntax to create dataclass fields for parsing with `HfArgumentParser`.\n@@ -125,7 +125,7 @@ class HfArgumentParser(ArgumentParser):\n \n     dataclass_types: Iterable[DataClassType]\n \n-    def __init__(self, dataclass_types: Optional[Union[DataClassType, Iterable[DataClassType]]] = None, **kwargs):\n+    def __init__(self, dataclass_types: DataClassType | Iterable[DataClassType] | None = None, **kwargs):\n         # Make sure dataclass_types is an iterable\n         if dataclass_types is None:\n             dataclass_types = []\n@@ -201,7 +201,7 @@ def _parse_dataclass_field(parser: ArgumentParser, field: dataclasses.Field):\n                 kwargs[\"default\"] = field.default\n             else:\n                 kwargs[\"required\"] = True\n-        elif field.type is bool or field.type == Optional[bool]:\n+        elif field.type is bool or field.type == bool | None:\n             # Copy the correct kwargs to use to instantiate a `no_*` complement argument below.\n             # We do not initialize it here because the `no_*` alternative must be instantiated after the real argument\n             bool_kwargs = copy(kwargs)\n@@ -238,7 +238,7 @@ def _parse_dataclass_field(parser: ArgumentParser, field: dataclasses.Field):\n         # Order is important for arguments with the same destination!\n         # We use a copy of earlier kwargs because the original kwargs have changed a lot before reaching down\n         # here and we do not need those changes/additional keys.\n-        if field.default is True and (field.type is bool or field.type == Optional[bool]):\n+        if field.default is True and (field.type is bool or field.type == bool | None):\n             bool_kwargs[\"default\"] = False\n             parser.add_argument(\n                 f\"--no_{field.name}\",\n@@ -383,9 +383,7 @@ def parse_dict(self, args: dict[str, Any], allow_extra_keys: bool = False) -> tu\n             raise ValueError(f\"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}\")\n         return tuple(outputs)\n \n-    def parse_json_file(\n-        self, json_file: Union[str, os.PathLike], allow_extra_keys: bool = False\n-    ) -> tuple[DataClass, ...]:\n+    def parse_json_file(self, json_file: str | os.PathLike, allow_extra_keys: bool = False) -> tuple[DataClass, ...]:\n         \"\"\"\n         Alternative helper method that does not use `argparse` at all, instead loading a json file and populating the\n         dataclass types.\n@@ -407,9 +405,7 @@ def parse_json_file(\n         outputs = self.parse_dict(data, allow_extra_keys=allow_extra_keys)\n         return tuple(outputs)\n \n-    def parse_yaml_file(\n-        self, yaml_file: Union[str, os.PathLike], allow_extra_keys: bool = False\n-    ) -> tuple[DataClass, ...]:\n+    def parse_yaml_file(self, yaml_file: str | os.PathLike, allow_extra_keys: bool = False) -> tuple[DataClass, ...]:\n         \"\"\"\n         Alternative helper method that does not use `argparse` at all, instead loading a yaml file and populating the\n         dataclass types."
        },
        {
            "sha": "267bb36a8e6eff68a7207a09eb32018d8ca532cd",
            "filename": "src/transformers/hyperparameter_search.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fhyperparameter_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fhyperparameter_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhyperparameter_search.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional\n \n from .integrations import (\n     is_optuna_available,\n@@ -35,7 +34,7 @@\n \n class HyperParamSearchBackendBase:\n     name: str\n-    pip_package: Optional[str] = None\n+    pip_package: str | None = None\n \n     @staticmethod\n     def is_available():"
        },
        {
            "sha": "f59fd41f7b85a09969bfbebea3f4a7aa7cf094fa",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -15,7 +15,7 @@\n import copy\n import json\n import os\n-from typing import Any, Optional, TypeVar, Union\n+from typing import Any, TypeVar\n \n import numpy as np\n from huggingface_hub import create_repo, is_offline_mode\n@@ -84,11 +84,11 @@ def __init__(self, **kwargs):\n     @classmethod\n     def from_pretrained(\n         cls: type[ImageProcessorType],\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        pretrained_model_name_or_path: str | os.PathLike,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         **kwargs,\n     ) -> ImageProcessorType:\n@@ -180,7 +180,7 @@ def from_pretrained(\n \n         return cls.from_dict(image_processor_dict, **kwargs)\n \n-    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n+    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n         \"\"\"\n         Save an image processor object to the directory `save_directory`, so that it can be re-loaded using the\n         [`~image_processing_utils.ImageProcessingMixin.from_pretrained`] class method.\n@@ -230,7 +230,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n \n     @classmethod\n     def get_image_processor_dict(\n-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n+        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n@@ -393,7 +393,7 @@ def to_dict(self) -> dict[str, Any]:\n         return output\n \n     @classmethod\n-    def from_json_file(cls, json_file: Union[str, os.PathLike]):\n+    def from_json_file(cls, json_file: str | os.PathLike):\n         \"\"\"\n         Instantiates a image processor of type [`~image_processing_utils.ImageProcessingMixin`] from the path to a JSON\n         file of parameters.\n@@ -426,7 +426,7 @@ def to_json_string(self) -> str:\n \n         return json.dumps(dictionary, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+    def to_json_file(self, json_file_path: str | os.PathLike):\n         \"\"\"\n         Save this instance to a JSON file.\n \n@@ -462,7 +462,7 @@ def register_for_auto_class(cls, auto_class=\"AutoImageProcessor\"):\n \n         cls._auto_class = auto_class\n \n-    def fetch_images(self, image_url_or_urls: Union[str, list[str], list[list[str]]]):\n+    def fetch_images(self, image_url_or_urls: str | list[str] | list[list[str]]):\n         \"\"\"\n         Convert a single or a list of urls into the corresponding `PIL.Image` objects.\n "
        },
        {
            "sha": "c1339500326f806efddbf16067f92ae89a54a46a",
            "filename": "src/transformers/image_processing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -14,7 +14,6 @@\n \n import math\n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -60,8 +59,8 @@ def rescale(\n         self,\n         image: np.ndarray,\n         scale: float,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -91,10 +90,10 @@ def rescale(\n     def normalize(\n         self,\n         image: np.ndarray,\n-        mean: Union[float, Iterable[float]],\n-        std: Union[float, Iterable[float]],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        mean: float | Iterable[float],\n+        std: float | Iterable[float],\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -129,8 +128,8 @@ def center_crop(\n         self,\n         image: np.ndarray,\n         size: dict[str, int],\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n         **kwargs,\n     ) -> np.ndarray:\n         \"\"\"\n@@ -191,7 +190,7 @@ def is_valid_size_dict(size_dict):\n \n \n def convert_to_size_dict(\n-    size, max_size: Optional[int] = None, default_to_square: bool = True, height_width_order: bool = True\n+    size, max_size: int | None = None, default_to_square: bool = True, height_width_order: bool = True\n ):\n     # By default, if size is an int we assume it represents a tuple of (size, size).\n     if isinstance(size, int) and default_to_square:\n@@ -219,8 +218,8 @@ def convert_to_size_dict(\n \n \n def get_size_dict(\n-    size: Optional[Union[int, Iterable[int], dict[str, int]]] = None,\n-    max_size: Optional[int] = None,\n+    size: int | Iterable[int] | dict[str, int] | None = None,\n+    max_size: int | None = None,\n     height_width_order: bool = True,\n     default_to_square: bool = True,\n     param_name=\"size\","
        },
        {
            "sha": "fca1ad1b8ce6a2f8b79840c7ff65c562337db13f",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -45,8 +45,8 @@\n \n def to_channel_dimension_format(\n     image: np.ndarray,\n-    channel_dim: Union[ChannelDimension, str],\n-    input_channel_dim: Optional[Union[ChannelDimension, str]] = None,\n+    channel_dim: ChannelDimension | str,\n+    input_channel_dim: ChannelDimension | str | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Converts `image` to the channel dimension format specified by `channel_dim`. The input\n@@ -89,9 +89,9 @@ def to_channel_dimension_format(\n def rescale(\n     image: np.ndarray,\n     scale: float,\n-    data_format: Optional[ChannelDimension] = None,\n+    data_format: ChannelDimension | None = None,\n     dtype: np.dtype = np.float32,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Rescales `image` by `scale`.\n@@ -134,14 +134,14 @@ def _rescale_for_pil_conversion(image):\n     if image.dtype == np.uint8:\n         do_rescale = False\n     elif np.allclose(image, image.astype(int)):\n-        if np.all(0 <= image) and np.all(image <= 255):\n+        if np.all(image >= 0) and np.all(image <= 255):\n             do_rescale = False\n         else:\n             raise ValueError(\n                 \"The image to be converted to a PIL image contains values outside the range [0, 255], \"\n                 f\"got [{image.min()}, {image.max()}] which cannot be converted to uint8.\"\n             )\n-    elif np.all(0 <= image) and np.all(image <= 1):\n+    elif np.all(image >= 0) and np.all(image <= 1):\n         do_rescale = True\n     else:\n         raise ValueError(\n@@ -153,9 +153,9 @@ def _rescale_for_pil_conversion(image):\n \n def to_pil_image(\n     image: Union[np.ndarray, \"PIL.Image.Image\", \"torch.Tensor\"],\n-    do_rescale: Optional[bool] = None,\n-    image_mode: Optional[str] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    do_rescale: bool | None = None,\n+    image_mode: str | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> \"PIL.Image.Image\":\n     \"\"\"\n     Converts `image` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if\n@@ -245,10 +245,10 @@ def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, in\n # Logic adapted from torchvision resizing logic: https://github.com/pytorch/vision/blob/511924c1ced4ce0461197e5caa64ce5b9e558aab/torchvision/transforms/functional.py#L366\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, tuple[int, int], list[int], tuple[int, ...]],\n+    size: int | tuple[int, int] | list[int] | tuple[int, ...],\n     default_to_square: bool = True,\n-    max_size: Optional[int] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    max_size: int | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> tuple:\n     \"\"\"\n     Find the target (height, width) dimension of the output image after resizing given the input image and the desired\n@@ -314,10 +314,10 @@ def resize(\n     image: np.ndarray,\n     size: tuple[int, int],\n     resample: Optional[\"PILImageResampling\"] = None,\n-    reducing_gap: Optional[int] = None,\n-    data_format: Optional[ChannelDimension] = None,\n+    reducing_gap: int | None = None,\n+    data_format: ChannelDimension | None = None,\n     return_numpy: bool = True,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Resizes `image` to `(height, width)` specified by `size` using the PIL library.\n@@ -383,10 +383,10 @@ def resize(\n \n def normalize(\n     image: np.ndarray,\n-    mean: Union[float, Collection[float]],\n-    std: Union[float, Collection[float]],\n-    data_format: Optional[ChannelDimension] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    mean: float | Collection[float],\n+    std: float | Collection[float],\n+    data_format: ChannelDimension | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Normalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n@@ -445,8 +445,8 @@ def normalize(\n def center_crop(\n     image: np.ndarray,\n     size: tuple[int, int],\n-    data_format: Optional[Union[str, ChannelDimension]] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    data_format: str | ChannelDimension | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Crops the `image` to the specified `size` using a center crop. Note that if the image is too small to be cropped to\n@@ -654,11 +654,11 @@ class PaddingMode(ExplicitEnum):\n \n def pad(\n     image: np.ndarray,\n-    padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n+    padding: int | tuple[int, int] | Iterable[tuple[int, int]],\n     mode: PaddingMode = PaddingMode.CONSTANT,\n-    constant_values: Union[float, Iterable[float]] = 0.0,\n-    data_format: Optional[Union[str, ChannelDimension]] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    constant_values: float | Iterable[float] = 0.0,\n+    data_format: str | ChannelDimension | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Pads the `image` with the specified (height, width) `padding` and `mode`.\n@@ -761,8 +761,8 @@ def convert_to_rgb(image: ImageInput) -> ImageInput:\n \n def flip_channel_order(\n     image: np.ndarray,\n-    data_format: Optional[ChannelDimension] = None,\n-    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    data_format: ChannelDimension | None = None,\n+    input_data_format: str | ChannelDimension | None = None,\n ) -> np.ndarray:\n     \"\"\"\n     Flips the channel order of the image.\n@@ -911,7 +911,7 @@ def _iterate_items(items, is_nested: bool):\n def group_images_by_shape(\n     images: Union[list[\"torch.Tensor\"], \"torch.Tensor\"],\n     *paired_inputs,\n-    disable_grouping: Optional[bool],\n+    disable_grouping: bool | None,\n     is_nested: bool = False,\n ) -> tuple[dict, ...]:\n     \"\"\"\n@@ -971,7 +971,7 @@ def group_images_by_shape(\n \n def reorder_images(\n     processed_images: dict[tuple[int, int], \"torch.Tensor\"],\n-    grouped_images_index: dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]],\n+    grouped_images_index: dict[int | tuple[int, int], tuple[tuple[int, int], int]],\n     is_nested: bool = False,\n ) -> Union[list[\"torch.Tensor\"], \"torch.Tensor\"]:\n     \"\"\""
        },
        {
            "sha": "61227a6d2341566f7507d67535e44c02e0665412",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 25,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -199,7 +199,7 @@ def make_list_of_images(images, expected_ndims: int = 3) -> list[ImageInput]:\n \n \n def make_flat_list_of_images(\n-    images: Union[list[ImageInput], ImageInput],\n+    images: list[ImageInput] | ImageInput,\n     expected_ndims: int = 3,\n ) -> ImageInput:\n     \"\"\"\n@@ -237,7 +237,7 @@ def make_flat_list_of_images(\n \n \n def make_nested_list_of_images(\n-    images: Union[list[ImageInput], ImageInput],\n+    images: list[ImageInput] | ImageInput,\n     expected_ndims: int = 3,\n ) -> list[ImageInput]:\n     \"\"\"\n@@ -285,7 +285,7 @@ def to_numpy_array(img) -> np.ndarray:\n \n \n def infer_channel_dimension_format(\n-    image: np.ndarray, num_channels: Optional[Union[int, tuple[int, ...]]] = None\n+    image: np.ndarray, num_channels: int | tuple[int, ...] | None = None\n ) -> ChannelDimension:\n     \"\"\"\n     Infers the channel dimension format of `image`.\n@@ -323,9 +323,7 @@ def infer_channel_dimension_format(\n     raise ValueError(\"Unable to infer channel dimension format\")\n \n \n-def get_channel_dimension_axis(\n-    image: np.ndarray, input_data_format: Optional[Union[ChannelDimension, str]] = None\n-) -> int:\n+def get_channel_dimension_axis(image: np.ndarray, input_data_format: ChannelDimension | str | None = None) -> int:\n     \"\"\"\n     Returns the channel dimension axis of the image.\n \n@@ -347,7 +345,7 @@ def get_channel_dimension_axis(\n     raise ValueError(f\"Unsupported data format: {input_data_format}\")\n \n \n-def get_image_size(image: np.ndarray, channel_dim: Optional[ChannelDimension] = None) -> tuple[int, int]:\n+def get_image_size(image: np.ndarray, channel_dim: ChannelDimension | None = None) -> tuple[int, int]:\n     \"\"\"\n     Returns the (height, width) dimensions of the image.\n \n@@ -402,7 +400,7 @@ def get_image_size_for_max_height_width(\n     return new_height, new_width\n \n \n-def is_valid_annotation_coco_detection(annotation: dict[str, Union[list, tuple]]) -> bool:\n+def is_valid_annotation_coco_detection(annotation: dict[str, list | tuple]) -> bool:\n     if (\n         isinstance(annotation, dict)\n         and \"image_id\" in annotation\n@@ -417,7 +415,7 @@ def is_valid_annotation_coco_detection(annotation: dict[str, Union[list, tuple]]\n     return False\n \n \n-def is_valid_annotation_coco_panoptic(annotation: dict[str, Union[list, tuple]]) -> bool:\n+def is_valid_annotation_coco_panoptic(annotation: dict[str, list | tuple]) -> bool:\n     if (\n         isinstance(annotation, dict)\n         and \"image_id\" in annotation\n@@ -433,15 +431,15 @@ def is_valid_annotation_coco_panoptic(annotation: dict[str, Union[list, tuple]])\n     return False\n \n \n-def valid_coco_detection_annotations(annotations: Iterable[dict[str, Union[list, tuple]]]) -> bool:\n+def valid_coco_detection_annotations(annotations: Iterable[dict[str, list | tuple]]) -> bool:\n     return all(is_valid_annotation_coco_detection(ann) for ann in annotations)\n \n \n-def valid_coco_panoptic_annotations(annotations: Iterable[dict[str, Union[list, tuple]]]) -> bool:\n+def valid_coco_panoptic_annotations(annotations: Iterable[dict[str, list | tuple]]) -> bool:\n     return all(is_valid_annotation_coco_panoptic(ann) for ann in annotations)\n \n \n-def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] = None) -> \"PIL.Image.Image\":\n+def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: float | None = None) -> \"PIL.Image.Image\":\n     \"\"\"\n     Loads `image` to a PIL Image.\n \n@@ -484,7 +482,7 @@ def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] =\n \n \n def load_images(\n-    images: Union[list, tuple, str, \"PIL.Image.Image\"], timeout: Optional[float] = None\n+    images: Union[list, tuple, str, \"PIL.Image.Image\"], timeout: float | None = None\n ) -> Union[\"PIL.Image.Image\", list[\"PIL.Image.Image\"], list[list[\"PIL.Image.Image\"]]]:\n     \"\"\"Loads images, handling different levels of nesting.\n \n@@ -505,17 +503,17 @@ def load_images(\n \n \n def validate_preprocess_arguments(\n-    do_rescale: Optional[bool] = None,\n-    rescale_factor: Optional[float] = None,\n-    do_normalize: Optional[bool] = None,\n-    image_mean: Optional[Union[float, list[float]]] = None,\n-    image_std: Optional[Union[float, list[float]]] = None,\n-    do_pad: Optional[bool] = None,\n-    pad_size: Optional[Union[dict[str, int], int]] = None,\n-    do_center_crop: Optional[bool] = None,\n-    crop_size: Optional[dict[str, int]] = None,\n-    do_resize: Optional[bool] = None,\n-    size: Optional[dict[str, int]] = None,\n+    do_rescale: bool | None = None,\n+    rescale_factor: float | None = None,\n+    do_normalize: bool | None = None,\n+    image_mean: float | list[float] | None = None,\n+    image_std: float | list[float] | None = None,\n+    do_pad: bool | None = None,\n+    pad_size: dict[str, int] | int | None = None,\n+    do_center_crop: bool | None = None,\n+    crop_size: dict[str, int] | None = None,\n+    do_resize: bool | None = None,\n+    size: dict[str, int] | None = None,\n     resample: Optional[\"PILImageResampling\"] = None,\n     interpolation: Optional[\"InterpolationMode\"] = None,\n ):\n@@ -612,7 +610,7 @@ def convert_rgb(self, image):\n \n         return image.convert(\"RGB\")\n \n-    def rescale(self, image: np.ndarray, scale: Union[float, int]) -> np.ndarray:\n+    def rescale(self, image: np.ndarray, scale: float | int) -> np.ndarray:\n         \"\"\"\n         Rescale a numpy image by scale amount\n         \"\"\""
        },
        {
            "sha": "059343ea5c78dc43cfd03924dc27d9a497a464e9",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"AWQ (Activation aware Weight Quantization) integration file\"\n \n-from typing import Optional, Union\n-\n from ..quantizers.quantizers_utils import should_convert_module\n from ..utils import is_torch_available, logging\n \n@@ -59,7 +57,7 @@ def replace_with_awq_linear(\n     model,\n     modules_to_not_convert=None,\n     quantization_config=None,\n-    device_map: Optional[Union[str, dict]] = None,\n+    device_map: str | dict | None = None,\n ) -> bool:\n     \"\"\"\n     Public method that replaces the linear layers of the given model with awq quantized layers."
        },
        {
            "sha": "11df0f9e655b18b947ae30bcb707518f936501cb",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -54,7 +54,7 @@ def is_deepspeed_available():\n     from builtins import object as DeepSpeedConfig\n \n \n-class HfDeepSpeedConfig(DeepSpeedConfig):\n+class HfDeepSpeedConfig(DeepSpeedConfig):  # noqa UP004\n     \"\"\"\n     This object contains a DeepSpeed configuration dictionary and can be quickly queried for things like zero stage.\n "
        },
        {
            "sha": "1e9a82b3d48a75cb2756dddf54a6067abbc32f39",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 NetEase, Inc. and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "37603d9788eaa016d99be285958bac23ad2864e3",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n from functools import lru_cache\n-from typing import Optional\n \n from ..activations import ACT2FN\n from ..core_model_loading import ConversionOps\n@@ -49,7 +48,7 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor | list[torch.Tensor]],\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         target_key, value = tuple(input_dict.items())[0]"
        },
        {
            "sha": "4fdafb49c6d9abfc8395a66263a4eda12b12935a",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "10737a984225e78ad619fbaaeb06c68a0f083beb",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -102,7 +102,7 @@ def compile_friendly_flex_attention(\n     )\n \n \n-Offset = Union[torch.Tensor, int]\n+Offset = torch.Tensor | int\n \n \n # TODO: deprecate / rename to make_flex_block_mask for clarity as it's not only causal anymore"
        },
        {
            "sha": "ea02151c5094f73cf5d6a65385228ca19990a40a",
            "filename": "src/transformers/integrations/fp_quant.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffp_quant.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"FP-Quant integration file\"\n \n-from typing import Optional\n-\n import torch\n \n from ..utils import (\n@@ -39,8 +37,8 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: torch.Tensor,\n-        model: Optional[torch.nn.Module] = None,\n-        missing_keys: Optional[list[str]] = None,\n+        model: torch.nn.Module | None = None,\n+        missing_keys: list[str] | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         target_key, value = tuple(input_dict.items())[0]\n@@ -76,9 +74,9 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: torch.Tensor,\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         full_layer_name: str | None = None,\n-        missing_keys: Optional[list[str]] = None,\n+        missing_keys: list[str] | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         target_key, value = tuple(input_dict.items())[0]"
        },
        {
            "sha": "748d649b4ef09a1525ff3869bb783fe68f6ca849",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The ggml.ai team and The HuggingFace Inc. team. and pygguf author (github.com/99991)\n # https://github.com/99991/pygguf\n #"
        },
        {
            "sha": "31e38ab566ded98c2102b6087c793af9b35952dc",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -2403,9 +2403,9 @@ def get_reporting_integration_callbacks(report_to):\n         return []\n \n     if isinstance(report_to, str):\n-        if \"none\" == report_to:\n+        if report_to == \"none\":\n             return []\n-        elif \"all\" == report_to:\n+        elif report_to == \"all\":\n             report_to = get_available_reporting_integrations()\n         else:\n             report_to = [report_to]"
        },
        {
            "sha": "f0a8de789f481a46119c43f5329175578f1d772b",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,6 @@\n     import torch\n     from torch import nn\n from contextlib import contextmanager\n-from typing import Optional\n \n from ..core_model_loading import ConversionOps\n from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n@@ -76,8 +75,8 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n-        model: Optional[torch.nn.Module] = None,\n-        missing_keys: Optional[list[str]] = None,\n+        model: torch.nn.Module | None = None,\n+        missing_keys: list[str] | None = None,\n         full_layer_name: str | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n@@ -124,7 +123,7 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         full_layer_name: str | None = None,\n         missing_keys=None,\n         **kwargs,\n@@ -153,9 +152,9 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         full_layer_name: str | None = None,\n-        missing_keys: Optional[list[str]] = None,\n+        missing_keys: list[str] | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         param_data = {}"
        },
        {
            "sha": "009ab0fd0b140ecda2b45e8b4fa21d39d709b300",
            "filename": "src/transformers/integrations/quark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fquark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Fquark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquark.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -13,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n \n from ..core_model_loading import ConversionOps\n from ..utils import is_torch_available\n@@ -30,8 +28,8 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: torch.Tensor,\n-        model: Optional[torch.nn.Module] = None,\n-        missing_keys: Optional[list[str]] = None,\n+        model: torch.nn.Module | None = None,\n+        missing_keys: list[str] | None = None,\n         full_layer_name: str | None = None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:"
        },
        {
            "sha": "63cc7a3242754483c62390825392df960ace440b",
            "filename": "src/transformers/integrations/torchao.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftorchao.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n import importlib.metadata\n import re\n import types\n-from typing import Optional\n \n import torch\n from packaging import version\n@@ -41,7 +39,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def fuzzy_match_size(config_name: str) -> Optional[str]:\n+def fuzzy_match_size(config_name: str) -> str | None:\n     \"\"\"\n     Extract the size digit from strings like \"4weight\", \"8weight\".\n     Returns the digit as an integer if found, otherwise None.\n@@ -82,7 +80,7 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         full_layer_name: str | None = None,\n         missing_keys=None,\n         **kwargs,\n@@ -211,7 +209,7 @@ def convert(\n         self,\n         input_dict: dict[str, torch.Tensor],\n         source_patterns: list[str] | None = None,\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         full_layer_name: str | None = None,\n         missing_keys=None,\n         **kwargs,\n@@ -251,7 +249,7 @@ def convert(\n         if is_unsafe_serialization:\n             return {full_layer_name: weight}\n         # Sanity check for the new serialization format\n-        elif not (TORCHAO_VERSION >= version.parse(\"0.15.0\") and is_metadata_torchao(self.hf_quantizer.metadata)):\n+        elif not (version.parse(\"0.15.0\") <= TORCHAO_VERSION and is_metadata_torchao(self.hf_quantizer.metadata)):\n             raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.15.0` installed\")\n \n         unflattened_state_dict, leftover_state_dict = unflatten_tensor_state_dict("
        },
        {
            "sha": "fa62d6f422466b7beca598db6833dbd28e3871fa",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 47,
            "deletions": 51,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,7 +13,6 @@\n # limitations under the License.\n import itertools\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n@@ -163,9 +161,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n     return inner_mask\n \n \n-def prepare_padding_mask(\n-    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int\n-) -> Optional[torch.Tensor]:\n+def prepare_padding_mask(attention_mask: torch.Tensor | None, kv_length: int, kv_offset: int) -> torch.Tensor | None:\n     \"\"\"\n     From the 2D attention mask, prepare the correct padding mask to use by potentially padding it.\n     \"\"\"\n@@ -178,10 +174,10 @@ def prepare_padding_mask(\n \n \n def _can_skip_causal_mask_xpu(\n-    padding_mask: Optional[torch.Tensor],\n+    padding_mask: torch.Tensor | None,\n     query_length: int,\n     kv_length: int,\n-    local_attention_size: Optional[int],\n+    local_attention_size: int | None,\n ) -> bool:\n     \"\"\"\n     XPU-specific logic for determining if we can skip causal mask creation.\n@@ -214,11 +210,11 @@ def _can_skip_causal_mask_xpu(\n \n \n def _ignore_causal_mask_sdpa(\n-    padding_mask: Optional[torch.Tensor],\n+    padding_mask: torch.Tensor | None,\n     query_length: int,\n     kv_length: int,\n     kv_offset: int,\n-    local_attention_size: Optional[int] = None,\n+    local_attention_size: int | None = None,\n ) -> bool:\n     \"\"\"\n     Detects whether the causal mask can be ignored in case PyTorch's SDPA is used, rather relying on SDPA's `is_causal` argument.\n@@ -257,7 +253,7 @@ def _ignore_causal_mask_sdpa(\n     return False\n \n \n-def _ignore_bidirectional_mask_sdpa(padding_mask: Optional[torch.Tensor]) -> bool:\n+def _ignore_bidirectional_mask_sdpa(padding_mask: torch.Tensor | None) -> bool:\n     \"\"\"\n     Detects whether the bidirectional mask can be ignored in case PyTorch's SDPA is used, i.e. when there is full\n     attention with no padding.\n@@ -308,14 +304,14 @@ def sdpa_mask(\n     kv_length: int,\n     kv_offset: int = 0,\n     mask_function: Callable = causal_mask_function,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    local_size: Optional[int] = None,\n+    attention_mask: torch.Tensor | None = None,\n+    local_size: int | None = None,\n     allow_is_causal_skip: bool = True,\n     allow_is_bidirectional_skip: bool = False,\n     allow_torch_fix: bool = True,\n     use_vmap: bool = False,\n     **kwargs,\n-) -> Optional[torch.Tensor]:\n+) -> torch.Tensor | None:\n     \"\"\"\n     Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n     the element should take part in the attention computation, and False that it should not.\n@@ -475,7 +471,7 @@ def eager_mask(\n     kv_length: int,\n     kv_offset: int = 0,\n     mask_function: Callable = causal_mask_function,\n-    attention_mask: Optional[torch.Tensor] = None,\n+    attention_mask: torch.Tensor | None = None,\n     dtype: torch.dtype = torch.float32,\n     allow_is_bidirectional_skip: bool = False,\n     use_vmap: bool = False,\n@@ -538,7 +534,7 @@ def flash_attention_mask(\n     kv_length: int,\n     kv_offset: int = 0,\n     mask_function: Callable = causal_mask_function,\n-    attention_mask: Optional[torch.Tensor] = None,\n+    attention_mask: torch.Tensor | None = None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -577,7 +573,7 @@ def flex_attention_mask(\n     kv_length: int,\n     kv_offset: int = 0,\n     mask_function: Callable = causal_mask_function,\n-    attention_mask: Optional[torch.Tensor] = None,\n+    attention_mask: torch.Tensor | None = None,\n     **kwargs,\n ) -> BlockMask:\n     \"\"\"\n@@ -644,7 +640,7 @@ class AttentionMaskInterface(GeneralInterface):\n ALL_MASK_ATTENTION_FUNCTIONS: AttentionMaskInterface = AttentionMaskInterface()\n \n \n-def find_packed_sequence_indices(position_ids: torch.Tensor) -> Optional[torch.Tensor]:\n+def find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor | None:\n     \"\"\"\n     Find the indices of the sequence to which each new query token in the sequence belongs when using packed\n     tensor format (i.e. several sequences packed in the same batch dimension).\n@@ -679,12 +675,12 @@ def find_packed_sequence_indices(position_ids: torch.Tensor) -> Optional[torch.T\n def _preprocess_mask_arguments(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[Union[torch.Tensor, BlockMask]],\n+    attention_mask: torch.Tensor | BlockMask | None,\n     cache_position: torch.Tensor,\n-    past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor],\n-    layer_idx: Optional[int],\n-) -> tuple[bool, Optional[Union[torch.Tensor, BlockMask]], int, int]:\n+    past_key_values: Cache | None,\n+    position_ids: torch.Tensor | None,\n+    layer_idx: int | None,\n+) -> tuple[bool, torch.Tensor | BlockMask | None, int, int]:\n     \"\"\"\n     Perform some common pre-processing of the mask arguments we get from the modeling code. Mostly determine the\n     key-value length and offsets, and if we should early exit or not.\n@@ -770,13 +766,13 @@ def _preprocess_mask_arguments(\n def create_causal_mask(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     cache_position: torch.Tensor,\n-    past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor] = None,\n-    or_mask_function: Optional[Callable] = None,\n-    and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, BlockMask]]:\n+    past_key_values: Cache | None,\n+    position_ids: torch.Tensor | None = None,\n+    or_mask_function: Callable | None = None,\n+    and_mask_function: Callable | None = None,\n+) -> torch.Tensor | BlockMask | None:\n     \"\"\"\n     Create a standard causal mask based on the attention implementation used (stored in the config). If `past_key_values`\n     has an hybrid cache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n@@ -873,11 +869,11 @@ def create_causal_mask(\n def create_bidirectional_mask(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    encoder_hidden_states: Optional[torch.Tensor] = None,\n-    or_mask_function: Optional[Callable] = None,\n-    and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, BlockMask]]:\n+    attention_mask: torch.Tensor | None,\n+    encoder_hidden_states: torch.Tensor | None = None,\n+    or_mask_function: Callable | None = None,\n+    and_mask_function: Callable | None = None,\n+) -> torch.Tensor | BlockMask | None:\n     \"\"\"\n     Create a standard bidirectional mask based on the attention implementation used (stored in the config).\n \n@@ -961,13 +957,13 @@ def create_bidirectional_mask(\n def create_sliding_window_causal_mask(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     cache_position: torch.Tensor,\n-    past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor] = None,\n-    or_mask_function: Optional[Callable] = None,\n-    and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, BlockMask]]:\n+    past_key_values: Cache | None,\n+    position_ids: torch.Tensor | None = None,\n+    or_mask_function: Callable | None = None,\n+    and_mask_function: Callable | None = None,\n+) -> torch.Tensor | BlockMask | None:\n     \"\"\"\n     Create a sliding window causal mask based on the attention implementation used (stored in the config). This type\n     of attention pattern was mostly democratized by Mistral. If `past_key_values` has an hybrid cache structure, this\n@@ -1065,13 +1061,13 @@ def create_sliding_window_causal_mask(\n def create_chunked_causal_mask(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     cache_position: torch.Tensor,\n-    past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor] = None,\n-    or_mask_function: Optional[Callable] = None,\n-    and_mask_function: Optional[Callable] = None,\n-) -> Optional[Union[torch.Tensor, BlockMask]]:\n+    past_key_values: Cache | None,\n+    position_ids: torch.Tensor | None = None,\n+    or_mask_function: Callable | None = None,\n+    and_mask_function: Callable | None = None,\n+) -> torch.Tensor | BlockMask | None:\n     \"\"\"\n     Create a chunked attention causal mask based on the attention implementation used (stored in the config). This type\n     of attention pattern was mostly democratized by Llama4. If `past_key_values` has an hybrid cache structure, this\n@@ -1190,12 +1186,12 @@ def create_chunked_causal_mask(\n def create_masks_for_generate(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     cache_position: torch.Tensor,\n-    past_key_values: Optional[Cache],\n-    position_ids: Optional[torch.Tensor] = None,\n-    or_mask_function: Optional[Callable] = None,\n-    and_mask_function: Optional[Callable] = None,\n+    past_key_values: Cache | None,\n+    position_ids: torch.Tensor | None = None,\n+    or_mask_function: Callable | None = None,\n+    and_mask_function: Callable | None = None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -1375,7 +1371,7 @@ def __str__(self):\n         return self.to_string()\n \n     @classmethod\n-    def from_tensor(cls, tensor: torch.Tensor, style: Optional[str] = None) -> \"AttentionMask\":\n+    def from_tensor(cls, tensor: torch.Tensor, style: str | None = None) -> \"AttentionMask\":\n         res = cls(tensor)\n         res.style = style\n         return res"
        },
        {
            "sha": "5b230836c6e7ec63a83b276e3ced23e30e6dd300",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,6 @@\n import re\n from contextlib import contextmanager, redirect_stdout\n from io import StringIO\n-from typing import Optional\n \n from .utils import logging\n from .utils.import_utils import is_torch_available, requires\n@@ -68,7 +67,7 @@ def _dtensor_repr(x):\n \n \n def _serialize_tensor_like_io(\n-    value, debug_path: Optional[str] = None, use_repr: bool = True, path_to_value: Optional[str] = None\n+    value, debug_path: str | None = None, use_repr: bool = True, path_to_value: str | None = None\n ):\n     \"\"\"\n     Converts Tensors and DTensors to a JSON-serializable dictionary representation.\n@@ -116,7 +115,7 @@ def _serialize_tensor_like_io(\n     return out\n \n \n-def _serialize_io(value, debug_path: Optional[str] = None, use_repr: bool = True, path_to_value: Optional[str] = None):\n+def _serialize_io(value, debug_path: str | None = None, use_repr: bool = True, path_to_value: str | None = None):\n     \"\"\"\n     Recursively build a JSON-serializable Python structure from `value`.\n     Tensors and DTensors become either sanitized repr strings, or are saved to disk as SafeTensors files and their\n@@ -225,7 +224,7 @@ def prune_intermediate_layers(node):\n         prune_intermediate_layers(child)\n \n \n-def log_model_debug_trace(debug_path: Optional[str], model):\n+def log_model_debug_trace(debug_path: str | None, model):\n     if debug_path:\n         try:\n             os.makedirs(debug_path, exist_ok=True)\n@@ -399,7 +398,7 @@ def top_wrapped_forward(*inps, **kws):\n @contextmanager\n def model_addition_debugger_context(\n     model,\n-    debug_path: Optional[str] = None,\n+    debug_path: str | None = None,\n     do_prune_layers: bool = True,\n     use_repr: bool = True,\n ):"
        },
        {
            "sha": "256e4d932a8d8b2053b125e76f84009c0dfe964a",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -19,7 +19,7 @@\n import warnings\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import httpx\n import yaml\n@@ -352,19 +352,19 @@ def _get_mapping_values(mapping):\n @dataclass\n class TrainingSummary:\n     model_name: str\n-    language: Optional[Union[str, list[str]]] = None\n-    license: Optional[str] = None\n-    tags: Optional[Union[str, list[str]]] = None\n-    finetuned_from: Optional[str] = None\n-    tasks: Optional[Union[str, list[str]]] = None\n-    dataset: Optional[Union[str, list[str]]] = None\n-    dataset_tags: Optional[Union[str, list[str]]] = None\n-    dataset_args: Optional[Union[str, list[str]]] = None\n-    dataset_metadata: Optional[dict[str, Any]] = None\n-    eval_results: Optional[dict[str, float]] = None\n-    eval_lines: Optional[list[str]] = None\n-    hyperparameters: Optional[dict[str, Any]] = None\n-    source: Optional[str] = \"trainer\"\n+    language: str | list[str] | None = None\n+    license: str | None = None\n+    tags: str | list[str] | None = None\n+    finetuned_from: str | None = None\n+    tasks: str | list[str] | None = None\n+    dataset: str | list[str] | None = None\n+    dataset_tags: str | list[str] | None = None\n+    dataset_args: str | list[str] | None = None\n+    dataset_metadata: dict[str, Any] | None = None\n+    eval_results: dict[str, float] | None = None\n+    eval_lines: list[str] | None = None\n+    hyperparameters: dict[str, Any] | None = None\n+    source: str | None = \"trainer\"\n \n     def __post_init__(self):\n         # Infer default license from the checkpoint used, if possible."
        },
        {
            "sha": "8eeb3eccd183e29b6d1d58ed232bd6411dd42d79",
            "filename": "src/transformers/modeling_attn_mask_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_attn_mask_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -18,7 +18,7 @@\n \"\"\"\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Union\n \n import torch\n \n@@ -60,7 +60,7 @@ class AttentionMaskConverter:\n     is_causal: bool\n     sliding_window: int\n \n-    def __init__(self, is_causal: bool, sliding_window: Optional[int] = None):\n+    def __init__(self, is_causal: bool, sliding_window: int | None = None):\n         self.is_causal = is_causal\n         self.sliding_window = sliding_window\n \n@@ -76,7 +76,7 @@ def to_causal_4d(\n         key_value_length: int,\n         dtype: torch.dtype,\n         device: Union[torch.device, \"str\"] = \"cpu\",\n-    ) -> Optional[torch.Tensor]:\n+    ) -> torch.Tensor | None:\n         \"\"\"\n         Creates a causal 4D mask of (bsz, head_dim=1, query_length, key_value_length) shape and adds large negative\n         bias to upper right hand triangular matrix (causal mask).\n@@ -107,7 +107,7 @@ def to_4d(\n         attention_mask_2d: torch.Tensor,\n         query_length: int,\n         dtype: torch.dtype,\n-        key_value_length: Optional[int] = None,\n+        key_value_length: int | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Converts 2D attention mask to 4D attention mask by expanding mask to (bsz, head_dim=1, query_length,\n@@ -155,7 +155,7 @@ def _make_causal_mask(\n         dtype: torch.dtype,\n         device: torch.device,\n         past_key_values_length: int = 0,\n-        sliding_window: Optional[int] = None,\n+        sliding_window: int | None = None,\n     ):\n         \"\"\"\n         Make causal mask used for bi-directional self-attention.\n@@ -184,7 +184,7 @@ def _make_causal_mask(\n         return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n \n     @staticmethod\n-    def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n+    def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: int | None = None):\n         \"\"\"\n         Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n         \"\"\"\n@@ -248,10 +248,10 @@ def _unmask_unattended(\n \n     @staticmethod\n     def _ignore_causal_mask_sdpa(\n-        attention_mask: Optional[torch.Tensor],\n+        attention_mask: torch.Tensor | None,\n         inputs_embeds: torch.Tensor,\n         past_key_values_length: int,\n-        sliding_window: Optional[int] = None,\n+        sliding_window: int | None = None,\n         is_training: bool = False,\n     ) -> bool:\n         \"\"\"\n@@ -306,11 +306,11 @@ def _ignore_causal_mask_sdpa(\n \n \n def _prepare_4d_causal_attention_mask(\n-    attention_mask: Optional[torch.Tensor],\n-    input_shape: Union[torch.Size, tuple, list],\n+    attention_mask: torch.Tensor | None,\n+    input_shape: torch.Size | tuple | list,\n     inputs_embeds: torch.Tensor,\n     past_key_values_length: int,\n-    sliding_window: Optional[int] = None,\n+    sliding_window: int | None = None,\n ):\n     \"\"\"\n     Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n@@ -359,11 +359,11 @@ def _prepare_4d_causal_attention_mask(\n \n # Adapted from _prepare_4d_causal_attention_mask\n def _prepare_4d_causal_attention_mask_for_sdpa(\n-    attention_mask: Optional[torch.Tensor],\n-    input_shape: Union[torch.Size, tuple, list],\n+    attention_mask: torch.Tensor | None,\n+    input_shape: torch.Size | tuple | list,\n     inputs_embeds: torch.Tensor,\n     past_key_values_length: int,\n-    sliding_window: Optional[int] = None,\n+    sliding_window: int | None = None,\n ):\n     \"\"\"\n     Prepares the correct `attn_mask` argument to be used by `torch.nn.functional.scaled_dot_product_attention`.\n@@ -416,7 +416,7 @@ def _prepare_4d_causal_attention_mask_for_sdpa(\n     return expanded_4d_mask\n \n \n-def _prepare_4d_attention_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n+def _prepare_4d_attention_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: int | None = None):\n     \"\"\"\n     Creates a non-causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n     `(batch_size, key_value_length)`\n@@ -432,7 +432,7 @@ def _prepare_4d_attention_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len:\n     return AttentionMaskConverter._expand_mask(mask=mask, dtype=dtype, tgt_len=tgt_len)\n \n \n-def _prepare_4d_attention_mask_for_sdpa(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n+def _prepare_4d_attention_mask_for_sdpa(mask: torch.Tensor, dtype: torch.dtype, tgt_len: int | None = None):\n     \"\"\"\n     Creates a non-causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n     `(batch_size, key_value_length)`\n@@ -456,12 +456,12 @@ def _prepare_4d_attention_mask_for_sdpa(mask: torch.Tensor, dtype: torch.dtype,\n \n \n def _create_4d_causal_attention_mask(\n-    input_shape: Union[torch.Size, tuple, list],\n+    input_shape: torch.Size | tuple | list,\n     dtype: torch.dtype,\n     device: torch.device,\n     past_key_values_length: int = 0,\n-    sliding_window: Optional[int] = None,\n-) -> Optional[torch.Tensor]:\n+    sliding_window: int | None = None,\n+) -> torch.Tensor | None:\n     \"\"\"\n     Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)`\n "
        },
        {
            "sha": "b5f59b4bb1f90b928c13dd98a195c27f50db6739",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -15,7 +15,7 @@\n import os\n from collections.abc import Callable\n from functools import partial\n-from typing import Optional, TypedDict\n+from typing import TypedDict\n \n import torch\n import torch.nn.functional as F\n@@ -71,7 +71,7 @@ def is_flash_attn_available():\n }\n \n \n-def _lazy_imports(implementation: Optional[str], attention_wrapper: Optional[Callable] = None):\n+def _lazy_imports(implementation: str | None, attention_wrapper: Callable | None = None):\n     \"\"\"\n     Lazy loads the respective flash attention implementations.\n \n@@ -147,7 +147,7 @@ def _lazy_define_process_function(flash_function):\n     return partial(_process_flash_attention_kwargs, supports_mapping=supports_mapping)\n \n \n-def lazy_import_flash_attention(implementation: Optional[str], attention_wrapper: Optional[Callable] = None):\n+def lazy_import_flash_attention(implementation: str | None, attention_wrapper: Callable | None = None):\n     \"\"\"\n     Lazily import flash attention and return the respective functions + flags.\n \n@@ -168,7 +168,7 @@ def lazy_import_flash_attention(implementation: Optional[str], attention_wrapper\n     return (_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn), _process_flash_kwargs_fn\n \n \n-def lazy_import_paged_flash_attention(implementation: Optional[str]):\n+def lazy_import_paged_flash_attention(implementation: str | None):\n     \"\"\"\n     Same as `lazy_import_flash_attention` but explicitly wrapping it with the paged implementation.\n     \"\"\"\n@@ -453,7 +453,7 @@ def fa_peft_integration_check(\n     q: torch.Tensor,\n     k: torch.Tensor,\n     v: torch.Tensor,\n-    target_dtype: Optional[torch.dtype] = None,\n+    target_dtype: torch.dtype | None = None,\n ):\n     \"\"\"\n     PEFT usually casts the layer norms in float32 for training stability reasons\n@@ -482,24 +482,24 @@ class FlashAttentionKwargs(TypedDict, total=False):\n             Maximum sequence length for key state.\n     \"\"\"\n \n-    cu_seq_lens_q: Optional[torch.LongTensor]\n-    cu_seq_lens_k: Optional[torch.LongTensor]\n-    max_length_q: Optional[int]\n-    max_length_k: Optional[int]\n+    cu_seq_lens_q: torch.LongTensor | None\n+    cu_seq_lens_k: torch.LongTensor | None\n+    max_length_q: int | None\n+    max_length_k: int | None\n \n \n def _process_flash_attention_kwargs(\n     query_length: int,\n     key_length: int,\n     is_causal: bool,\n     dropout: float = 0.0,\n-    softmax_scale: Optional[float] = None,\n-    sliding_window: Optional[int] = None,\n+    softmax_scale: float | None = None,\n+    sliding_window: int | None = None,\n     use_top_left_mask: bool = False,\n-    softcap: Optional[float] = None,\n-    deterministic: Optional[bool] = None,\n-    s_aux: Optional[torch.Tensor] = None,\n-    supports_mapping: Optional[dict[str, bool]] = None,\n+    softcap: float | None = None,\n+    deterministic: bool | None = None,\n+    s_aux: torch.Tensor | None = None,\n+    supports_mapping: dict[str, bool] | None = None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -568,22 +568,22 @@ def _flash_attention_forward(\n     query_states: torch.Tensor,\n     key_states: torch.Tensor,\n     value_states: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     query_length: int,\n     is_causal: bool,\n     dropout: float = 0.0,\n-    position_ids: Optional[torch.Tensor] = None,\n-    softmax_scale: Optional[float] = None,\n-    sliding_window: Optional[int] = None,\n+    position_ids: torch.Tensor | None = None,\n+    softmax_scale: float | None = None,\n+    sliding_window: int | None = None,\n     use_top_left_mask: bool = False,\n-    softcap: Optional[float] = None,\n-    deterministic: Optional[bool] = None,\n-    cu_seq_lens_q: Optional[torch.LongTensor] = None,\n-    cu_seq_lens_k: Optional[torch.LongTensor] = None,\n-    max_length_q: Optional[int] = None,\n-    max_length_k: Optional[int] = None,\n-    target_dtype: Optional[torch.dtype] = None,\n-    attn_implementation: Optional[str] = None,\n+    softcap: float | None = None,\n+    deterministic: bool | None = None,\n+    cu_seq_lens_q: torch.LongTensor | None = None,\n+    cu_seq_lens_k: torch.LongTensor | None = None,\n+    max_length_q: int | None = None,\n+    max_length_k: int | None = None,\n+    target_dtype: torch.dtype | None = None,\n+    attn_implementation: str | None = None,\n     **kwargs,\n ):\n     \"\"\""
        },
        {
            "sha": "ca960a9564b579f6c097be5cbd8057f34fdc1648",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n import re\n-from typing import NamedTuple, Optional\n+from typing import NamedTuple\n \n import numpy as np\n from tqdm.auto import tqdm\n@@ -103,7 +103,7 @@ def process(self, weights, name, **kwargs):\n         return GGUFTensor(weights, name, {})\n \n     def _reverse_permute_weights(\n-        self, weights: np.ndarray, n_head: int, num_kv_heads: Optional[int] = None\n+        self, weights: np.ndarray, n_head: int, num_kv_heads: int | None = None\n     ) -> np.ndarray:\n         # Original permutation implementation\n         # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L1402-L1408\n@@ -326,8 +326,8 @@ def read_field(reader, field):\n def get_gguf_hf_weights_map(\n     hf_model,\n     processor: TensorProcessor,\n-    model_type: Optional[str] = None,\n-    num_layers: Optional[int] = None,\n+    model_type: str | None = None,\n+    num_layers: int | None = None,\n     qual_name: str = \"\",\n ):\n     \"\"\"\n@@ -481,7 +481,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     # tie_word_embeddings is true otherwise false\n     exceptions = [\"falcon\", \"bloom\"]\n     parsed_parameters[\"config\"][\"tie_word_embeddings\"] = (\n-        all(\"output.weight\" != tensor.name for tensor in reader.tensors) or architecture in exceptions\n+        all(tensor.name != \"output.weight\" for tensor in reader.tensors) or architecture in exceptions\n     )\n \n     # Set GGUF-specific default values"
        },
        {
            "sha": "735212b7647153eb161603e5a326591eb60bd8c2",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from functools import partial\n-from typing import Optional\n \n import torch\n import torch.nn as nn\n@@ -112,13 +111,13 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> SequenceClassifierOutputWithPast:\n         transformer_outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)(\n@@ -192,13 +191,13 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> QuestionAnsweringModelOutput:\n         outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)(\n@@ -255,13 +254,13 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> TokenClassifierOutput:\n         outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)("
        },
        {
            "sha": "34a036b427232448f4837325fe7f3b5cf4ffbed4",
            "filename": "src/transformers/modeling_outputs.py",
            "status": "modified",
            "additions": 242,
            "deletions": 243,
            "changes": 485,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_outputs.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -14,7 +14,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional\n \n import torch\n \n@@ -43,9 +42,9 @@ class BaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -63,8 +62,8 @@ class BaseModelOutputWithNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -93,10 +92,10 @@ class BaseModelOutputWithPooling(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -116,9 +115,9 @@ class BaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -151,10 +150,10 @@ class BaseModelOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -184,10 +183,10 @@ class BaseModelOutputWithCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -228,12 +227,12 @@ class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n             input) to speed up sequential decoding.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    past_key_values: Optional[Cache] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    past_key_values: Cache | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -272,11 +271,11 @@ class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -317,14 +316,14 @@ class MoECausalLMOutputWithPast(ModelOutput):\n             modules.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    z_loss: Optional[torch.FloatTensor] = None\n-    aux_loss: Optional[torch.FloatTensor] = None\n-    router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    z_loss: torch.FloatTensor | None = None\n+    aux_loss: torch.FloatTensor | None = None\n+    router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -353,11 +352,11 @@ class MoEModelOutput(ModelOutput):\n             loss and the z_loss for Mixture of Experts models.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    router_probs: Optional[tuple[torch.FloatTensor]] = None\n-    router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    router_probs: tuple[torch.FloatTensor] | None = None\n+    router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -392,11 +391,11 @@ class MoeModelOutputWithPast(ModelOutput):\n             loss for Mixture of Experts models.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -438,13 +437,13 @@ class MoeCausalLMOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    aux_loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    aux_loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -489,13 +488,13 @@ class MoEModelOutputWithPastAndCrossAttentions(ModelOutput):\n             loss and the z_loss for Mixture of Experts models.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    router_probs: Optional[tuple[torch.FloatTensor]] = None\n-    router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    router_probs: tuple[torch.FloatTensor] | None = None\n+    router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -547,14 +546,14 @@ class Seq2SeqModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -615,16 +614,16 @@ class Seq2SeqMoEModelOutput(ModelOutput):\n             modules.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_router_logits: tuple[torch.FloatTensor] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -650,10 +649,10 @@ class CausalLMOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -684,11 +683,11 @@ class CausalLMOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -725,12 +724,12 @@ class CausalLMOutputWithCrossAttentions(ModelOutput):\n             `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -761,11 +760,11 @@ class SequenceClassifierOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -791,10 +790,10 @@ class MaskedLMOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -844,15 +843,15 @@ class Seq2SeqLMOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -911,21 +910,21 @@ class Seq2SeqMoEOutput(ModelOutput):\n             models.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    encoder_z_loss: Optional[torch.FloatTensor] = None\n-    decoder_z_loss: Optional[torch.FloatTensor] = None\n-    encoder_aux_loss: Optional[torch.FloatTensor] = None\n-    decoder_aux_loss: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    encoder_z_loss: torch.FloatTensor | None = None\n+    decoder_z_loss: torch.FloatTensor | None = None\n+    encoder_aux_loss: torch.FloatTensor | None = None\n+    decoder_aux_loss: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_router_logits: tuple[torch.FloatTensor] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_router_logits: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -952,10 +951,10 @@ class NextSentencePredictorOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -981,10 +980,10 @@ class SequenceClassifierOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1034,15 +1033,15 @@ class Seq2SeqSequenceClassifierOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1070,10 +1069,10 @@ class MultipleChoiceModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1099,10 +1098,10 @@ class TokenClassifierOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1130,11 +1129,11 @@ class QuestionAnsweringModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    start_logits: Optional[torch.FloatTensor] = None\n-    end_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    start_logits: torch.FloatTensor | None = None\n+    end_logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1186,16 +1185,16 @@ class Seq2SeqQuestionAnsweringModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    start_logits: Optional[torch.FloatTensor] = None\n-    end_logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    start_logits: torch.FloatTensor | None = None\n+    end_logits: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1230,10 +1229,10 @@ class SemanticSegmenterOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1258,10 +1257,10 @@ class ImageClassifierOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1280,9 +1279,9 @@ class ImageClassifierOutputWithNoAttention(ModelOutput):\n             called feature maps) of the model at the output of each stage.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1309,10 +1308,10 @@ class DepthEstimatorOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    predicted_depth: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    predicted_depth: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1337,10 +1336,10 @@ class ImageSuperResolutionOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    reconstruction: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    reconstruction: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1366,10 +1365,10 @@ class Wav2Vec2BaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    extract_features: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    extract_features: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1397,11 +1396,11 @@ class XVectorOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    embeddings: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    embeddings: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1426,9 +1425,9 @@ class BackboneOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    feature_maps: Optional[tuple[torch.FloatTensor]] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    feature_maps: tuple[torch.FloatTensor] | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1461,11 +1460,11 @@ class BaseModelOutputWithPoolingAndProjection(ModelOutput):\n             Text embeddings before the projection layer, used to mimic the last hidden state of the teacher encoder.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    projection_state: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    projection_state: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -1515,15 +1514,15 @@ class Seq2SeqSpectrogramOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    spectrogram: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    spectrogram: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -1583,17 +1582,17 @@ class Seq2SeqTSModelOutput(ModelOutput):\n             Static features of each time series' in a batch which are copied to the covariates at inference time.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    loc: Optional[torch.FloatTensor] = None\n-    scale: Optional[torch.FloatTensor] = None\n-    static_features: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    loc: torch.FloatTensor | None = None\n+    scale: torch.FloatTensor | None = None\n+    static_features: torch.FloatTensor | None = None\n \n \n @dataclass\n@@ -1652,18 +1651,18 @@ class Seq2SeqTSPredictionOutput(ModelOutput):\n             Static features of each time series' in a batch which are copied to the covariates at inference time.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    params: Optional[tuple[torch.FloatTensor, ...]] = None\n-    past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    loc: Optional[torch.FloatTensor] = None\n-    scale: Optional[torch.FloatTensor] = None\n-    static_features: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    params: tuple[torch.FloatTensor, ...] | None = None\n+    past_key_values: EncoderDecoderCache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    cross_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor, ...] | None = None\n+    loc: torch.FloatTensor | None = None\n+    scale: torch.FloatTensor | None = None\n+    static_features: torch.FloatTensor | None = None\n \n \n @dataclass\n@@ -1677,7 +1676,7 @@ class SampleTSPredictionOutput(ModelOutput):\n             Sampled values from the chosen distribution.\n     \"\"\"\n \n-    sequences: Optional[torch.FloatTensor] = None\n+    sequences: torch.FloatTensor | None = None\n \n \n @dataclass\n@@ -1702,10 +1701,10 @@ class MaskedImageModelingOutput(ModelOutput):\n             the self-attention heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    reconstruction: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    reconstruction: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n     @property\n     def logits(self):"
        },
        {
            "sha": "c873e61c8eac9859fd37382e8abde228bdb0a727",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -132,8 +132,8 @@ def wrapper(self, x, position_ids, layer_type=None):\n def _compute_linear_scaling_rope_parameters(\n     config: Optional[\"PreTrainedConfig\"] = None,\n     device: Optional[\"torch.device\"] = None,\n-    seq_len: Optional[int] = None,\n-    layer_type: Optional[str] = None,\n+    seq_len: int | None = None,\n+    layer_type: str | None = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n@@ -186,8 +186,8 @@ def _compute_linear_scaling_rope_parameters(\n def _compute_dynamic_ntk_parameters(\n     config: Optional[\"PreTrainedConfig\"] = None,\n     device: Optional[\"torch.device\"] = None,\n-    seq_len: Optional[int] = None,\n-    layer_type: Optional[str] = None,\n+    seq_len: int | None = None,\n+    layer_type: str | None = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n@@ -256,8 +256,8 @@ def _compute_dynamic_ntk_parameters(\n def _compute_yarn_parameters(\n     config: \"PreTrainedConfig\",\n     device: Optional[\"torch.device\"] = None,\n-    seq_len: Optional[int] = None,\n-    layer_type: Optional[str] = None,\n+    seq_len: int | None = None,\n+    layer_type: str | None = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Please refer to the\n@@ -391,8 +391,8 @@ def linear_ramp_factor(min, max, dim):\n def _compute_longrope_parameters(\n     config: \"PreTrainedConfig\",\n     device: Optional[\"torch.device\"] = None,\n-    seq_len: Optional[int] = None,\n-    layer_type: Optional[str] = None,\n+    seq_len: int | None = None,\n+    layer_type: str | None = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n@@ -479,8 +479,8 @@ def _compute_longrope_parameters(\n def _compute_llama3_parameters(\n     config: \"PreTrainedConfig\",\n     device: Optional[\"torch.device\"] = None,\n-    seq_len: Optional[int] = None,\n-    layer_type: Optional[str] = None,\n+    seq_len: int | None = None,\n+    layer_type: str | None = None,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies for llama 3.1.\n@@ -609,17 +609,17 @@ class RopeParameters(TypedDict, total=False):\n     \"\"\"\n \n     rope_theta: float\n-    rope_type: Optional[str]\n-    partial_rotary_factor: Optional[float]\n-    factor: Optional[float]\n-    original_max_position_embeddings: Optional[int]\n-    attention_factor: Optional[float]\n-    beta_fast: Optional[float]\n-    beta_slow: Optional[float]\n-    short_factor: Optional[list[float]]\n-    long_factor: Optional[list[float]]\n-    low_freq_factor: Optional[float]\n-    high_freq_factor: Optional[float]\n+    rope_type: str | None\n+    partial_rotary_factor: float | None\n+    factor: float | None\n+    original_max_position_embeddings: int | None\n+    attention_factor: float | None\n+    beta_fast: float | None\n+    beta_slow: float | None\n+    short_factor: list[float] | None\n+    long_factor: list[float] | None\n+    low_freq_factor: float | None\n+    high_freq_factor: float | None\n \n \n class RotaryEmbeddingConfigMixin:\n@@ -629,7 +629,7 @@ class RotaryEmbeddingConfigMixin:\n \n     default_theta = 10_000.0\n \n-    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: Optional[set] = None, **kwargs):\n+    def convert_rope_params_to_dict(self, ignore_keys_at_rope_validation: set | None = None, **kwargs):\n         rope_scaling = kwargs.pop(\"rope_scaling\", None)\n         self.rope_parameters = rope_scaling or self.rope_parameters\n         self.rope_parameters = self.rope_parameters if self.rope_parameters is not None else {}\n@@ -693,7 +693,7 @@ def standardize_rope_params(self):\n \n         self.rope_parameters = rope_parameters\n \n-    def validate_rope(self: \"PreTrainedConfig\", ignore_keys: Optional[set] = None):\n+    def validate_rope(self: \"PreTrainedConfig\", ignore_keys: set | None = None):\n         \"\"\"\n         Validate the RoPE config arguments, given a `\"PreTrainedConfig\"` object\n         \"\"\"\n@@ -720,13 +720,13 @@ def validate_rope(self: \"PreTrainedConfig\", ignore_keys: Optional[set] = None):\n                     f\"Missing validation function in 'RotaryEmbeddingConfigMixin' for 'rope_type'='{rope_type}'\"\n                 )\n \n-    def _validate_default_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+    def _validate_default_rope_parameters(self, rope_parameters: dict, ignore_keys: set | None = None):\n         required_keys = {\"rope_type\", \"rope_theta\"}\n         received_keys = set(rope_parameters.keys())\n         rope_type = rope_parameters[\"rope_type\"]\n         self._check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n \n-    def _validate_linear_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+    def _validate_linear_rope_parameters(self, rope_parameters: dict, ignore_keys: set | None = None):\n         required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n         received_keys = set(rope_parameters.keys())\n         rope_type = rope_parameters[\"rope_type\"]\n@@ -736,7 +736,7 @@ def _validate_linear_rope_parameters(self, rope_parameters: dict, ignore_keys: O\n         if factor is None or not isinstance(factor, float) or factor < 1.0:\n             logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n-    def _validate_dynamic_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+    def _validate_dynamic_rope_parameters(self, rope_parameters: dict, ignore_keys: set | None = None):\n         required_keys = {\"rope_type\", \"factor\"}\n         received_keys = set(rope_parameters.keys())\n         rope_type = rope_parameters[\"rope_type\"]\n@@ -746,7 +746,7 @@ def _validate_dynamic_rope_parameters(self, rope_parameters: dict, ignore_keys:\n         if factor is None or not isinstance(factor, float) or factor < 1.0:\n             logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n \n-    def _validate_yarn_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+    def _validate_yarn_rope_parameters(self, rope_parameters: dict, ignore_keys: set | None = None):\n         required_keys = {\"rope_type\", \"factor\", \"rope_theta\", \"original_max_position_embeddings\"}\n         optional_keys = {\n             \"attention_factor\",\n@@ -797,7 +797,7 @@ def _validate_yarn_rope_parameters(self, rope_parameters: dict, ignore_keys: Opt\n                 \"behaviour in model usage, please correct the 'original_max_position_embeddings' fields in the model config.\"\n             )\n \n-    def _validate_longrope_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+    def _validate_longrope_rope_parameters(self, rope_parameters: dict, ignore_keys: set | None = None):\n         required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\", \"original_max_position_embeddings\"}\n         optional_keys = {\"attention_factor\", \"factor\"}\n         received_keys = set(rope_parameters.keys())\n@@ -847,7 +847,7 @@ def _validate_longrope_rope_parameters(self, rope_parameters: dict, ignore_keys:\n                 f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n             )\n \n-    def _validate_llama3_rope_parameters(self, rope_parameters: dict, ignore_keys: Optional[set] = None):\n+    def _validate_llama3_rope_parameters(self, rope_parameters: dict, ignore_keys: set | None = None):\n         required_keys = {\n             \"rope_type\",\n             \"factor\",\n@@ -893,8 +893,8 @@ def _check_received_keys(\n         rope_type: str,\n         received_keys: set,\n         required_keys: set,\n-        optional_keys: Optional[set] = None,\n-        ignore_keys: Optional[set] = None,\n+        optional_keys: set | None = None,\n+        ignore_keys: set | None = None,\n     ):\n         \"\"\"Compare the received keys in `config.rope_parameters` against the expected and optional keys\"\"\"\n         # BC: \"rope_type\" was originally \"type\" -- let's check for \"rope_type\" when \"type\" is present\n@@ -919,7 +919,7 @@ def _check_received_keys(\n             logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")\n \n \n-def rope_config_validation(config: RotaryEmbeddingConfigMixin, ignore_keys: Optional[set] = None):\n+def rope_config_validation(config: RotaryEmbeddingConfigMixin, ignore_keys: set | None = None):\n     \"\"\"\n     This is a deprecated function.\n     It has been kept for backward compatibility with custom code models."
        },
        {
            "sha": "f97b09d053922d009bc11685a66e474cdc592ab3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 57,
            "deletions": 58,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors, Facebook AI Research authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -31,7 +30,7 @@\n from functools import partial, wraps\n from itertools import cycle\n from threading import Thread\n-from typing import Optional, TypeVar, Union, get_type_hints\n+from typing import Optional, TypeVar, get_type_hints\n from zipfile import is_zipfile\n \n import torch\n@@ -271,7 +270,7 @@ def get_state_dict_dtype(state_dict):\n \n \n def load_state_dict(\n-    checkpoint_file: Union[str, os.PathLike], map_location: Union[str, torch.device] = \"cpu\", weights_only: bool = True\n+    checkpoint_file: str | os.PathLike, map_location: str | torch.device = \"cpu\", weights_only: bool = True\n ) -> dict[str, torch.Tensor]:\n     \"\"\"\n     Reads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\n@@ -461,23 +460,23 @@ def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor\n     setattr(parent, param_type, tensor)\n \n \n-def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n+def _add_variant(weights_name: str, variant: str | None = None) -> str:\n     if variant is not None:\n         path, name = weights_name.rsplit(\".\", 1)\n         weights_name = f\"{path}.{variant}.{name}\"\n     return weights_name\n \n \n def _get_resolved_checkpoint_files(\n-    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n-    variant: Optional[str],\n-    gguf_file: Optional[str],\n-    use_safetensors: Optional[bool],\n+    pretrained_model_name_or_path: str | os.PathLike | None,\n+    variant: str | None,\n+    gguf_file: str | None,\n+    use_safetensors: bool | None,\n     download_kwargs: DownloadKwargs,\n     user_agent: dict,\n     is_remote_code: bool,  # Because we can't determine this inside this function, we need it to be passed in\n-    transformers_explicit_filename: Optional[str] = None,\n-) -> tuple[Optional[list[str]], Optional[dict]]:\n+    transformers_explicit_filename: str | None = None,\n+) -> tuple[list[str] | None, dict | None]:\n     \"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\n     checkpoints are sharded.\n     This function will download the data if necessary.\n@@ -745,13 +744,13 @@ def _get_resolved_checkpoint_files(\n \n \n def _get_dtype(\n-    dtype: Optional[Union[str, torch.dtype, dict]],\n-    checkpoint_files: Optional[list[str]],\n+    dtype: str | torch.dtype | dict | None,\n+    checkpoint_files: list[str] | None,\n     config: PreTrainedConfig,\n-    sharded_metadata: Optional[dict],\n-    state_dict: Optional[dict],\n+    sharded_metadata: dict | None,\n+    state_dict: dict | None,\n     weights_only: bool,\n-    hf_quantizer: Optional[HfQuantizer] = None,\n+    hf_quantizer: HfQuantizer | None = None,\n ) -> tuple[PreTrainedConfig, torch.dtype]:\n     \"\"\"Find the correct `dtype` to use based on provided arguments. Also update the `config` based on the\n     inferred dtype. We do the following:\n@@ -907,8 +906,8 @@ def get_extended_attention_mask(\n         self,\n         attention_mask: Tensor,\n         input_shape: tuple[int, ...],\n-        device: Optional[torch.device] = None,\n-        dtype: Optional[torch.dtype] = None,\n+        device: torch.device | None = None,\n+        dtype: torch.dtype | None = None,\n     ) -> Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n@@ -1128,7 +1127,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     # to also prevent bfloat16 casting, use the _keep_in_fp32_modules_strict flag\n     _keep_in_fp32_modules_strict = None\n \n-    dtype_plan: Optional[dict[str, torch.dtype]] = None\n+    dtype_plan: dict[str, torch.dtype] | None = None\n \n     # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n     # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n@@ -1188,7 +1187,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n \n     # Attributes used mainly in multimodal LLMs, though all models contain a valid field for these\n     # Possible values are: text, image, video, audio and time\n-    input_modalities: Union[str, list[str]] = \"text\"  # most models are text\n+    input_modalities: str | list[str] = \"text\"  # most models are text\n \n     @property\n     @torch._dynamo.allow_in_graph\n@@ -1417,7 +1416,7 @@ def _backward_compatibility_gradient_checkpointing(self):\n             # Remove the attribute now that is has been consumed, so it's no saved in the config.\n             delattr(self.config, \"gradient_checkpointing\")\n \n-    def add_model_tags(self, tags: Union[list[str], str]) -> None:\n+    def add_model_tags(self, tags: list[str] | str) -> None:\n         r\"\"\"\n         Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\n         not overwrite existing tags in the model.\n@@ -1784,7 +1783,7 @@ def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n         return True\n \n     def _check_and_adjust_attn_implementation(\n-        self, attn_implementation: Optional[str], is_init_check: bool = False\n+        self, attn_implementation: str | None, is_init_check: bool = False\n     ) -> str:\n         \"\"\"\n         Check that the `attn_implementation` exists and is supported by the models, and try to get the kernel from hub if\n@@ -1864,7 +1863,7 @@ def _check_and_adjust_attn_implementation(\n \n         return applicable_attn_implementation\n \n-    def _check_and_adjust_experts_implementation(self, experts_implementation: Optional[str]) -> str:\n+    def _check_and_adjust_experts_implementation(self, experts_implementation: str | None) -> str:\n         \"\"\"\n         Check that the `experts_implementation` exists and is supported by the models.\n \n@@ -1877,7 +1876,7 @@ def _check_and_adjust_experts_implementation(self, experts_implementation: Optio\n         applicable_experts_implementation = self.get_correct_experts_implementation(experts_implementation)\n         return applicable_experts_implementation\n \n-    def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n+    def get_correct_attn_implementation(self, requested_attention: str | None, is_init_check: bool = False) -> str:\n         applicable_attention = \"sdpa\" if requested_attention is None else requested_attention\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n@@ -1911,7 +1910,7 @@ def get_correct_attn_implementation(self, requested_attention: Optional[str], is\n \n         return applicable_attention\n \n-    def get_correct_experts_implementation(self, requested_experts: Optional[str]) -> str:\n+    def get_correct_experts_implementation(self, requested_experts: str | None) -> str:\n         applicable_experts = \"grouped_mm\" if requested_experts is None else requested_experts\n         if applicable_experts not in [\"eager\", \"grouped_mm\", \"batched_mm\"]:\n             message = (\n@@ -1960,7 +1959,7 @@ def _can_set_experts_implementation(cls) -> bool:\n         # heuristic -> if we the use_experts_implementation decorator is used, then we can set it\n         return \"@use_experts_implementation\" in code\n \n-    def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n+    def set_attn_implementation(self, attn_implementation: str | dict):\n         \"\"\"\n         Set the requested `attn_implementation` for this model.\n \n@@ -2059,7 +2058,7 @@ def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n                     if hasattr(subconfig, \"_attn_was_changed\"):\n                         del subconfig._attn_was_changed\n \n-    def set_experts_implementation(self, experts_implementation: Union[str, dict]):\n+    def set_experts_implementation(self, experts_implementation: str | dict):\n         \"\"\"\n         Set the requested `experts_implementation` for this model.\n \n@@ -2162,7 +2161,7 @@ def disable_input_require_grads(self):\n         if hasattr(self, \"_require_grads_hook\"):\n             del self._require_grads_hook\n \n-    def get_encoder(self, modality: Optional[str] = None):\n+    def get_encoder(self, modality: str | None = None):\n         \"\"\"\n         Best-effort lookup of the *encoder* module. If provided with `modality` argument,\n         it looks for a modality-specific encoder in multimodal models (e.g. \"image_encoder\")\n@@ -2194,7 +2193,7 @@ def get_encoder(self, modality: Optional[str] = None):\n         # If this is a base transformer model (no encoder/model attributes), return self\n         return self\n \n-    def set_encoder(self, encoder, modality: Optional[str] = None):\n+    def set_encoder(self, encoder, modality: str | None = None):\n         \"\"\"\n         Symmetric setter. Mirrors the lookup logic used in `get_encoder`.\n         \"\"\"\n@@ -2467,7 +2466,7 @@ def get_expanded_tied_weights_keys(self, all_submodels: bool = False) -> dict:\n \n         return expanded_tied_weights\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n+    def tie_weights(self, missing_keys: set[str] | None = None, recompute_mapping: bool = True):\n         \"\"\"\n         Tie the model weights. If `recompute_mapping=False` (default when called internally), it will rely on the\n         `model.all_tied_weights_keys` attribute, containing the `{target: source}` mapping for the tied params.\n@@ -2590,8 +2589,8 @@ def _get_no_split_modules(self, device_map: str):\n \n     def resize_token_embeddings(\n         self,\n-        new_num_tokens: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n+        new_num_tokens: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n         mean_resizing: bool = True,\n     ) -> nn.Embedding:\n         \"\"\"\n@@ -2692,8 +2691,8 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean\n     def _get_resized_embeddings(\n         self,\n         old_embeddings: nn.Embedding,\n-        new_num_tokens: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n+        new_num_tokens: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n         mean_resizing: bool = True,\n     ) -> nn.Embedding:\n         \"\"\"\n@@ -2850,7 +2849,7 @@ def _get_resized_embeddings(\n     def _get_resized_lm_head(\n         self,\n         old_lm_head: nn.Linear,\n-        new_num_tokens: Optional[int] = None,\n+        new_num_tokens: int | None = None,\n         transposed: bool = False,\n         mean_resizing: bool = True,\n     ) -> nn.Linear:\n@@ -3047,7 +3046,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n             f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n         )\n \n-    def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]:\n+    def get_position_embeddings(self) -> nn.Embedding | tuple[nn.Embedding]:\n         raise NotImplementedError(\n             f\"`get_position_embeddings` is not implemented for {self.__class__}`. To implement it, you should \"\n             f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n@@ -3158,13 +3157,13 @@ def is_gradient_checkpointing(self) -> bool:\n \n     def save_pretrained(\n         self,\n-        save_directory: Union[str, os.PathLike],\n+        save_directory: str | os.PathLike,\n         is_main_process: bool = True,\n-        state_dict: Optional[dict] = None,\n+        state_dict: dict | None = None,\n         push_to_hub: bool = False,\n-        max_shard_size: Union[int, str] = \"50GB\",\n-        variant: Optional[str] = None,\n-        token: Optional[Union[str, bool]] = None,\n+        max_shard_size: int | str = \"50GB\",\n+        variant: str | None = None,\n+        token: str | bool | None = None,\n         save_peft_format: bool = True,\n         save_original_format: bool = True,\n         **kwargs,\n@@ -3655,16 +3654,16 @@ def set_use_kernels(self, use_kernels, kernel_config):\n     @classmethod\n     def from_pretrained(\n         cls: type[SpecificPreTrainedModelType],\n-        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n+        pretrained_model_name_or_path: str | os.PathLike | None,\n         *model_args,\n-        config: Optional[Union[PreTrainedConfig, str, os.PathLike]] = None,\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        config: PreTrainedConfig | str | os.PathLike | None = None,\n+        cache_dir: str | os.PathLike | None = None,\n         ignore_mismatched_sizes: bool = False,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n-        use_safetensors: Optional[bool] = True,\n+        use_safetensors: bool | None = True,\n         weights_only: bool = True,\n         **kwargs,\n     ) -> SpecificPreTrainedModelType:\n@@ -4140,19 +4139,19 @@ def from_pretrained(\n     def _load_pretrained_model(\n         cls,\n         model: \"PreTrainedModel\",\n-        state_dict: Optional[dict],\n-        checkpoint_files: Optional[list[str]],\n-        pretrained_model_name_or_path: Optional[str],\n+        state_dict: dict | None,\n+        checkpoint_files: list[str] | None,\n+        pretrained_model_name_or_path: str | None,\n         ignore_mismatched_sizes: bool = False,\n-        sharded_metadata: Optional[dict] = None,\n-        device_map: Optional[dict] = None,\n-        disk_offload_folder: Optional[str] = None,\n+        sharded_metadata: dict | None = None,\n+        device_map: dict | None = None,\n+        disk_offload_folder: str | None = None,\n         offload_buffers: bool = False,\n-        dtype: Optional[torch.dtype] = None,\n-        hf_quantizer: Optional[HfQuantizer] = None,\n+        dtype: torch.dtype | None = None,\n+        hf_quantizer: HfQuantizer | None = None,\n         device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n         weights_only: bool = True,\n-        weight_mapping: Optional[Sequence[WeightConverter | WeightRenaming]] = None,\n+        weight_mapping: Sequence[WeightConverter | WeightRenaming] | None = None,\n     ):\n         is_quantized = hf_quantizer is not None\n         is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n@@ -4430,7 +4429,7 @@ def use_kernels(self, value: bool) -> None:\n                 )\n             self._use_kernels = False\n \n-    def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable:\n+    def get_compiled_call(self, compile_config: CompileConfig | None) -> Callable:\n         \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n         non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't\n         want to use compiled version to avoid recomputing the graph with new shapes) and iterative decoding\n@@ -4640,7 +4639,7 @@ def unwrap_model(model: nn.Module, recursive: bool = False) -> nn.Module:\n             return model\n \n \n-def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n+def is_accelerator_device(device: str | int | torch.device) -> bool:\n     \"\"\"Check if the device is an accelerator. We need to function, as device_map can be \"disk\" as well, which is not\n     a proper `torch.device`.\n     \"\"\"\n@@ -4651,7 +4650,7 @@ def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n \n \n def get_total_byte_count(\n-    model: PreTrainedModel, accelerator_device_map: dict, hf_quantizer: Optional[HfQuantizer] = None\n+    model: PreTrainedModel, accelerator_device_map: dict, hf_quantizer: HfQuantizer | None = None\n ):\n     \"\"\"\n     This utility function calculates the total bytes count needed to load the model on each device.\n@@ -4684,7 +4683,7 @@ def get_total_byte_count(\n     return total_byte_count\n \n \n-def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]):\n+def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: HfQuantizer | None):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n     the model, which is actually the loading speed bottleneck."
        },
        {
            "sha": "ddf03465e524e5771dd88e3a24819341093f1c7c",
            "filename": "src/transformers/models/afmoe/configuration_afmoe.py",
            "status": "modified",
            "additions": 26,
            "deletions": 29,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fafmoe%2Fconfiguration_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fafmoe%2Fconfiguration_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fconfiguration_afmoe.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"AFMoE model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n@@ -132,32 +129,32 @@ class AfmoeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 200192,\n-        hidden_size: Optional[int] = 2048,\n-        intermediate_size: Optional[int] = 6144,\n-        moe_intermediate_size: Optional[int] = 1408,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_dense_layers: Optional[int] = 1,\n-        num_attention_heads: Optional[int] = 16,\n-        num_key_value_heads: Optional[int] = None,\n-        head_dim: Optional[int] = 128,\n-        hidden_act: Optional[str] = \"silu\",\n-        max_position_embeddings: Optional[int] = 16384,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_theta: Optional[float] = 10000.0,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        num_experts: Optional[int] = 64,\n-        num_experts_per_tok: Optional[int] = 6,\n-        num_shared_experts: Optional[int] = 2,\n-        route_scale: Optional[float] = 1.0,\n-        global_attn_every_n_layers: Optional[int] = 4,\n-        sliding_window: Optional[int] = 1024,\n-        layer_types: Optional[list] = None,\n-        attention_dropout: Optional[float] = 0.0,\n-        mup_enabled: Optional[bool] = False,\n+        vocab_size: int | None = 200192,\n+        hidden_size: int | None = 2048,\n+        intermediate_size: int | None = 6144,\n+        moe_intermediate_size: int | None = 1408,\n+        num_hidden_layers: int | None = 32,\n+        num_dense_layers: int | None = 1,\n+        num_attention_heads: int | None = 16,\n+        num_key_value_heads: int | None = None,\n+        head_dim: int | None = 128,\n+        hidden_act: str | None = \"silu\",\n+        max_position_embeddings: int | None = 16384,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: float | None = 1e-5,\n+        use_cache: bool | None = True,\n+        tie_word_embeddings: bool | None = False,\n+        rope_theta: float | None = 10000.0,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        num_experts: int | None = 64,\n+        num_experts_per_tok: int | None = 6,\n+        num_shared_experts: int | None = 2,\n+        route_scale: float | None = 1.0,\n+        global_attn_every_n_layers: int | None = 4,\n+        sliding_window: int | None = 1024,\n+        layer_types: list | None = None,\n+        attention_dropout: float | None = 0.0,\n+        mup_enabled: bool | None = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "e9d8c2743760ca5a83ed3637957894a566550c7e",
            "filename": "src/transformers/models/afmoe/modeling_afmoe.py",
            "status": "modified",
            "additions": 29,
            "deletions": 30,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_afmoe.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n # limitations under the License.\n \n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -62,9 +61,9 @@ def __init__(self, config: AfmoeConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[AfmoeConfig] = None,\n+        config: AfmoeConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -317,7 +316,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -383,9 +382,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None,\n+        past_key_value: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -464,12 +463,12 @@ def __init__(self, config: AfmoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_value: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n@@ -566,13 +565,13 @@ def __init__(self, config: AfmoeConfig):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -652,15 +651,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "904f7dc69b2d73d164bb3197ddc8f6de5a326b7c",
            "filename": "src/transformers/models/afmoe/modular_afmoe.py",
            "status": "modified",
            "additions": 16,
            "deletions": 18,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"PyTorch AFMoE model.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -203,9 +201,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_value: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None,\n+        past_key_value: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -284,12 +282,12 @@ def __init__(self, config: AfmoeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_value: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n@@ -386,13 +384,13 @@ def __init__(self, config: AfmoeConfig):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):"
        },
        {
            "sha": "4d070e95d9ce9bc55968603f543c951b45536e58",
            "filename": "src/transformers/models/aimv2/configuration_aimv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aimv2.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 Apple Inc. and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,8 +18,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n@@ -188,8 +185,8 @@ def __init__(\n         qkv_bias: bool = False,\n         mlp_bias: bool = False,\n         hidden_act: str = \"silu\",\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = None,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = None,\n         eos_token_id: int = 49407,\n         max_position_embeddings: int = 77,\n         initializer_range: bool = 0.02,"
        },
        {
            "sha": "b544bb8d9d0f1aa20452606bdc53a21166b7befd",
            "filename": "src/transformers/models/aimv2/convert_aimv2_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n import gc\n import os\n import re\n-from typing import Optional\n \n import torch\n from huggingface_hub import snapshot_download\n@@ -90,7 +88,7 @@\n }\n \n \n-def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> dict[str, torch.Tensor]:\n+def load_original_state_dict(model_id: str, revision: str | None = None) -> dict[str, torch.Tensor]:\n     # Download only the model.safetensors file\n     directory_path = snapshot_download(\n         repo_id=model_id,"
        },
        {
            "sha": "c9e3c5f1b50fc6b34d8af2b3493cb7cae5b859c8",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aimv2.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 Apple Inc. and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -23,7 +22,7 @@\n import math\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional\n+from typing import Any\n \n import torch\n import torch.nn.functional as F\n@@ -64,11 +63,11 @@ class Aimv2Output(ModelOutput):\n         The output of the [`Aimv2VisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -183,9 +182,9 @@ def __init__(self, config: Aimv2TextConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n         max_position_embedding = self.position_embedding.weight.shape[0]\n@@ -213,7 +212,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n@@ -256,9 +255,9 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -303,7 +302,7 @@ def __init__(self, config: Aimv2VisionConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         norm_hidden_states = self.rms_norm1(hidden_states)\n@@ -337,7 +336,7 @@ def __init__(self, config: Aimv2Config):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         hidden_states = inputs_embeds\n@@ -530,7 +529,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         hidden_states = self.embeddings(input_ids)\n@@ -609,8 +608,8 @@ def __init__(self, config: Aimv2Config):\n     def get_text_features(\n         self,\n         input_ids: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -684,9 +683,9 @@ def get_image_features(\n     @can_return_tuple\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Aimv2Output:\n         r\"\"\""
        },
        {
            "sha": "b0d48079e7870bbd1f7fcba2f0b1ce05ff5aceb2",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 Apple Inc. and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \"\"\"Pytorch implementation of AIMv2 Model\"\"\"\n \n import math\n-from typing import Optional\n \n import torch\n import torch.nn.functional as F\n@@ -198,8 +196,8 @@ def __init__(\n         qkv_bias: bool = False,\n         mlp_bias: bool = False,\n         hidden_act: str = \"silu\",\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = None,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = None,\n         eos_token_id: int = 49407,\n         max_position_embeddings: int = 77,\n         initializer_range: bool = 0.02,\n@@ -376,7 +374,7 @@ def __init__(self, config: Aimv2VisionConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         norm_hidden_states = self.rms_norm1(hidden_states)\n@@ -573,7 +571,7 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         input_ids,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         hidden_states = self.embeddings(input_ids)\n@@ -638,9 +636,9 @@ def __init__(self, config: Aimv2Config):\n     @can_return_tuple\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Aimv2Output:\n         r\"\"\""
        },
        {
            "sha": "c3a97b74c384455926a7e4b9c9a15a7dcc42c2d6",
            "filename": "src/transformers/models/albert/configuration_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "4d31b44a478c96709ddf4cbafefb0ab3eef2175d",
            "filename": "src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Fconvert_albert_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Fconvert_albert_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconvert_albert_original_tf_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "70c179cfd6a35f6d44a6992d5f7a77ff3c064f95",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 67,
            "deletions": 69,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -71,10 +69,10 @@ def __init__(self, config: AlbertConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n             input_shape = input_ids.size()\n@@ -117,8 +115,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -172,7 +170,7 @@ def __init__(self, config: AlbertConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -223,7 +221,7 @@ def __init__(self, config: AlbertConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         attention_output, _ = self.attention(hidden_states, attention_mask, **kwargs)\n@@ -253,9 +251,9 @@ def __init__(self, config: AlbertConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n+    ) -> tuple[torch.Tensor | tuple[torch.Tensor], ...]:\n         for layer_index, albert_layer in enumerate(self.albert_layers):\n             hidden_states = albert_layer(hidden_states, attention_mask, **kwargs)\n         return hidden_states\n@@ -272,9 +270,9 @@ def __init__(self, config: AlbertConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[BaseModelOutput, tuple]:\n+    ) -> BaseModelOutput | tuple:\n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n \n         for i in range(self.config.num_hidden_layers):\n@@ -343,11 +341,11 @@ class AlbertForPreTrainingOutput(ModelOutput):\n         before SoftMax).\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: Optional[torch.FloatTensor] = None\n-    sop_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    prediction_logits: torch.FloatTensor | None = None\n+    sop_logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n @auto_docstring\n@@ -387,13 +385,13 @@ def set_input_embeddings(self, value: nn.Embedding) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[BaseModelOutputWithPooling, tuple]:\n+    ) -> BaseModelOutputWithPooling | tuple:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -459,15 +457,15 @@ def get_input_embeddings(self) -> nn.Embedding:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        sentence_order_label: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        sentence_order_label: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[AlbertForPreTrainingOutput, tuple]:\n+    ) -> AlbertForPreTrainingOutput | tuple:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -589,14 +587,14 @@ def get_input_embeddings(self) -> nn.Embedding:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[MaskedLMOutput, tuple]:\n+    ) -> MaskedLMOutput | tuple:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -681,14 +679,14 @@ def __init__(self, config: AlbertConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[SequenceClassifierOutput, tuple]:\n+    ) -> SequenceClassifierOutput | tuple:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -763,14 +761,14 @@ def __init__(self, config: AlbertConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[TokenClassifierOutput, tuple]:\n+    ) -> TokenClassifierOutput | tuple:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -819,15 +817,15 @@ def __init__(self, config: AlbertConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[AlbertForPreTrainingOutput, tuple]:\n+    ) -> AlbertForPreTrainingOutput | tuple:\n         outputs = self.albert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -887,14 +885,14 @@ def __init__(self, config: AlbertConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[AlbertForPreTrainingOutput, tuple]:\n+    ) -> AlbertForPreTrainingOutput | tuple:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary."
        },
        {
            "sha": "45f651703de15d5dbe056ee76ae6acf7190264f0",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"Tokenization classes for ALBERT model.\"\"\"\n \n-from typing import Optional, Union\n-\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n \n@@ -86,7 +83,7 @@ class AlbertTokenizer(TokenizersBackend):\n \n     def __init__(\n         self,\n-        vocab: Optional[Union[str, list[tuple[str, float]]]] = None,\n+        vocab: str | list[tuple[str, float]] | None = None,\n         do_lower_case: bool = True,\n         keep_accents: bool = False,\n         bos_token: str = \"[CLS]\","
        },
        {
            "sha": "cb9cc8138cda90b6abcd867a5deaa31e0aea00bd",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "6a9151185f82568fa4a0248cb6621d0061208f6d",
            "filename": "src/transformers/models/align/convert_align_tf_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fconvert_align_tf_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fconvert_align_tf_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconvert_align_tf_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "43dfe428ffdb532bf024c62c183644b122895920",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 61,
            "deletions": 62,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Google Research Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,7 @@\n import math\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n from torch import nn\n@@ -52,9 +51,9 @@ class AlignVisionModelOutput(ModelOutput):\n         The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    image_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -69,10 +68,10 @@ class AlignTextModelOutput(ModelOutput):\n         The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    text_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -97,11 +96,11 @@ class AlignOutput(ModelOutput):\n         The output of the [`AlignVisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPoolingAndNoAttention = None\n \n@@ -141,7 +140,7 @@ def round_filters(config: AlignVisionConfig, num_channels: int):\n \n \n # Copied from transformers.models.efficientnet.modeling_efficientnet.correct_pad\n-def correct_pad(kernel_size: Union[int, tuple], adjust: bool = True):\n+def correct_pad(kernel_size: int | tuple, adjust: bool = True):\n     r\"\"\"\n     Utility function to get the tuple padding value for the depthwise convolution.\n \n@@ -490,8 +489,8 @@ def round_repeats(repeats):\n     def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         all_hidden_states = (hidden_states,) if output_hidden_states else None\n \n@@ -530,10 +529,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n             input_shape = input_ids.size()\n@@ -574,7 +573,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n@@ -617,8 +616,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -672,8 +671,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -730,8 +729,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n@@ -767,12 +766,12 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutput]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -889,16 +888,16 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> tuple | BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n \n@@ -1007,11 +1006,11 @@ def __init__(self, config: AlignVisionConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n+    ) -> tuple | BaseModelOutputWithPoolingAndNoAttention:\n         r\"\"\"\n         Examples:\n \n@@ -1097,11 +1096,11 @@ def __init__(self, config: AlignConfig):\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1166,18 +1165,18 @@ def get_image_features(self, pixel_values: torch.FloatTensor) -> torch.FloatTens\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, AlignOutput]:\n+    ) -> tuple | AlignOutput:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "addf0df79e37a076ad6067ce212ba4762d5fc8c1",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "d49c0f50e5e3aca91b2c6f8ee3d9afa4509d1fba",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 WenXiang ZhongzhiCheng LedellWu LiuGuang BoWenZhang and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "7d24e886bf880a364517a8eaabd2f90afe583e7d",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 76,
            "deletions": 77,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The BAAI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,7 @@\n import math\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n import torch.nn as nn\n@@ -75,11 +74,11 @@ class AltCLIPOutput(ModelOutput):\n         The output of the [`AltCLIPVisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -116,10 +115,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if position_ids is None:\n@@ -219,8 +218,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.attention_head_size)\n@@ -284,8 +283,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n@@ -341,8 +340,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n@@ -379,12 +378,12 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutput]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -435,7 +434,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n@@ -479,10 +478,10 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        attention_mask: torch.Tensor | None = None,\n+        causal_attention_mask: torch.Tensor | None = None,\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -556,7 +555,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -611,12 +610,12 @@ def __init__(self, config: AltCLIPConfig):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n+        attention_mask: torch.Tensor | None = None,\n+        causal_attention_mask: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+    ) -> tuple | BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -836,12 +835,12 @@ def __init__(self, config: AltCLIPVisionConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        pixel_values: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        interpolate_pos_encoding: bool | None = False,\n+    ) -> tuple | BaseModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -890,13 +889,13 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> tuple | BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n \n@@ -967,16 +966,16 @@ def set_input_embeddings(self, value):\n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1052,23 +1051,23 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value: nn.Embedding) -> None:\n         self.roberta.embeddings.word_embeddings = value\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> nn.Embedding:\n+    def resize_token_embeddings(self, new_num_tokens: int | None = None) -> nn.Embedding:\n         return super().resize_token_embeddings(new_num_tokens)\n \n     @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        return_dict: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPoolingAndProjection]:\n+    ) -> tuple | BaseModelOutputWithPoolingAndProjection:\n         r\"\"\"\n         Examples:\n \n@@ -1159,9 +1158,9 @@ def __init__(self, config: AltCLIPConfig):\n     def get_text_features(\n         self,\n         input_ids: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1233,18 +1232,18 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, AltCLIPOutput]:\n+    ) -> tuple | AltCLIPOutput:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "0891234659dcbfa6bad8a32420e4e9cc1a5f91a1",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 WenXiang ZhongzhiCheng LedellWu LiuGuang BoWenZhang The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "726e3d61c3bdcd7bb1ee85b0dfbaebe8e4815ecf",
            "filename": "src/transformers/models/apertus/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2F__init__.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team and the Swiss AI Initiative. All rights reserved.\n #\n # This code is based on HuggingFace's LLaMA implementation in this library."
        },
        {
            "sha": "dc62c8963a5ddfdc20d0199d71bf292cda456539",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 18,
            "deletions": 21,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_apertus.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 the HuggingFace Inc. team and the Swiss AI Initiative. All rights reserved.\n #\n #\n@@ -19,8 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n \n@@ -116,31 +113,31 @@ class ApertusConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 131072,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 14336,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"xielu\",\n-        max_position_embeddings: Optional[int] = 65536,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = 3,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters] = {\n+        vocab_size: int | None = 131072,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 14336,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"xielu\",\n+        max_position_embeddings: int | None = 65536,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: float | None = 1e-5,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = 3,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | None = {\n             \"rope_type\": \"llama3\",\n             \"rope_theta\": 12000000.0,\n             \"factor\": 8.0,\n             \"original_max_position_embeddings\": 8192,\n             \"low_freq_factor\": 1.0,\n             \"high_freq_factor\": 4.0,\n         },\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "0e5efea4bf3777ce326677b9f5598cd638e82309",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_apertus.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 the HuggingFace Inc. team and the Swiss AI Initiative. All rights reserved.\n #\n #\n@@ -20,7 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -98,9 +97,9 @@ def __init__(self, config: ApertusConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[ApertusConfig] = None,\n+        config: ApertusConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -194,7 +193,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -219,7 +218,7 @@ def eager_attention_forward(\n class ApertusAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: ApertusConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: ApertusConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n@@ -248,9 +247,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -303,12 +302,12 @@ def __init__(self, config: ApertusConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         residual = hidden_states\n@@ -374,13 +373,13 @@ def __init__(self, config: ApertusConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -451,15 +450,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "43448fe33746eb9b416ca744b722dffdc86e4f3b",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 28,
            "deletions": 30,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 the HuggingFace Inc. team and the Swiss AI Initiative. All rights reserved.\n #\n #\n@@ -14,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -135,31 +133,31 @@ class ApertusConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 131072,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 14336,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"xielu\",\n-        max_position_embeddings: Optional[int] = 65536,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = 3,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters] = {\n+        vocab_size: int | None = 131072,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 14336,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"xielu\",\n+        max_position_embeddings: int | None = 65536,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: float | None = 1e-5,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = 3,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | None = {\n             \"rope_type\": \"llama3\",\n             \"rope_theta\": 12000000.0,\n             \"factor\": 8.0,\n             \"original_max_position_embeddings\": 8192,\n             \"low_freq_factor\": 1.0,\n             \"high_freq_factor\": 4.0,\n         },\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -209,7 +207,7 @@ class ApertusRotaryEmbedding(LlamaRotaryEmbedding):\n \n \n class ApertusAttention(LlamaAttention):\n-    def __init__(self, config: ApertusConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: ApertusConfig, layer_idx: int | None = None):\n         super().__init__(config, layer_idx)\n         self.q_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n         self.k_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n@@ -218,9 +216,9 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -271,12 +269,12 @@ def __init__(self, config: ApertusConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         residual = hidden_states"
        },
        {
            "sha": "79c1e7bc94e2004d3e9130ce4dae407ab17d63ef",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_arcee.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,8 +18,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n \n@@ -121,26 +118,26 @@ class ArceeConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 32000,\n-        hidden_size: Optional[int] = 2560,\n-        intermediate_size: Optional[int] = 18432,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"relu2\",\n-        max_position_embeddings: Optional[int] = 4096,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 128000,\n-        eos_token_id: Optional[int] = 128001,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n-        mlp_bias: Optional[bool] = False,\n-        head_dim: Optional[int] = None,\n+        vocab_size: int | None = 32000,\n+        hidden_size: int | None = 2560,\n+        intermediate_size: int | None = 18432,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"relu2\",\n+        max_position_embeddings: int | None = 4096,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-5,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 128000,\n+        eos_token_id: int | None = 128001,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n+        mlp_bias: bool | None = False,\n+        head_dim: int | None = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "becd99f0c403cf04dcfb389ab4dc08055f41d177",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_arcee.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n # limitations under the License.\n \n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -103,9 +102,9 @@ def __init__(self, config: ArceeConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[ArceeConfig] = None,\n+        config: ArceeConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -199,7 +198,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -250,10 +249,10 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -305,12 +304,12 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -377,13 +376,13 @@ def __init__(self, config: ArceeConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -454,15 +453,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "19352ec82c9b4746999b6929cf16422141ec941a",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"PyTorch Arcee model.\"\"\"\n \n-from typing import Optional\n-\n from transformers.utils import auto_docstring, logging\n \n from ...modeling_rope_utils import RopeParameters\n@@ -122,26 +119,26 @@ class ArceeConfig(LlamaConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 32000,\n-        hidden_size: Optional[int] = 2560,\n-        intermediate_size: Optional[int] = 18432,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"relu2\",\n-        max_position_embeddings: Optional[int] = 4096,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 128000,\n-        eos_token_id: Optional[int] = 128001,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n-        mlp_bias: Optional[bool] = False,\n-        head_dim: Optional[int] = None,\n+        vocab_size: int | None = 32000,\n+        hidden_size: int | None = 2560,\n+        intermediate_size: int | None = 18432,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"relu2\",\n+        max_position_embeddings: int | None = 4096,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-5,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 128000,\n+        eos_token_id: int | None = 128001,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n+        mlp_bias: bool | None = False,\n+        head_dim: int | None = None,\n         **kwargs,\n     ):\n         super().__init__("
        },
        {
            "sha": "b2c367e7942b1862cf7a7096b2901f1a1f859648",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aria.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,8 +17,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -117,27 +114,27 @@ class AriaTextConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 32000,\n-        hidden_size: Optional[int] = 4096,\n+        vocab_size: int | None = 32000,\n+        hidden_size: int | None = 4096,\n         intermediate_size: int = 4096,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        max_position_embeddings: Optional[int] = 2048,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-6,\n-        use_cache: Optional[bool] = True,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = None,\n+        hidden_act: str | None = \"silu\",\n+        max_position_embeddings: int | None = 2048,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-6,\n+        use_cache: bool | None = True,\n         pad_token_id=2,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        pretraining_tp: Optional[int] = 1,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[float] = 0.0,\n-        mlp_bias: Optional[bool] = False,\n-        head_dim: Optional[int] = None,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        pretraining_tp: int | None = 1,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: bool | None = False,\n+        attention_dropout: float | None = 0.0,\n+        mlp_bias: bool | None = False,\n+        head_dim: int | None = None,\n         moe_num_experts: int = 8,\n         moe_topk: int = 2,\n         moe_num_shared_experts: int = 2,\n@@ -227,7 +224,7 @@ def __init__(\n         vision_config=None,\n         vision_feature_layer: int = -1,\n         text_config: AriaTextConfig = None,\n-        projector_patch_to_query_dict: Optional[dict] = None,\n+        projector_patch_to_query_dict: dict | None = None,\n         image_token_index: int = 9,\n         initializer_range: float = 0.02,\n         **kwargs,"
        },
        {
            "sha": "8baefbc413190d6ab417e1552eabfe37e5b43ed2",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 25,
            "deletions": 27,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aria.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,7 +18,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -107,16 +105,16 @@ class AriaImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        image_mean: Optional[list[float]] = None,\n-        image_std: Optional[list[float]] = None,\n+        image_mean: list[float] | None = None,\n+        image_std: list[float] | None = None,\n         max_image_size: int = 980,\n         min_image_size: int = 336,\n-        split_resolutions: Optional[list[tuple[int, int]]] = None,\n-        split_image: Optional[bool] = False,\n-        do_convert_rgb: Optional[bool] = True,\n+        split_resolutions: list[tuple[int, int]] | None = None,\n+        split_image: bool | None = False,\n+        do_convert_rgb: bool | None = True,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: Optional[bool] = True,\n+        rescale_factor: int | float = 1 / 255,\n+        do_normalize: bool | None = True,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         **kwargs,\n     ):\n@@ -143,20 +141,20 @@ def __init__(\n \n     def preprocess(\n         self,\n-        images: Union[ImageInput, list[ImageInput]],\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        max_image_size: Optional[int] = None,\n-        min_image_size: Optional[int] = None,\n-        split_image: Optional[bool] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | list[ImageInput],\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        max_image_size: int | None = None,\n+        min_image_size: int | None = None,\n+        split_image: bool | None = None,\n+        do_convert_rgb: bool | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        resample: PILImageResampling | None = None,\n+        return_tensors: str | TensorType | None = \"pt\",\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Process a list of images.\n@@ -386,11 +384,11 @@ def _pad_for_patching(\n     def pad(\n         self,\n         image: np.ndarray,\n-        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n+        padding: int | tuple[int, int] | Iterable[tuple[int, int]],\n         mode: PaddingMode = PaddingMode.CONSTANT,\n-        constant_values: Union[float, Iterable[float]] = 0.0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0.0,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)"
        },
        {
            "sha": "1c2882cd79eef72ddbad9a2e9ee28f9816c69856",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 62,
            "deletions": 63,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aria.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n # limitations under the License.\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -175,7 +174,7 @@ def __init__(\n         self.layer_norm = nn.LayerNorm(self.in_features)\n         self.feed_forward = AriaProjectorMLP(self.in_features, self.hidden_features, self.output_dim)\n \n-    def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n+    def forward(self, key_value_states: torch.Tensor, attn_mask: torch.Tensor | None = None):\n         \"\"\"\n         Forward pass of the Projector module.\n \n@@ -423,7 +422,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -474,10 +473,10 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -540,12 +539,12 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -640,9 +639,9 @@ def __init__(self, config: AriaTextConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[AriaTextConfig] = None,\n+        config: AriaTextConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -706,13 +705,13 @@ def __init__(self, config: AriaTextConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -782,15 +781,15 @@ def __init__(self, config: AriaTextConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n@@ -861,12 +860,12 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n+    image_hidden_states: torch.FloatTensor | None = None\n \n \n @dataclass\n@@ -887,7 +886,7 @@ class AriaModelOutputWithPast(BaseModelOutputWithPast):\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+    image_hidden_states: torch.FloatTensor | None = None\n \n \n @auto_docstring(\n@@ -916,7 +915,7 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.FloatTensor] = None,\n+        pixel_mask: torch.FloatTensor | None = None,\n         vision_feature_layer: int = -1,\n     ):\n         \"\"\"\n@@ -978,17 +977,17 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, AriaModelOutputWithPast]:\n+    ) -> tuple | AriaModelOutputWithPast:\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -1075,7 +1074,7 @@ def get_output_embeddings(self) -> nn.Module:\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.FloatTensor] = None,\n+        pixel_mask: torch.FloatTensor | None = None,\n         vision_feature_layer: int = -1,\n     ):\n         return self.model.get_image_features(\n@@ -1088,19 +1087,19 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, AriaCausalLMOutputWithPast]:\n+    ) -> tuple | AriaCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "cba7c237cd4f933dff02c4780129efb4194afa13",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 56,
            "deletions": 58,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -13,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Iterable\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -244,7 +242,7 @@ def __init__(\n         vision_config=None,\n         vision_feature_layer: int = -1,\n         text_config: AriaTextConfig = None,\n-        projector_patch_to_query_dict: Optional[dict] = None,\n+        projector_patch_to_query_dict: dict | None = None,\n         image_token_index: int = 9,\n         initializer_range: float = 0.02,\n         **kwargs,\n@@ -395,7 +393,7 @@ def __init__(\n         self.layer_norm = nn.LayerNorm(self.in_features)\n         self.feed_forward = AriaProjectorMLP(self.in_features, self.hidden_features, self.output_dim)\n \n-    def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):\n+    def forward(self, key_value_states: torch.Tensor, attn_mask: torch.Tensor | None = None):\n         \"\"\"\n         Forward pass of the Projector module.\n \n@@ -465,16 +463,16 @@ class AriaImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        image_mean: Optional[list[float]] = None,\n-        image_std: Optional[list[float]] = None,\n+        image_mean: list[float] | None = None,\n+        image_std: list[float] | None = None,\n         max_image_size: int = 980,\n         min_image_size: int = 336,\n-        split_resolutions: Optional[list[tuple[int, int]]] = None,\n-        split_image: Optional[bool] = False,\n-        do_convert_rgb: Optional[bool] = True,\n+        split_resolutions: list[tuple[int, int]] | None = None,\n+        split_image: bool | None = False,\n+        do_convert_rgb: bool | None = True,\n         do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: Optional[bool] = True,\n+        rescale_factor: int | float = 1 / 255,\n+        do_normalize: bool | None = True,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         **kwargs,\n     ):\n@@ -501,20 +499,20 @@ def __init__(\n \n     def preprocess(\n         self,\n-        images: Union[ImageInput, list[ImageInput]],\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        max_image_size: Optional[int] = None,\n-        min_image_size: Optional[int] = None,\n-        split_image: Optional[bool] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        images: ImageInput | list[ImageInput],\n+        image_mean: float | list[float] | None = None,\n+        image_std: float | list[float] | None = None,\n+        max_image_size: int | None = None,\n+        min_image_size: int | None = None,\n+        split_image: bool | None = None,\n+        do_convert_rgb: bool | None = None,\n+        do_rescale: bool | None = None,\n+        rescale_factor: float | None = None,\n+        do_normalize: bool | None = None,\n+        resample: PILImageResampling | None = None,\n+        return_tensors: str | TensorType | None = \"pt\",\n+        data_format: ChannelDimension | None = ChannelDimension.FIRST,\n+        input_data_format: str | ChannelDimension | None = None,\n     ):\n         \"\"\"\n         Process a list of images.\n@@ -744,11 +742,11 @@ def _pad_for_patching(\n     def pad(\n         self,\n         image: np.ndarray,\n-        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n+        padding: int | tuple[int, int] | Iterable[tuple[int, int]],\n         mode: PaddingMode = PaddingMode.CONSTANT,\n-        constant_values: Union[float, Iterable[float]] = 0.0,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        constant_values: float | Iterable[float] = 0.0,\n+        data_format: str | ChannelDimension | None = None,\n+        input_data_format: str | ChannelDimension | None = None,\n     ) -> np.ndarray:\n         \"\"\"\n         Pads the `image` with the specified `padding` and `mode`. Padding can be in the (`height`, `width`)\n@@ -919,9 +917,9 @@ class AriaProcessor(ProcessorMixin):\n     def __init__(\n         self,\n         image_processor=None,\n-        tokenizer: Union[AutoTokenizer, str] = None,\n-        chat_template: Optional[str] = None,\n-        size_conversion: Optional[dict[Union[float, int], int]] = None,\n+        tokenizer: AutoTokenizer | str = None,\n+        chat_template: str | None = None,\n+        size_conversion: dict[float | int, int] | None = None,\n     ):\n         if size_conversion is None:\n             size_conversion = {490: 128, 980: 256}\n@@ -936,8 +934,8 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n-        images: Optional[ImageInput] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput],\n+        images: ImageInput | None = None,\n         **kwargs: Unpack[AriaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1276,7 +1274,7 @@ def _create_patch_attention_mask(self, pixel_mask):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.FloatTensor] = None,\n+        pixel_mask: torch.FloatTensor | None = None,\n         vision_feature_layer: int = -1,\n     ):\n         \"\"\"\n@@ -1312,17 +1310,17 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, AriaModelOutputWithPast]:\n+    ) -> tuple | AriaModelOutputWithPast:\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -1372,7 +1370,7 @@ class AriaForConditionalGeneration(LlavaForConditionalGeneration):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.FloatTensor] = None,\n+        pixel_mask: torch.FloatTensor | None = None,\n         vision_feature_layer: int = -1,\n     ):\n         return self.model.get_image_features(\n@@ -1385,19 +1383,19 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, AriaCausalLMOutputWithPast]:\n+    ) -> tuple | AriaCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "e36e7773b8b87d8a2c57c5f7c41c850da256910a",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aria.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2024 The Rhymes-AI Teams Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n \n import numpy as np\n \n@@ -70,9 +68,9 @@ class AriaProcessor(ProcessorMixin):\n     def __init__(\n         self,\n         image_processor=None,\n-        tokenizer: Union[AutoTokenizer, str] = None,\n-        chat_template: Optional[str] = None,\n-        size_conversion: Optional[dict[Union[float, int], int]] = None,\n+        tokenizer: AutoTokenizer | str = None,\n+        chat_template: str | None = None,\n+        size_conversion: dict[float | int, int] | None = None,\n     ):\n         if size_conversion is None:\n             size_conversion = {490: 128, 980: 256}\n@@ -87,8 +85,8 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n-        images: Optional[ImageInput] = None,\n+        text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput],\n+        images: ImageInput | None = None,\n         **kwargs: Unpack[AriaProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "1b89a93d29dc51ee6427b095bf39124e575f2008",
            "filename": "src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 Google AI and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b099b8b6b8ef7df1655337c6e228ff4b55babff2",
            "filename": "src/transformers/models/audio_spectrogram_transformer/convert_audio_spectrogram_transformer_original_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "ee69d1d0b991b33eb22e4b65893efdae78b7d942",
            "filename": "src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,8 +15,6 @@\n Feature extractor class for Audio Spectrogram Transformer.\n \"\"\"\n \n-from typing import Optional, Union\n-\n import numpy as np\n \n from ...audio_utils import mel_filter_bank, spectrogram, window_function\n@@ -160,9 +157,9 @@ def normalize(self, input_values: np.ndarray) -> np.ndarray:\n \n     def __call__(\n         self,\n-        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n-        sampling_rate: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        raw_speech: np.ndarray | list[float] | list[np.ndarray] | list[list[float]],\n+        sampling_rate: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "fe12caa0e6df0d6c471dd4c5df386e5e05d2f1c0",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 MIT and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"PyTorch Audio Spectrogram Transformer (AST) model.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -103,8 +101,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -302,7 +300,7 @@ class ASTPreTrainedModel(PreTrainedModel):\n     }\n \n     @torch.no_grad()\n-    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+    def _init_weights(self, module: nn.Linear | nn.Conv2d | nn.LayerNorm) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n             init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n@@ -338,7 +336,7 @@ def get_input_embeddings(self) -> ASTPatchEmbeddings:\n     @auto_docstring\n     def forward(\n         self,\n-        input_values: Optional[torch.Tensor] = None,\n+        input_values: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -401,8 +399,8 @@ def __init__(self, config: ASTConfig) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        input_values: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        input_values: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> SequenceClassifierOutput:\n         r\"\"\""
        },
        {
            "sha": "28a2a4aa0066d153c9b909deaf1c287e9772b0f2",
            "filename": "src/transformers/models/audioflamingo3/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2F__init__.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n # reserved.\n #"
        },
        {
            "sha": "9eca494dbd79e105b1d923cd59fb88c1ae37e523",
            "filename": "src/transformers/models/audioflamingo3/configuration_audioflamingo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconfiguration_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconfiguration_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconfiguration_audioflamingo3.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n # reserved.\n #"
        },
        {
            "sha": "246e37edd7297422546cc10541e8afa1e58c82d1",
            "filename": "src/transformers/models/audioflamingo3/convert_audioflamingo3_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconvert_audioflamingo3_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconvert_audioflamingo3_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconvert_audioflamingo3_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n # reserved.\n #"
        },
        {
            "sha": "a56e685e1ec1f76591b63b3c64c611c1bd9cc1d6",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_audioflamingo3.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n # reserved.\n #\n@@ -20,9 +19,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+\n import math\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -49,8 +48,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs,\n ):\n@@ -81,8 +80,8 @@ def __init__(\n         is_decoder: bool = False,\n         bias: bool = True,\n         is_causal: bool = False,\n-        layer_idx: Optional[int] = None,\n-        config: Optional[AudioFlamingo3Config] = None,\n+        layer_idx: int | None = None,\n+        config: AudioFlamingo3Config | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -116,15 +115,15 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -322,7 +321,7 @@ def set_input_embeddings(self, value: nn.Module):\n     def forward(\n         self,\n         input_features: torch.Tensor,\n-        input_features_mask: Optional[torch.Tensor] = None,\n+        input_features_mask: torch.Tensor | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -477,17 +476,17 @@ def get_audio_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        input_features: Optional[torch.FloatTensor] = None,\n-        input_features_mask: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        input_features: torch.FloatTensor | None = None,\n+        input_features_mask: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "82222dccef95efd2e22f2f5dd245072dda9c34b6",
            "filename": "src/transformers/models/audioflamingo3/modular_audioflamingo3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n # reserved.\n #\n@@ -14,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -59,7 +57,7 @@ class AudioFlamingo3Encoder(Qwen2AudioEncoder):\n     def forward(\n         self,\n         input_features: torch.Tensor,\n-        input_features_mask: Optional[torch.Tensor] = None,\n+        input_features_mask: torch.Tensor | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -177,17 +175,17 @@ def get_audio_embeds(self):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        input_features: Optional[torch.FloatTensor] = None,\n-        input_features_mask: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        input_features: torch.FloatTensor | None = None,\n+        input_features_mask: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "1dd766c14da2870a4e4208c86ed03f64a57d0a8d",
            "filename": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n # reserved.\n #"
        },
        {
            "sha": "93e7cc4e69730df9881870d41e2caeca07d81f89",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n import os\n from collections import OrderedDict\n from collections.abc import Iterator\n-from typing import Any, TypeVar, Union\n+from typing import Any, TypeVar\n \n from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n@@ -46,7 +45,7 @@\n \n _T = TypeVar(\"_T\")\n # Tokenizers will depend on packages installed, too much variance and there are no common base or Protocol\n-_LazyAutoMappingValue = tuple[Union[type[Any], None], Union[type[Any], None]]\n+_LazyAutoMappingValue = tuple[type[Any] | None, type[Any] | None]\n \n CLASS_DOCSTRING = \"\"\"\n     This is a generic model class that will be instantiated as one of the model classes of the library when created\n@@ -247,7 +246,7 @@ def _prepare_config_for_auto_class(cls, config: PreTrainedConfig) -> PreTrainedC\n         return config\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[str]], *model_args, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike[str], *model_args, **kwargs):\n         config = kwargs.pop(\"config\", None)\n         trust_remote_code = kwargs.get(\"trust_remote_code\")\n         kwargs[\"_from_auto\"] = True\n@@ -592,7 +591,7 @@ def keys(self) -> list[type[PreTrainedConfig]]:\n         ]\n         return mapping_keys + list(self._extra_content.keys())\n \n-    def get(self, key: type[PreTrainedConfig], default: _T) -> Union[_LazyAutoMappingValue, _T]:\n+    def get(self, key: type[PreTrainedConfig], default: _T) -> _LazyAutoMappingValue | _T:\n         try:\n             return self.__getitem__(key)\n         except KeyError:"
        },
        {
            "sha": "a110207c5e67236a02b65d376d914c5c4b7bd682",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,7 +18,7 @@\n import re\n from collections import OrderedDict\n from collections.abc import Callable, Iterator, KeysView, ValuesView\n-from typing import Any, TypeVar, Union\n+from typing import Any, TypeVar\n \n from ...configuration_utils import PreTrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n@@ -1035,7 +1034,7 @@ def model_type_to_module_name(key) -> str:\n     return key\n \n \n-def config_class_to_model_type(config) -> Union[str, None]:\n+def config_class_to_model_type(config) -> str | None:\n     \"\"\"Converts a config class name to the corresponding model type\"\"\"\n     for key, cls in CONFIG_MAPPING_NAMES.items():\n         if cls == config:\n@@ -1152,7 +1151,7 @@ def __contains__(self, item: object) -> bool:\n         return item in self._data\n \n \n-def _get_class_name(model_class: Union[str, list[str]]):\n+def _get_class_name(model_class: str | list[str]):\n     if isinstance(model_class, (list, tuple)):\n         return \" or \".join([f\"[`{c}`]\" for c in model_class if c is not None])\n     return f\"[`{model_class}`]\"\n@@ -1245,7 +1244,7 @@ def for_model(cls, model_type: str, *args, **kwargs) -> PreTrainedConfig:\n \n     @classmethod\n     @replace_list_option_in_docstrings()\n-    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[str]], **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike[str], **kwargs):\n         r\"\"\"\n         Instantiate one of the configuration classes of the library from a pretrained model configuration.\n "
        },
        {
            "sha": "25b980443cf87a0c0c00ca94bbea2750a3a61e10",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n import importlib\n import os\n from collections import OrderedDict\n-from typing import Optional, Union\n \n # Build the list of all feature extractors\n from ...configuration_utils import PreTrainedConfig\n@@ -115,12 +113,12 @@ def feature_extractor_class_from_name(class_name: str):\n \n \n def get_feature_extractor_config(\n-    pretrained_model_name_or_path: Union[str, os.PathLike],\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n+    pretrained_model_name_or_path: str | os.PathLike,\n+    cache_dir: str | os.PathLike | None = None,\n     force_download: bool = False,\n-    proxies: Optional[dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n+    proxies: dict[str, str] | None = None,\n+    token: bool | str | None = None,\n+    revision: str | None = None,\n     local_files_only: bool = False,\n     **kwargs,\n ):"
        },
        {
            "sha": "f330c3c4bab6e807cee5c48a3a0baf5d9203eeaf",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "ef135b8db03b22b0f3a04da249a1b153696a6c42",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,7 @@\n import os\n import warnings\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING\n \n from ...utils import logging\n from .auto_factory import (\n@@ -1982,7 +1981,7 @@ class AutoModelForCausalLM(_BaseAutoModelClass):\n     @classmethod\n     def from_pretrained(\n         cls: type[\"AutoModelForCausalLM\"],\n-        pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n+        pretrained_model_name_or_path: str | os.PathLike[str],\n         *model_args,\n         **kwargs,\n     ) -> \"_BaseModelWithGenerate\":\n@@ -2186,7 +2185,7 @@ class AutoModelForImageTextToText(_BaseAutoModelClass):\n     @classmethod\n     def from_pretrained(\n         cls: type[\"AutoModelForImageTextToText\"],\n-        pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n+        pretrained_model_name_or_path: str | os.PathLike[str],\n         *model_args,\n         **kwargs,\n     ) -> \"_BaseModelWithGenerate\":"
        },
        {
            "sha": "8ec4f0254d2421badedce202df9c13879cb77191",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "de522d4ef0caa21e3ace2000f259bd07f1c062e3",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,7 @@\n import json\n import os\n from collections import OrderedDict\n-from typing import Any, Optional, Union\n+from typing import Any\n \n from transformers.utils.import_utils import is_mistral_common_available\n \n@@ -61,7 +60,7 @@\n REGISTERED_TOKENIZER_CLASSES: dict[str, type[Any]] = {}\n REGISTERED_FAST_ALIASES: dict[str, type[Any]] = {}\n \n-TOKENIZER_MAPPING_NAMES = OrderedDict[str, Optional[str]](\n+TOKENIZER_MAPPING_NAMES = OrderedDict[str, str | None](\n     [\n         (\"aimv2\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"albert\", \"AlbertTokenizer\" if is_tokenizers_available() else None),\n@@ -335,7 +334,7 @@ def load_merges(merges_file):\n     return merges\n \n \n-def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n+def tokenizer_class_from_name(class_name: str) -> type[Any] | None:\n     # Bloom tokenizer classes were removed but should map to the fast backend for BC\n     if class_name in {\"BloomTokenizer\", \"BloomTokenizerFast\"}:\n         return TokenizersBackend\n@@ -380,12 +379,12 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n \n \n def get_tokenizer_config(\n-    pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n-    cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n+    pretrained_model_name_or_path: str | os.PathLike[str],\n+    cache_dir: str | os.PathLike[str] | None = None,\n     force_download: bool = False,\n-    proxies: Optional[dict[str, str]] = None,\n-    token: Optional[Union[bool, str]] = None,\n-    revision: Optional[str] = None,\n+    proxies: dict[str, str] | None = None,\n+    token: bool | str | None = None,\n+    revision: str | None = None,\n     local_files_only: bool = False,\n     subfolder: str = \"\",\n     **kwargs,\n@@ -493,7 +492,7 @@ def __init__(self):\n     @replace_list_option_in_docstrings(TOKENIZER_MAPPING_NAMES)\n     def from_pretrained(\n         cls, pretrained_model_name_or_path, *inputs, **kwargs\n-    ) -> Union[TokenizersBackend, SentencePieceBackend]:\n+    ) -> TokenizersBackend | SentencePieceBackend:\n         r\"\"\"\n         Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n "
        },
        {
            "sha": "9185e8aa4b51fd66b53c973f5c157392d8ab57dd",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "3ea6fb5af3276e6ea417e3da29966a65eeb7af5c",
            "filename": "src/transformers/models/autoformer/configuration_autoformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"Autoformer model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n \n@@ -137,8 +134,8 @@ class AutoformerConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        prediction_length: Optional[int] = None,\n-        context_length: Optional[int] = None,\n+        prediction_length: int | None = None,\n+        context_length: int | None = None,\n         distribution_output: str = \"student_t\",\n         loss: str = \"nll\",\n         input_size: int = 1,\n@@ -148,8 +145,8 @@ def __init__(\n         num_dynamic_real_features: int = 0,\n         num_static_categorical_features: int = 0,\n         num_static_real_features: int = 0,\n-        cardinality: Optional[list[int]] = None,\n-        embedding_dimension: Optional[list[int]] = None,\n+        cardinality: list[int] | None = None,\n+        embedding_dimension: list[int] | None = None,\n         d_model: int = 64,\n         encoder_attention_heads: int = 2,\n         decoder_attention_heads: int = 2,"
        },
        {
            "sha": "75a23adb125f3d95559d0980fbef323d8ffa4c17",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 98,
            "deletions": 100,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright (c) 2021 THUML @ Tsinghua University\n # Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n@@ -18,7 +17,6 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -75,12 +73,12 @@ class AutoFormerDecoderOutput(ModelOutput):\n         weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    trend: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    trend: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n+    cross_attentions: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -113,18 +111,18 @@ class AutoformerModelOutput(ModelOutput):\n         Static features of each time series' in a batch which are copied to the covariates at inference time.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    trend: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    loc: Optional[torch.FloatTensor] = None\n-    scale: Optional[torch.FloatTensor] = None\n-    static_features: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    trend: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    decoder_hidden_states: tuple[torch.FloatTensor] | None = None\n+    decoder_attentions: tuple[torch.FloatTensor] | None = None\n+    cross_attentions: tuple[torch.FloatTensor] | None = None\n+    encoder_last_hidden_state: torch.FloatTensor | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor] | None = None\n+    loc: torch.FloatTensor | None = None\n+    scale: torch.FloatTensor | None = None\n+    static_features: torch.FloatTensor | None = None\n \n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesFeatureEmbedder with TimeSeries->Autoformer\n@@ -265,7 +263,7 @@ def __init__(self, config: AutoformerConfig):\n         self.keepdim = config.keepdim if hasattr(config, \"keepdim\") else True\n \n     def forward(\n-        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = None\n+        self, data: torch.Tensor, observed_indicator: torch.Tensor | None = None\n     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:\n@@ -282,7 +280,7 @@ def forward(\n \n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.weighted_average\n-def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = None, dim=None) -> torch.Tensor:\n+def weighted_average(input_tensor: torch.Tensor, weights: torch.Tensor | None = None, dim=None) -> torch.Tensor:\n     \"\"\"\n     Computes the weighted average of a given tensor across a given `dim`, masking values associated with weight zero,\n     meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n@@ -318,7 +316,7 @@ def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.\n class AutoformerSinusoidalPositionalEmbedding(nn.Embedding):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n-    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n+    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: int | None = None) -> None:\n         super().__init__(num_positions, embedding_dim, _freeze=True)\n \n     def create_weight(self):\n@@ -338,7 +336,7 @@ def create_weight(self):\n \n     @torch.no_grad()\n     def forward(\n-        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: torch.Tensor | None = None\n     ) -> torch.Tensor:\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n         if position_ids is None:\n@@ -418,11 +416,11 @@ def __init__(\n         self,\n         embed_dim: int,\n         num_heads: int,\n-        dropout: Optional[float] = 0.0,\n-        is_decoder: Optional[bool] = False,\n-        bias: Optional[bool] = True,\n-        autocorrelation_factor: Optional[int] = 3,\n-        layer_idx: Optional[int] = None,\n+        dropout: float | None = 0.0,\n+        is_decoder: bool | None = False,\n+        bias: bool | None = True,\n+        autocorrelation_factor: int | None = 3,\n+        layer_idx: int | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -449,12 +447,12 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        output_attentions: bool | None = False,\n+        cache_position: torch.Tensor | None = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -641,8 +639,8 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -736,14 +734,14 @@ def __init__(self, config: AutoformerConfig, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        cache_position: torch.Tensor | None = None,\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -848,7 +846,7 @@ def _init_weights(self, module: nn.Module):\n     # copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask(\n         self,\n-        attention_mask: Union[torch.Tensor, None],\n+        attention_mask: torch.Tensor | None,\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n@@ -898,13 +896,13 @@ def __init__(self, config: AutoformerConfig):\n \n     def forward(\n         self,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> tuple | BaseModelOutput:\n         r\"\"\"\n         Args:\n             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1014,19 +1012,19 @@ def __init__(self, config: AutoformerConfig):\n \n     def forward(\n         self,\n-        trend: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        trend: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, AutoFormerDecoderOutput]:\n+    ) -> tuple | AutoFormerDecoderOutput:\n         r\"\"\"\n         Args:\n             trend (`torch.FloatTensor` of shape `(batch_size, prediction_length, feature_size)`, *optional*):\n@@ -1248,11 +1246,11 @@ def create_network_inputs(\n         self,\n         past_values: torch.Tensor,\n         past_time_features: torch.Tensor,\n-        static_categorical_features: Optional[torch.Tensor] = None,\n-        static_real_features: Optional[torch.Tensor] = None,\n-        past_observed_mask: Optional[torch.Tensor] = None,\n-        future_values: Optional[torch.Tensor] = None,\n-        future_time_features: Optional[torch.Tensor] = None,\n+        static_categorical_features: torch.Tensor | None = None,\n+        static_real_features: torch.Tensor | None = None,\n+        past_observed_mask: torch.Tensor | None = None,\n+        future_values: torch.Tensor | None = None,\n+        future_time_features: torch.Tensor | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Creates the inputs for the network given the past and future values, time features, and static features.\n@@ -1350,20 +1348,20 @@ def forward(\n         past_values: torch.Tensor,\n         past_time_features: torch.Tensor,\n         past_observed_mask: torch.Tensor,\n-        static_categorical_features: Optional[torch.Tensor] = None,\n-        static_real_features: Optional[torch.Tensor] = None,\n-        future_values: Optional[torch.Tensor] = None,\n-        future_time_features: Optional[torch.Tensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        use_cache: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        static_categorical_features: torch.Tensor | None = None,\n+        static_real_features: torch.Tensor | None = None,\n+        future_values: torch.Tensor | None = None,\n+        future_time_features: torch.Tensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        output_hidden_states: bool | None = None,\n+        output_attentions: bool | None = None,\n+        use_cache: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[AutoformerModelOutput, tuple]:\n+    ) -> AutoformerModelOutput | tuple:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Past values of the time series, that serve as context in order to predict the future. These values may\n@@ -1601,20 +1599,20 @@ def forward(\n         past_values: torch.Tensor,\n         past_time_features: torch.Tensor,\n         past_observed_mask: torch.Tensor,\n-        static_categorical_features: Optional[torch.Tensor] = None,\n-        static_real_features: Optional[torch.Tensor] = None,\n-        future_values: Optional[torch.Tensor] = None,\n-        future_time_features: Optional[torch.Tensor] = None,\n-        future_observed_mask: Optional[torch.Tensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        use_cache: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        static_categorical_features: torch.Tensor | None = None,\n+        static_real_features: torch.Tensor | None = None,\n+        future_values: torch.Tensor | None = None,\n+        future_time_features: torch.Tensor | None = None,\n+        future_observed_mask: torch.Tensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        output_hidden_states: bool | None = None,\n+        output_attentions: bool | None = None,\n+        use_cache: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[Seq2SeqTSPredictionOutput, tuple]:\n+    ) -> Seq2SeqTSPredictionOutput | tuple:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Past values of the time series, that serve as context in order to predict the future. These values may\n@@ -1839,11 +1837,11 @@ def generate(\n         past_values: torch.Tensor,\n         past_time_features: torch.Tensor,\n         future_time_features: torch.Tensor,\n-        past_observed_mask: Optional[torch.Tensor] = None,\n-        static_categorical_features: Optional[torch.Tensor] = None,\n-        static_real_features: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        past_observed_mask: torch.Tensor | None = None,\n+        static_categorical_features: torch.Tensor | None = None,\n+        static_real_features: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n     ) -> SampleTSPredictionOutput:\n         r\"\"\"\n         Greedily generate sequences of sample predictions from a model with a probability distribution head."
        },
        {
            "sha": "671026a8b7143eb899d64ee23f4a0410579ca1d5",
            "filename": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 Cohere team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0204f171dd3bd1dee27e18cafeb0b4a1d05f84d6",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 35,
            "deletions": 37,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_aya_vision.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 the Cohere Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,6 @@\n # limitations under the License.\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -128,12 +126,12 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    past_key_values: Cache | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n+    image_hidden_states: torch.FloatTensor | None = None\n \n \n @dataclass\n@@ -154,7 +152,7 @@ class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n         image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+    image_hidden_states: torch.FloatTensor | None = None\n \n \n @auto_docstring(\n@@ -184,8 +182,8 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -264,18 +262,18 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n+    ) -> tuple | AyaVisionModelOutputWithPast:\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -354,8 +352,8 @@ def get_output_embeddings(self) -> nn.Module:\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n         **kwargs,\n     ):\n         return self.model.get_image_features(\n@@ -369,20 +367,20 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n+        labels: torch.LongTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n+        image_sizes: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, AyaVisionCausalLMOutputWithPast]:\n+    ) -> tuple | AyaVisionCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "e0c4b23c4edb89d0038e87a56211ca61d94443ec",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 26,
            "deletions": 29,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 the Cohere Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"PyTorch AyaVision model.\"\"\"\n \n-from typing import Optional, Union\n-\n import torch\n from torch import nn\n \n@@ -110,8 +107,8 @@ class AyaVisionModel(LlavaModel):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -166,18 +163,18 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n+    ) -> tuple | AyaVisionModelOutputWithPast:\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -227,20 +224,20 @@ def forward(\n class AyaVisionForConditionalGeneration(LlavaForConditionalGeneration):\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        vision_feature_layer: int | list[int] | None = None,\n+        vision_feature_select_strategy: str | None = None,\n+        labels: torch.LongTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n+        image_sizes: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, AyaVisionCausalLMOutputWithPast]:\n+    ) -> tuple | AyaVisionCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "5dd80e4b0f4605cbf054dbe2792d9858c412b132",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "09fce8659ae3022dc55a0b020f8617e1df59ea78",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 29,
            "deletions": 32,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"Bamba model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n@@ -116,35 +113,35 @@ class BambaConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 128000,\n-        tie_word_embeddings: Optional[bool] = False,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 14336,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = 8,\n-        hidden_act: Optional[str] = \"silu\",\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        num_logits_to_keep: Optional[int] = 1,\n-        pad_token_id: Optional[int] = 0,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        max_position_embeddings: Optional[int] = 262144,\n-        attention_dropout: Optional[float] = 0.0,\n-        attn_layer_indices: Optional[list[int]] = None,\n-        mamba_n_heads: Optional[int] = 128,\n-        mamba_d_head: Optional[str] = \"auto\",\n-        mamba_n_groups: Optional[int] = 1,\n-        mamba_d_state: Optional[int] = 256,\n-        mamba_d_conv: Optional[int] = 4,\n-        mamba_expand: Optional[int] = 2,\n-        mamba_chunk_size: Optional[int] = 256,\n-        mamba_conv_bias: Optional[bool] = True,\n-        mamba_proj_bias: Optional[bool] = False,\n-        z_loss_coefficient: Optional[float] = 0.0,\n-        rope_parameters: Optional[RopeParameters] = None,\n+        vocab_size: int | None = 128000,\n+        tie_word_embeddings: bool | None = False,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 14336,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = 8,\n+        hidden_act: str | None = \"silu\",\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: float | None = 1e-5,\n+        use_cache: bool | None = True,\n+        num_logits_to_keep: int | None = 1,\n+        pad_token_id: int | None = 0,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        max_position_embeddings: int | None = 262144,\n+        attention_dropout: float | None = 0.0,\n+        attn_layer_indices: list[int] | None = None,\n+        mamba_n_heads: int | None = 128,\n+        mamba_d_head: str | None = \"auto\",\n+        mamba_n_groups: int | None = 1,\n+        mamba_d_state: int | None = 256,\n+        mamba_d_conv: int | None = 4,\n+        mamba_expand: int | None = 2,\n+        mamba_chunk_size: int | None = 256,\n+        mamba_conv_bias: bool | None = True,\n+        mamba_proj_bias: bool | None = False,\n+        z_loss_coefficient: float | None = 0.0,\n+        rope_parameters: RopeParameters | None = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "be92f7f62dca1283465cdb77e7fbbddc2bad9505",
            "filename": "src/transformers/models/bamba/convert_mamba_ssm_checkpoint.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,7 +18,6 @@\n import os\n import re\n from os import path\n-from typing import Optional, Union\n \n import torch\n from huggingface_hub import split_torch_state_dict_into_shards\n@@ -144,7 +142,7 @@ def save_sharded_safetensors(\n     state_dict: dict,\n     save_directory: str,\n     metadata: dict,\n-    max_shard_size: Union[int, str] = \"5GB\",\n+    max_shard_size: int | str = \"5GB\",\n ):\n     filename_pattern = SAFE_WEIGHTS_NAME.replace(\".bin\", \"{suffix}.bin\").replace(\n         \".safetensors\", \"{suffix}.safetensors\"\n@@ -172,8 +170,8 @@ def convert_mamba_ssm_checkpoint_file_to_huggingface_model_file(\n     mamba_ssm_checkpoint_path: str,\n     precision: str,\n     output_dir: str,\n-    tokenizer_path: Optional[str] = None,\n-    save_model: Union[bool, str] = True,\n+    tokenizer_path: str | None = None,\n+    save_model: bool | str = True,\n ) -> None:\n     # load tokenizer if provided, this will be used to set the\n     # token_ids in the config file"
        },
        {
            "sha": "944d97fa342f3d131a95b81a94483ea8086363ff",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 49,
            "deletions": 50,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_bamba.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n # This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n@@ -25,7 +24,7 @@\n # limitations under the License.\n \n from collections.abc import Callable\n-from typing import Any, Optional, TypedDict, Union\n+from typing import Any, Optional, TypedDict\n \n import torch\n from torch import nn\n@@ -140,7 +139,7 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n+        cache_kwargs: dict[str, Any] | None = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         # Update the cache\n         if self.key_cache[layer_idx].shape[-1] == 0:\n@@ -173,7 +172,7 @@ def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[\n         kv_length = self.get_seq_length(layer_idx) + query_length\n         return kv_length, kv_offset\n \n-    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+    def get_seq_length(self, layer_idx: int | None = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # take any layer that contains cache and not empty tensor\n         layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n@@ -203,9 +202,9 @@ def __init__(self, config: BambaConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[BambaConfig] = None,\n+        config: BambaConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -271,7 +270,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -363,10 +362,10 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -585,10 +584,10 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n     def cuda_kernels_forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        seq_idx: Optional[torch.IntTensor] = None,\n+        cache_params: HybridMambaAttentionDynamicCache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        seq_idx: torch.IntTensor | None = None,\n     ):\n         # 1. Gated MLP's linear projection\n         hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n@@ -755,9 +754,9 @@ def cuda_kernels_forward(\n     def torch_forward(\n         self,\n         input_states,\n-        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        cache_params: HybridMambaAttentionDynamicCache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n     ):\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n@@ -962,10 +961,10 @@ def torch_forward(\n     def forward(\n         self,\n         hidden_states,\n-        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        seq_idx: Optional[torch.IntTensor] = None,\n+        cache_params: HybridMambaAttentionDynamicCache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        seq_idx: torch.IntTensor | None = None,\n         **kwargs,\n     ):\n         if is_fast_path_available and \"cuda\" in self.in_proj.weight.device.type:\n@@ -1040,15 +1039,15 @@ def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: HybridMambaAttentionDynamicCache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -1161,15 +1160,15 @@ def __init__(self, config: BambaConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: HybridMambaAttentionDynamicCache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1403,17 +1402,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: HybridMambaAttentionDynamicCache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "593935be004dce16ff7160affdae9e0fe87d5dd2",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 40,
            "deletions": 41,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n # This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n@@ -19,7 +18,7 @@\n # limitations under the License.\n \"\"\"PyTorch Bamba model.\"\"\"\n \n-from typing import Optional, TypedDict, Union\n+from typing import TypedDict\n \n import torch\n from torch import nn\n@@ -288,10 +287,10 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n     def cuda_kernels_forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        seq_idx: Optional[torch.IntTensor] = None,\n+        cache_params: HybridMambaAttentionDynamicCache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        seq_idx: torch.IntTensor | None = None,\n     ):\n         # 1. Gated MLP's linear projection\n         hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n@@ -458,9 +457,9 @@ def cuda_kernels_forward(\n     def torch_forward(\n         self,\n         input_states,\n-        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        cache_params: HybridMambaAttentionDynamicCache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n     ):\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n@@ -665,10 +664,10 @@ def torch_forward(\n     def forward(\n         self,\n         hidden_states,\n-        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        seq_idx: Optional[torch.IntTensor] = None,\n+        cache_params: HybridMambaAttentionDynamicCache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        seq_idx: torch.IntTensor | None = None,\n         **kwargs,\n     ):\n         if is_fast_path_available and \"cuda\" in self.in_proj.weight.device.type:\n@@ -714,15 +713,15 @@ def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: HybridMambaAttentionDynamicCache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -835,15 +834,15 @@ def __init__(self, config: BambaConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: HybridMambaAttentionDynamicCache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1067,17 +1066,17 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: HybridMambaAttentionDynamicCache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "2b66c1e82db834fa24391389b5b8d9008dcbf534",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Suno AI Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"BARK model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import add_start_docstrings, logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -243,10 +240,10 @@ class BarkConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        semantic_config: Optional[dict] = None,\n-        coarse_acoustics_config: Optional[dict] = None,\n-        fine_acoustics_config: Optional[dict] = None,\n-        codec_config: Optional[dict] = None,\n+        semantic_config: dict | None = None,\n+        coarse_acoustics_config: dict | None = None,\n+        fine_acoustics_config: dict | None = None,\n+        codec_config: dict | None = None,\n         initializer_range=0.02,\n         **kwargs,\n     ):"
        },
        {
            "sha": "a1f60580aecf1a5dd042b92d44c534041077dfac",
            "filename": "src/transformers/models/bark/generation_configuration_bark.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Suno AI Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"BARK model generation configuration\"\"\"\n \n import copy\n-from typing import Optional\n \n from ...generation.configuration_utils import GenerationConfig\n from ...utils import logging\n@@ -244,9 +242,9 @@ class BarkGenerationConfig(GenerationConfig):\n \n     def __init__(\n         self,\n-        semantic_config: Optional[dict] = None,\n-        coarse_acoustics_config: Optional[dict] = None,\n-        fine_acoustics_config: Optional[dict] = None,\n+        semantic_config: dict | None = None,\n+        coarse_acoustics_config: dict | None = None,\n+        fine_acoustics_config: dict | None = None,\n         sample_rate=24_000,\n         codebook_size=1024,\n         **kwargs,"
        },
        {
            "sha": "22cef72dfec1cb15a247817c83f5162e08e3f124",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 39,
            "deletions": 41,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Suno AI Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -424,19 +422,19 @@ def prepare_inputs_for_generation(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        input_embeds: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        input_embeds: torch.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n+    ) -> tuple[torch.Tensor] | CausalLMOutputWithPast:\n         r\"\"\"\n         input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n@@ -569,9 +567,9 @@ class BarkSemanticModel(BarkCausalModel):\n     def generate(\n         self,\n         input_ids: torch.Tensor,\n-        semantic_generation_config: Optional[BarkSemanticGenerationConfig] = None,\n-        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        semantic_generation_config: BarkSemanticGenerationConfig | None = None,\n+        history_prompt: dict[str, torch.Tensor] | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n         \"\"\"\n@@ -686,7 +684,7 @@ def preprocess_histories(\n         batch_size: int,\n         semantic_generation_config: int,\n         codebook_size: int,\n-        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n+        history_prompt: dict[str, torch.Tensor] | None = None,\n     ):\n         \"\"\"\n         Preprocess the optional `Bark` speaker prompts before `self.generate`.\n@@ -756,13 +754,13 @@ def preprocess_histories(\n     def generate(\n         self,\n         semantic_output: torch.Tensor,\n-        semantic_generation_config: Optional[BarkSemanticGenerationConfig] = None,\n-        coarse_generation_config: Optional[BarkCoarseGenerationConfig] = None,\n+        semantic_generation_config: BarkSemanticGenerationConfig | None = None,\n+        coarse_generation_config: BarkCoarseGenerationConfig | None = None,\n         codebook_size: int = 1024,\n-        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n-        return_output_lengths: Optional[bool] = None,\n+        history_prompt: dict[str, torch.Tensor] | None = None,\n+        return_output_lengths: bool | None = None,\n         **kwargs,\n-    ) -> Union[torch.LongTensor, tuple[torch.LongTensor, torch.LongTensor]]:\n+    ) -> torch.LongTensor | tuple[torch.LongTensor, torch.LongTensor]:\n         \"\"\"\n         Generates coarse acoustics tokens from input text semantic tokens and an additional optional `Bark` speaker\n         prompt.\n@@ -978,8 +976,8 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean\n \n     def resize_token_embeddings(\n         self,\n-        new_num_tokens: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n+        new_num_tokens: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n         mean_resizing: bool = True,\n     ) -> nn.Embedding:\n         \"\"\"\n@@ -1030,16 +1028,16 @@ def resize_token_embeddings(\n     def forward(\n         self,\n         codebook_idx: int,  # an additional idx corresponding to the id of the codebook that will be predicted\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        input_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        input_embeds: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> tuple[torch.Tensor] | MaskedLMOutput:\n         r\"\"\"\n         codebook_idx (`int`):\n             Index of the codebook that will be predicted.\n@@ -1149,11 +1147,11 @@ def forward(\n     def generate(\n         self,\n         coarse_output: torch.Tensor,\n-        semantic_generation_config: Optional[BarkSemanticGenerationConfig] = None,\n-        coarse_generation_config: Optional[BarkCoarseGenerationConfig] = None,\n+        semantic_generation_config: BarkSemanticGenerationConfig | None = None,\n+        coarse_generation_config: BarkCoarseGenerationConfig | None = None,\n         fine_generation_config: BarkFineGenerationConfig = None,\n         codebook_size: int = 1024,\n-        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n+        history_prompt: dict[str, torch.Tensor] | None = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n         \"\"\"\n@@ -1345,7 +1343,7 @@ def device(self) -> torch.device:\n \n     def enable_cpu_offload(\n         self,\n-        accelerator_id: Optional[int] = 0,\n+        accelerator_id: int | None = 0,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -1422,9 +1420,9 @@ def codec_decode(self, fine_output, output_lengths=None):\n     @torch.no_grad()\n     def generate(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n-        return_output_lengths: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        history_prompt: dict[str, torch.Tensor] | None = None,\n+        return_output_lengths: bool | None = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n         \"\"\""
        },
        {
            "sha": "8fa04d4ee032abb04546f95a397b3adefb813fbd",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Suno AI Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "6ecc0c58b4a510f878a613de040922c1887beca0",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "ebb295e5b046575c3032ef75b87194613a42a648",
            "filename": "src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbart%2Fconvert_bart_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbart%2Fconvert_bart_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconvert_bart_original_pytorch_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c47311f978b78a42ceee482194a6f4584bcbfea6",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 115,
            "deletions": 117,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Fairseq Authors and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n import math\n import warnings\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -81,7 +79,7 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings + self.offset, embedding_dim)\n \n     def forward(\n-        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+        self, input_ids: torch.Tensor, past_key_values_length: int = 0, position_ids: torch.Tensor | None = None\n     ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n@@ -101,7 +99,7 @@ class BartScaledWordEmbedding(nn.Embedding):\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n     \"\"\"\n \n-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float | None = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n         self.embed_scale = embed_scale\n \n@@ -115,8 +113,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -150,8 +148,8 @@ def __init__(\n         is_decoder: bool = False,\n         bias: bool = True,\n         is_causal: bool = False,\n-        config: Optional[BartConfig] = None,\n-        layer_idx: Optional[int] = None,\n+        config: BartConfig | None = None,\n+        layer_idx: int | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -184,15 +182,15 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -265,7 +263,7 @@ def forward(\n \n \n class BartEncoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BartConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -288,8 +286,8 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -332,7 +330,7 @@ def forward(\n \n \n class BartDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BartConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -366,14 +364,14 @@ def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        cache_position: torch.Tensor | None = None,\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -547,14 +545,14 @@ def __init__(self, config: BartConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> tuple | BaseModelOutput:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -690,19 +688,19 @@ def __init__(self, config: BartConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple | BaseModelOutputWithPastAndCrossAttentions:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -916,21 +914,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqModelOutput]:\n+    ) -> tuple | Seq2SeqModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1044,7 +1042,7 @@ def __init__(self, config: BartConfig):\n         self.post_init()\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+        self, new_num_tokens: int, pad_to_multiple_of: int | None = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n         new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n@@ -1062,22 +1060,22 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqLMOutput]:\n+    ) -> tuple | Seq2SeqLMOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1225,21 +1223,21 @@ def __init__(self, config: BartConfig, **kwargs):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n+    ) -> tuple | Seq2SeqSequenceClassifierOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1357,22 +1355,22 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n+    ) -> tuple | Seq2SeqQuestionAnsweringModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1505,21 +1503,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "c356f25cf3106dcc8e4c5002e8fe595996e50073",
            "filename": "src/transformers/models/barthez/tokenization_barthez.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 Ecole Polytechnique and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License\n \"\"\"Tokenization classes for the BARThez model.\"\"\"\n \n-from typing import Optional, Union\n-\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers\n from tokenizers.models import Unigram\n \n@@ -92,7 +89,7 @@ class BarthezTokenizer(TokenizersBackend):\n \n     def __init__(\n         self,\n-        vocab: Optional[Union[str, dict, list]] = None,\n+        vocab: str | dict | list | None = None,\n         bos_token=\"<s>\",\n         eos_token=\"</s>\",\n         sep_token=\"</s>\","
        },
        {
            "sha": "2905b34ee7f8d219de860dc195312c11fe4b8deb",
            "filename": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 VinAI Research and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,7 @@\n \n import os\n from shutil import copyfile\n-from typing import Any, Optional\n+from typing import Any\n \n from ...tokenization_python import AddedToken\n from ...tokenization_utils_sentencepiece import SentencePieceBackend\n@@ -115,7 +114,7 @@ def __init__(\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        sp_model_kwargs: dict[str, Any] | None = None,\n         **kwargs,\n     ) -> None:\n         # Mask token behave like a normal word, i.e. include the space before it\n@@ -159,7 +158,7 @@ def __init__(\n         self._align_added_tokens_with_fairseq_vocab()\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n     ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n@@ -185,7 +184,7 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None, already_has_special_tokens: bool = False\n     ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n@@ -213,7 +212,7 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n     ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTPho does not\n@@ -289,7 +288,7 @@ def _align_added_tokens_with_fairseq_vocab(self):\n         self._added_tokens_decoder = remapped_decoder\n         self._added_tokens_encoder = {token.content: idx for idx, token in remapped_decoder.items()}\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "dbf435789ff1fd890835581557f5e2049c282f49",
            "filename": "src/transformers/models/beit/configuration_beit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright Microsoft Research and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "bb716f67c90b7400a97808c8bfecd8f81ec54f7b",
            "filename": "src/transformers/models/beit/convert_beit_unilm_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fconvert_beit_unilm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fconvert_beit_unilm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconvert_beit_unilm_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "eab886b4d946c1673ebaf7cd52901ac6ac5c647c",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "a936bbe4f975c5ce63b32e3b68f986c900cca19b",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0c773b48e4a5c8681142cfb4c1d4bc82364a2d9e",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 51,
            "deletions": 53,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 Microsoft Research and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n import collections.abc\n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import Tensor, nn\n@@ -77,7 +75,7 @@ def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = Fals\n class BeitDropPath(nn.Module):\n     \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n \n-    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+    def __init__(self, drop_prob: float | None = None) -> None:\n         super().__init__()\n         self.drop_prob = drop_prob\n \n@@ -162,7 +160,7 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        bool_masked_pos: torch.BoolTensor | None = None,\n     ) -> torch.Tensor:\n         _, _, height, width = pixel_values.shape\n         embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n@@ -224,7 +222,7 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n \n \n class BeitSelfAttention(nn.Module):\n-    def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> None:\n+    def __init__(self, config: BeitConfig, window_size: tuple | None = None) -> None:\n         super().__init__()\n         self.config = config\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n@@ -251,10 +249,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[torch.Tensor] = None,\n+        relative_position_bias: torch.Tensor | None = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int]] = None,\n-    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: tuple[int] | None = None,\n+    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]:\n         batch_size, seq_length, _ = hidden_states.shape\n         query_layer = (\n             self.query(hidden_states)\n@@ -312,10 +310,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[torch.Tensor] = None,\n+        relative_position_bias: torch.Tensor | None = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int]] = None,\n-    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: tuple[int] | None = None,\n+    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]:\n         if output_attentions:\n             logger.warning_once(\n                 f\"{self.__class__.__name__} does not support `output_attentions=True`. The returned attention weights will \"\n@@ -394,7 +392,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, gamma\n \n \n class BeitAttention(nn.Module):\n-    def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> None:\n+    def __init__(self, config: BeitConfig, window_size: tuple | None = None) -> None:\n         super().__init__()\n         self.attention = BEIT_SELF_ATTENTION_CLASSES[config._attn_implementation](config, window_size=window_size)\n         self.output = BeitSelfOutput(config)\n@@ -403,10 +401,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[torch.Tensor] = None,\n+        relative_position_bias: torch.Tensor | None = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int]] = None,\n-    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: tuple[int] | None = None,\n+    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]:\n         self_outputs = self.attention(\n             hidden_states, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution\n         )\n@@ -449,7 +447,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class BeitLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n-    def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None, drop_path_rate: float = 0.0) -> None:\n+    def __init__(self, config: BeitConfig, window_size: tuple | None = None, drop_path_rate: float = 0.0) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n@@ -471,10 +469,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[torch.Tensor] = None,\n+        relative_position_bias: torch.Tensor | None = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int, int]] = None,\n-    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: tuple[int, int] | None = None,\n+    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in BEiT, layernorm is applied before self-attention\n             output_attentions=output_attentions,\n@@ -593,7 +591,7 @@ def forward(self, window_size, interpolate_pos_encoding: bool = False, dim_size=\n \n \n class BeitEncoder(nn.Module):\n-    def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> None:\n+    def __init__(self, config: BeitConfig, window_size: tuple | None = None) -> None:\n         super().__init__()\n         self.config = config\n         self.has_relative_position_bias = config.use_shared_relative_position_bias\n@@ -620,9 +618,9 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int, int]] = None,\n+        resolution: tuple[int, int] | None = None,\n         return_dict: bool = True,\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> tuple | BaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -721,13 +719,13 @@ def get_input_embeddings(self):\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        bool_masked_pos: torch.BoolTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BeitModelOutputWithPooling]:\n+    ) -> tuple | BeitModelOutputWithPooling:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n@@ -812,15 +810,15 @@ def get_output_embeddings(self):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.Tensor | None = None,\n+        bool_masked_pos: torch.BoolTensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, MaskedLMOutput]:\n+    ) -> tuple | MaskedLMOutput:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n@@ -907,14 +905,14 @@ def __init__(self, config: BeitConfig) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, ImageClassifierOutput]:\n+    ) -> tuple | ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -962,10 +960,10 @@ def __init__(\n         self,\n         in_channels: int,\n         out_channels: int,\n-        kernel_size: Union[int, tuple[int, int]],\n-        padding: Union[int, tuple[int, int], str] = 0,\n+        kernel_size: int | tuple[int, int],\n+        padding: int | tuple[int, int] | str = 0,\n         bias: bool = False,\n-        dilation: Union[int, tuple[int, int]] = 1,\n+        dilation: int | tuple[int, int] = 1,\n     ) -> None:\n         super().__init__()\n         self.conv = nn.Conv2d(\n@@ -1142,7 +1140,7 @@ class BeitFCNHead(nn.Module):\n     \"\"\"\n \n     def __init__(\n-        self, config: BeitConfig, in_index: int = 2, kernel_size: int = 3, dilation: Union[int, tuple[int, int]] = 1\n+        self, config: BeitConfig, in_index: int = 2, kernel_size: int = 3, dilation: int | tuple[int, int] = 1\n     ) -> None:\n         super().__init__()\n         self.in_channels = config.hidden_size\n@@ -1241,14 +1239,14 @@ def compute_loss(self, logits, auxiliary_logits, labels):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, SemanticSegmenterOutput]:\n+    ) -> tuple | SemanticSegmenterOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,\n@@ -1372,9 +1370,9 @@ def get_input_embeddings(self):\n     def forward(\n         self,\n         pixel_values: Tensor,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        output_attentions: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\""
        },
        {
            "sha": "017269482c01159404344306fabe7d89d15648a9",
            "filename": "src/transformers/models/bert/configuration_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "67775234673d2264c3fe88133d7272edf24df969",
            "filename": "src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_original_tf_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "d4637793785bfb8eb752b2f44548d96c5b649add",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 111,
            "deletions": 113,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -18,7 +17,6 @@\n import warnings\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -73,10 +71,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n@@ -119,8 +117,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -171,9 +169,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -243,9 +241,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[EncoderDecoderCache] = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: EncoderDecoderCache | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -318,11 +316,11 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n@@ -390,11 +388,11 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n@@ -443,14 +441,14 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         for i, layer_module in enumerate(self.layer):\n             hidden_states = layer_module(\n                 hidden_states,\n@@ -592,11 +590,11 @@ class BertForPreTrainingOutput(ModelOutput):\n         before SoftMax).\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: Optional[torch.FloatTensor] = None\n-    seq_relationship_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    prediction_logits: torch.FloatTensor | None = None\n+    seq_relationship_logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n @auto_docstring(\n@@ -641,18 +639,18 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n@@ -783,15 +781,15 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        next_sentence_label: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        next_sentence_label: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BertForPreTrainingOutput]:\n+    ) -> tuple[torch.Tensor] | BertForPreTrainingOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -883,20 +881,20 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n@@ -973,16 +971,16 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> tuple[torch.Tensor] | MaskedLMOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1060,14 +1058,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], NextSentencePredictorOutput]:\n+    ) -> tuple[torch.Tensor] | NextSentencePredictorOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n@@ -1156,14 +1154,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | SequenceClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1235,14 +1233,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+    ) -> tuple[torch.Tensor] | MultipleChoiceModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1334,14 +1332,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1390,15 +1388,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        start_positions: Optional[torch.Tensor] = None,\n-        end_positions: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        start_positions: torch.Tensor | None = None,\n+        end_positions: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+    ) -> tuple[torch.Tensor] | QuestionAnsweringModelOutput:\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "9c68ad916e5f65c40cd83e236f46c387af0ba8ea",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"Tokenization classes for Bert.\"\"\"\n \n import collections\n-from typing import Optional, Union\n \n from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import WordPiece\n@@ -80,15 +78,15 @@ class BertTokenizer(TokenizersBackend):\n \n     def __init__(\n         self,\n-        vocab: Optional[Union[str, dict[str, int]]] = None,\n+        vocab: str | dict[str, int] | None = None,\n         do_lower_case: bool = False,\n         unk_token: str = \"[UNK]\",\n         sep_token: str = \"[SEP]\",\n         pad_token: str = \"[PAD]\",\n         cls_token: str = \"[CLS]\",\n         mask_token: str = \"[MASK]\",\n         tokenize_chinese_chars: bool = True,\n-        strip_accents: Optional[bool] = None,\n+        strip_accents: bool | None = None,\n         **kwargs,\n     ):\n         self.do_lower_case = do_lower_case"
        },
        {
            "sha": "cb96aa3eb29eee725edda716d00c8e56240c4e1e",
            "filename": "src/transformers/models/bert/tokenization_bert_legacy.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_legacy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_legacy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_legacy.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n import collections\n import os\n import unicodedata\n-from typing import Optional\n \n from ...tokenization_python import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n from ...utils import logging\n@@ -184,7 +182,7 @@ def convert_tokens_to_string(self, tokens):\n         return out_string\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n     ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n@@ -209,7 +207,7 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None, already_has_special_tokens: bool = False\n     ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n@@ -236,7 +234,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):\n             vocab_file = os.path.join("
        },
        {
            "sha": "b96f69df0b459854a82ea866ec81ea31aa4f2489",
            "filename": "src/transformers/models/bert_generation/configuration_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fconfiguration_bert_generation.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The Google AI Language Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "bccb17124aa9d6ad9a60390cd449b88fff6c6f94",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 47,
            "deletions": 49,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The Google AI Language Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"PyTorch BERT model specific for generation.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -63,8 +61,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -116,9 +114,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -189,9 +187,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[EncoderDecoderCache] = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: EncoderDecoderCache | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -251,11 +249,11 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n@@ -326,11 +324,11 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n@@ -381,14 +379,14 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         for i, layer_module in enumerate(self.layer):\n             hidden_states = layer_module(\n                 hidden_states,\n@@ -510,17 +508,17 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n@@ -663,19 +661,19 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        past_key_values: tuple[tuple[torch.FloatTensor]] | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in"
        },
        {
            "sha": "c463a11b72e1684b80002af3a628fb69ec199047",
            "filename": "src/transformers/models/bert_generation/tokenization_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,7 +13,7 @@\n # limitations under the License.\n \"\"\"Tokenization class for model BertGeneration.\"\"\"\n \n-from typing import Any, Optional\n+from typing import Any\n \n from ...tokenization_utils_sentencepiece import SentencePieceBackend\n from ...utils import logging\n@@ -81,7 +80,7 @@ def __init__(\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         sep_token=\"<::::>\",\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        sp_model_kwargs: dict[str, Any] | None = None,\n         **kwargs,\n     ) -> None:\n         self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs"
        },
        {
            "sha": "b9249113b5af27e014cf611c2288eb2a849f2d54",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,7 @@\n import copy\n import os\n import unicodedata\n-from typing import Any, Optional\n+from typing import Any\n \n from ...tokenization_python import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n from ...utils import is_sentencepiece_available, is_sudachi_projection_available, logging\n@@ -261,7 +260,7 @@ def convert_tokens_to_string(self, tokens):\n         out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n         if os.path.isdir(save_directory):\n             if self.subword_tokenizer_type == \"sentencepiece\":\n                 vocab_file = os.path.join(\n@@ -302,8 +301,8 @@ def __init__(\n         do_lower_case=False,\n         never_split=None,\n         normalize_text=True,\n-        mecab_dic: Optional[str] = \"unidic_lite\",\n-        mecab_option: Optional[str] = None,\n+        mecab_dic: str | None = \"unidic_lite\",\n+        mecab_option: str | None = None,\n     ):\n         \"\"\"\n         Constructs a MecabTokenizer.\n@@ -842,7 +841,7 @@ def __init__(\n         do_lower_case=False,\n         remove_space=True,\n         keep_accents=True,\n-        sp_model_kwargs: Optional[dict[str, Any]] = None,\n+        sp_model_kwargs: dict[str, Any] | None = None,\n     ):\n         self.vocab = vocab\n         self.unk_token = unk_token"
        },
        {
            "sha": "95e9f46787e8fb087e0bd1bac400f10a172e4710",
            "filename": "src/transformers/models/bertweet/tokenization_bertweet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright (c) 2020, VinAI Research and the HuggingFace Inc. team.\n # Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n #\n@@ -18,7 +17,6 @@\n import html\n import os\n import re\n-from typing import Optional\n \n import regex\n \n@@ -301,7 +299,7 @@ def convert_tokens_to_string(self, tokens):\n     #     tokens_generated_so_far = re.sub('(@@ ?$)', '', string=tokens_generated_so_far)\n     #     return ''.join(tokens_generated_so_far)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str, ...]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str, ...]:\n         \"\"\"\n         Save the vocabulary and merges files to a directory.\n         \"\"\""
        },
        {
            "sha": "df1e525800a53ef216eda2187d10b913d2096743",
            "filename": "src/transformers/models/big_bird/configuration_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 Google Research and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "22e685472ed6fb942874665dabbfa79ad5b89bbf",
            "filename": "src/transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconvert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconvert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconvert_bigbird_original_tf_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b33aae21fc6b34a437e21f3a2a1b9a19ae6aa99f",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 107,
            "deletions": 109,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 Google Research and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -1376,7 +1374,7 @@ def forward(\n         blocked_encoder_mask=None,\n         return_dict=True,\n         cache_position=None,\n-    ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple]:\n+    ) -> BaseModelOutputWithPastAndCrossAttentions | tuple:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -1544,11 +1542,11 @@ class BigBirdForPreTrainingOutput(ModelOutput):\n         before SoftMax).\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: Optional[torch.FloatTensor] = None\n-    seq_relationship_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    prediction_logits: torch.FloatTensor | None = None\n+    seq_relationship_logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -1565,12 +1563,12 @@ class BigBirdForQuestionAnsweringModelOutput(ModelOutput):\n         pooler output from BigBigModel\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    start_logits: Optional[torch.FloatTensor] = None\n-    end_logits: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    start_logits: torch.FloatTensor | None = None\n+    end_logits: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n @auto_docstring\n@@ -1638,21 +1636,21 @@ def set_attention_type(self, value: str):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, tuple[torch.FloatTensor]]:\n+    ) -> BaseModelOutputWithPoolingAndCrossAttentions | tuple[torch.FloatTensor]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1911,18 +1909,18 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.FloatTensor] = None,\n-        next_sentence_label: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.FloatTensor | None = None,\n+        next_sentence_label: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[BigBirdForPreTrainingOutput, tuple[torch.FloatTensor]]:\n+    ) -> BigBirdForPreTrainingOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -2021,19 +2019,19 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n+    ) -> MaskedLMOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -2164,23 +2162,23 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[CausalLMOutputWithCrossAttentions, tuple[torch.FloatTensor]]:\n+    ) -> CausalLMOutputWithCrossAttentions | tuple[torch.FloatTensor]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n@@ -2273,17 +2271,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n+    ) -> SequenceClassifierOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -2391,17 +2389,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[MultipleChoiceModelOutput, tuple[torch.FloatTensor]]:\n+    ) -> MultipleChoiceModelOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -2498,17 +2496,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[TokenClassifierOutput, tuple[torch.FloatTensor]]:\n+    ) -> TokenClassifierOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -2588,19 +2586,19 @@ def __init__(self, config, add_pooling_layer=False):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        question_lengths: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        question_lengths: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[BigBirdForQuestionAnsweringModelOutput, tuple[torch.FloatTensor]]:\n+    ) -> BigBirdForQuestionAnsweringModelOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         question_lengths (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*):\n             The lengths of the questions in the batch."
        },
        {
            "sha": "91bbb090766bbb7d87ea81e945bbb6eb8ef3558f",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"Tokenization classes for Big Bird model.\"\"\"\n \n-from typing import Optional, Union\n-\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n \n@@ -86,7 +83,7 @@ class BigBirdTokenizer(TokenizersBackend):\n \n     def __init__(\n         self,\n-        vocab: Optional[Union[str, dict, list]] = None,\n+        vocab: str | dict | list | None = None,\n         unk_token=\"<unk>\",\n         bos_token=\"<s>\",\n         eos_token=\"</s>\","
        },
        {
            "sha": "c361b149acc67e1f9c5c62f5c191249ded22e330",
            "filename": "src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright Google Research and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0108e12ef6b07bee960182afc3f474edd243c113",
            "filename": "src/transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b7c950e6587ea788b3075e83d3b80bf564ca9315",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 109,
            "deletions": 111,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 Google Research The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import math\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -76,7 +74,7 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n     def forward(\n-        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: torch.Tensor | None = None\n     ):\n         \"\"\"`input_ids' shape is expected to be [bsz x seqlen].\"\"\"\n \n@@ -94,7 +92,7 @@ class BigBirdPegasusScaledWordEmbedding(nn.Embedding):\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n     \"\"\"\n \n-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float | None = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n         self.embed_scale = embed_scale\n \n@@ -1160,8 +1158,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -1196,8 +1194,8 @@ def __init__(\n         is_decoder: bool = False,\n         bias: bool = True,\n         is_causal: bool = False,\n-        config: Optional[BigBirdPegasusConfig] = None,\n-        layer_idx: Optional[int] = None,\n+        config: BigBirdPegasusConfig | None = None,\n+        layer_idx: int | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -1230,15 +1228,15 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -1396,7 +1394,7 @@ def set_attention_type(self, value: str):\n \n \n class BigBirdPegasusDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BigBirdPegasusConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BigBirdPegasusConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n         self.self_attn = BigBirdPegasusDecoderAttention(\n@@ -1431,13 +1429,13 @@ def __init__(self, config: BigBirdPegasusConfig, layer_idx: Optional[int] = None\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        cache_position: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -1594,12 +1592,12 @@ def __init__(self, config: BigBirdPegasusConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -1863,17 +1861,17 @@ def __init__(self, config: BigBirdPegasusConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -2091,21 +2089,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqModelOutput]:\n+    ) -> tuple | Seq2SeqModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2210,7 +2208,7 @@ def __init__(self, config: BigBirdPegasusConfig):\n \n     # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.resize_token_embeddings with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n     def resize_token_embeddings(\n-        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+        self, new_num_tokens: int, pad_to_multiple_of: int | None = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n         new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n@@ -2229,22 +2227,22 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqLMOutput]:\n+    ) -> tuple | Seq2SeqLMOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2365,21 +2363,21 @@ def __init__(self, config: BigBirdPegasusConfig, **kwargs):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n+    ) -> tuple | Seq2SeqSequenceClassifierOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2486,22 +2484,22 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: list[torch.FloatTensor] | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n+    ) -> tuple | Seq2SeqQuestionAnsweringModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2615,21 +2613,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "d29691649a5413e87727d4b878ffa83df87fec1f",
            "filename": "src/transformers/models/biogpt/configuration_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconfiguration_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconfiguration_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconfiguration_biogpt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team and Microsoft Research AI4Science All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "e8c82bba94a23014fcb7d62e99c84e3cbfd082ae",
            "filename": "src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fconvert_biogpt_original_pytorch_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "533417dcc12dd3e42926a64ec8ad2db3012480d3",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 69,
            "deletions": 71,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_biogpt.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team and Microsoft Research AI4Science All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -21,7 +20,6 @@\n \n import math\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -63,7 +61,7 @@ def forward(\n         self,\n         attention_mask: torch.LongTensor,\n         past_key_values_length: int = 0,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_ids: torch.LongTensor | None = None,\n     ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n \n@@ -81,7 +79,7 @@ class BioGptScaledWordEmbedding(nn.Embedding):\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n     \"\"\"\n \n-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float | None = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n         self.embed_scale = embed_scale\n \n@@ -94,8 +92,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -129,8 +127,8 @@ def __init__(\n         is_decoder: bool = False,\n         bias: bool = True,\n         is_causal: bool = False,\n-        config: Optional[BioGptConfig] = None,\n-        layer_idx: Optional[int] = None,\n+        config: BioGptConfig | None = None,\n+        layer_idx: int | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -163,15 +161,15 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -244,7 +242,7 @@ def forward(\n \n \n class BioGptDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BioGptConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n@@ -270,14 +268,14 @@ def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        position_ids: torch.LongTensor | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -367,18 +365,18 @@ def __init__(self, config: BioGptConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple | BaseModelOutputWithPastAndCrossAttentions:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -530,20 +528,20 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n@@ -608,20 +606,20 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, TokenClassifierOutput]:\n+    ) -> tuple | TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -700,20 +698,20 @@ def __init__(self, config: BioGptConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n+    ) -> tuple | SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "f1bfbcee698854b5c0cc6b60b317bc5320561b02",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 59,
            "deletions": 61,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team and Microsoft Research AI4Science All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"PyTorch BioGPT model.\"\"\"\n \n import math\n-from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -52,7 +50,7 @@ def forward(\n         self,\n         attention_mask: torch.LongTensor,\n         past_key_values_length: int = 0,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_ids: torch.LongTensor | None = None,\n     ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n         super().forward(attention_mask, past_key_values_length, position_ids)\n@@ -67,7 +65,7 @@ class BioGptAttention(BartAttention):\n \n \n class BioGptDecoderLayer(BartDecoderLayer):\n-    def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BioGptConfig, layer_idx: int | None = None):\n         super().__init__(config)\n         self.embed_dim = config.hidden_size\n \n@@ -92,14 +90,14 @@ def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        position_ids: torch.LongTensor | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -189,18 +187,18 @@ def __init__(self, config: BioGptConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple | BaseModelOutputWithPastAndCrossAttentions:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -352,20 +350,20 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n@@ -430,20 +428,20 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple, TokenClassifierOutput]:\n+    ) -> tuple | TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -522,20 +520,20 @@ def __init__(self, config: BioGptConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n+    ) -> tuple | SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "1bf3e103a0eb837238bdd0f620c7ef423f73387c",
            "filename": "src/transformers/models/biogpt/tokenization_biogpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Team and Microsoft Research AI4Science. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import json\n import os\n-from typing import Optional\n \n from ...tokenization_python import PreTrainedTokenizer\n from ...utils import logging\n@@ -231,7 +229,7 @@ def convert_tokens_to_string(self, tokens):\n         return text\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n     ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n@@ -255,7 +253,7 @@ def build_inputs_with_special_tokens(\n         return sep + token_ids_0 + sep + token_ids_1\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None, already_has_special_tokens: bool = False\n     ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n@@ -281,7 +279,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n         return [1] + ([0] * len(token_ids_0))\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "3f2270845415d7ba482a8678cf37a6bf787dd33c",
            "filename": "src/transformers/models/bit/configuration_bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "064f93037595f0f24487f31778cc66800b5df710",
            "filename": "src/transformers/models/bit/convert_bit_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fconvert_bit_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fconvert_bit_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fconvert_bit_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0bc17b3157cfdaf4d8583c0ae65f869d61ef4dbb",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "050b46b05b67856c5b4ed4559daea5f2b5a55425",
            "filename": "src/transformers/models/bit/image_processing_bit_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "86f06f901a577c0ea605bc9fded873826afaf5b4",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 Google AI and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import collections\n import math\n-from typing import Optional\n \n import numpy as np\n import torch\n@@ -294,7 +292,7 @@ def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = Fals\n class BitDropPath(nn.Module):\n     \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n \n-    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+    def __init__(self, drop_prob: float | None = None) -> None:\n         super().__init__()\n         self.drop_prob = drop_prob\n \n@@ -672,8 +670,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         pixel_values: Tensor,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         output_hidden_states = (\n@@ -725,10 +723,10 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n@@ -778,8 +776,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         pixel_values: Tensor,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\""
        },
        {
            "sha": "e903ce2da2975a4da4bc64c9e2ad548b3459a67d",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 18,
            "deletions": 21,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The BitNet Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -13,8 +12,6 @@\n # See the License for the specific language governing permissions and\n \"\"\"BitNet model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n@@ -101,24 +98,24 @@ class BitNetConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 128256,\n-        hidden_size: Optional[int] = 2560,\n-        intermediate_size: Optional[int] = 6912,\n-        num_hidden_layers: Optional[int] = 30,\n-        num_attention_heads: Optional[int] = 20,\n-        num_key_value_heads: Optional[int] = 5,\n-        hidden_act: Optional[str] = \"relu2\",\n-        max_position_embeddings: Optional[int] = 2048,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-5,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 128000,\n-        eos_token_id: Optional[int] = 128001,\n-        tie_word_embeddings: Optional[bool] = False,\n-        attention_bias: Optional[bool] = False,\n-        attention_dropout: Optional[str] = 0.0,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        vocab_size: int | None = 128256,\n+        hidden_size: int | None = 2560,\n+        intermediate_size: int | None = 6912,\n+        num_hidden_layers: int | None = 30,\n+        num_attention_heads: int | None = 20,\n+        num_key_value_heads: int | None = 5,\n+        hidden_act: str | None = \"relu2\",\n+        max_position_embeddings: int | None = 2048,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-5,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 128000,\n+        eos_token_id: int | None = 128001,\n+        tie_word_embeddings: bool | None = False,\n+        attention_bias: bool | None = False,\n+        attention_dropout: str | None = 0.0,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "f92ac06f446159d196011589164b02b24df2861b",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_bitnet.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 The BitNet Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,7 +18,7 @@\n # See the License for the specific language governing permissions and\n \n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -130,7 +129,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -183,11 +182,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -239,12 +238,12 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         residual = hidden_states\n@@ -291,9 +290,9 @@ def __init__(self, config: BitNetConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[BitNetConfig] = None,\n+        config: BitNetConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -376,13 +375,13 @@ def __init__(self, config: BitNetConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -453,15 +452,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\""
        },
        {
            "sha": "d3aae65a8ad46d53611ef1ab48372910c279e49b",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The BitNet Team and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,7 +13,6 @@\n \"\"\"PyTorch BitNet model.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n \n@@ -63,11 +61,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n-        attention_mask: Optional[torch.Tensor],\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "6c4ff517e63b042551166d8833aa25f3e6765c34",
            "filename": "src/transformers/models/blenderbot/configuration_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Facebook, Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "1ba9a3d570deed84d5154609b9b3ac0b4141f268",
            "filename": "src/transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconvert_blenderbot_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconvert_blenderbot_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconvert_blenderbot_original_pytorch_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "072ca4083b6684f0bb1cc1242abf81253e1da134",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 66,
            "deletions": 68,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Facebook, Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,6 @@\n import os\n import warnings\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -79,7 +77,7 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n     def forward(\n-        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: torch.Tensor | None = None\n     ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n         if position_ids is None:\n@@ -96,7 +94,7 @@ class BlenderbotScaledWordEmbedding(nn.Embedding):\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n     \"\"\"\n \n-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float] = 1.0):\n+    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: float | None = 1.0):\n         super().__init__(num_embeddings, embedding_dim, padding_idx)\n         self.embed_scale = embed_scale\n \n@@ -110,8 +108,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -146,8 +144,8 @@ def __init__(\n         is_decoder: bool = False,\n         bias: bool = True,\n         is_causal: bool = False,\n-        config: Optional[BlenderbotConfig] = None,\n-        layer_idx: Optional[int] = None,\n+        config: BlenderbotConfig | None = None,\n+        layer_idx: int | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -180,15 +178,15 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -322,7 +320,7 @@ def forward(\n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n class BlenderbotDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BlenderbotConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -356,13 +354,13 @@ def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        cache_position: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Args:\n@@ -649,7 +647,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -857,7 +855,7 @@ def __init__(self, config: BlenderbotConfig):\n         self.post_init()\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike | None, *model_args, **kwargs):\n         if pretrained_model_name_or_path == \"facebook/blenderbot-90M\":\n             warnings.warn(\n                 \"The checkpoint `facebook/blenderbot-90M` is deprecated. In the future, please use the identical\"\n@@ -880,21 +878,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: tuple | BaseModelOutput | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n+    ) -> tuple[torch.FloatTensor] | Seq2SeqModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1003,7 +1001,7 @@ def __init__(self, config: BlenderbotConfig):\n         self.post_init()\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike | None, *model_args, **kwargs):\n         if pretrained_model_name_or_path == \"facebook/blenderbot-90M\":\n             warnings.warn(\n                 \"The checkpoint `facebook/blenderbot-90M` is deprecated. In the future, please use the identical\"\n@@ -1016,7 +1014,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.P\n         return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+        self, new_num_tokens: int, pad_to_multiple_of: int | None = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n         new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n@@ -1034,22 +1032,22 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: tuple | BaseModelOutput | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+    ) -> tuple[torch.FloatTensor] | Seq2SeqLMOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1194,21 +1192,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "f324c14eaae47ba223c00da5c9f6d8455f01d866",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Facebook Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "161f856e7c4ac8d63d88ab955b2b8a7106651213",
            "filename": "src/transformers/models/blenderbot_small/configuration_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Facebook, Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c48db96f731069f0a8d6bc2bac64bd9e69e6833a",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 66,
            "deletions": 68,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Facebook, Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import math\n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -77,7 +75,7 @@ def __init__(self, num_embeddings: int, embedding_dim: int):\n         super().__init__(num_embeddings, embedding_dim)\n \n     def forward(\n-        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: Optional[torch.Tensor] = None\n+        self, input_ids_shape: torch.Size, past_key_values_length: int = 0, position_ids: torch.Tensor | None = None\n     ):\n         \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n         if position_ids is None:\n@@ -94,8 +92,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -130,8 +128,8 @@ def __init__(\n         is_decoder: bool = False,\n         bias: bool = True,\n         is_causal: bool = False,\n-        config: Optional[BlenderbotSmallConfig] = None,\n-        layer_idx: Optional[int] = None,\n+        config: BlenderbotSmallConfig | None = None,\n+        layer_idx: int | None = None,\n     ):\n         super().__init__()\n         self.embed_dim = embed_dim\n@@ -164,15 +162,15 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        key_value_states: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        key_value_states: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        cache_position: torch.Tensor | None = None,\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -246,7 +244,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n class BlenderbotSmallEncoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BlenderbotSmallConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -269,8 +267,8 @@ def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n         attention_mask: torch.FloatTensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -314,7 +312,7 @@ def forward(\n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n class BlenderbotSmallDecoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BlenderbotSmallConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n \n@@ -348,14 +346,14 @@ def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = Non\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = True,\n+        cache_position: torch.Tensor | None = None,\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -853,21 +851,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: tuple | BaseModelOutput | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n+    ) -> tuple[torch.FloatTensor] | Seq2SeqModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -976,7 +974,7 @@ def __init__(self, config: BlenderbotSmallConfig):\n         self.post_init()\n \n     def resize_token_embeddings(\n-        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n+        self, new_num_tokens: int, pad_to_multiple_of: int | None = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n         new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n@@ -994,22 +992,22 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        encoder_outputs: tuple | BaseModelOutput | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        decoder_inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+    ) -> tuple[torch.FloatTensor] | Seq2SeqLMOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1154,21 +1152,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "6d31cbe4432cfa9c75572e4265cba8a9a6830f13",
            "filename": "src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Facebook Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import json\n import os\n-from typing import Optional\n \n import regex as re\n \n@@ -191,7 +189,7 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n         out_string = \" \".join(tokens).replace(\"@@ \", \"\").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "ea169147e8a06894c7026c88ec80ccc6156c403a",
            "filename": "src/transformers/models/blip/configuration_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b4cfb95d613b7e2daff7545bf6b6675411eae3aa",
            "filename": "src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fconvert_blip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fconvert_blip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fconvert_blip_original_pytorch_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c90b4ac19e40cda161933cdf424cf2b44d1048d6",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "122cda61254acfb1d06bed86a1e94b1dee960d57",
            "filename": "src/transformers/models/blip/image_processing_blip_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "082ef2a694e81a103b82f90f122b4dcc07afaefc",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 60,
            "deletions": 61,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The Salesforce Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n from torch import nn\n@@ -78,12 +77,12 @@ class BlipForConditionalGenerationModelOutput(ModelOutput):\n         heads.\n     \"\"\"\n \n-    loss: Optional[tuple[torch.FloatTensor]] = None\n-    logits: Optional[tuple[torch.FloatTensor]] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: tuple[torch.FloatTensor] | None = None\n+    logits: tuple[torch.FloatTensor] | None = None\n+    image_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n     @property\n     def decoder_logits(self):\n@@ -110,11 +109,11 @@ class BlipTextVisionModelOutput(ModelOutput):\n         The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    loss: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -139,14 +138,14 @@ class BlipImageTextMatchingModelOutput(ModelOutput):\n         The question embeddings obtained by the text projection layer.\n     \"\"\"\n \n-    itm_score: Optional[torch.FloatTensor] = None\n-    loss: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    vision_pooler_output: Optional[torch.FloatTensor] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n-    question_embeds: Optional[tuple[torch.FloatTensor]] = None\n+    itm_score: torch.FloatTensor | None = None\n+    loss: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    vision_pooler_output: torch.FloatTensor | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n+    question_embeds: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -171,11 +170,11 @@ class BlipOutput(ModelOutput):\n         The output of the [`BlipVisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -276,9 +275,9 @@ def __init__(self, config: BlipTextConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n         max_position_embedding = self.position_embedding.weight.shape[0]\n@@ -455,7 +454,7 @@ def forward(\n         self,\n         inputs_embeds,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> tuple | BaseModelOutput:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n@@ -490,10 +489,10 @@ def __init__(self, config: BlipVisionConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> tuple | BaseModelOutputWithPooling:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -572,9 +571,9 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def get_text_features(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -606,7 +605,7 @@ def get_text_features(\n     @auto_docstring\n     def get_image_features(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -645,9 +644,9 @@ def get_image_features(\n     @auto_docstring\n     def get_multimodal_features(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -695,14 +694,14 @@ def get_multimodal_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        return_loss: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        return_loss: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BlipOutput]:\n+    ) -> tuple | BlipOutput:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss.\n@@ -812,13 +811,13 @@ def set_input_embeddings(self, value):\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BlipForConditionalGenerationModelOutput]:\n+    ) -> tuple | BlipForConditionalGenerationModelOutput:\n         r\"\"\"\n         Examples:\n \n@@ -870,8 +869,8 @@ def forward(\n     def generate(\n         self,\n         pixel_values: torch.FloatTensor,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -983,13 +982,13 @@ def forward(\n         self,\n         input_ids: torch.LongTensor,\n         pixel_values: torch.FloatTensor,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BlipTextVisionModelOutput]:\n+    ) -> tuple | BlipTextVisionModelOutput:\n         r\"\"\"\n         Examples:\n \n@@ -1080,7 +1079,7 @@ def generate(\n         self,\n         input_ids: torch.LongTensor,\n         pixel_values: torch.FloatTensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -1213,11 +1212,11 @@ def forward(\n         self,\n         input_ids: torch.LongTensor,\n         pixel_values: torch.FloatTensor,\n-        use_itm_head: Optional[bool] = True,\n-        attention_mask: Optional[torch.LongTensor] = None,\n+        use_itm_head: bool | None = True,\n+        attention_mask: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BlipTextVisionModelOutput]:\n+    ) -> tuple | BlipTextVisionModelOutput:\n         r\"\"\"\n         use_itm_head (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the image-text matching head."
        },
        {
            "sha": "b81e23c201e0d86f12c4517f31bc9e7945e580c6",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 63,
            "deletions": 65,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The Salesforce Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the BSD-3-clause license (the \"License\");\n@@ -15,7 +14,6 @@\n \n \n import math\n-from typing import Optional, Union\n \n import torch\n from torch import Tensor, device, nn\n@@ -61,9 +59,9 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n@@ -130,12 +128,12 @@ def get_attention_map(self):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        cache_position: torch.Tensor | None = None,\n     ) -> tuple[torch.Tensor]:\n         batch_size, seq_length, _ = hidden_states.shape\n         query_layer = (\n@@ -238,11 +236,11 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        cache_position: torch.Tensor | None = None,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n@@ -306,12 +304,12 @@ def __init__(self, config, layer_num):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        cache_position: torch.Tensor | None = None,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n@@ -356,16 +354,16 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+        cache_position: torch.Tensor | None = None,\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning(\n@@ -601,22 +599,22 @@ def get_extended_attention_mask(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        is_decoder: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        is_decoder: bool | None = False,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -763,25 +761,25 @@ def set_output_embeddings(self, new_embeddings):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        return_logits: Optional[bool] = False,\n-        is_decoder: Optional[bool] = True,\n-        reduction: Optional[str] = \"mean\",\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        return_logits: bool | None = False,\n+        is_decoder: bool | None = True,\n+        reduction: str | None = \"mean\",\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states (`torch.FloatTensor`, *optional*): Sequence of\n             hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is"
        },
        {
            "sha": "097b4f008ac6dae67e3de971709e440ce3389de2",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c8dd4812883e5e846e19a2ee15d0474bebd87cd1",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "4075b97b9910473248cd37e4b6a216f14b2eec96",
            "filename": "src/transformers/models/blip_2/convert_blip_2_original_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconvert_blip_2_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconvert_blip_2_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconvert_blip_2_original_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "e23c96d8dffb47a499abe047f9bb2721f50e36e8",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 70,
            "deletions": 71,
            "changes": 141,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Salesforce Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,7 @@\n import math\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n from torch import nn\n@@ -75,11 +74,11 @@ class Blip2ForConditionalGenerationModelOutput(ModelOutput):\n         Outputs of the language model.\n     \"\"\"\n \n-    loss: Optional[tuple[torch.FloatTensor]] = None\n-    logits: Optional[tuple[torch.FloatTensor]] = None\n-    vision_outputs: Optional[torch.FloatTensor] = None\n-    qformer_outputs: Optional[tuple[torch.FloatTensor]] = None\n-    language_model_outputs: Optional[tuple[torch.FloatTensor]] = None\n+    loss: tuple[torch.FloatTensor] | None = None\n+    logits: tuple[torch.FloatTensor] | None = None\n+    vision_outputs: torch.FloatTensor | None = None\n+    qformer_outputs: tuple[torch.FloatTensor] | None = None\n+    language_model_outputs: tuple[torch.FloatTensor] | None = None\n \n     def to_tuple(self) -> tuple[Any]:\n         return tuple(\n@@ -112,11 +111,11 @@ class Blip2ImageTextMatchingModelOutput(ModelOutput):\n         The output of the [`Blip2VisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -140,10 +139,10 @@ class Blip2TextModelOutput(ModelOutput):\n         The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    text_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -159,10 +158,10 @@ class Blip2VisionModelOutput(ModelOutput):\n         The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    image_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->Blip2\n@@ -246,7 +245,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n@@ -305,7 +304,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -454,7 +453,7 @@ def forward(\n         self,\n         inputs_embeds,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BaseModelOutput]:\n+    ) -> tuple | BaseModelOutput:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n@@ -491,10 +490,10 @@ def __init__(self, config: Blip2VisionConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> tuple | BaseModelOutputWithPooling:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -643,9 +642,9 @@ def __init__(self, config, is_cross_attention=False):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         attn_output, _ = self.attention(\n@@ -827,9 +826,9 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.FloatTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        query_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.FloatTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        query_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n             seq_length = input_ids.size()[1]\n@@ -945,12 +944,12 @@ def get_extended_attention_mask(\n     def forward(\n         self,\n         query_embeds: torch.FloatTensor,\n-        query_length: Optional[int] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        query_length: int | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n             Hidden states to be used in the attention computation. If cross-attention,\n@@ -1073,11 +1072,11 @@ def get_encoder(self, modality=None):\n     def get_text_features(\n         self,\n         input_ids: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        decoder_input_ids: Optional[torch.Tensor] = None,\n-        decoder_attention_mask: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-    ) -> Union[torch.FloatTensor, CausalLMOutputWithPast]:\n+        attention_mask: torch.Tensor | None = None,\n+        decoder_input_ids: torch.Tensor | None = None,\n+        decoder_attention_mask: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+    ) -> torch.FloatTensor | CausalLMOutputWithPast:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1138,7 +1137,7 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[torch.FloatTensor, CausalLMOutputWithPast]:\n+    ) -> torch.FloatTensor | CausalLMOutputWithPast:\n         r\"\"\"\n         Returns:\n             vision_outputs (`torch.FloatTensor`):\n@@ -1174,7 +1173,7 @@ def get_qformer_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[torch.FloatTensor, BaseModelOutputWithPooling]:\n+    ) -> torch.FloatTensor | BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n             qformer_outputs (`torch.FloatTensor`):\n@@ -1239,13 +1238,13 @@ def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n         input_ids: torch.FloatTensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, Blip2ForConditionalGenerationModelOutput]:\n+    ) -> tuple | Blip2ForConditionalGenerationModelOutput:\n         r\"\"\"\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n@@ -1386,11 +1385,11 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, Blip2TextModelOutput]:\n+    ) -> tuple | Blip2TextModelOutput:\n         r\"\"\"\n         Examples:\n \n@@ -1470,9 +1469,9 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, Blip2VisionModelOutput]:\n+    ) -> tuple | Blip2VisionModelOutput:\n         r\"\"\"\n         Examples:\n \n@@ -1612,8 +1611,8 @@ def _preprocess_accelerate(self):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        interpolate_pos_encoding: Optional[bool] = False,\n-        return_dict: Optional[bool] = False,\n+        interpolate_pos_encoding: bool | None = False,\n+        return_dict: bool | None = False,\n     ):\n         \"\"\"\n         Encodes images into continuous embeddings that can be forwarded to the language model.\n@@ -1674,14 +1673,14 @@ def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n         input_ids: torch.LongTensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        decoder_input_ids: Optional[torch.LongTensor] = None,\n-        decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        decoder_input_ids: torch.LongTensor | None = None,\n+        decoder_attention_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, Blip2ForConditionalGenerationModelOutput]:\n+    ) -> tuple | Blip2ForConditionalGenerationModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n@@ -1816,9 +1815,9 @@ def forward(\n     def generate(\n         self,\n         pixel_values: torch.FloatTensor,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -1945,13 +1944,13 @@ def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n         input_ids: torch.LongTensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        use_image_text_matching_head: Optional[bool] = False,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        use_image_text_matching_head: bool | None = False,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, Blip2ImageTextMatchingModelOutput]:\n+    ) -> tuple | Blip2ImageTextMatchingModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be"
        },
        {
            "sha": "3b91304c73f02b9715ed62882d35ec94630daab4",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "98cba005ecd44965bfcce3154ec1213a50a9e0e4",
            "filename": "src/transformers/models/bloom/configuration_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 the Big Science Workshop and HuggingFace Inc. team.  All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c8fb5aab004fdc227d3168d359a0346e366783f1",
            "filename": "src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "cd84f58712bb5fc4bbf1055a79d6d8dc9b627690",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 58,
            "deletions": 59,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 HuggingFace Inc. team and BigScience workshop.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,7 @@\n \"\"\"PyTorch BLOOM model.\"\"\"\n \n import math\n-from typing import Optional, Union\n+from typing import Union\n \n import torch\n from torch import nn\n@@ -170,7 +169,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n \n \n class BloomAttention(nn.Module):\n-    def __init__(self, config: BloomConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BloomConfig, layer_idx: int | None = None):\n         super().__init__()\n \n         self.pretraining_tp = config.pretraining_tp\n@@ -254,10 +253,10 @@ def forward(\n         residual: torch.Tensor,\n         alibi: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_past: Optional[Cache] = None,\n+        layer_past: Cache | None = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        cache_position: torch.LongTensor | None = None,\n     ):\n         batch_size, q_length, _ = hidden_states.shape\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n@@ -350,7 +349,7 @@ def forward(self, hidden_states: torch.Tensor, residual: torch.Tensor) -> torch.\n \n \n class BloomBlock(GradientCheckpointingLayer):\n-    def __init__(self, config: BloomConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: BloomConfig, layer_idx: int | None = None):\n         super().__init__()\n         hidden_size = config.hidden_size\n \n@@ -369,10 +368,10 @@ def forward(\n         hidden_states: torch.Tensor,\n         alibi: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        layer_past: Optional[Cache] = None,\n+        layer_past: Cache | None = None,\n         use_cache: bool = False,\n         output_attentions: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        cache_position: torch.LongTensor | None = None,\n     ):\n         # hidden_states: [batch_size, seq_length, hidden_size]\n \n@@ -456,17 +455,17 @@ def set_input_embeddings(self, new_embeddings: torch.Tensor):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor, ...] | BaseModelOutputWithPastAndCrossAttentions:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n@@ -746,19 +745,19 @@ def prepare_inputs_for_generation(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n@@ -844,17 +843,17 @@ def __init__(self, config: BloomConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n+    ) -> tuple[torch.Tensor] | SequenceClassifierOutputWithPast:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n@@ -968,17 +967,17 @@ def __init__(self, config: BloomConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | TokenClassifierOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`\n@@ -1048,16 +1047,16 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, QuestionAnsweringModelOutput]:\n+    ) -> tuple | QuestionAnsweringModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values.get_seq_length()`"
        },
        {
            "sha": "577c001fee86b1b7c5a97be831350d3f2b5afbaf",
            "filename": "src/transformers/models/blt/configuration_blt.py",
            "status": "modified",
            "additions": 71,
            "deletions": 74,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"Blt model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n@@ -34,21 +31,21 @@ class BltLocalEncoderConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 260,\n-        cross_attn_all_layers: Optional[bool] = False,\n-        cross_attn_k: Optional[int] = 2,\n-        hidden_size_global: Optional[int] = 2048,\n-        hidden_size: Optional[int] = 1024,\n-        num_attention_heads: Optional[int] = 16,\n-        num_key_value_heads: Optional[int] = None,\n-        num_hidden_layers: Optional[int] = 1,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        dropout: Optional[float] = 0.0,\n-        max_position_embeddings: Optional[int] = 24576,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        intermediate_size: Optional[int] = 2816,\n-        initializer_range: Optional[float] = 0.02,\n+        vocab_size: int | None = 260,\n+        cross_attn_all_layers: bool | None = False,\n+        cross_attn_k: int | None = 2,\n+        hidden_size_global: int | None = 2048,\n+        hidden_size: int | None = 1024,\n+        num_attention_heads: int | None = 16,\n+        num_key_value_heads: int | None = None,\n+        num_hidden_layers: int | None = 1,\n+        rms_norm_eps: float | None = 1e-5,\n+        dropout: float | None = 0.0,\n+        max_position_embeddings: int | None = 24576,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        hidden_act: str | None = \"silu\",\n+        intermediate_size: int | None = 2816,\n+        initializer_range: float | None = 0.02,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -83,21 +80,21 @@ class BltLocalDecoderConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 260,\n-        cross_attn_all_layers: Optional[bool] = True,\n-        cross_attn_k: Optional[int] = 2,\n-        hidden_size_global: Optional[int] = 2048,\n-        hidden_size: Optional[int] = 1024,\n-        num_attention_heads: Optional[int] = 16,\n-        num_key_value_heads: Optional[int] = None,\n-        num_hidden_layers: Optional[int] = 9,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        dropout: Optional[float] = 0.0,\n-        max_position_embeddings: Optional[int] = 24576,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        intermediate_size: Optional[int] = 2816,\n-        initializer_range: Optional[float] = 0.02,\n+        vocab_size: int | None = 260,\n+        cross_attn_all_layers: bool | None = True,\n+        cross_attn_k: int | None = 2,\n+        hidden_size_global: int | None = 2048,\n+        hidden_size: int | None = 1024,\n+        num_attention_heads: int | None = 16,\n+        num_key_value_heads: int | None = None,\n+        num_hidden_layers: int | None = 9,\n+        rms_norm_eps: float | None = 1e-5,\n+        dropout: float | None = 0.0,\n+        max_position_embeddings: int | None = 24576,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        hidden_act: str | None = \"silu\",\n+        intermediate_size: int | None = 2816,\n+        initializer_range: float | None = 0.02,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -132,17 +129,17 @@ class BltGlobalTransformerConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        hidden_size: Optional[int] = 2048,\n-        num_attention_heads: Optional[int] = 16,\n-        num_key_value_heads: Optional[int] = None,\n-        num_hidden_layers: Optional[int] = 25,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        dropout: Optional[float] = 0.0,\n-        max_position_embeddings: Optional[int] = 4096,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        hidden_act: Optional[str] = \"silu\",\n-        intermediate_size: Optional[int] = 5632,\n-        initializer_range: Optional[float] = 0.02,\n+        hidden_size: int | None = 2048,\n+        num_attention_heads: int | None = 16,\n+        num_key_value_heads: int | None = None,\n+        num_hidden_layers: int | None = 25,\n+        rms_norm_eps: float | None = 1e-5,\n+        dropout: float | None = 0.0,\n+        max_position_embeddings: int | None = 4096,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        hidden_act: str | None = \"silu\",\n+        intermediate_size: int | None = 5632,\n+        initializer_range: float | None = 0.02,\n         **kwargs,\n     ):\n         self.hidden_size = hidden_size\n@@ -205,17 +202,17 @@ class BltPatcherConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 260,\n-        hidden_size: Optional[int] = 768,\n-        num_hidden_layers: Optional[int] = 14,\n-        num_attention_heads: Optional[int] = 12,\n-        num_key_value_heads: Optional[int] = None,\n-        max_position_embeddings: Optional[int] = 8192,\n-        rms_norm_eps: Optional[float] = 1e-5,\n-        dropout: Optional[float] = 0.0,\n-        intermediate_size: Optional[int] = 2048,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        initializer_range: Optional[float] = 0.02,\n+        vocab_size: int | None = 260,\n+        hidden_size: int | None = 768,\n+        num_hidden_layers: int | None = 14,\n+        num_attention_heads: int | None = 12,\n+        num_key_value_heads: int | None = None,\n+        max_position_embeddings: int | None = 8192,\n+        rms_norm_eps: float | None = 1e-5,\n+        dropout: float | None = 0.0,\n+        intermediate_size: int | None = 2048,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        initializer_range: float | None = 0.02,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -316,25 +313,25 @@ class BltConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 260,\n-        max_position_embeddings: Optional[int] = 4096,\n-        patch_in_forward: Optional[bool] = True,\n-        patch_size: Optional[int] = 4,\n-        patching_mode: Optional[str] = \"entropy\",\n-        patching_threshold: Optional[float] = 1.335442066192627,\n-        patching_batch_size: Optional[int] = 1,\n-        max_patch_length: Optional[int] = None,\n-        cross_attn_k: Optional[int] = 2,\n-        encoder_hash_byte_group_size: Optional[int] = None,\n-        encoder_hash_byte_group_vocab: Optional[int] = 500002,\n-        encoder_hash_byte_group_nb_functions: Optional[int] = 1,\n-        patcher_config: Optional[dict] = None,\n-        encoder_config: Optional[dict] = None,\n-        decoder_config: Optional[dict] = None,\n-        global_config: Optional[dict] = None,\n-        tie_word_embeddings: Optional[bool] = False,\n-        initializer_range: Optional[float] = 0.02,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        vocab_size: int | None = 260,\n+        max_position_embeddings: int | None = 4096,\n+        patch_in_forward: bool | None = True,\n+        patch_size: int | None = 4,\n+        patching_mode: str | None = \"entropy\",\n+        patching_threshold: float | None = 1.335442066192627,\n+        patching_batch_size: int | None = 1,\n+        max_patch_length: int | None = None,\n+        cross_attn_k: int | None = 2,\n+        encoder_hash_byte_group_size: int | None = None,\n+        encoder_hash_byte_group_vocab: int | None = 500002,\n+        encoder_hash_byte_group_nb_functions: int | None = 1,\n+        patcher_config: dict | None = None,\n+        encoder_config: dict | None = None,\n+        decoder_config: dict | None = None,\n+        global_config: dict | None = None,\n+        tie_word_embeddings: bool | None = False,\n+        initializer_range: float | None = 0.02,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n         **kwargs,\n     ):\n         # Basic model configuration"
        },
        {
            "sha": "054ca9f84d164d0f2cbe620c5287e484a53de6ed",
            "filename": "src/transformers/models/blt/convert_blt_weights_to_hf.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fconvert_blt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fconvert_blt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconvert_blt_weights_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -2,7 +2,7 @@\n import json\n import logging\n import os\n-from typing import Any, Optional\n+from typing import Any\n \n import torch\n from huggingface_hub import hf_hub_download, upload_folder\n@@ -342,7 +342,7 @@ def push_to_hub(\n     repo_id: str,\n     commit_message: str = \"Upload converted Blt model\",\n     private: bool = False,\n-    token: Optional[str] = None,\n+    token: str | None = None,\n ) -> None:\n     try:\n         upload_folder(\n@@ -364,10 +364,10 @@ def convert_hf_blt_to_unified(\n     output_dir: str,\n     config_name: str = \"config.json\",\n     weights_name: str = \"model.bin\",\n-    cache_dir: Optional[str] = None,\n-    push_to_hub_repo: Optional[str] = None,\n+    cache_dir: str | None = None,\n+    push_to_hub_repo: str | None = None,\n     hub_private: bool = False,\n-    hub_token: Optional[str] = None,\n+    hub_token: str | None = None,\n ) -> None:\n     # Download model files\n     config_path = hf_hub_download(repo_id=model_id, filename=\"config.json\", cache_dir=cache_dir)"
        },
        {
            "sha": "ee41a54d5e360ec18d61fa2589f7f45ef4bc62e7",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 72,
            "deletions": 73,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_blt.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2025 HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n # limitations under the License.\n \n from collections.abc import Callable\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n import torch.distributions\n@@ -107,9 +106,9 @@ def __init__(self, config: BltConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[BltConfig] = None,\n+        config: BltConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -167,17 +166,17 @@ def __init__(self, config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cross_attention_states: Optional[torch.Tensor] = None,\n-        cross_attention_mask: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        cross_attention_states: torch.Tensor | None = None,\n+        cross_attention_mask: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        full_text_row_masked_out_mask: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor] | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -241,7 +240,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -369,7 +368,7 @@ def forward(\n class BltCrossAttention(nn.Module):\n     \"\"\"Cross-attention module for Blt, following transformers style\"\"\"\n \n-    def __init__(self, config: BltConfig, layer_idx: int, hidden_size: Optional[int] = None):\n+    def __init__(self, config: BltConfig, layer_idx: int, hidden_size: int | None = None):\n         super().__init__()\n         self.config = config\n         self.num_heads = self.config.num_attention_heads\n@@ -392,10 +391,10 @@ def __init__(self, config: BltConfig, layer_idx: int, hidden_size: Optional[int]\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cross_attention_states: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        cross_attention_states: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         bsz, q_len, _ = hidden_states.size()\n         query_states = self.q_norm(hidden_states)\n@@ -634,16 +633,16 @@ def __init__(self, config: BltLocalEncoderConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        patch_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        num_patches: Optional[int] = None,\n-        patch_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        patch_embeds: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        num_patches: int | None = None,\n+        patch_ids: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         if inputs_embeds is None:\n@@ -748,14 +747,14 @@ def __init__(self, config: BltLocalDecoderConfig):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        patch_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        patch_embeds: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         batch_size = inputs_embeds.shape[0]\n@@ -824,10 +823,10 @@ def __init__(self, config: BltGlobalTransformerConfig):\n     def forward(\n         self,\n         input_embeds: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         batch_size, seq_len, _ = input_embeds.shape\n@@ -850,7 +849,7 @@ def forward(\n         return hidden_states\n \n \n-def process_patch_lengths(patch_lengths: torch.Tensor, max_patch_length: Optional[int]) -> torch.Tensor:\n+def process_patch_lengths(patch_lengths: torch.Tensor, max_patch_length: int | None) -> torch.Tensor:\n     \"\"\"\n     Splits patch lengths into smaller segments if they exceed `max_patch_length`.\n     Pads the result to uniform length across the batch.\n@@ -915,16 +914,16 @@ def __init__(self, config: BltPatcherConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        patch_size: Optional[int] = None,\n-        threshold: Optional[float] = None,\n-        max_patch_length: Optional[int] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        patch_size: int | None = None,\n+        threshold: float | None = None,\n+        max_patch_length: int | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -1220,14 +1219,14 @@ def __init__(self, config: BltConfig):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        patch_lengths: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        patch_lengths: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -1393,20 +1392,20 @@ def __init__(self, config: BltConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        cross_attention_states: Optional[torch.LongTensor] = None,  # Keep for compatibility\n-        cross_attention_mask: Optional[torch.LongTensor] = None,\n-        full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        cross_attention_states: torch.LongTensor | None = None,  # Keep for compatibility\n+        cross_attention_mask: torch.LongTensor | None = None,\n+        full_text_row_masked_out_mask: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n+    ) -> tuple | CausalLMOutputWithPast:\n         r\"\"\"\n         cross_attention_states (`torch.FloatTensor`, *optional*):\n             Output of the vision model, used for cross-attention. This tensor contains the processed image features that"
        },
        {
            "sha": "421c2e286c40e07a275c1c16d05e02901779579e",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 57,
            "deletions": 59,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"Blt modular model, inheriting from Mllama where appropriate.\"\"\"\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n import torch.distributions\n@@ -218,7 +216,7 @@ def _prepare_patch_cross_attention_mask(\n     return cross_attention_mask\n \n \n-def process_patch_lengths(patch_lengths: torch.Tensor, max_patch_length: Optional[int]) -> torch.Tensor:\n+def process_patch_lengths(patch_lengths: torch.Tensor, max_patch_length: int | None) -> torch.Tensor:\n     \"\"\"\n     Splits patch lengths into smaller segments if they exceed `max_patch_length`.\n     Pads the result to uniform length across the batch.\n@@ -305,7 +303,7 @@ def __init__(self, config: BltConfig, layer_idx: int):\n class BltCrossAttention(MllamaTextCrossAttention):\n     \"\"\"Cross-attention module for Blt, following transformers style\"\"\"\n \n-    def __init__(self, config: BltConfig, layer_idx: int, hidden_size: Optional[int] = None):\n+    def __init__(self, config: BltConfig, layer_idx: int, hidden_size: int | None = None):\n         super().__init__()\n         self.is_causal = False\n         self.q_norm = BltRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n@@ -314,8 +312,8 @@ def __init__(self, config: BltConfig, layer_idx: int, hidden_size: Optional[int]\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cross_attention_states: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        cross_attention_states: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         bsz, q_len, _ = hidden_states.size()\n@@ -564,16 +562,16 @@ def __init__(self, config: BltLocalEncoderConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        patch_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        num_patches: Optional[int] = None,\n-        patch_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        patch_embeds: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        num_patches: int | None = None,\n+        patch_ids: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         if inputs_embeds is None:\n@@ -678,14 +676,14 @@ def __init__(self, config: BltLocalDecoderConfig):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        patch_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        patch_embeds: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         batch_size = inputs_embeds.shape[0]\n@@ -754,10 +752,10 @@ def __init__(self, config: BltGlobalTransformerConfig):\n     def forward(\n         self,\n         input_embeds: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         batch_size, seq_len, _ = input_embeds.shape\n@@ -801,16 +799,16 @@ def __init__(self, config: BltPatcherConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        patch_size: Optional[int] = None,\n-        threshold: Optional[float] = None,\n-        max_patch_length: Optional[int] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        patch_size: int | None = None,\n+        threshold: float | None = None,\n+        max_patch_length: int | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -943,14 +941,14 @@ def __init__(self, config: BltConfig):\n     @check_model_inputs\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        patch_lengths: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        patch_lengths: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n@@ -1107,20 +1105,20 @@ def __init__(self, config: BltConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        cross_attention_states: Optional[torch.LongTensor] = None,  # Keep for compatibility\n-        cross_attention_mask: Optional[torch.LongTensor] = None,\n-        full_text_row_masked_out_mask: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        cross_attention_states: torch.LongTensor | None = None,  # Keep for compatibility\n+        cross_attention_mask: torch.LongTensor | None = None,\n+        full_text_row_masked_out_mask: tuple[torch.Tensor, torch.Tensor] | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n+    ) -> tuple | CausalLMOutputWithPast:\n         # Call parent forward but exclude cross_attention_states from model call\n         outputs = self.model(\n             input_ids=input_ids,"
        },
        {
            "sha": "4013c7a217f1ee584d7b32043aa67c0690ba36ca",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License=, Version 2.0 (the \"License\");"
        },
        {
            "sha": "5a1089039fbcf4840938ad6c6a42c954c2177dd3",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "c858ffccc37ba4f33a71b6a9604a8eda537dc150",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "15553adbf3151e743d25856583fe3c58f4612b43",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 107,
            "deletions": 109,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n from collections import OrderedDict\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -65,11 +63,11 @@ class BridgeTowerModelOutput(ModelOutput):\n         token), respectively, after further processing through layers used for auxiliary pretraining tasks.\n     \"\"\"\n \n-    text_features: Optional[torch.FloatTensor] = None\n-    image_features: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    text_features: torch.FloatTensor | None = None\n+    image_features: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n @dataclass\n@@ -95,13 +93,13 @@ class BridgeTowerContrastiveOutput(ModelOutput):\n         sequence_length)`.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[tuple[torch.FloatTensor]] = None\n-    image_embeds: Optional[tuple[torch.FloatTensor]] = None\n-    cross_embeds: Optional[tuple[torch.FloatTensor]] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    logits: torch.FloatTensor | None = None\n+    text_embeds: tuple[torch.FloatTensor] | None = None\n+    image_embeds: tuple[torch.FloatTensor] | None = None\n+    cross_embeds: tuple[torch.FloatTensor] | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n class BridgeTowerResidualAttention(nn.Module):\n@@ -139,7 +137,7 @@ def attention(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor):\n             key_padding_mask=attention_mask,\n         )[0]\n \n-    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n+    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor | None = None):\n         residual_state = hidden_state + self.attention(self.ln_1(hidden_state), attention_mask)\n         hidden_state = self.ln_2(residual_state)\n         for layer in self.mlp.values():\n@@ -163,7 +161,7 @@ def __init__(self, config):\n             )\n         self.stop_gradient = config.stop_gradient\n \n-    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n+    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor | None = None):\n         hidden_states = []\n         for block in self.resblocks:\n             hidden_state = block(hidden_state, attention_mask)\n@@ -409,8 +407,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -462,9 +460,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -535,9 +533,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[EncoderDecoderCache] = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: EncoderDecoderCache | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -597,11 +595,11 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n@@ -700,11 +698,11 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         outputs = ()\n@@ -761,16 +759,16 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -830,10 +828,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if position_ids is None:\n@@ -1018,20 +1016,20 @@ def set_input_embeddings(self, value):\n     # copied from transformers.models.bert.modeling_bert.BertModel.forward\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1216,21 +1214,21 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        image_embeds: Optional[torch.FloatTensor] = None,\n-        image_token_type_idx: Optional[int] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        image_embeds: torch.FloatTensor | None = None,\n+        image_token_type_idx: int | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        labels: torch.LongTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BridgeTowerModelOutput]:\n+    ) -> tuple[torch.Tensor] | BridgeTowerModelOutput:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n@@ -1526,19 +1524,19 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        image_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        image_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n+    ) -> MaskedLMOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n@@ -1627,19 +1625,19 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        image_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        image_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs,\n-    ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n+    ) -> SequenceClassifierOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n@@ -1740,19 +1738,19 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        image_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = True,\n-        return_dict: Optional[bool] = None,\n-        return_loss: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        pixel_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        image_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = True,\n+        return_dict: bool | None = None,\n+        return_loss: bool | None = None,\n         **kwargs,\n-    ) -> Union[BridgeTowerContrastiveOutput, tuple[torch.FloatTensor]]:\n+    ) -> BridgeTowerContrastiveOutput | tuple[torch.FloatTensor]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation."
        },
        {
            "sha": "0339573fa62d7b0c1c8ece5c917744fbf71a4677",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The Intel Labs Team Authors, The Microsoft Research Team Authors and HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "dab7328ec3960edac35cb59edcbbc3d07f020e75",
            "filename": "src/transformers/models/bros/configuration_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fconfiguration_bros.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023-present NAVER Corp, The Microsoft Research Asia LayoutLM Team Authors and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "f8799e045ad4c8459be31991eafd1b73df84bf3d",
            "filename": "src/transformers/models/bros/convert_bros_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b2dee3ef27d9aac92cb40dbe5770ef1fd16d863e",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 77,
            "deletions": 79,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023-present NAVER Corp, The Microsoft Research Asia LayoutLM Team Authors and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,6 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -55,11 +53,11 @@ class BrosSpadeOutput(ModelOutput):\n         Classification scores for entity sequence tokens (before SoftMax).\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    initial_token_logits: Optional[torch.FloatTensor] = None\n-    subsequent_token_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    loss: torch.FloatTensor | None = None\n+    initial_token_logits: torch.FloatTensor | None = None\n+    subsequent_token_logits: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n class BrosPositionalEmbedding1D(nn.Module):\n@@ -143,10 +141,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n             input_shape = input_ids.size()\n@@ -204,10 +202,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[torch.Tensor] = False,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        output_attentions: torch.Tensor | None = False,\n     ) -> tuple[torch.Tensor]:\n         hidden_shape = (hidden_states.shape[0], -1, self.num_attention_heads, self.attention_head_size)\n         query_layer = self.query(hidden_states).view(hidden_shape).transpose(1, 2)\n@@ -286,10 +284,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states=hidden_states,\n@@ -353,10 +351,10 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n@@ -419,13 +417,13 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         bbox_pos_emb: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithCrossAttentions]:\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithCrossAttentions:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -560,19 +558,19 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        bbox: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        bbox: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n@@ -699,19 +697,19 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        bbox: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        bbox_first_token_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        bbox: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        bbox_first_token_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | TokenClassifierOutput:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n@@ -819,20 +817,20 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        bbox: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        bbox_first_token_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        initial_token_labels: Optional[torch.Tensor] = None,\n-        subsequent_token_labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        bbox: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        bbox_first_token_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        initial_token_labels: torch.Tensor | None = None,\n+        subsequent_token_labels: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BrosSpadeOutput]:\n+    ) -> tuple[torch.Tensor] | BrosSpadeOutput:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n@@ -957,19 +955,19 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        bbox: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        bbox_first_token_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        bbox: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        bbox_first_token_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | TokenClassifierOutput:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values"
        },
        {
            "sha": "4691834e6277b5f640638d7aaaed2530b74f024a",
            "filename": "src/transformers/models/bros/processing_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b73bc5aaf75e5ed89904c314b834125923562c77",
            "filename": "src/transformers/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbyt5%2Fconvert_byt5_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbyt5%2Fconvert_byt5_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbyt5%2Fconvert_byt5_original_tf_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The T5 authors and HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "ce4ed37a4ec387b157dd0e862602b7c40c1704b2",
            "filename": "src/transformers/models/byt5/tokenization_byt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 T5 Authors and HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,6 @@\n \"\"\"Tokenization class for model ByT5.\"\"\"\n \n import warnings\n-from typing import Optional\n \n from ...tokenization_python import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n@@ -108,7 +106,7 @@ def get_vocab(self):\n         return vocab\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None, already_has_special_tokens: bool = False\n     ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n@@ -147,7 +145,7 @@ def _add_eos_if_not_present(self, token_ids: list[int]) -> list[int]:\n             return token_ids + [self.eos_token_id]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n     ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. ByT5 does not\n@@ -169,7 +167,7 @@ def create_token_type_ids_from_sequences(\n         return len(token_ids_0 + eos + token_ids_1 + eos) * [0]\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+        self, token_ids_0: list[int], token_ids_1: list[int] | None = None\n     ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n@@ -229,7 +227,7 @@ def convert_tokens_to_string(self, tokens):\n         return string\n \n     # ByT5Tokenizer has no vocab file\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n         return ()\n \n "
        },
        {
            "sha": "492fbceeb740e710a6eb82ec125b626f8fb90b58",
            "filename": "src/transformers/models/camembert/configuration_camembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #"
        },
        {
            "sha": "7bdcb24be61e18926d45208541e06ce433cceea2",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 91,
            "deletions": 93,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_camembert.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-# coding=utf-8\n # Copyright 2019 Inria, Facebook AI Research and the HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -21,7 +20,6 @@\n # limitations under the License.\n \n from collections.abc import Callable\n-from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -79,10 +77,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if position_ids is None:\n@@ -165,8 +163,8 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: Optional[float] = None,\n+    attention_mask: torch.Tensor | None,\n+    scaling: float | None = None,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n ):\n@@ -217,9 +215,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -289,9 +287,9 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[EncoderDecoderCache] = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: EncoderDecoderCache | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         # determine input shapes\n@@ -364,11 +362,11 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n@@ -436,11 +434,11 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         self_attention_output, _ = self.attention(\n@@ -537,14 +535,14 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n         for i, layer_module in enumerate(self.layer):\n             hidden_states = layer_module(\n                 hidden_states,\n@@ -619,18 +617,18 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n@@ -761,16 +759,16 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> tuple[torch.Tensor] | MaskedLMOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -859,14 +857,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | SequenceClassifierOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -943,14 +941,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+    ) -> tuple[torch.Tensor] | MultipleChoiceModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1044,14 +1042,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | TokenClassifierOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -1111,15 +1109,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+    ) -> tuple[torch.Tensor] | QuestionAnsweringModelOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -1207,20 +1205,20 @@ def set_output_embeddings(self, new_embeddings):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        past_key_values: tuple[tuple[torch.FloatTensor]] | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:"
        },
        {
            "sha": "a7d98b33498339bcab887c0d514d6d32419c744f",
            "filename": "src/transformers/models/camembert/modular_camembert.py",
            "status": "modified",
            "additions": 51,
            "deletions": 54,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2019 Inria, Facebook AI Research and the HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -15,8 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch CamemBERT model.\"\"\"\n \n-from typing import Optional, Union\n-\n import torch\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -68,16 +65,16 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> tuple[torch.Tensor] | MaskedLMOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -133,14 +130,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | SequenceClassifierOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -212,14 +209,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+    ) -> tuple[torch.Tensor] | MultipleChoiceModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -304,14 +301,14 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> tuple[torch.Tensor] | TokenClassifierOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -366,15 +363,15 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+    ) -> tuple[torch.Tensor] | QuestionAnsweringModelOutput:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -440,20 +437,20 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        use_cache: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        encoder_hidden_states: torch.FloatTensor | None = None,\n+        encoder_attention_mask: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        past_key_values: tuple[tuple[torch.FloatTensor]] | None = None,\n+        use_cache: bool | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | CausalLMOutputWithCrossAttentions:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:"
        },
        {
            "sha": "dca65b28f73cbbf9b87a5fa13963e55cc783d375",
            "filename": "src/transformers/models/camembert/tokenization_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License\n \"\"\"Tokenization classes for Camembert model.\"\"\"\n \n-from typing import Optional, Union\n-\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import Unigram\n \n@@ -105,7 +102,7 @@ def __init__(\n         additional_special_tokens=None,\n         add_prefix_space=True,\n         vocab_file=None,\n-        vocab: Optional[Union[str, dict, list]] = None,\n+        vocab: str | dict | list | None = None,\n         **kwargs,\n     ):\n         self.vocab_file = vocab_file"
        },
        {
            "sha": "dd94c99706dc787600995f32b3f724722cfe135d",
            "filename": "src/transformers/models/canine/configuration_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Fconfiguration_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Fconfiguration_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fconfiguration_canine.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright Google AI and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "766ec566ccfc1cca5b5e8958f6b9a8f62fcb810e",
            "filename": "src/transformers/models/canine/convert_canine_original_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Fconvert_canine_original_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Fconvert_canine_original_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fconvert_canine_original_tf_checkpoint_to_pytorch.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "1aed9699846cfe7434b10380a3a8135465782d48",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 73,
            "deletions": 75,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 Google AI The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -17,7 +16,6 @@\n import copy\n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -78,10 +76,10 @@ class CanineModelOutputWithPooling(ModelOutput):\n         attention softmax, used to compute the weighted average in the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    pooler_output: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[torch.FloatTensor] | None = None\n \n \n class CanineEmbeddings(nn.Module):\n@@ -147,10 +145,10 @@ def _embed_hash_buckets(self, input_ids, embedding_size: int, num_hashes: int, n\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.FloatTensor:\n         if input_ids is not None:\n             input_shape = input_ids.size()\n@@ -247,7 +245,7 @@ def __init__(self, config):\n     def forward(\n         self,\n         inputs: torch.Tensor,\n-        final_seq_char_positions: Optional[torch.Tensor] = None,\n+        final_seq_char_positions: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         # inputs has shape [batch, mol_seq, molecule_hidden_size+char_hidden_final]\n         # we transpose it to be [batch, molecule_hidden_size+char_hidden_final, mol_seq]\n@@ -304,9 +302,9 @@ def forward(\n         self,\n         from_tensor: torch.Tensor,\n         to_tensor: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         batch_size, seq_length, _ = from_tensor.shape\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -430,9 +428,9 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor | None]:\n         if not self.local:\n             self_outputs = self.self(hidden_states, hidden_states, attention_mask, output_attentions)\n             attention_output = self_outputs[0]\n@@ -561,9 +559,9 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor | None]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n@@ -620,11 +618,11 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple, BaseModelOutput]:\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+    ) -> tuple | BaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -834,16 +832,16 @@ def _repeat_molecules(self, molecules: torch.Tensor, char_seq_length: int) -> to\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, CanineModelOutputWithPooling]:\n+    ) -> tuple | CanineModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1004,17 +1002,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, SequenceClassifierOutput]:\n+    ) -> tuple | SequenceClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1088,17 +1086,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, MultipleChoiceModelOutput]:\n+    ) -> tuple | MultipleChoiceModelOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1192,17 +1190,17 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, TokenClassifierOutput]:\n+    ) -> tuple | TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1287,18 +1285,18 @@ def __init__(self, config):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        attention_mask: torch.FloatTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        start_positions: torch.LongTensor | None = None,\n+        end_positions: torch.LongTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, QuestionAnsweringModelOutput]:\n+    ) -> tuple | QuestionAnsweringModelOutput:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.canine("
        },
        {
            "sha": "4b533f2513cb167a5209a5f04a2a89835fc52746",
            "filename": "src/transformers/models/canine/tokenization_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright Google AI and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "e98689a53c9c762ac7fede969bf29312345acbee",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 24,
            "deletions": 27,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 Meta Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"chameleon model configuration\"\"\"\n \n-from typing import Optional\n-\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_rope_utils import RopeParameters\n from ...utils import logging\n@@ -76,7 +73,7 @@ def __init__(\n         base_channels: int = 128,\n         channel_multiplier: list[int] = [1, 1, 2, 2, 4],\n         num_res_blocks: int = 2,\n-        attn_resolutions: Optional[list[int]] = None,\n+        attn_resolutions: list[int] | None = None,\n         dropout: float = 0.0,\n         attn_type: str = \"vanilla\",\n         initializer_range=0.02,\n@@ -188,29 +185,29 @@ class ChameleonConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        vocab_size: Optional[int] = 65536,\n-        hidden_size: Optional[int] = 4096,\n-        intermediate_size: Optional[int] = 11008,\n-        num_hidden_layers: Optional[int] = 32,\n-        num_attention_heads: Optional[int] = 32,\n-        num_key_value_heads: Optional[int] = 32,\n-        hidden_act: Optional[int] = \"silu\",\n-        max_position_embeddings: Optional[int] = 4096,\n-        initializer_range: Optional[float] = 0.02,\n-        rms_norm_eps: Optional[int] = 1e-05,\n-        use_cache: Optional[bool] = True,\n-        pad_token_id: Optional[int] = None,\n-        bos_token_id: Optional[int] = 1,\n-        eos_token_id: Optional[int] = 2,\n-        tie_word_embeddings: Optional[bool] = False,\n-        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n-        attention_bias: Optional[int] = False,\n-        attention_dropout: Optional[float] = 0.0,\n-        model_parallel_size: Optional[int] = 1,\n-        swin_norm: Optional[bool] = False,\n-        vq_config: Optional[dict] = None,\n-        vocabulary_map: Optional[dict] = None,\n-        mlp_bias: Optional[bool] = False,\n+        vocab_size: int | None = 65536,\n+        hidden_size: int | None = 4096,\n+        intermediate_size: int | None = 11008,\n+        num_hidden_layers: int | None = 32,\n+        num_attention_heads: int | None = 32,\n+        num_key_value_heads: int | None = 32,\n+        hidden_act: int | None = \"silu\",\n+        max_position_embeddings: int | None = 4096,\n+        initializer_range: float | None = 0.02,\n+        rms_norm_eps: int | None = 1e-05,\n+        use_cache: bool | None = True,\n+        pad_token_id: int | None = None,\n+        bos_token_id: int | None = 1,\n+        eos_token_id: int | None = 2,\n+        tie_word_embeddings: bool | None = False,\n+        rope_parameters: RopeParameters | dict[str, RopeParameters] | None = None,\n+        attention_bias: int | None = False,\n+        attention_dropout: float | None = 0.0,\n+        model_parallel_size: int | None = 1,\n+        swin_norm: bool | None = False,\n+        vq_config: dict | None = None,\n+        vocabulary_map: dict | None = None,\n+        mlp_bias: bool | None = False,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size"
        },
        {
            "sha": "040c3c567b4af2413a3999747d463ba0496f4ff8",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 Meta Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "1ee85b9c4a62adccec17ab9ddb529164408ff675",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 Meta Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "26486409f6f8518a161069d404009c09584e84c7",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 52,
            "deletions": 53,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 Meta Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,7 @@\n \n from collections.abc import Callable\n from functools import cached_property\n-from typing import Optional, Union\n+from typing import Optional\n \n import torch\n import torch.nn.functional as F\n@@ -88,9 +87,9 @@ def __init__(self, config: ChameleonConfig, device=None):\n \n     @staticmethod\n     def compute_default_rope_parameters(\n-        config: Optional[ChameleonConfig] = None,\n+        config: ChameleonConfig | None = None,\n         device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n+        seq_len: int | None = None,\n     ) -> tuple[\"torch.Tensor\", float]:\n         \"\"\"\n         Computes the inverse frequencies according to the original RoPE implementation\n@@ -223,7 +222,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -247,7 +246,7 @@ def eager_attention_forward(\n class ChameleonAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: ChameleonConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: ChameleonConfig, layer_idx: int | None = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n@@ -286,15 +285,15 @@ def __init__(self, config: ChameleonConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)\n@@ -355,15 +354,15 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -429,15 +428,15 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n+        cache_position: torch.LongTensor | None = None,\n+        position_embeddings: torch.Tensor | None = None,\n         **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, tuple[torch.FloatTensor, torch.FloatTensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`):\n@@ -895,19 +894,19 @@ def get_placeholder_mask(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> tuple | BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1031,20 +1030,20 @@ def get_image_features(self, pixel_values):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        past_key_values: Cache | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        labels: torch.LongTensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        cache_position: torch.LongTensor | None = None,\n+        logits_to_keep: int | torch.Tensor = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> Union[tuple, CausalLMOutputWithPast]:\n+    ) -> tuple | CausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "df934cbfdf678efc131697eb0908536adccff0bf",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 Meta Inc. and The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "fe8a48859fa237987638fe2b0297164de8a1d6ef",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "e097f84193668519affd2eaf4c75189259ef5e56",
            "filename": "src/transformers/models/chinese_clip/convert_chinese_clip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconvert_chinese_clip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconvert_chinese_clip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconvert_chinese_clip_original_pytorch_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "de30c962dccbbeda9e5bc8ac3a9779350006a62d",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0358c511665b253819ca0e8de7a229f65b7fe142",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2025 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b14942c2a482fc289d28deaf0796590bec02aa85",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 65,
            "deletions": 66,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,7 @@\n \n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n from torch import nn\n@@ -75,11 +74,11 @@ class ChineseCLIPOutput(ModelOutput):\n         The output of the [`ChineseCLIPVisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n     vision_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n \n@@ -112,10 +111,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n             input_shape = input_ids.size()\n@@ -241,7 +240,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n@@ -285,8 +284,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -341,8 +340,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -379,8 +378,8 @@ def __init__(self, config):\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n \n     def forward(\n-        self, hidden_states: torch.Tensor, output_attentions: Optional[bool] = False, **kwargs\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        self, hidden_states: torch.Tensor, output_attentions: bool | None = False, **kwargs\n+    ) -> tuple[torch.Tensor, torch.Tensor | None, tuple[torch.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         input_shape = hidden_states.shape[:-1]\n@@ -471,8 +470,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n@@ -509,7 +508,7 @@ def __init__(self, config: ChineseCLIPConfig):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -627,12 +626,12 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutput]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -680,10 +679,10 @@ def __init__(self, config: ChineseCLIPConfig):\n     def forward(\n         self,\n         inputs_embeds,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+    ) -> tuple | BaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -745,12 +744,12 @@ def __init__(self, config: ChineseCLIPVisionConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        return_dict: bool | None = None,\n+    ) -> tuple | BaseModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -830,20 +829,20 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n-        encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        encoder_hidden_states: torch.Tensor | None = None,\n+        encoder_attention_mask: torch.Tensor | None = None,\n+        past_key_values: Cache | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPooling]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -925,13 +924,13 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> tuple | BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n \n@@ -1006,9 +1005,9 @@ def __init__(self, config: ChineseCLIPConfig):\n     def get_text_features(\n         self,\n         input_ids: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1086,18 +1085,18 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, ChineseCLIPOutput]:\n+    ) -> tuple | ChineseCLIPOutput:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "c76008d5259b2a4931355e2a007bec7bacabbdf8",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The OFA-Sys Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "522edc2885f3f8793317c261c6f920bb568a0858",
            "filename": "src/transformers/models/clap/configuration_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "379e86fd8bbe5d0e477469fe5c667f6138bad2b1",
            "filename": "src/transformers/models/clap/convert_clap_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fconvert_clap_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fconvert_clap_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconvert_clap_original_pytorch_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "8f0a34d2cf4e39d6910b6c9711a3fbc7e97fa7db",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -15,7 +14,7 @@\n \"\"\"Feature extractor class for CLAP.\"\"\"\n \n import copy\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import torch\n@@ -94,7 +93,7 @@ def __init__(\n         return_attention_mask=False,  # pad inputs to max length with silence token (zero) and no attention mask\n         frequency_min: float = 0,\n         frequency_max: float = 14_000,\n-        top_db: Optional[int] = None,\n+        top_db: int | None = None,\n         truncation: str = \"fusion\",\n         padding: str = \"repeatpad\",\n         **kwargs,\n@@ -152,7 +151,7 @@ def to_dict(self) -> dict[str, Any]:\n             del output[\"mel_filters_slaney\"]\n         return output\n \n-    def _np_extract_fbank_features(self, waveform: np.ndarray, mel_filters: Optional[np.ndarray] = None) -> np.ndarray:\n+    def _np_extract_fbank_features(self, waveform: np.ndarray, mel_filters: np.ndarray | None = None) -> np.ndarray:\n         \"\"\"\n         Compute the log-mel spectrogram of the provided `waveform` using the Hann window. In CLAP, two different filter\n         banks are used depending on the truncation pattern:\n@@ -259,12 +258,12 @@ def _get_input_mel(self, waveform: np.ndarray, max_length, truncation, padding)\n \n     def __call__(\n         self,\n-        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n-        truncation: Optional[str] = None,\n-        padding: Optional[str] = None,\n-        max_length: Optional[int] = None,\n-        sampling_rate: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        raw_speech: np.ndarray | list[float] | list[np.ndarray] | list[list[float]],\n+        truncation: str | None = None,\n+        padding: str | None = None,\n+        max_length: int | None = None,\n+        sampling_rate: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "1fbade060c913e526e9d4fad499f89c8db14ebe8",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 88,
            "deletions": 89,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The LAION-AI Team and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -18,7 +17,7 @@\n import math\n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n import torch.nn.functional as F\n@@ -119,10 +118,10 @@ class ClapTextModelOutput(ModelOutput):\n         The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    text_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -137,10 +136,10 @@ class ClapAudioModelOutput(ModelOutput):\n         The Audio embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    audio_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    audio_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -166,11 +165,11 @@ class ClapOutput(ModelOutput):\n         The output of the [`ClapAudioModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_audio: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    audio_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_audio: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    audio_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     audio_model_output: BaseModelOutputWithPooling = None\n \n@@ -376,8 +375,8 @@ def __init__(self, config, dim, num_heads, window_size):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         batch_size, dim, num_channels = hidden_states.shape\n         hidden_shape = (batch_size, dim, -1, self.attention_head_size)\n@@ -463,8 +462,8 @@ def __init__(self, config, dim, num_heads, window_size):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(hidden_states, attention_mask, output_attentions)\n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -563,8 +562,8 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        output_attentions: Optional[bool] = False,\n-        always_partition: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n+        always_partition: bool | None = False,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         if not always_partition:\n             self.set_shift_and_window_size(input_dimensions)\n@@ -656,8 +655,8 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         input_dimensions: tuple[int, int],\n-        output_attentions: Optional[bool] = False,\n-        always_partition: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n+        always_partition: bool | None = False,\n     ) -> tuple[torch.Tensor]:\n         height, width = input_dimensions\n         for i, layer_module in enumerate(self.blocks):\n@@ -815,13 +814,13 @@ def reshape_mel2img(self, normalized_input_features):\n     def forward(\n         self,\n         input_features,\n-        is_longer: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        output_hidden_states_before_downsampling: Optional[bool] = False,\n-        always_partition: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple, ClapAudioModelOutput]:\n+        is_longer: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        output_hidden_states_before_downsampling: bool | None = False,\n+        always_partition: bool | None = False,\n+        return_dict: bool | None = True,\n+    ) -> tuple | ClapAudioModelOutput:\n         input_features = input_features.transpose(1, 3)\n         normalized_input_features = self.batch_norm(input_features)\n         normalized_input_features = normalized_input_features.transpose(1, 3)\n@@ -928,7 +927,7 @@ def forward(\n \n \n class ClapProjectionLayer(nn.Module):\n-    def __init__(self, config: Union[ClapAudioConfig, ClapTextConfig]):\n+    def __init__(self, config: ClapAudioConfig | ClapTextConfig):\n         super().__init__()\n         self.config = config\n         hidden_size = config.hidden_size\n@@ -971,10 +970,10 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        token_type_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        token_type_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n         past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if position_ids is None:\n@@ -1058,7 +1057,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs,\n@@ -1102,8 +1101,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -1158,8 +1157,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n@@ -1217,8 +1216,8 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor]:\n         self_attention_outputs = self.attention(\n@@ -1255,12 +1254,12 @@ def __init__(self, config):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        attention_mask: torch.FloatTensor | None = None,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutput]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -1361,13 +1360,13 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        input_features: Optional[torch.FloatTensor] = None,\n-        is_longer: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_features: torch.FloatTensor | None = None,\n+        is_longer: torch.BoolTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+    ) -> tuple | BaseModelOutputWithPooling:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n@@ -1449,16 +1448,16 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        token_type_ids: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        token_type_ids: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> tuple[torch.Tensor] | BaseModelOutputWithPoolingAndCrossAttentions:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1558,8 +1557,8 @@ def __init__(self, config: ClapConfig):\n     def get_text_features(\n         self,\n         input_ids: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -1592,8 +1591,8 @@ def get_text_features(\n     def get_audio_features(\n         self,\n         input_features: torch.Tensor,\n-        is_longer: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        is_longer: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n@@ -1630,17 +1629,17 @@ def get_audio_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        input_features: Optional[torch.FloatTensor] = None,\n-        is_longer: Optional[torch.BoolTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        input_features: torch.FloatTensor | None = None,\n+        is_longer: torch.BoolTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, ClapOutput]:\n+    ) -> tuple | ClapOutput:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n@@ -1747,14 +1746,14 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, ClapTextModelOutput]:\n+    ) -> tuple | ClapTextModelOutput:\n         r\"\"\"\n         Examples:\n \n@@ -1812,13 +1811,13 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        input_features: Optional[torch.FloatTensor] = None,\n-        is_longer: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        input_features: torch.FloatTensor | None = None,\n+        is_longer: torch.BoolTensor | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         **kwargs,\n-    ) -> Union[tuple, ClapAudioModelOutput]:\n+    ) -> tuple | ClapAudioModelOutput:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance"
        },
        {
            "sha": "e8178229a753e72253b56b156e19509f98211e7f",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2023 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "694dc16dadd60a48a4cdc16e12691b023f657023",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "0987e7785988e186b11b72a42e89bb5920d580b5",
            "filename": "src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fconvert_clip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fconvert_clip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconvert_clip_original_pytorch_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "5244299fb34b401d775ea28eadc62dbee2bb7b0e",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "665ba49e507fe5dd88a7ff57badbeaefc8377045",
            "filename": "src/transformers/models/clip/image_processing_clip_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip_fast.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "1154a3b47500543c8e9433196faa5e83019166e7",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 45,
            "deletions": 46,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The OpenAI Team Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,7 +15,7 @@\n \n from collections.abc import Callable\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import torch\n from torch import nn\n@@ -79,10 +78,10 @@ class CLIPVisionModelOutput(ModelOutput):\n         The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    image_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -97,10 +96,10 @@ class CLIPTextModelOutput(ModelOutput):\n         The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n-    text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    text_embeds: torch.FloatTensor | None = None\n+    last_hidden_state: torch.FloatTensor | None = None\n+    hidden_states: tuple[torch.FloatTensor, ...] | None = None\n+    attentions: tuple[torch.FloatTensor, ...] | None = None\n \n \n @dataclass\n@@ -125,11 +124,11 @@ class CLIPOutput(ModelOutput):\n         The output of the [`CLIPVisionModel`].\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: Optional[torch.FloatTensor] = None\n-    logits_per_text: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[torch.FloatTensor] = None\n-    image_embeds: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    logits_per_image: torch.FloatTensor | None = None\n+    logits_per_text: torch.FloatTensor | None = None\n+    text_embeds: torch.FloatTensor | None = None\n+    image_embeds: torch.FloatTensor | None = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -238,9 +237,9 @@ def __init__(self, config: CLIPTextConfig):\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n         max_position_embedding = self.position_embedding.weight.shape[0]\n@@ -268,7 +267,7 @@ def eager_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     scaling: float,\n     dropout: float = 0.0,\n     **kwargs: Unpack[TransformersKwargs],\n@@ -287,7 +286,7 @@ def eager_attention_forward(\n class CLIPAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n+    def __init__(self, config: CLIPVisionConfig | CLIPTextConfig):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size\n@@ -310,9 +309,9 @@ def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -362,7 +361,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class CLIPEncoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n+    def __init__(self, config: CLIPVisionConfig | CLIPTextConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n         self.self_attn = CLIPAttention(config)\n@@ -487,7 +486,7 @@ def __init__(self, config: CLIPConfig):\n     def forward(\n         self,\n         inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -532,9 +531,9 @@ def __init__(self, config: CLIPTextConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         if input_ids is None:\n@@ -619,9 +618,9 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -662,8 +661,8 @@ def __init__(self, config: CLIPVisionConfig):\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n+        pixel_values: torch.FloatTensor | None = None,\n+        interpolate_pos_encoding: bool | None = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         if pixel_values is None:\n@@ -711,7 +710,7 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n@@ -788,8 +787,8 @@ def __init__(self, config: CLIPConfig):\n     def get_text_features(\n         self,\n         input_ids: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -863,11 +862,11 @@ def get_image_features(\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        return_loss: Optional[bool] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        pixel_values: torch.FloatTensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.LongTensor | None = None,\n+        return_loss: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CLIPOutput:\n@@ -969,9 +968,9 @@ def set_input_embeddings(self, value):\n     @auto_docstring\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        attention_mask: torch.Tensor | None = None,\n+        position_ids: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CLIPTextModelOutput:\n         r\"\"\"\n@@ -1030,7 +1029,7 @@ def get_input_embeddings(self) -> nn.Module:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values: torch.FloatTensor | None = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CLIPVisionModelOutput:\n@@ -1098,8 +1097,8 @@ def __init__(self, config: CLIPConfig) -> None:\n     @auto_docstring\n     def forward(\n         self,\n-        pixel_values: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        pixel_values: torch.Tensor | None = None,\n+        labels: torch.Tensor | None = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n         r\"\"\""
        },
        {
            "sha": "d526c6d832945feb33d320b0db35a4f59c0fb623",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "018c630afbce1cc439477a94779044a2e724a7c5",
            "filename": "src/transformers/models/clip/tokenization_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The Open AI Team Authors and The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -14,8 +13,6 @@\n # limitations under the License.\n \"\"\"Tokenization classes for CLIP.\"\"\"\n \n-from typing import Optional, Union\n-\n from tokenizers import Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE\n \n@@ -58,8 +55,8 @@ class CLIPTokenizer(TokenizersBackend):\n \n     def __init__(\n         self,\n-        vocab: Optional[Union[str, dict[str, int]]] = None,\n-        merges: Optional[Union[str, list[str]]] = None,\n+        vocab: str | dict[str, int] | None = None,\n+        merges: str | list[str] | None = None,\n         unk_token: str = \"<|endoftext|>\",\n         bos_token: str = \"<|startoftext|>\",\n         eos_token: str = \"<|endoftext|>\","
        },
        {
            "sha": "bd11c45f288f136199375c16c355b6e76e2cab19",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "b730fca7d1f72a67eee758b57252f324610f104d",
            "filename": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");"
        },
        {
            "sha": "492d8e9f12b3404ae51ed509ebc55b20726b9387",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "a1d530e721a3973a578a32a575901c60dfb25c97",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "cb487792d909470f3dfe03f55302147acf105e06",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "806c27f9509996a0654f5ccd4af1682526acd968",
            "filename": "src/transformers/models/clvp/convert_clvp_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fconvert_clvp_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fconvert_clvp_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconvert_clvp_to_hf.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "cc39e6aca6770655ae9fd72f1a1c95cdc5260e76",
            "filename": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "b5163e72f3d6824b2a5c8d66b804b07e7fcd4c9d",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 111,
            "deletions": 113,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "d8fd399ca40ba2f2e199ef3774812996bad0fa66",
            "filename": "src/transformers/models/clvp/number_normalizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fnumber_normalizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fnumber_normalizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fnumber_normalizer.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "163e476f076ac3f33c961a603bd5753d482eb919",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "2993fe663fa703263a0e29e58d46a84447484ad0",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "5e5d00f770c94dbcaa565c50cfb19c7487742bf5",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "496b492f1d26abeacaefa4661ee1a899ea87c2f6",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "27aa831b0d2950f0849adb44941c4130844e9186",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 48,
            "deletions": 48,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "8d54a80592b5395fa63d4122cb15c3bbe33dcd1d",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "29706d09408453f42ee51e02f407aac8eede0146",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "d79d5f0f9f00a2a87a7a5f7a1cef4a86b9bb2267",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 34,
            "deletions": 35,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "5846e63710d6a6d26261b4fe26324033316121f2",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 23,
            "deletions": 25,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "fb25704360db9489b32443c6816a0dc7c441feeb",
            "filename": "src/transformers/models/cohere/tokenization_cohere.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        },
        {
            "sha": "1ad5346974a05510872af81fbf874738215ec9e8",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 24,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a275d3581c0ecf962f7412aa764c2047331fd6b/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=3a275d3581c0ecf962f7412aa764c2047331fd6b"
        }
    ],
    "stats": {
        "total": 65081,
        "additions": 31218,
        "deletions": 33863
    }
}