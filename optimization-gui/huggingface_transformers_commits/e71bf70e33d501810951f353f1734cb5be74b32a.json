{
    "author": "amyeroberts",
    "message": "Pixtral update example checkpoint (#33633)\n\n* Update pixtral example checkpoint\r\n\r\n* Fix typo",
    "sha": "e71bf70e33d501810951f353f1734cb5be74b32a",
    "files": [
        {
            "sha": "03b9630bfd985b89222664da8d7c7cdffd5343fc",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e71bf70e33d501810951f353f1734cb5be74b32a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e71bf70e33d501810951f353f1734cb5be74b32a/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=e71bf70e33d501810951f353f1734cb5be74b32a",
            "patch": "@@ -39,7 +39,7 @@ Here is an example of how to run it:\n from transformers import LlavaForConditionalGeneration, AutoProcessor\n from PIL import Image\n \n-model_id = \"hf-internal-testing/pixtral-12b\"\n+model_id = \"mistral-community/pixtral-12b\"\n model = LlavaForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")\n processor = AutoProcessor.from_pretrained(model_id)\n \n@@ -53,7 +53,7 @@ PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n \n inputs = processor(images=IMG_URLS, text=PROMPT, return_tensors=\"pt\").to(\"cuda\")\n generate_ids = model.generate(**inputs, max_new_tokens=500)\n-ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n \n EXPECTED_GENERATION = \"\"\"\n Describe the images."
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}