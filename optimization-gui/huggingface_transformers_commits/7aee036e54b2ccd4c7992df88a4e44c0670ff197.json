{
    "author": "yaswanth19",
    "message": "Iterative generation using Input embeds and `past_key_values` (#35890)\n\n* Iterative generation using input embeds\r\n\r\n* ruff fix\r\n\r\n* Added Testcase\r\n\r\n* Updated comment\r\n\r\n* ♻️ Refactored testcase\r\n\r\n* Skip test for these models\r\n\r\n* Continue generation using input embeds and cache\r\n\r\n* Skip generate_continue_from_embeds test\r\n\r\n* Refactor `prepare_input_for_generation` func\r\n\r\n* Continue generation using input embeds and cache\r\n\r\n* Modular changes fix\r\n\r\n* Overwrite 'prepare_inputs_for_generation' function",
    "sha": "7aee036e54b2ccd4c7992df88a4e44c0670ff197",
    "files": [
        {
            "sha": "f0f7f2b0b6b5f09433d1827e8cfb80bc13f87613",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -381,9 +381,13 @@ def prepare_inputs_for_generation(\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         #              (we can't check exception 3 while compiling)\n+        # Excpetion 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n             model_inputs[\"past_key_values\"] = past_key_values\n-            if (\n+            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            elif (\n                 inputs_embeds is not None  # Exception 1\n                 or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n             ):\n@@ -393,9 +397,9 @@ def prepare_inputs_for_generation(\n \n         # 3. Prepare base model inputs\n         input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step for every prompt.\n         if not self.config.is_encoder_decoder:\n-            if inputs_embeds is not None and cache_position[0] == 0:\n+            if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n                 model_inputs[input_ids_key] = None\n                 model_inputs[\"inputs_embeds\"] = inputs_embeds\n             else:"
        },
        {
            "sha": "6f74f5b839f5d89f9a87450911955948b14b57ce",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -895,8 +895,12 @@ def prepare_inputs_for_generation(\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         #              (we can't check exception 3 while compiling)\n+        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n-            if (\n+            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            elif (\n                 inputs_embeds is not None  # Exception 1\n                 or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n             ):\n@@ -905,7 +909,7 @@ def prepare_inputs_for_generation(\n                 input_ids = input_ids[:, cache_position]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n+        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n             model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n         else:\n             # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the"
        },
        {
            "sha": "3bc6a43d6f5614aa8d86bfe7cfa9fa65da09769d",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1654,8 +1654,12 @@ def prepare_inputs_for_generation(\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         #              (we can't check exception 3 while compiling)\n+        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n-            if (\n+            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            elif (\n                 inputs_embeds is not None  # Exception 1\n                 or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n             ):\n@@ -1668,10 +1672,13 @@ def prepare_inputs_for_generation(\n             position_ids = attention_mask.long().cumsum(-1) - 1\n             position_ids.masked_fill_(attention_mask == 0, 1)\n             if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n+                if inputs_embeds is not None and input_ids.shape[1] == 0:\n+                    position_ids = position_ids[:, -inputs_embeds.shape[1] :]\n+                else:\n+                    position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n+        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "6857fb624c0fcb04394c89e65c215875d409345b",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1674,10 +1674,13 @@ def prepare_inputs_for_generation(\n         else:\n             model_inputs[\"pixel_values\"] = pixel_values\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # If we have cache: let's slice `input_ids` or `input embeds` through `cache_position`, to keep only the unprocessed tokens\n         if past_key_values is not None:\n             if inputs_embeds is not None:\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n+                if input_ids.shape[1] == 0:\n+                    inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+                else:\n+                    input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:\n                 input_ids = input_ids[:, cache_position]\n                 if image_attention_mask is not None:\n@@ -1687,14 +1690,19 @@ def prepare_inputs_for_generation(\n             # create position_ids on the fly for batch generation\n             position_ids = attention_mask.long().cumsum(-1) - 1\n             position_ids.masked_fill_(attention_mask == 0, 1)\n+\n+            # If past_key_values are present then slice the postion ids for only only the unprocessed tokens.\n             if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n+                if inputs_embeds is not None and input_ids.shape[1] == 0:\n+                    position_ids = position_ids[:, -inputs_embeds.shape[1] :]\n+                else:\n+                    position_ids = position_ids[:, -input_ids.shape[1] :]\n \n                 # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n                 position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n+        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n             model_inputs.update({\"inputs_embeds\": inputs_embeds, \"input_ids\": None})\n         else:\n             # The clone here is for the same reason as for `position_ids`."
        },
        {
            "sha": "6bde89f9aab5e053d1daf44ba96c7b9017290d19",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 52,
            "deletions": 14,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1901,8 +1901,7 @@ def forward(\n \n \n @add_start_docstrings(\n-    \"The original Moshi model with an audio encoder, a Moshi depth decoder and a Moshi decoder, \"\n-    \"for speech-to-speech.\",\n+    \"The original Moshi model with an audio encoder, a Moshi depth decoder and a Moshi decoder, for speech-to-speech.\",\n     MOSHI_START_DOCSTRING,\n )\n class MoshiForConditionalGeneration(MoshiPreTrainedModel, GenerationMixin):\n@@ -2458,18 +2457,57 @@ def prepare_inputs_for_generation(\n         blank_user_audio_codes: Optional[torch.FloatTensor] = None,\n         **kwargs,\n     ):\n-        # Overwritten -- Moshi has custom post-processing\n-        # 1. Do usual operations done on LLMs like Gemma - because we pre-processed inputs, the first pass always has inputs_embeds\n-        model_inputs = super().prepare_inputs_for_generation(\n-            input_ids=input_ids,\n-            past_key_values=past_key_values,\n-            attention_mask=attention_mask,\n-            inputs_embeds=inputs_embeds,\n-            cache_position=cache_position,\n-            position_ids=position_ids,\n-            use_cache=use_cache,\n-            logits_to_keep=logits_to_keep,\n-            **kwargs,\n+        # Overwritten -- Moshi has custom post-processing on the prepared inputs.\n+\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        # (we can't check exception 3 while compiling)\n+\n+        if past_key_values is not None:\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = inputs_embeds.shape\n+                device = inputs_embeds.device\n+            else:\n+                batch_size, sequence_length = input_ids.shape\n+                device = input_ids.device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n+            )\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+            }\n         )\n \n         # 2. Now that everything is prepared, generate audio_codes using the depth decoder"
        },
        {
            "sha": "82b112ad366546cb808f6daddafa398b65cf37fe",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1261,7 +1261,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -1872,8 +1872,12 @@ def prepare_inputs_for_generation(\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         #              (we can't check exception 3 while compiling)\n+        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n-            if (\n+            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            elif (\n                 inputs_embeds is not None  # Exception 1\n                 or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n             ):\n@@ -1886,7 +1890,7 @@ def prepare_inputs_for_generation(\n             pixel_values_videos = None\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n+        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n             model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n         else:\n             model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}"
        },
        {
            "sha": "601ad373771c886dd0a73548edc8c423711f8b67",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -770,8 +770,12 @@ def prepare_inputs_for_generation(\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         #              (we can't check exception 3 while compiling)\n+        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n-            if (\n+            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            elif (\n                 inputs_embeds is not None  # Exception 1\n                 or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n             ):\n@@ -784,7 +788,7 @@ def prepare_inputs_for_generation(\n             pixel_values_videos = None\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n+        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n             model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n         else:\n             model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}"
        },
        {
            "sha": "51d8fe9430b5726a9bd10baf0a0461093e5a9ae8",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1735,8 +1735,12 @@ def prepare_inputs_for_generation(\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         #              (we can't check exception 3 while compiling)\n+        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n         if past_key_values is not None:\n-            if (\n+            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n+                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n+            elif (\n                 inputs_embeds is not None  # Exception 1\n                 or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n             ):\n@@ -1749,7 +1753,7 @@ def prepare_inputs_for_generation(\n             pixel_values_videos = None\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n+        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n             model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n         else:\n             model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}"
        },
        {
            "sha": "8a5642e0f5d11c6302914eb940442836023b3019",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1557,7 +1557,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path."
        },
        {
            "sha": "c7c8c7f8c10883b5d808cb62e81c7190d468a7fe",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -1857,6 +1857,83 @@ def test_generate_continue_from_past_key_values(self):\n                         )\n                     )\n \n+    @pytest.mark.generate\n+    def test_generate_continue_from_inputs_embeds(self):\n+        \"\"\"Tests that we can continue generation from `inputs_embeds` and past key values returned from a previous `generate` call.\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\"]):\n+                self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n+                self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n+\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n+            if \"token_type_ids\" in inputs_dict:\n+                del inputs_dict[\"token_type_ids\"]\n+\n+            if config.is_encoder_decoder:\n+                self.skipTest(reason=\"This model is encoder-decoder\")\n+            if not hasattr(config, \"use_cache\"):\n+                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n+\n+            model = model_class(config).to(torch_device).eval()\n+\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+                self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n+\n+            # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n+            outputs = model(**inputs_dict)\n+            if \"past_key_values\" not in outputs:\n+                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n+\n+            pixel_values_is_mutually_exclusive = any(\n+                model_name in model_class.__name__.lower()\n+                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\"]\n+            )\n+            if pixel_values_is_mutually_exclusive:\n+                inputs_dict.pop(\"pixel_values\", None)\n+                inputs_dict.pop(\"pixel_values_videos\", None)\n+                inputs_dict.pop(\"pixel_values_images\", None)\n+\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+\n+            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n+            model.generation_config.forced_eos_token_id = None\n+            model.config.is_decoder = True\n+            model.generation_config.use_cache = True\n+\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"do_sample\": False,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values.\n+            input_embeds = model.get_input_embeddings()(input_ids)\n+            outputs = model.generate(inputs_embeds=input_embeds, max_new_tokens=4, **generation_kwargs)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens)\n+            initial_output = model.generate(inputs_embeds=input_embeds, max_new_tokens=3, **generation_kwargs)\n+            continued_embeds = torch.cat([input_embeds, model.get_input_embeddings()(initial_output.sequences)], dim=1)\n+            cached_output = model.generate(\n+                inputs_embeds=continued_embeds,\n+                max_new_tokens=1,\n+                past_key_values=initial_output.past_key_values,\n+                **generation_kwargs,\n+            )\n+\n+            # Combine the (3 + 1) generated tokens and verify it matches with full generation.\n+            combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n+            self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n+            # The two sets of past kv should be equal to each other\n+            for layer_idx in range(len(cached_output.past_key_values)):\n+                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            outputs.past_key_values[layer_idx][kv_idx],\n+                            cached_output.past_key_values[layer_idx][kv_idx],\n+                        )\n+                    )\n+\n     @parameterized.expand([(\"offloaded\",)])  # (\"offloaded_static\",) TODO: @raushan fixme in some models (eg T5)\n     @require_torch_gpu\n     @pytest.mark.generate"
        },
        {
            "sha": "334f01004936d75204f2d15ef47d15107764953f",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -334,6 +334,10 @@ def test_training_gradient_checkpointing(self):\n         loss = model(**inputs).loss\n         loss.backward()\n \n+    @unittest.skip(reason=\"Clvp `prepare_inputs_for_generation` function doesn't have cache position.\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n \n class ClvpModelForConditionalGenerationTester:\n     def __init__(self, parent, is_training=False):"
        },
        {
            "sha": "81ea53b49f884e5eec72e7e4dfda6026f016b893",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -131,6 +131,10 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    @unittest.skip(\"Cohere2 has HybridCache and doesn't support progressive generation using input embeds.\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n     # overwrite because HybridCache has fixed length for key/values\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1"
        },
        {
            "sha": "634dfcf615650f22979b641f6958488d65cdc0db",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -325,6 +325,10 @@ def test_disk_offload_safetensors(self):\n     def test_model_parallelism(self):\n         super().test_model_parallelism()\n \n+    @unittest.skip(reason=\"Fuyu `prepare_inputs_for_generation` function doesn't have cache position.\")\n+    def test_generate_continue_from_inputs_embeds():\n+        pass\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "a0563aed90cb7d71e93e055c473186d631cec88d",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -146,6 +146,10 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n     # overwrite because HybridCache has fixed length for key/values\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1"
        },
        {
            "sha": "c854a7e7116788a2db57468851f07c5a101b94bc",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -450,6 +450,10 @@ def test_disk_offload(self):\n     def test_past_key_values_format(self):\n         pass\n \n+    @unittest.skip(reason=\"BigCodeGPT has a non-standard KV cache format and breaks this test.\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n     def test_gpt_bigcode_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_gpt_bigcode_model(*config_and_inputs)"
        },
        {
            "sha": "cc9efc967db210b8a1cb76dbee9c382344b60364",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -755,6 +755,65 @@ def test_generate_without_input_ids(self):\n             )\n             self.assertIsNotNone(output_ids_generate)\n \n+    @pytest.mark.generate\n+    def test_generate_continue_from_inputs_embeds(self):\n+        \"\"\"Overwrite for IDEFICS: Ensure image attention mask is processed while continuing from `inputs_embeds`.\"\"\"\n+\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+            print(inputs)\n+\n+            model = model_class(config).to(torch_device).eval()\n+\n+            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n+            model.generation_config.forced_eos_token_id = None\n+            model.generation_config.use_cache = True\n+\n+            input_ids = inputs.pop(\"input_ids\")\n+            input_embeds = model.get_input_embeddings()(input_ids)\n+\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"do_sample\": False,\n+            }\n+\n+            inputs[\"inputs_embeds\"] = input_embeds\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, max_new_tokens=4, **generation_kwargs)\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            initial_output = model.generate(**inputs, max_new_tokens=3, **generation_kwargs)\n+            inputs[\"past_key_values\"] = initial_output.past_key_values\n+\n+            new_attention_len = input_ids.shape[1] + initial_output.sequences.shape[-1]\n+            continued_embeds = torch.cat([input_embeds, model.get_input_embeddings()(initial_output.sequences)], dim=1)\n+            inputs[\"inputs_embeds\"] = continued_embeds\n+\n+            if \"attention_mask\" in inputs:\n+                inputs[\"attention_mask\"] = torch.nn.functional.pad(\n+                    inputs[\"attention_mask\"],\n+                    (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+            if \"image_attention_mask\" in inputs:\n+                inputs[\"image_attention_mask\"] = inputs[\"image_attention_mask\"][..., -1:, :]\n+\n+            cached_output = model.generate(**inputs, max_new_tokens=1, **generation_kwargs)\n+\n+            # Verify that the combined outputs match the full generation.\n+            combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n+            self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n+            for layer_idx in range(len(cached_output.past_key_values)):\n+                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            outputs.past_key_values[layer_idx][kv_idx],\n+                            cached_output.past_key_values[layer_idx][kv_idx],\n+                        )\n+                    )\n+\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n     ):"
        },
        {
            "sha": "09278f0d24c4e1f32564abc17a35e0fce99c9e5c",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -358,6 +358,10 @@ def test_disk_offload_bin(self):\n     def test_disk_offload_safetensors(self):\n         pass\n \n+    @unittest.skip(reason=\"Test becomes too complex with Moshi requiring multiple input modalities.\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n     @is_flaky(max_attempts=5, description=\"flaky on some models.\")\n     def test_save_load(self):\n         super().test_save_load()\n@@ -824,6 +828,7 @@ def test_generate_without_input_ids(self):\n             output_ids_generate = model.generate(\n                 do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True\n             )\n+            print(output_ids_generate)\n             self.assertIsNotNone(output_ids_generate)\n \n     @unittest.skip(reason=\"The audio encoder has no gradients.\")\n@@ -919,6 +924,10 @@ def test_disk_offload_bin(self):\n     def test_disk_offload_safetensors(self):\n         pass\n \n+    @unittest.skip(reason=\"Test becomes too complex with Moshi requiring multiple modalities\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n     @is_flaky(max_attempts=5, description=\"flaky on some models.\")\n     def test_save_load(self):\n         super().test_save_load()"
        },
        {
            "sha": "c876e598e867899369f914b2661313daf3cb4007",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7aee036e54b2ccd4c7992df88a4e44c0670ff197/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=7aee036e54b2ccd4c7992df88a4e44c0670ff197",
            "patch": "@@ -333,6 +333,10 @@ def test_past_key_values_format(self):\n         \"\"\"\n         pass\n \n+    @unittest.skip(reason=\"Zamba2 has hybrid cache.\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n     @unittest.skip(reason=\"A large mamba2 would be necessary (and costly) for that\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        }
    ],
    "stats": {
        "total": 310,
        "additions": 276,
        "deletions": 34
    }
}