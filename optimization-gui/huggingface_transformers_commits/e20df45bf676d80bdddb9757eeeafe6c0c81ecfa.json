{
    "author": "merveenoyan",
    "message": "Add Backbone API fine-tuning tutorial (#41590)\n\n\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "e20df45bf676d80bdddb9757eeeafe6c0c81ecfa",
    "files": [
        {
            "sha": "7853dae7c11b314751dad7f9182b5361d3b9291f",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=e20df45bf676d80bdddb9757eeeafe6c0c81ecfa",
            "patch": "@@ -284,6 +284,8 @@\n         title: Knowledge Distillation for Computer Vision\n       - local: tasks/keypoint_matching\n         title: Keypoint matching\n+      - local: tasks/training_vision_backbone\n+        title: Training vision models using Backbone API\n       title: Computer vision\n     - sections:\n       - local: tasks/image_captioning"
        },
        {
            "sha": "6febc61aaefb8795ccf657bfeb27a6a782d6fe18",
            "filename": "docs/source/en/tasks/training_vision_backbone.md",
            "status": "added",
            "additions": 250,
            "deletions": 0,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/docs%2Fsource%2Fen%2Ftasks%2Ftraining_vision_backbone.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e20df45bf676d80bdddb9757eeeafe6c0c81ecfa/docs%2Fsource%2Fen%2Ftasks%2Ftraining_vision_backbone.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftraining_vision_backbone.md?ref=e20df45bf676d80bdddb9757eeeafe6c0c81ecfa",
            "patch": "@@ -0,0 +1,250 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Training Vision Models using Backbone API\n+\n+Computer vision workflows follow a common pattern. Use a pre-trained backbone for feature extraction ([ViT](../model_doc/vit), [DINOv3](../model_doc/dinov3)). Add a \"neck\" for feature enhancement. Attach a task-specific head ([DETR](../model_doc/detr) for object detection, [MaskFormer](../model_doc/maskformer) for segmentation).\n+\n+The Transformers library implements these models and the [backbone API](../backbones) lets you swap different backbones and heads with minimal code.\n+\n+![Backbone Explanation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Backbone.png)\n+\n+This guide combines [DINOv3 with ConvNext architecture](https://huggingface.co/facebook/dinov3-convnext-large-pretrain-lvd1689m) and a [DETR head](https://huggingface.co/facebook/detr-resnet-50). You'll train on the [license plate detection dataset](https://huggingface.co/datasets/merve/license-plates). DINOv3 delivers the best performance as of this writing.\n+\n+> [!NOTE]\n+> This model requires access approval. Visit [the model repository](https://huggingface.co/facebook/dinov3-convnext-large-pretrain-lvd1689m) to request access.\n+\n+Install [trackio](https://github.com/gradio-app/trackio) for experiment tracking and [albumentations](https://albumentations.ai/) for data augmentation. Use the latest transformers version.\n+\n+```bash\n+pip install -Uq albumentations trackio transformers datasets\n+```\n+\n+Initialize [`DetrConfig`] with the pre-trained DINOv3 ConvNext backbone. Use `num_labels=1` to detect the license plate bounding boxes. Create [`DetrForObjectDetection`] with this configuration. Freeze the backbone to preserve DINOv3 features without updating weights. Load the [`DetrImageProcessor`].\n+\n+```py\n+from transformers import DetrConfig, DetrForObjectDetection, AutoImageProcessor\n+\n+config = DetrConfig(backbone=\"facebook/dinov3-convnext-large-pretrain-lvd1689m\",\n+                    use_pretrained_backbone=True, use_timm_backbone=False,\n+                    num_labels=1, id2label={0: \"license_plate\"}, label2id={\"license_plate\": 0})\n+model = DetrForObjectDetection(config)\n+\n+for param in model.model.backbone.parameters():\n+    param.requires_grad = False\n+image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n+```\n+\n+Load the dataset and split it for training.\n+\n+```py\n+from datasets import load_dataset\n+ds = load_dataset(\"merve/license-plates\")\n+ds = ds[\"train\"]\n+\n+ds = ds.train_test_split(test_size=0.05)\n+train_dataset = ds[\"train\"]\n+val_dataset = ds[\"test\"]\n+len(train_dataset)\n+# 5867\n+```\n+\n+Augment the dataset. Rescale images to a maximum size, flip them, and apply affine transforms. Eliminate invalid bounding boxes and ensure annotations stay clean with `rebuild_objects`.\n+\n+```py\n+import albumentations as A\n+import numpy as np\n+from PIL import Image\n+\n+train_aug = A.Compose(\n+    [\n+        A.LongestMaxSize(max_size=1024, p=1.0),\n+        A.HorizontalFlip(p=0.5),\n+        A.Affine(rotate=(-5, 5), shear=(-5, 5), translate_percent=(0.05, 0.05), p=0.5),\n+    ],\n+    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_id\"], min_visibility=0.0),\n+)\n+\n+def train_transform(batch):\n+    imgs_out, objs_out = [], []\n+    original_imgs, original_objs = batch[\"image\"], batch[\"objects\"]\n+\n+    for i, (img_pil, objs) in enumerate(zip(original_imgs, original_objs)):\n+        img = np.array(img_pil)\n+        labels = [0] * len(objs[\"bbox\"])\n+\n+        out = train_aug(image=img, bboxes=list(objs[\"bbox\"]), category_id=labels)\n+\n+        if len(out[\"bboxes\"]) == 0:\n+            imgs_out.append(img_pil) # if no boxes left after augmentation, use original\n+            objs_out.append(objs)\n+            continue\n+\n+        H, W = out[\"image\"].shape[:2]\n+        clamped = []\n+        for (x, y, w, h) in out[\"bboxes\"]:\n+            x = max(0.0, min(x, W - 1.0))\n+            y = max(0.0, min(y, H - 1.0))\n+            w = max(1.0, min(w, W - x))\n+            h = max(1.0, min(h, H - y))\n+            clamped.append([x, y, w, h])\n+\n+        imgs_out.append(Image.fromarray(out[\"image\"]))\n+        objs_out.append(rebuild_objects(clamped, out[\"category_id\"]))\n+\n+    batch[\"image\"] = imgs_out\n+    batch[\"objects\"] = objs_out\n+    return batch\n+\n+\n+\n+def rebuild_objects(bboxes, labels):\n+    bboxes = [list(map(float, b)) for b in bboxes]\n+    areas  = [float(w*h) for (_, _, w, h) in bboxes]\n+    ids    = list(range(len(bboxes)))\n+    return {\n+        \"id\": ids,\n+        \"bbox\": bboxes,\n+        \"category_id\": list(map(int, labels)),\n+        \"area\": areas,\n+        \"iscrowd\": [0]*len(bboxes),\n+    }\n+\n+train_dataset = train_dataset.with_transform(train_transform)\n+```\n+\n+\n+Build COCO-style annotations for the image processor.\n+\n+```py\n+import torch\n+\n+def format_annotations(image, objects, image_id):\n+    n = len(objects[\"id\"])\n+    anns = []\n+    iscrowd_list = objects.get(\"iscrowd\", [0] * n)\n+    area_list = objects.get(\"area\", None)\n+\n+    for i in range(n):\n+        x, y, w, h = objects[\"bbox\"][i]\n+        area = area_list[i] if area_list is not None else float(w * h)\n+\n+        anns.append({\n+            \"id\": int(objects[\"id\"][i]),\n+            \"iscrowd\": int(iscrowd_list[i]),\n+            \"bbox\": [float(x), float(y), float(w), float(h)],\n+            \"category_id\": int(objects.get(\"category_id\", objects.get(\"category\"))[i]),\n+            \"area\": float(area),\n+        })\n+\n+    return {\"image_id\": int(image_id), \"annotations\": anns}\n+```\n+\n+Create batches in the data collator. Format annotations and pass them with transformed images to the image processor.\n+\n+```py\n+def collate_fn(examples):\n+    images = [example[\"image\"] for example in examples]\n+    ann_batch = [format_annotations(example[\"image\"], example[\"objects\"], example[\"image_id\"]) for example in examples]\n+\n+    inputs = image_processor(images=images, annotations=ann_batch, return_tensors=\"pt\")\n+    return inputs\n+```\n+\n+Initialize the [`Trainer`] and set up [`TrainingArguments`] for model convergence. Pass datasets, data collator, arguments, and model to `Trainer` to start training.\n+\n+```py\n+from transformers import Trainer, TrainingArguments\n+\n+training_args = TrainingArguments(\n+    output_dir=\"./license-plate-detr-dinov3\",\n+    per_device_train_batch_size=4,\n+    per_device_eval_batch_size=4,\n+    num_train_epochs=8,\n+    learning_rate=1e-5,\n+    weight_decay=1e-4,\n+    warmup_steps=500,\n+    eval_strategy=\"steps\",\n+    eval_steps=500,\n+    save_total_limit=2,\n+    dataloader_pin_memory=False,\n+    fp16=True,\n+    report_to=\"trackio\",\n+    load_best_model_at_end=True,\n+    remove_unused_columns=False,\n+    push_to_hub=True,\n+)\n+\n+trainer = Trainer(\n+    model=model,\n+    args=training_args,\n+    train_dataset=train_dataset,\n+    eval_dataset=val_dataset,\n+    data_collator=collate_fn,\n+)\n+\n+trainer.train()\n+```\n+\n+Push the trainer and image processor to the Hub.\n+\n+```py\n+trainer.push_to_hub()\n+image_processor.push_to_hub(\"merve/license-plate-detr-dinov3\")\n+```\n+\n+Test the model with an object detection pipeline.\n+\n+```py\n+from transformers import pipeline\n+\n+obj_detector = pipeline(\n+    \"object-detection\", model=\"merve/license-plate-detr-dinov3\"\n+)\n+results = obj_detector(\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/license-plates.jpg\", threshold=0.05)\n+print(results)\n+```\n+\n+Visualize the results.\n+\n+```py\n+from PIL import Image, ImageDraw\n+import numpy as np\n+import requests\n+\n+\n+def plot_results(image, results, threshold):\n+    image = Image.fromarray(np.uint8(image))\n+    draw = ImageDraw.Draw(image)\n+    width, height = image.size\n+\n+    for result in results:\n+        score = result[\"score\"]\n+        label = result[\"label\"]\n+        box = list(result[\"box\"].values())\n+\n+        if score > threshold:\n+            x1, y1, x2, y2 = tuple(box)\n+            draw.rectangle((x1, y1, x2, y2), outline=\"red\")\n+            draw.text((x1 + 5, y1 + 10), f\"{score:.2f}\", fill=\"green\" if score > 0.7 else \"red\")\n+\n+    return image\n+\n+image = Image.open(requests.get(\"https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/license-plates.jpg\", stream=True).raw)\n+plot_results(image, results, threshold=0.05)\n+```\n+\n+![Results](https://huggingface.co/datasets/huggingface/documentation-images/results/main/transformers/tasks/backbone_training_results.png)\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 252,
        "additions": 252,
        "deletions": 0
    }
}