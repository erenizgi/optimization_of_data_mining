{
    "author": "Cyrilvallez",
    "message": "Align gpt2 mask preparation to #37612 (#37787)\n\nUpdate modeling_gpt2.py",
    "sha": "ba3bd372537d993b24406eac7a6949938a486621",
    "files": [
        {
            "sha": "78a40f6d537d29631ec94bdce69e5b643564fdd6",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba3bd372537d993b24406eac7a6949938a486621/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba3bd372537d993b24406eac7a6949938a486621/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=ba3bd372537d993b24406eac7a6949938a486621",
            "patch": "@@ -1119,7 +1119,7 @@ def _update_causal_mask(\n             ):\n                 return None\n \n-        dtype, device = input_tensor.dtype, input_tensor.device\n+        dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1136,7 +1136,6 @@ def _update_causal_mask(\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n-            device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1161,7 +1160,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         sequence_length: int,\n         target_length: int,\n         dtype: torch.dtype,\n-        device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n         **kwargs,\n@@ -1181,8 +1179,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1194,11 +1190,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         else:\n             min_dtype = torch.finfo(dtype).min\n             causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n             )\n             if sequence_length != 1:\n                 causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 3,
        "deletions": 7
    }
}