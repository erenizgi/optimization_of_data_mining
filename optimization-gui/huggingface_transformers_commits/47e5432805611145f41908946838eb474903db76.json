{
    "author": "zucchini-nlp",
    "message": "Deprecate #36741 and map Causal to Conditional (#36917)\n\n* deprecate the prev fix\n\n* reword warning and update docs\n\n* reword warning\n\n* tests\n\n* dont bloat `get_text_config()`",
    "sha": "47e5432805611145f41908946838eb474903db76",
    "files": [
        {
            "sha": "4c7d978d3b7b9f92ed6c76ff7ca244eebeb4d1d9",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/47e5432805611145f41908946838eb474903db76/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/47e5432805611145f41908946838eb474903db76/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=47e5432805611145f41908946838eb474903db76",
            "patch": "@@ -204,7 +204,7 @@ visualizer(\"<img>What is shown in this image?\")\n     +   do_pan_and_scan=True,\n         ).to(\"cuda\")\n     ```\n-- For text-only inputs, use [`AutoModelForCausalLM`] instead to skip loading the vision components and save resources.\n+- For Gemma-3 1B checkpoint trained in text-only mode, use [`AutoModelForCausalLM`] instead.\n \n     ```py\n     import torch"
        },
        {
            "sha": "d3b9e04b2b8001ed9c7234e68c8fc5533932c09d",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/47e5432805611145f41908946838eb474903db76/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47e5432805611145f41908946838eb474903db76/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=47e5432805611145f41908946838eb474903db76",
            "patch": "@@ -1118,7 +1118,9 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n         Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n         itself. On specific composite models, it is under a set of valid names.\n \n-        If `decoder` is set to `True`, then only search for decoder config names.\n+        Args:\n+            decoder (`Optional[bool]`, *optional*, defaults to `False`):\n+                If set to `True`, then only search for decoder config names.\n         \"\"\"\n         decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n         encoder_possible_text_config_names = (\"text_encoder\",)\n@@ -1140,8 +1142,10 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n                 \"case, using `get_text_config()` would be ambiguous. Please specify the desied text config directly.\"\n             )\n         elif len(valid_text_config_names) == 1:\n-            return getattr(self, valid_text_config_names[0])\n-        return self\n+            config_to_return = getattr(self, valid_text_config_names[0])\n+        else:\n+            config_to_return = self\n+        return config_to_return\n \n \n def get_configuration_file(configuration_files: list[str]) -> str:"
        },
        {
            "sha": "2dcd1fa3efaf28fd112046457a5b588648c2dd4c",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/47e5432805611145f41908946838eb474903db76/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47e5432805611145f41908946838eb474903db76/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=47e5432805611145f41908946838eb474903db76",
            "patch": "@@ -522,7 +522,7 @@\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"gemma\", \"GemmaForCausalLM\"),\n         (\"gemma2\", \"Gemma2ForCausalLM\"),\n-        (\"gemma3\", \"Gemma3ForCausalLM\"),\n+        (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n         (\"gemma3_text\", \"Gemma3ForCausalLM\"),\n         (\"git\", \"GitForCausalLM\"),\n         (\"glm\", \"GlmForCausalLM\"),\n@@ -1671,7 +1671,20 @@ def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedC\n         Under the hood, multimodal models mapped by AutoModelForCausalLM assume the text decoder receives its own\n         config, rather than the config for the whole model. This is used e.g. to load the text-only part of a VLM.\n         \"\"\"\n-        return config.get_text_config(decoder=True)\n+        possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n+        text_config_names = []\n+        for text_config_name in possible_text_config_names:\n+            if hasattr(config, text_config_name):\n+                text_config_names += [text_config_name]\n+\n+        text_config = config.get_text_config(decoder=True)\n+        if text_config_names and type(text_config) in cls._model_mapping.keys():\n+            warnings.warn(\n+                \"Loading a multimodal model with `AutoModelForCausalLM` is deprecated and will be removed in v5. \"\n+                \"`AutoModelForCausalLM` will be used to load only the text-to-text generation module.\",\n+                FutureWarning,\n+            )\n+        return config\n \n \n AutoModelForCausalLM = auto_class_update(AutoModelForCausalLM, head_doc=\"causal language modeling\")"
        },
        {
            "sha": "7aba3454bd63b881e4dc12595599a927d4d869a5",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/47e5432805611145f41908946838eb474903db76/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47e5432805611145f41908946838eb474903db76/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=47e5432805611145f41908946838eb474903db76",
            "patch": "@@ -344,15 +344,15 @@ def test_flex_attention_with_grads(self):\n \n     def test_automodelforcausallm(self):\n         \"\"\"\n-        Regression test for #36741 -- make sure `AutoModelForCausalLM` works with a Gemma3 config, i.e. that\n+        Regression test for #36741/#36917 -- make sure `AutoModelForCausalLM` works with a Gemma3 config, i.e. that\n         `AutoModelForCausalLM.from_pretrained` pulls the text config before loading the model\n         \"\"\"\n         config = self.model_tester.get_config()\n         model = Gemma3ForConditionalGeneration(config)\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             model.save_pretrained(tmp_dir)\n             for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n-            self.assertIsInstance(for_causal_lm, Gemma3ForCausalLM)\n+            self.assertIsInstance(for_causal_lm, Gemma3ForConditionalGeneration)\n \n \n @slow"
        },
        {
            "sha": "916ba668f0138a0900cecd7c9d4e28062bbd6145",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/47e5432805611145f41908946838eb474903db76/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/47e5432805611145f41908946838eb474903db76/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=47e5432805611145f41908946838eb474903db76",
            "patch": "@@ -20,7 +20,7 @@\n import requests\n \n from transformers import PixtralProcessor\n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import require_read_token, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -35,6 +35,7 @@\n \n \n @require_vision\n+@require_read_token\n class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     \"\"\"This tests Pixtral processor with the new `spatial_merge_size` argument in Mistral3.\"\"\"\n "
        }
    ],
    "stats": {
        "total": 36,
        "additions": 27,
        "deletions": 9
    }
}