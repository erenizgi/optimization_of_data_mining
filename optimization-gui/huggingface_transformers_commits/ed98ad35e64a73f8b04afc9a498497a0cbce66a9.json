{
    "author": "pavelgein",
    "message": "Fix usage of unpad_input function (#35925)\n\nFix usage of unpad_function\n\nSee https://github.com/huggingface/transformers/issues/35899\n\nIn the [commit](https://github.com/Dao-AILab/flash-attention/commit/cdbbe844b1c0bcba3362e1f8c8af4d6f6d0bf300) return type of `unpad_input` was changed.\nNow the code support older and newer versions\n\nCo-authored-by: Pavel Gein <pavel.gein@gmail.com>",
    "sha": "ed98ad35e64a73f8b04afc9a498497a0cbce66a9",
    "files": [
        {
            "sha": "d4c7bec07902d107e4ddaa5669a1a83b34be7655",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed98ad35e64a73f8b04afc9a498497a0cbce66a9/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed98ad35e64a73f8b04afc9a498497a0cbce66a9/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=ed98ad35e64a73f8b04afc9a498497a0cbce66a9",
            "patch": "@@ -121,7 +121,7 @@ def _upad_input(\n     else:\n         # The -q_len: slice assumes left padding.\n         attention_mask = attention_mask[:, -query_length:]\n-        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n+        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q, *_ = unpad_input(query_layer, attention_mask)\n \n     return (\n         query_layer,"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}