{
    "author": "Cyrilvallez",
    "message": "Fix doc formatting in forward passes & modular (#36243)\n\n* fix indentation issues + modular without magic keyword\n\n* style\n\n* Update doc.py\n\n* style\n\n* Fix all decorators indentation\n\n* all models\n\n* style\n\n* style\n\n* Update doc.py\n\n* fix\n\n* general fix\n\n* style",
    "sha": "da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
    "files": [
        {
            "sha": "ea2e1a2b9a106229209d7edecfbad5373a07b008",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -349,7 +349,6 @@ def forward(\n         num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "d08ecfab7e6fcadaaee16587865b8bf384faacc6",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1193,7 +1193,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1458,7 +1457,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`)."
        },
        {
            "sha": "c62c074218dd6cb5900a314a466280732136c08d",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1437,7 +1437,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`)."
        },
        {
            "sha": "95f9b1a0a7a25f558a8bf9354e27f42834e3b138",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1495,7 +1495,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "755552036fc4ecacd5e99e9af1bf3fd499010a04",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1205,7 +1205,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "ecc954f5e6ed9731dd692234e394d09883300336",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1554,7 +1554,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "12b1d740bd797763221c676d40ac40e1b5cf8a7c",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -833,7 +833,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "a39489a346dfa23340210e32bd2e0ac24caaf9ef",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -321,7 +321,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "93763b2cab3b6ce231d1523081e5637a4ab656ee",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -834,7 +834,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "71484691c25692d9d4856af70e82a9e288acefbc",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1283,9 +1283,7 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n-        r\"\"\"Forward function for causal language modeling.\n-\n-        Args:\n+        r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "2a74517d9f13345a7a715fb9c7bcd8babea3326e",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -716,7 +716,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "4182e1a20325919863fa63cf69792eff98a455b9",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1070,7 +1070,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "a1e740306724a527153acc21a60f1d0a50dd6202",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1650,7 +1650,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1878,7 +1877,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "792d482681273285fffaa639c1ba5945be1a7216",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1077,7 +1077,6 @@ def __init__(self, config):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward(**super_kwargs):\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1186,7 +1185,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "afef380494da72541791137714fe968e4a374a69",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -803,7 +803,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "564576be76077784022e5933f557b5be98585c8d",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -483,7 +483,6 @@ def forward(\n class GemmaForCausalLM(LlamaForCausalLM):\n     def forward(**super_kwargs):\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "6ac249bfce02b9c91ebfbe859ee89bad017f8951",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -841,7 +841,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -859,9 +858,9 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from transformers import AutoTokenizer, GemmaForCausalLM\n+        >>> from transformers import AutoTokenizer, Gemma2ForCausalLM\n \n-        >>> model = GemmaForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n+        >>> model = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n \n         >>> prompt = \"What is your favorite condiment?\""
        },
        {
            "sha": "cd3ae3ed0efc990aa30a0d753a42a5c55d674eb0",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -591,10 +591,26 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+\n+        Example:\n+\n         ```python\n-        >>> from transformers import AutoTokenizer, GemmaForCausalLM\n+        >>> from transformers import AutoTokenizer, Gemma2ForCausalLM\n \n-        >>> model = GemmaForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n+        >>> model = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n \n         >>> prompt = \"What is your favorite condiment?\""
        },
        {
            "sha": "858c03ec214920b42e0ba6d22bca1d1be0344f40",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -812,7 +812,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "7fbb0d39ef4bc14241fe5f47e5d082eed9cef652",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -769,7 +769,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "e8b0d770d3bc9fd25833c4ba8d2542c0c7079e4d",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -848,7 +848,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "2a553e04cc041827f43728cffac1c0ce9801f0c7",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -815,7 +815,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "4a0ab379ffbf042c40471eca49c2c53c110e0409",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1287,7 +1287,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "7d4336a9dc9abc77f71252c862ec6c3a891cba18",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1313,7 +1313,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "d2a081efe77cc364e9e04f2ca9963528fc593afc",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -799,7 +799,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "4cd4ced761b47174dfb01e246f8446916d99e6bc",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1559,7 +1559,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, IdeficsCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "8ca859f8cf36bad7fd5044be14acb9e5fcd253ca",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1687,7 +1687,6 @@ def call(\n         training=False,\n     ) -> Union[TFIdeficsCausalLMOutputWithPast, Tuple[tf.Tensor]]:\n         r\"\"\"\n-        Args:\n             labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "872ba10a417611eab42489cf014f98035dacb8b0",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1537,7 +1537,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics2CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics2ForConditionalGeneration`)."
        },
        {
            "sha": "7e9e33c218ab44221742a7387c4df1b84dcf0432",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1121,7 +1121,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics3CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`)."
        },
        {
            "sha": "2e11d2f7be4cc990e6e2a9867d29ac673a1d8311",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1456,7 +1456,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "9cb66828d55d10704b812c06ba8aebee408f5b19",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1299,7 +1299,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "2a9117025776226e02d0c286bf71db67380a6ac1",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -801,7 +801,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "b20ecf2ca930398fd0b1f6922a5231660f120478",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -348,7 +348,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "1b0b4b93c83779773d6e7dfe8ed1546737260c23",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -561,7 +561,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "6d86d9c4d42627fbc29dd2467647f5457124f9c6",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -601,7 +601,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n                 The tensors corresponding to the input videos. Pixel values can be obtained using\n                 [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses"
        },
        {
            "sha": "804f6f58355b5ff162d26e5b16493e6c189acb6a",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -360,7 +360,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n                 The tensors corresponding to the input videos. Pixel values can be obtained using\n                 [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses"
        },
        {
            "sha": "5d41f8489e9058092b32db55917d610b9428665e",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -623,7 +623,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "50500fa5857bfb897211bbe8feadd74d36e08e3b",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -802,7 +802,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "53ae7cec7e4ccbda90c561cc9a0c7653a262623b",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -849,11 +849,10 @@ def call(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, TFCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n-                or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n+            or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n@@ -975,11 +974,10 @@ def call(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, TFSequenceClassifierOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n \n         transformer_outputs = self.model("
        },
        {
            "sha": "367ba34b80fc67a1cdb6c78eb7b0b1956aad422e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1022,7 +1022,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "b32a8d7987b2b3b1bb8103e90b997ac174ed3cab",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -480,7 +480,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "923e62434873ecdb4bad330fcd1a30498c64382c",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1901,7 +1901,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -2048,7 +2047,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "f7d712cd8b98d2b13276b2714d131e55f3a3bcf0",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1813,7 +1813,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoshiCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "7c39af637c7c45aad1846a39dad1841c4d6f0280",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1047,7 +1047,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "f48cf3d89dbbe8cfbad8aa47fab8e685d1541a40",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -777,7 +777,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "b26f55626e2d3b9a1dee7fee03aedb93ed5d9375",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -778,7 +778,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "ae830dc5a5581f763df7ae26628675054408ebd2",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1206,7 +1206,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "2bf456047d9f04c956171b94fb0c07983bcddb43",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -438,7 +438,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "7ccd4c2ba06c2874df43b429ec101f09a40f7999",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -852,7 +852,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "448f21260504b709c5114057ea5cea2b57b80928",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -775,7 +775,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "c140af1f3e4ed7f2498f0d014615dfc933054ada",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -877,7 +877,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "3f17690d6a465043e28a4ede305d4ac9849fc8b7",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1388,7 +1388,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "4ae09fbb70d80b4dc51c06b7ad59698fb3bb8281",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -815,7 +815,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "ef610b2251227968f011ab9becdd076d1c0cb3d4",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1742,7 +1742,6 @@ def forward(\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "d12a59926db9ad810c87e4bddd20c9b668f91d79",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -608,7 +608,6 @@ def forward(\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "a6c87e9950ee873d807e328094d6d9d2267cec84",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1112,7 +1112,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Qwen2AudioCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "9b3308914dbaa6e9d2887ddd976b5b83d1c23cff",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1272,7 +1272,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "9648de7298a3c4463c00a965aa0e58b0d492c9ea",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1619,7 +1619,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "4ae3de5a83111b080e494b2a10eec0293f36d93e",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -821,7 +821,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutput]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "042c950b094e265f7bfd198375d19567fd12dc00",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1109,7 +1109,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "e55e855b698551958202217e5812f9211699f8ae",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -798,7 +798,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "19170049b6376e5c9d7d105c1e83dbc8c6bcc823",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -383,7 +383,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, VideoLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "6216ef88dacf439566465a1b28b5e7981f8fa06c",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -323,7 +323,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, VipLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "7a728f4f9b1ee53b2c2c158c6c090612c4e33de0",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1228,7 +1228,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "ab03da6a1a8a5decba082526f3918f65bddb917b",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -1665,7 +1665,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "f01ffc28442cd5a5dcda3ea55d3ea08eb249b54f",
            "filename": "src/transformers/utils/doc.py",
            "status": "modified",
            "additions": 32,
            "deletions": 4,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Futils%2Fdoc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/src%2Ftransformers%2Futils%2Fdoc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdoc.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -16,10 +16,23 @@\n \"\"\"\n \n import functools\n+import inspect\n import re\n+import textwrap\n import types\n \n \n+def get_docstring_indentation_level(func):\n+    \"\"\"Return the indentation level of the start of the docstring of a class or function (or method).\"\"\"\n+    # We assume classes are always defined in the global scope\n+    if inspect.isclass(func):\n+        return 4\n+    source = inspect.getsource(func)\n+    first_line = source.splitlines()[0]\n+    function_def_level = len(first_line) - len(first_line.lstrip())\n+    return 4 + function_def_level\n+\n+\n def add_start_docstrings(*docstr):\n     def docstring_decorator(fn):\n         fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n@@ -30,10 +43,8 @@ def docstring_decorator(fn):\n \n def add_start_docstrings_to_model_forward(*docstr):\n     def docstring_decorator(fn):\n-        docstring = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n         class_name = f\"[`{fn.__qualname__.split('.')[0]}`]\"\n-        intro = f\"   The {class_name} forward method, overrides the `__call__` special method.\"\n-        note = r\"\"\"\n+        intro = rf\"\"\"    The {class_name} forward method, overrides the `__call__` special method.\n \n     <Tip>\n \n@@ -44,7 +55,23 @@ def docstring_decorator(fn):\n     </Tip>\n \"\"\"\n \n-        fn.__doc__ = intro + note + docstring\n+        correct_indentation = get_docstring_indentation_level(fn)\n+        current_doc = fn.__doc__ if fn.__doc__ is not None else \"\"\n+        try:\n+            first_non_empty = next(line for line in current_doc.splitlines() if line.strip() != \"\")\n+            doc_indentation = len(first_non_empty) - len(first_non_empty.lstrip())\n+        except StopIteration:\n+            doc_indentation = correct_indentation\n+\n+        docs = docstr\n+        # In this case, the correct indentation level (class method, 2 Python levels) was respected, and we should\n+        # correctly reindent everything. Otherwise, the doc uses a single indentation level\n+        if doc_indentation == 4 + correct_indentation:\n+            docs = [textwrap.indent(textwrap.dedent(doc), \" \" * correct_indentation) for doc in docstr]\n+            intro = textwrap.indent(textwrap.dedent(intro), \" \" * correct_indentation)\n+\n+        docstring = \"\".join(docs) + current_doc\n+        fn.__doc__ = intro + docstring\n         return fn\n \n     return docstring_decorator\n@@ -1153,6 +1180,7 @@ def docstring_decorator(fn):\n             built_doc = built_doc.replace(\n                 f'from_pretrained(\"{checkpoint}\")', f'from_pretrained(\"{checkpoint}\", revision=\"{revision}\")'\n             )\n+\n         fn.__doc__ = func_doc + output_doc + built_doc\n         return fn\n "
        },
        {
            "sha": "728c5628b1f30a8df498274e28fd282ab18b3edb",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 21,
            "deletions": 2,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da4ab2a1b66e2367f94ea34438d344dd53e2d66e/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
            "patch": "@@ -253,10 +253,29 @@ def get_docstring_indent(docstring):\n     return 0\n \n \n+def is_full_docstring(new_docstring: str) -> bool:\n+    \"\"\"Check if `new_docstring` is a full docstring, or if it is only part of a docstring that should then\n+    be merged with the existing old one.\n+    \"\"\"\n+    # libcst returns the docstrinbgs with litteral `r\"\"\"` quotes in front\n+    new_docstring = new_docstring.split('\"\"\"', 1)[1]\n+    # The docstring contains Args definition, so it is self-contained\n+    if re.search(r\"\\n\\s*Args:\\n\", new_docstring):\n+        return True\n+    # If it contains Returns, but starts with text indented with an additional 4 spaces before, it is self-contained\n+    # (this is the scenario when using `@add_start_docstrings_to_model_forward`, but adding more args to docstring)\n+    match_object = re.search(r\"\\n([^\\S\\n]*)Returns:\\n\", new_docstring)\n+    if match_object is not None:\n+        full_indent = match_object.group(1)\n+        striped_doc = new_docstring.strip(\"\\n\")\n+        if striped_doc.startswith(full_indent + \" \" * 4) or striped_doc.startswith(full_indent + \"\\t\"):\n+            return True\n+    return False\n+\n+\n def merge_docstrings(original_docstring, updated_docstring):\n-    # indent_level = get_docstring_indent(updated_docstring)\n     original_level = get_docstring_indent(original_docstring)\n-    if not re.findall(r\"\\n\\s*Args:\\n\", updated_docstring):\n+    if not is_full_docstring(updated_docstring):\n         # Split the docstring at the example section, assuming `\"\"\"` is used to define the docstring\n         parts = original_docstring.split(\"```\")\n         if \"```\" in updated_docstring and len(parts) > 1:"
        }
    ],
    "stats": {
        "total": 171,
        "additions": 82,
        "deletions": 89
    }
}