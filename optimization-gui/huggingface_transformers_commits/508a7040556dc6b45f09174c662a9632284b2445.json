{
    "author": "Rocketknight1",
    "message": "No more Tuple, List, Dict (#38797)\n\n* No more Tuple, List, Dict\n\n* make fixup\n\n* More style fixes\n\n* Docstring fixes with regex replacement\n\n* Trigger tests\n\n* Redo fixes after rebase\n\n* Fix copies\n\n* [test all]\n\n* update\n\n* [test all]\n\n* update\n\n* [test all]\n\n* make style after rebase\n\n* Patch the hf_argparser test\n\n* Patch the hf_argparser test\n\n* style fixes\n\n* style fixes\n\n* style fixes\n\n* Fix docstrings in Cohere test\n\n* [test all]\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "508a7040556dc6b45f09174c662a9632284b2445",
    "files": [
        {
            "sha": "add1d704172b9c0ef17d075e23a637f1f4b7bbe8",
            "filename": "benchmark/benchmarks_entrypoint.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/benchmark%2Fbenchmarks_entrypoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/benchmark%2Fbenchmarks_entrypoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenchmarks_entrypoint.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -28,7 +28,7 @@ def __init__(\n         self.commit_id = commit_id\n         self.commit_msg = commit_msg\n \n-    def initialise_benchmark(self, metadata: Dict[str, str]) -> int:\n+    def initialise_benchmark(self, metadata: dict[str, str]) -> int:\n         \"\"\"\n         Creates a new benchmark, returns the benchmark id\n         \"\"\"\n@@ -55,7 +55,7 @@ def collect_device_measurements(self, benchmark_id: int, cpu_util, mem_megabytes\n             f\"inserted device measurements for benchmark #{benchmark_id} [CPU util: {cpu_util}, mem MBs: {mem_megabytes}, GPU util: {gpu_util}, GPU mem MBs: {gpu_mem_megabytes}]\"\n         )\n \n-    def collect_model_measurements(self, benchmark_id: int, measurements: Dict[str, float]):\n+    def collect_model_measurements(self, benchmark_id: int, measurements: dict[str, float]):\n         with self.conn.cursor() as cur:\n             cur.execute(\n                 \"\"\"\n@@ -85,7 +85,7 @@ def close(self):\n logger.addHandler(handler)\n \n \n-def parse_arguments() -> Tuple[str, str, str, str]:\n+def parse_arguments() -> tuple[str, str, str, str]:\n     \"\"\"\n     Parse command line arguments for the benchmarking CLI.\n     \"\"\""
        },
        {
            "sha": "c295392210e3a68f506fe3c98465d6af5778ba9a",
            "filename": "docs/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2FREADME.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -278,7 +278,7 @@ Here's an example of a single value return:\n \n ```python\n     Returns:\n-        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.\n+        `list[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.\n ```\n \n Here's an example of a tuple return, comprising several objects:"
        },
        {
            "sha": "26956af811f51ce7877ba28675d4e59aac9e634e",
            "filename": "docs/source/ar/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Far%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Far%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -30,7 +30,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "a9d4109bd505e3c02b1b928ad7c25b109014c839",
            "filename": "docs/source/en/add_new_model.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_model.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -571,7 +571,7 @@ The processor should call the appropriate modality-specific processors within it\n def __call__(\n     self,\n     images: ImageInput = None,\n-    text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+    text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n     audio=None,\n     videos=None,\n     **kwargs: Unpack[YourModelProcessorKwargs],"
        },
        {
            "sha": "d78e21413e0e20dc0bd75cbfdc063f40eaa5015a",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -92,7 +92,7 @@ def custom_attention(\n     a_new_kwargs = None,  # You can now add as many kwargs as you need\n     another_new_kwargs = None,  # You can now add as many kwargs as you need\n     **kwargs,  # You need to accept **kwargs as models will pass other args\n-) -> Tuple[torch.Tensor, Optional[torch.Tensor]]\n+) -> tuple[torch.Tensor, Optional[torch.Tensor]]\n     ...  # do your magic!\n     return attn_output, attn_weights  # attn_weights are optional here\n "
        },
        {
            "sha": "a6f9d1238e00044410a51f403f1562db90f3ab7e",
            "filename": "docs/source/en/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -47,7 +47,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "68de9169986ec84972f2c62ab4a38f123eaeea8b",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -152,7 +152,7 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n | `temperature` | `float` | How unpredictable the next selected token will be. High values (`>0.8`) are good for creative tasks, low values (e.g. `<0.4`) for tasks that require \"thinking\". Requires `do_sample=True`. |\n | `num_beams` | `int` | When set to `>1`, activates the beam search algorithm. Beam search is good on input-grounded tasks. Check [this guide](./generation_strategies.md) for more information. |\n | `repetition_penalty` | `float` | Set it to `>1.0` if you're seeing the model repeat itself often. Larger values apply a larger penalty. |\n-| `eos_token_id` | `List[int]` | The token(s) that will cause generation to stop. The default value is usually good, but you can specify a different token. |\n+| `eos_token_id` | `list[int]` | The token(s) that will cause generation to stop. The default value is usually good, but you can specify a different token. |\n \n \n ## Pitfalls"
        },
        {
            "sha": "9ef37e8ea79a9aedd2d96e744e971ec5730d0065",
            "filename": "docs/source/en/model_doc/bros.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -62,11 +62,11 @@ def make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):\n \n     box_first_token_mask = np.zeros(max_seq_length, dtype=np.bool_)\n \n-    # encode(tokenize) each word from words (List[str])\n-    input_ids_list: List[List[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]\n+    # encode(tokenize) each word from words (list[str])\n+    input_ids_list: list[list[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]\n \n     # get the length of each box\n-    tokens_length_list: List[int] = [len(l) for l in input_ids_list]\n+    tokens_length_list: list[int] = [len(l) for l in input_ids_list]\n \n     box_end_token_indices = np.array(list(itertools.accumulate(tokens_length_list)))\n     box_start_token_indices = box_end_token_indices - np.array(tokens_length_list)"
        },
        {
            "sha": "aa942b231944b53727fc326d5db0339ead5572fd",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -149,7 +149,7 @@ As a summary, consider the following table:\n | **Description** | Predicting bounding boxes and class labels around objects in an image | Predicting masks around objects (i.e. instances) in an image | Predicting masks around both objects (i.e. instances) as well as \"stuff\" (i.e. background things like trees and roads) in an image |\n | **Model** | [`~transformers.DetrForObjectDetection`] | [`~transformers.DetrForSegmentation`] | [`~transformers.DetrForSegmentation`] |\n | **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO panoptic  |                                                                        |\n-| **Format of annotations to provide to**  [`~transformers.DetrImageProcessor`] | {'image_id': `int`, 'annotations': `List[Dict]`} each Dict being a COCO object annotation  | {'image_id': `int`, 'annotations': `List[Dict]`}  (in case of COCO detection) or {'file_name': `str`, 'image_id': `int`, 'segments_info': `List[Dict]`} (in case of COCO panoptic) | {'file_name': `str`, 'image_id': `int`, 'segments_info': `List[Dict]`} and masks_path (path to directory containing PNG files of the masks) |\n+| **Format of annotations to provide to**  [`~transformers.DetrImageProcessor`] | {'image_id': `int`, 'annotations': `list[Dict]`} each Dict being a COCO object annotation  | {'image_id': `int`, 'annotations': `list[Dict]`}  (in case of COCO detection) or {'file_name': `str`, 'image_id': `int`, 'segments_info': `list[Dict]`} (in case of COCO panoptic) | {'file_name': `str`, 'image_id': `int`, 'segments_info': `list[Dict]`} and masks_path (path to directory containing PNG files of the masks) |\n | **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformers.DetrImageProcessor.post_process`] | [`~transformers.DetrImageProcessor.post_process_segmentation`] | [`~transformers.DetrImageProcessor.post_process_segmentation`], [`~transformers.DetrImageProcessor.post_process_panoptic`] |\n | **evaluators** | `CocoEvaluator` with `iou_types=\"bbox\"` | `CocoEvaluator` with `iou_types=\"bbox\"` or `\"segm\"` | `CocoEvaluator` with `iou_tupes=\"bbox\"` or `\"segm\"`, `PanopticEvaluator` |\n "
        },
        {
            "sha": "b2051a91f2d2494809b55ff5d11d9ba8e9a510dd",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -83,7 +83,7 @@ def read_video_pyav(container, indices):\n     Decode the video with PyAV decoder.\n     Args:\n         container (`av.container.input.InputContainer`): PyAV container.\n-        indices (`List[int]`): List of frame indices to decode.\n+        indices (`list[int]`): List of frame indices to decode.\n     Returns:\n         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n     '''"
        },
        {
            "sha": "1cd8f0cbb1bb0da4d08858ca9372a0e145b67306",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -216,12 +216,12 @@ class Olmo2Attention(OlmoAttention):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -294,9 +294,9 @@ class Olmo2DecoderLayer(OlmoDecoderLayer):\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n \n         # Self Attention\n@@ -494,7 +494,7 @@ class LlamaForCausalLM(nn.Module):\n       input_ids: torch.LongTensor = None,\n       attention_mask: Optional[torch.Tensor] = None,\n       position_ids: Optional[torch.LongTensor] = None,\n-      past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+      past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n       inputs_embeds: Optional[torch.FloatTensor] = None,\n       labels: Optional[torch.LongTensor] = None,\n       use_cache: Optional[bool] = None,\n@@ -520,7 +520,7 @@ class NewModelForCausalLM(LlamaForCausalLM):    |    class LlamaForCausalLM(nn.M\n                                                 |         input_ids: torch.LongTensor = None,\n                                                 |         attention_mask: Optional[torch.Tensor] = None,\n                                                 |         position_ids: Optional[torch.LongTensor] = None,\n-                                                |         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = |None,\n+                                                |         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = |None,\n                                                 |         inputs_embeds: Optional[torch.FloatTensor] = None,\n                                                 |         labels: Optional[torch.LongTensor] = None,\n                                                 |         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "760f701cf5cc2d5dc29b55e6041d3efa96adb9cd",
            "filename": "docs/source/en/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fasr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -170,7 +170,7 @@ Unlike other data collators, this specific data collator needs to apply a differ\n ...     processor: AutoProcessor\n ...     padding: Union[bool, str] = \"longest\"\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         # split inputs and labels since they have to be of different lengths and need\n ...         # different padding methods\n ...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]"
        },
        {
            "sha": "61db61b7db2dbd670f42acef761087b74cc4e4f4",
            "filename": "docs/source/en/tasks/object_detection.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -243,7 +243,7 @@ and it uses the exact same dataset as an example. Apply some geometric and color\n ... )\n ```\n \n-The `image_processor` expects the annotations to be in the following format: `{'image_id': int, 'annotations': List[Dict]}`,\n+The `image_processor` expects the annotations to be in the following format: `{'image_id': int, 'annotations': list[Dict]}`,\n  where each dictionary is a COCO object annotation. Let's add a function to reformat annotations for a single example:\n \n ```py\n@@ -252,9 +252,9 @@ The `image_processor` expects the annotations to be in the following format: `{'\n \n ...     Args:\n ...         image_id (str): image id. e.g. \"0001\"\n-...         categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n-...         areas (List[float]): list of corresponding areas to provided bounding boxes\n-...         bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n+...         categories (list[int]): list of categories/class labels corresponding to provided bounding boxes\n+...         areas (list[float]): list of corresponding areas to provided bounding boxes\n+...         bboxes (list[tuple[float]]): list of bounding boxes provided in COCO format\n ...             ([center_x, center_y, width, height] in absolute coordinates)\n \n ...     Returns:\n@@ -397,7 +397,7 @@ Intermediate format of boxes used for training is `YOLO` (normalized) but we wil\n \n ...     Args:\n ...         boxes (torch.Tensor): Bounding boxes in YOLO format\n-...         image_size (Tuple[int, int]): Image size in format (height, width)\n+...         image_size (tuple[int, int]): Image size in format (height, width)\n \n ...     Returns:\n ...         torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)"
        },
        {
            "sha": "6715e7b7c6bd3595fd7970bb68babbe4ae784eba",
            "filename": "docs/source/en/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -408,7 +408,7 @@ instructs the model to ignore that part of the spectrogram when calculating the\n ... class TTSDataCollatorWithPadding:\n ...     processor: Any\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n ...         label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n ...         speaker_features = [feature[\"speaker_embeddings\"] for feature in features]"
        },
        {
            "sha": "7e00505b8df6a18132d6495fada5ace506325b80",
            "filename": "docs/source/es/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fes%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fes%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -48,7 +48,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "d5bb614e70dabf78a39637538f73a7e3445d935b",
            "filename": "docs/source/es/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fes%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fes%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fasr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -166,7 +166,7 @@ A diferencia de otros collators de datos, este tiene que aplicarle un método de\n ...     processor: AutoProcessor\n ...     padding: Union[bool, str] = \"longest\"\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         # particiona las entradas y las etiquetas ya que tienen que tener longitudes distintas y\n ...         # requieren métodos de padding diferentes\n ...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]"
        },
        {
            "sha": "a564ea606c75430cb163fba2456a7b7dc2e165e6",
            "filename": "docs/source/it/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fit%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fit%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -47,7 +47,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "cd0e70b1f48464b661d6a5c6003687be5714db4b",
            "filename": "docs/source/ja/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -39,7 +39,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "ff82d7138b033ae5fb434cef6e1ef64b37f1ddd9",
            "filename": "docs/source/ja/hpo_train.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fhpo_train.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -56,7 +56,7 @@ Optunaに関しては、[object_parameter](https://optuna.readthedocs.io/en/stab\n ...     }\n ```\n \n-Optunaは、多目的のハイパーパラメータ最適化（HPO）を提供しています。 `hyperparameter_search` で `direction` を渡し、複数の目的関数値を返すための独自の `compute_objective` を定義することができます。 Pareto Front（`List[BestRun]`）は `hyperparameter_search` で返され、[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py) のテストケース `TrainerHyperParameterMultiObjectOptunaIntegrationTest` を参照する必要があります。これは以下のようになります。\n+Optunaは、多目的のハイパーパラメータ最適化（HPO）を提供しています。 `hyperparameter_search` で `direction` を渡し、複数の目的関数値を返すための独自の `compute_objective` を定義することができます。 Pareto Front（`list[BestRun]`）は `hyperparameter_search` で返され、[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py) のテストケース `TrainerHyperParameterMultiObjectOptunaIntegrationTest` を参照する必要があります。これは以下のようになります。\n \n \n ```py"
        },
        {
            "sha": "58b59e2524610d6c03ba7fbb042c270ae686e07a",
            "filename": "docs/source/ja/model_doc/bros.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fmodel_doc%2Fbros.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fmodel_doc%2Fbros.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbros.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -57,11 +57,11 @@ def make_box_first_token_mask(bboxes, words, tokenizer, max_seq_length=512):\n \n     box_first_token_mask = np.zeros(max_seq_length, dtype=np.bool_)\n \n-    # encode(tokenize) each word from words (List[str])\n-    input_ids_list: List[List[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]\n+    # encode(tokenize) each word from words (list[str])\n+    input_ids_list: list[list[int]] = [tokenizer.encode(e, add_special_tokens=False) for e in words]\n \n     # get the length of each box\n-    tokens_length_list: List[int] = [len(l) for l in input_ids_list]\n+    tokens_length_list: list[int] = [len(l) for l in input_ids_list]\n \n     box_end_token_indices = np.array(list(itertools.accumulate(tokens_length_list)))\n     box_start_token_indices = box_end_token_indices - np.array(tokens_length_list)"
        },
        {
            "sha": "bef1a8fce770f725b3022cc43e37f38b25255349",
            "filename": "docs/source/ja/model_doc/detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -149,7 +149,7 @@ DETR モデルをインスタンス化するには 3 つの方法があります\n | **Description** |画像内のオブジェクトの周囲の境界ボックスとクラス ラベルを予測する | 画像内のオブジェクト (つまりインスタンス) の周囲のマスクを予測する | 画像内のオブジェクト (インスタンス) と「もの」 (木や道路などの背景) の両方の周囲のマスクを予測します |\n | **Model** | [`~transformers.DetrForObjectDetection`] | [`~transformers.DetrForSegmentation`] | [`~transformers.DetrForSegmentation`] |\n | **Example dataset** | COCO detection | COCO detection, COCO panoptic | COCO panoptic  |                                                                        |\n-| **Format of annotations to provide to**  [`~transformers.DetrImageProcessor`] | {'image_id': `int`, 'annotations': `List[Dict]`} each Dict being a COCO object annotation  | {'image_id': `int`, 'annotations': `List[Dict]`}  (in case of COCO detection) or {'file_name': `str`, 'image_id': `int`, 'segments_info': `List[Dict]`} (in case of COCO panoptic) | {'file_name': `str`, 'image_id': `int`, 'segments_info': `List[Dict]`} and masks_path (path to directory containing PNG files of the masks) |\n+| **Format of annotations to provide to**  [`~transformers.DetrImageProcessor`] | {'image_id': `int`, 'annotations': `list[Dict]`} each Dict being a COCO object annotation  | {'image_id': `int`, 'annotations': `list[Dict]`}  (in case of COCO detection) or {'file_name': `str`, 'image_id': `int`, 'segments_info': `list[Dict]`} (in case of COCO panoptic) | {'file_name': `str`, 'image_id': `int`, 'segments_info': `list[Dict]`} and masks_path (path to directory containing PNG files of the masks) |\n | **Postprocessing** (i.e. converting the output of the model to Pascal VOC format) | [`~transformers.DetrImageProcessor.post_process`] | [`~transformers.DetrImageProcessor.post_process_segmentation`] | [`~transformers.DetrImageProcessor.post_process_segmentation`], [`~transformers.DetrImageProcessor.post_process_panoptic`] |\n | **evaluators** | `CocoEvaluator` with `iou_types=\"bbox\"` | `CocoEvaluator` with `iou_types=\"bbox\"` or `\"segm\"` | `CocoEvaluator` with `iou_tupes=\"bbox\"` or `\"segm\"`, `PanopticEvaluator` |\n "
        },
        {
            "sha": "5e460a102f440899ef3bbb29e744b5087b2d7946",
            "filename": "docs/source/ja/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fasr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -170,7 +170,7 @@ MInDS-14 データセットのサンプリング レートは 8000kHz です (\n ...     processor: AutoProcessor\n ...     padding: Union[bool, str] = \"longest\"\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         # split inputs and labels since they have to be of different lengths and need\n ...         # different padding methods\n ...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]"
        },
        {
            "sha": "1b6c03ee1032bdfd6fe08f4992642229c05b6add",
            "filename": "docs/source/ja/tasks/object_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fobject_detection.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -208,7 +208,7 @@ DETR モデルをトレーニングできる「ラベル」。画像プロセッ\n ... )\n ```\n \n-`image_processor` は、注釈が次の形式であることを期待します: `{'image_id': int, 'annotations': List[Dict]}`,\n+`image_processor` は、注釈が次の形式であることを期待します: `{'image_id': int, 'annotations': list[Dict]}`,\n  ここで、各辞書は COCO オブジェクトの注釈です。 1 つの例として、注釈を再フォーマットする関数を追加してみましょう。\n \n  ```py"
        },
        {
            "sha": "1c22dfd71a7b6a2c139f55dbd442a1c86762484f",
            "filename": "docs/source/ja/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fja%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Ftext-to-speech.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -408,7 +408,7 @@ Y 軸が反転され、スペクトログラムが上下逆に表示されます\n ... class TTSDataCollatorWithPadding:\n ...     processor: Any\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n ...         label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n ...         speaker_features = [feature[\"speaker_embeddings\"] for feature in features]"
        },
        {
            "sha": "1a230a04b2838079945e18ef6cb29de71d7299f3",
            "filename": "docs/source/ko/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fko%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fko%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -46,7 +46,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "6c8ad6fc320123861935d232592c18fefc71094f",
            "filename": "docs/source/ko/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fko%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fko%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fasr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -172,7 +172,7 @@ MInDS-14 데이터 세트의 샘플링 레이트는 8000kHz이므로([데이터\n ...     processor: AutoProcessor\n ...     padding: Union[bool, str] = \"longest\"\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         # 입력과 레이블을 분할합니다\n ...         # 길이가 다르고, 각각 다른 패딩 방법을 사용해야 하기 때문입니다\n ...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]"
        },
        {
            "sha": "75319d93c24e1fcd35936a7ca0eb145c8d7e78df",
            "filename": "docs/source/ko/tasks/object_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fko%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fko%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fobject_detection.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -201,7 +201,7 @@ DatasetDict({\n ... )\n ```\n \n-이미지 프로세서는 어노테이션이 다음과 같은 형식일 것으로 예상합니다: `{'image_id': int, 'annotations': List[Dict]}`, 여기서 각 딕셔너리는 COCO 객체 어노테이션입니다. 단일 예제에 대해 어노테이션의 형식을 다시 지정하는 함수를 추가해 보겠습니다:\n+이미지 프로세서는 어노테이션이 다음과 같은 형식일 것으로 예상합니다: `{'image_id': int, 'annotations': list[Dict]}`, 여기서 각 딕셔너리는 COCO 객체 어노테이션입니다. 단일 예제에 대해 어노테이션의 형식을 다시 지정하는 함수를 추가해 보겠습니다:\n \n ```py\n >>> def formatted_anns(image_id, category, area, bbox):"
        },
        {
            "sha": "75376ff6e50f710c56b23a8dd20451878c34b706",
            "filename": "docs/source/pt/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fpt%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fpt%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -47,7 +47,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "a96f0f545dff46a9ef639f41bb88aa41933ecb21",
            "filename": "docs/source/zh/custom_models.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fzh%2Fcustom_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fzh%2Fcustom_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcustom_models.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -39,7 +39,7 @@ class ResnetConfig(PretrainedConfig):\n     def __init__(\n         self,\n         block_type=\"bottleneck\",\n-        layers: List[int] = [3, 4, 6, 3],\n+        layers: list[int] = [3, 4, 6, 3],\n         num_classes: int = 1000,\n         input_channels: int = 3,\n         cardinality: int = 1,"
        },
        {
            "sha": "9bc04109ac0c3c427c5dd7f824b3190dcb533eea",
            "filename": "docs/source/zh/hpo_train.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fhpo_train.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -56,7 +56,7 @@ pip install optuna/sigopt/wandb/ray[tune]\n ...     }\n ```\n \n-Optuna提供了多目标HPO。您可以在`hyperparameter_search`中传递`direction`参数，并定义自己的`compute_objective`以返回多个目标值。在`hyperparameter_search`中将返回Pareto Front（`List[BestRun]`），您应该参考[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py)中的测试用例`TrainerHyperParameterMultiObjectOptunaIntegrationTest`。它类似于以下内容：\n+Optuna提供了多目标HPO。您可以在`hyperparameter_search`中传递`direction`参数，并定义自己的`compute_objective`以返回多个目标值。在`hyperparameter_search`中将返回Pareto Front（`list[BestRun]`），您应该参考[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py)中的测试用例`TrainerHyperParameterMultiObjectOptunaIntegrationTest`。它类似于以下内容：\n \n ```py\n >>> best_trials = trainer.hyperparameter_search("
        },
        {
            "sha": "3b66888bc1078e56affecaf0b84a9df8f6c94614",
            "filename": "docs/source/zh/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -181,7 +181,7 @@ Wav2Vec2 分词器仅训练了大写字符，因此您需要确保文本与分\n ...     processor: AutoProcessor\n ...     padding: Union[bool, str] = \"longest\"\n \n-...     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n+...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n ...         # split inputs and labels since they have to be of different lengths and need\n ...         # different padding methods\n ...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]"
        },
        {
            "sha": "b30322b0071fff2f919bb0b2c48a77513430b6b4",
            "filename": "examples/flax/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fflax%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fflax%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Futils_qa.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -47,7 +47,7 @@ def postprocess_qa_predictions(\n     Args:\n         examples: The non-preprocessed dataset (see the main script for more information).\n         features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n+        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n             first dimension must match the number of elements of :obj:`features`.\n         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n@@ -270,7 +270,7 @@ def postprocess_qa_predictions_with_beam_search(\n     Args:\n         examples: The non-preprocessed dataset (see the main script for more information).\n         features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n+        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n             first dimension must match the number of elements of :obj:`features`.\n         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):"
        },
        {
            "sha": "fb430c00c26ba9deba7ebf248394faa436bf7788",
            "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -184,7 +184,7 @@ def prediction_step(\n         Args:\n             model (:obj:`nn.Module`):\n                 The model to evaluate.\n-            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n+            inputs (:obj:`dict[str, Union[torch.Tensor, Any]]`):\n                 The inputs and targets of the model.\n \n                 The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n@@ -193,7 +193,7 @@ def prediction_step(\n                 Whether or not to return the loss only.\n \n         Return:\n-            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n+            tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n             A tuple with the loss, logits and labels (each being optional).\n         \"\"\"\n         inputs = self._prepare_inputs(inputs)"
        },
        {
            "sha": "221b1405aa269131c696115957102383cef7fbb4",
            "filename": "examples/legacy/seq2seq/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Futils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -530,7 +530,7 @@ def calculate_rouge(\n         on multi sentence summaries (CNN/DM dataset).\n \n     Returns:\n-         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys\n+         dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys\n \n     \"\"\"\n     scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)"
        },
        {
            "sha": "4e9b055dcf9f45d3ea4ebc9a83f726959d8fff97",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -91,11 +91,11 @@ class MyNewModelConfig(PretrainedConfig):\n                 `beta_slow` (`float`, *optional*):\n                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                     ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n+                `short_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n+                `long_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2"
        },
        {
            "sha": "4614c8cdaa5248aed4f5dccf0ce1f586844e93e7",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_new_imgproc_model.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -57,11 +57,11 @@ class ImgprocModelImageProcessor(BaseImageProcessor):\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n             overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n@@ -74,13 +74,13 @@ class ImgprocModelImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -101,7 +101,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -113,7 +113,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n@@ -151,13 +151,13 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n@@ -172,7 +172,7 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Controls the size of the image after `resize`. The shortest edge of the image is resized to\n                 `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n                 is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n@@ -185,9 +185,9 @@ def preprocess(\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to normalize the image by if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB."
        },
        {
            "sha": "fdc768237bed38941bc25615db0a0926269488a1",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_add_function.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n # Note that zamba does not have the `apply_rotary_pos_emb` function!\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n from torch import nn\n@@ -62,5 +62,5 @@ class TestAttention(nn.Module):\n     def __init__(self):\n         pass\n \n-    def forward(self) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    def forward(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "0fe4ae497b4967b7027ddb0fc186295c44a2b135",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_dummy.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -210,12 +210,12 @@ def __init__(self, config: DummyConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -278,9 +278,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n "
        },
        {
            "sha": "8b2e8aed90bc6368d59c1ef4ada61febaf5b742b",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -6,7 +6,7 @@\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n import math\n import os\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from packaging import version\n@@ -136,9 +136,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -245,9 +245,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -386,9 +386,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -454,9 +454,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -532,12 +532,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None"
        },
        {
            "sha": "98daf0e8079a88e16c44d3285721f177daeb5356",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_from_uppercase_model.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -71,7 +71,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -153,7 +153,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`"
        },
        {
            "sha": "ec1a3346c9ba999b260750bce6ea433b9ebd33fb",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_multimodal1.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -210,12 +210,12 @@ def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -278,9 +278,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n "
        },
        {
            "sha": "69e7e454754c5eb9fdb2d9e5d13f8b7cd76c08b2",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_multimodal2.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n \n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -81,7 +81,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -177,7 +177,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -244,7 +244,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`"
        },
        {
            "sha": "d8e10885ef8a847869f4bbc5cafabc19c9c36c76",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -208,12 +208,12 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -276,9 +276,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n \n@@ -469,7 +469,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "77e4efa172e8371b450faf6f084eea85a632c44f",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_new_task_model.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n from dataclasses import dataclass\n-from typing import ClassVar, List, Optional, Tuple, Union\n+from typing import ClassVar, Optional, Union\n \n import torch\n from torch import nn\n@@ -88,9 +88,9 @@ class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -249,7 +249,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -259,7 +259,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, NewTaskModelModelOutputWithPast]:\n+    ) -> Union[tuple, NewTaskModelModelOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -442,7 +442,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         num_logits_to_keep: int = 0,\n-    ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n+    ) -> Union[tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "e1bd313a424396f0abf67d20271a65038e2e5e36",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -6,7 +6,7 @@\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n import math\n import os\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -139,9 +139,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -248,9 +248,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -389,9 +389,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -457,9 +457,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -535,12 +535,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -903,12 +903,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if"
        },
        {
            "sha": "fdcfa41d3f6e80e6102ee27144322776939ff364",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_super.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -211,12 +211,12 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -279,9 +279,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n "
        },
        {
            "sha": "d0ec849b9490bffe0095958777b6f9764469d803",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -5,7 +5,7 @@\n #                          modular_switch_function.py file directly. One of our CI enforces this.\n #                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n # Note that llama and cohere have different definitions for rotate_half\n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n@@ -123,12 +123,12 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "de1084de727713f13c1e366764c1b630661e49e6",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -7,7 +7,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n@@ -43,7 +43,7 @@ def forward(\n         self,\n         value: Tensor,\n         value_spatial_shapes: Tensor,\n-        value_spatial_shapes_list: List[Tuple],\n+        value_spatial_shapes_list: list[tuple],\n         level_start_index: Tensor,\n         sampling_locations: Tensor,\n         attention_weights: Tensor,\n@@ -124,9 +124,9 @@ class TestDetrDecoderOutput(ModelOutput):\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -177,12 +177,12 @@ class TestDetrModelOutput(ModelOutput):\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n     enc_outputs_class: Optional[torch.FloatTensor] = None\n     enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n \n@@ -557,7 +557,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, target_len, embed_dim = hidden_states.size()\n@@ -1431,7 +1431,7 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n         Args:\n             enc_output (Tensor[batch_size, sequence_length, hidden_size]): Output of the encoder.\n             padding_mask (Tensor[batch_size, sequence_length]): Padding mask for `enc_output`.\n-            spatial_shapes (List[Tuple[int, int]]): Spatial shapes of the feature maps.\n+            spatial_shapes (list[tuple[int, int]]): Spatial shapes of the feature maps.\n \n         Returns:\n             `tuple(torch.FloatTensor)`: A tuple of feature map and bbox prediction.\n@@ -1499,7 +1499,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.FloatTensor], TestDetrModelOutput]:\n+    ) -> Union[tuple[torch.FloatTensor], TestDetrModelOutput]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "792a7c8c165500d2c15f798f6b7b9d5eebf5558f",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -33,7 +33,7 @@\n import os\n from collections.abc import Iterable\n from contextlib import nullcontext\n-from typing import Dict, Optional\n+from typing import Optional\n \n import torch\n import torch.distributed as dist\n@@ -589,7 +589,7 @@ class ContextParallelCollator:\n     def __init__(self, cp_mesh: Optional[DeviceMesh] = None):\n         self.cp_mesh = cp_mesh\n \n-    def __call__(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n+    def __call__(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n         batch = default_collate(batch)\n         if self.cp_mesh is not None and self.cp_mesh.size() > 1:\n             # Get sequence length from the input batch"
        },
        {
            "sha": "3f0e8fea4bd2aead2d0903adc4bccb347dfb6db6",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -66,9 +66,9 @@ def format_image_annotations_as_coco(\n \n     Args:\n         image_id (str): image id. e.g. \"0001\"\n-        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n-        areas (List[float]): list of corresponding areas to provided bounding boxes\n-        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n+        categories (list[int]): list of categories/class labels corresponding to provided bounding boxes\n+        areas (list[float]): list of corresponding areas to provided bounding boxes\n+        bboxes (list[tuple[float]]): list of bounding boxes provided in COCO format\n             ([center_x, center_y, width, height] in absolute coordinates)\n \n     Returns:\n@@ -101,7 +101,7 @@ def convert_bbox_yolo_to_pascal(boxes: torch.Tensor, image_size: tuple[int, int]\n \n     Args:\n         boxes (torch.Tensor): Bounding boxes in YOLO format\n-        image_size (Tuple[int, int]): Image size in format (height, width)\n+        image_size (tuple[int, int]): Image size in format (height, width)\n \n     Returns:\n         torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)"
        },
        {
            "sha": "fe60ebaa847959b8eb1173bceace5d4b3de8e963",
            "filename": "examples/pytorch/object-detection/run_object_detection_no_trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -67,9 +67,9 @@ def format_image_annotations_as_coco(\n \n     Args:\n         image_id (str): image id. e.g. \"0001\"\n-        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n-        areas (List[float]): list of corresponding areas to provided bounding boxes\n-        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n+        categories (list[int]): list of categories/class labels corresponding to provided bounding boxes\n+        areas (list[float]): list of corresponding areas to provided bounding boxes\n+        bboxes (list[tuple[float]]): list of bounding boxes provided in COCO format\n             ([center_x, center_y, width, height] in absolute coordinates)\n \n     Returns:\n@@ -103,7 +103,7 @@ def convert_bbox_yolo_to_pascal(boxes: torch.Tensor, image_size: tuple[int, int]\n \n     Args:\n         boxes (torch.Tensor): Bounding boxes in YOLO format\n-        image_size (Tuple[int, int]): Image size in format (height, width)\n+        image_size (tuple[int, int]): Image size in format (height, width)\n \n     Returns:\n         torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)"
        },
        {
            "sha": "b30322b0071fff2f919bb0b2c48a77513430b6b4",
            "filename": "examples/pytorch/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Futils_qa.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -47,7 +47,7 @@ def postprocess_qa_predictions(\n     Args:\n         examples: The non-preprocessed dataset (see the main script for more information).\n         features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n+        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n             first dimension must match the number of elements of :obj:`features`.\n         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n@@ -270,7 +270,7 @@ def postprocess_qa_predictions_with_beam_search(\n     Args:\n         examples: The non-preprocessed dataset (see the main script for more information).\n         features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n+        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n             first dimension must match the number of elements of :obj:`features`.\n         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):"
        },
        {
            "sha": "b30322b0071fff2f919bb0b2c48a77513430b6b4",
            "filename": "examples/tensorflow/question-answering/utils_qa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -47,7 +47,7 @@ def postprocess_qa_predictions(\n     Args:\n         examples: The non-preprocessed dataset (see the main script for more information).\n         features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n+        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n             first dimension must match the number of elements of :obj:`features`.\n         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n@@ -270,7 +270,7 @@ def postprocess_qa_predictions_with_beam_search(\n     Args:\n         examples: The non-preprocessed dataset (see the main script for more information).\n         features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n+        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n             first dimension must match the number of elements of :obj:`features`.\n         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):"
        },
        {
            "sha": "af22cfe9c623ce3ecdeafb7b1a374632fd9ea3af",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -22,7 +22,8 @@ line-length = 119\n ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\" ]\n # RUF013: Checks for the use of implicit Optional\n #  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\"]\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"UP006\"]\n+extend-safe-fixes = [\"UP006\"]\n \n # Ignore import violations in all `__init__.py` files.\n [tool.ruff.lint.per-file-ignores]"
        },
        {
            "sha": "40a1a3963d6441b6d209c3a3e3625b2f629e06ef",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import warnings\n from io import BytesIO\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import requests\n@@ -70,7 +70,7 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n \n \n AudioInput = Union[\n-    np.ndarray, \"torch.Tensor\", List[np.ndarray], Tuple[np.ndarray], List[\"torch.Tensor\"], Tuple[\"torch.Tensor\"]  # noqa: F821\n+    np.ndarray, \"torch.Tensor\", list[np.ndarray], tuple[np.ndarray], list[\"torch.Tensor\"], tuple[\"torch.Tensor\"]  # noqa: F821\n ]\n \n \n@@ -88,7 +88,7 @@ def make_list_of_audio(\n     \"\"\"\n     Ensure that the output is a list of audio.\n     Args:\n-        audio (`Union[List[AudioInput], AudioInput]`):\n+        audio (`Union[list[AudioInput], AudioInput]`):\n             The input audio.\n     Returns:\n         list: A list of audio.\n@@ -246,7 +246,7 @@ def chroma_filter_bank(\n             Tuning deviation from A440 in fractions of a chroma bin.\n         power (`float`, *optional*, defaults to 2.0):\n             If 12.0, normalizes each column with their L2 norm. If 1.0, normalizes each column with their L1 norm.\n-        weighting_parameters (`Tuple[float, float]`, *optional*, defaults to `(5., 2.)`):\n+        weighting_parameters (`tuple[float, float]`, *optional*, defaults to `(5., 2.)`):\n             If specified, apply a Gaussian weighting parameterized by the first element of the tuple being the center and\n             the second element being the Gaussian half-width.\n         start_at_c_chroma (`float`, *optional*, defaults to `True`):\n@@ -733,7 +733,7 @@ def spectrogram_batch(\n     Note: This function is designed for efficient batch processing of multiple waveforms but retains compatibility with individual waveform processing methods like `librosa.stft`.\n \n     Args:\n-        waveform_list (`List[np.ndarray]` with arrays of shape `(length,)`):\n+        waveform_list (`list[np.ndarray]` with arrays of shape `(length,)`):\n             The list of input waveforms, each a single-channel (mono) signal.\n         window (`np.ndarray` of shape `(frame_length,)`):\n             The windowing function to apply, including zero-padding if necessary.\n@@ -775,7 +775,7 @@ def spectrogram_batch(\n             Data type of the output spectrogram.\n \n     Returns:\n-        List[`np.ndarray`]: A list of spectrogram arrays, one for each input waveform.\n+        list[`np.ndarray`]: A list of spectrogram arrays, one for each input waveform.\n     \"\"\"\n     window_length = len(window)\n "
        },
        {
            "sha": "10a868938881feb3f82f5aefd6a757fc6c5e43a4",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 74,
            "deletions": 74,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -4,7 +4,7 @@\n import os\n from collections.abc import Iterable\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n from packaging import version\n@@ -28,7 +28,7 @@ def _static_cache_update(\n     key_states: torch.Tensor,\n     value_states: torch.Tensor,\n     cache_position: Optional[torch.LongTensor],\n-) -> Tuple[torch.Tensor, torch.Tensor]:\n+) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"\n     Updates the static cache tensors in place.\n \n@@ -41,7 +41,7 @@ def _static_cache_update(\n                                                        If None, the entire cache is overwritten (prefill).\n \n     Returns:\n-        Tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value cache tensors (modified in-place).\n+        tuple[`torch.Tensor`, `torch.Tensor`]: The updated key and value cache tensors (modified in-place).\n     \"\"\"\n     if cache_position is None:\n         # Prefill phase where seq_len potentially equals max_cache_len. Directly copy.\n@@ -67,7 +67,7 @@ def _sliding_cache_update(\n     value_states: torch.Tensor,\n     cache_position: torch.LongTensor,\n     max_cache_len: int,\n-) -> Tuple[torch.Tensor, torch.Tensor]:\n+) -> tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"\n     Updates the sliding window cache tensors, returning the potentially modified tensors.\n \n@@ -80,7 +80,7 @@ def _sliding_cache_update(\n         max_cache_len (`int`): The maximum length of the sliding window cache.\n \n     Returns:\n-        Tuple[`torch.Tensor`, `torch.Tensor`]: The key and value tensors representing the cache state after the update.\n+        tuple[`torch.Tensor`, `torch.Tensor`]: The key and value tensors representing the cache state after the update.\n                                                For prefill > window, these are the full input states.\n                                                Otherwise, they are the updated cache tensors.\n     \"\"\"\n@@ -134,8 +134,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n \n@@ -146,7 +146,7 @@ def update(\n                 The new value states to cache.\n             layer_idx (`int`):\n                 The index of the layer to cache the states for.\n-            cache_kwargs (`Dict[str, Any]`, `optional`):\n+            cache_kwargs (`dict[str, Any]`, `optional`):\n                 Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n                 cache to be created.\n \n@@ -222,7 +222,7 @@ def from_dict(cls, config_dict, **kwargs):\n         \"\"\"\n         Constructs a CacheConfig instance from a dictionary of parameters.\n         Args:\n-            config_dict (Dict[str, Any]): Dictionary containing configuration parameters.\n+            config_dict (dict[str, Any]): Dictionary containing configuration parameters.\n             **kwargs: Additional keyword arguments to override dictionary values.\n \n         Returns:\n@@ -257,10 +257,10 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n             writer.write(json_string)\n \n     # Copied from transformers.utils.quantization_config.QuantizationConfigMixin.to_dict\n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         return copy.deepcopy(self.__dict__)\n \n@@ -289,11 +289,11 @@ def update(self, **kwargs):\n         returning all the unused kwargs.\n \n         Args:\n-            kwargs (`Dict[str, Any]`):\n+            kwargs (`dict[str, Any]`):\n                 Dictionary of attributes to tentatively update this class.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n+            `dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n         \"\"\"\n         to_remove = []\n         for key, value in kwargs.items():\n@@ -473,8 +473,8 @@ class DynamicCache(Cache):\n     def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n         super().__init__()\n         self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n \n         # `_distributed_cache_data` was originally added for compatibility with `torch.distributed` (DDP). See #36121\n         # and #36373 for more information. In a nutshell, it is `map(gather_map, zip(*caches))`, i.e. each item in the\n@@ -487,7 +487,7 @@ def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n                 self.key_cache.append(key_states)\n                 self.value_cache.append(value_states)\n \n-    def __getitem__(self, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n         sequence length.\n@@ -517,8 +517,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n \n@@ -529,7 +529,7 @@ def update(\n                 The new value states to cache.\n             layer_idx (`int`):\n                 The index of the layer to cache the states for.\n-            cache_kwargs (`Dict[str, Any]`, `optional`):\n+            cache_kwargs (`dict[str, Any]`, `optional`):\n                 Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n \n         Return:\n@@ -574,7 +574,7 @@ def get_max_cache_shape(self) -> Optional[int]:\n         \"\"\"Returns the maximum sequence length of the cache object. DynamicCache does not have a maximum length.\"\"\"\n         return None\n \n-    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor, torch.Tensor]]:\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:\n         \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format. Used for\n         backward compatibility.\"\"\"\n         legacy_cache = ()\n@@ -584,7 +584,7 @@ def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor, torch.Tensor]]:\n \n     @classmethod\n     def from_legacy_cache(\n-        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor, torch.FloatTensor]]] = None\n+        cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor, torch.FloatTensor]]] = None\n     ) -> \"DynamicCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`. Used for\n         backward compatibility.\"\"\"\n@@ -611,7 +611,7 @@ def crop(self, max_length: int):\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n                 self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n \n-    def batch_split(self, full_batch_size: int, split_size: int) -> List[\"DynamicCache\"]:\n+    def batch_split(self, full_batch_size: int, split_size: int) -> list[\"DynamicCache\"]:\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         out = []\n@@ -624,7 +624,7 @@ def batch_split(self, full_batch_size: int, split_size: int) -> List[\"DynamicCac\n         return out\n \n     @classmethod\n-    def from_batch_splits(cls, splits: List[\"DynamicCache\"]) -> \"DynamicCache\":\n+    def from_batch_splits(cls, splits: list[\"DynamicCache\"]) -> \"DynamicCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n         cache = cls()\n@@ -762,7 +762,7 @@ def evict_previous_layer(self, layer_idx: int):\n             self.key_cache[prev_layer_idx] = self.key_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n             self.value_cache[prev_layer_idx] = self.value_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n \n-    def __getitem__(self, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         \"Gets the cache for this layer to the device. Prefetches the next and evicts the previous layer.\"\n         if layer_idx < len(self):\n             # Evict the previous layer if necessary\n@@ -799,8 +799,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n         Parameters:\n@@ -810,7 +810,7 @@ def update(\n                 The new value states to cache.\n             layer_idx (`int`):\n                 The index of the layer to cache the states for.\n-            cache_kwargs (`Dict[str, Any]`, `optional`):\n+            cache_kwargs (`dict[str, Any]`, `optional`):\n                 Additional arguments for the cache subclass. No additional arguments are used in `OffloadedCache`.\n         Return:\n             A tuple containing the updated key and value states.\n@@ -857,8 +857,8 @@ class QuantizedCache(DynamicCache):\n \n     def __init__(self, cache_config: QuantizedCacheConfig) -> None:\n         super().__init__()\n-        self._quantized_key_cache: List[torch.Tensor] = []\n-        self._quantized_value_cache: List[torch.Tensor] = []\n+        self._quantized_key_cache: list[torch.Tensor] = []\n+        self._quantized_value_cache: list[torch.Tensor] = []\n \n         self.nbits = cache_config.nbits\n         self.residual_length = cache_config.residual_length\n@@ -875,8 +875,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         # Update the number of seen tokens\n         if layer_idx == 0:\n             self._seen_tokens += key_states.shape[-2]\n@@ -1094,7 +1094,7 @@ class StaticCache(Cache):\n             should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n+        layer_device_map (`Optional[dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n@@ -1129,7 +1129,7 @@ def __init__(\n         max_cache_len: Optional[int] = None,\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n         self.max_batch_size = max_batch_size\n@@ -1145,8 +1145,8 @@ def __init__(\n             else config.num_key_value_heads\n         )\n \n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n         # Note: There will be significant perf decrease if switching to use 5D tensors instead.\n         cache_shape = (self.max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n         device = torch.device(device) if device is not None else None\n@@ -1169,8 +1169,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n         It is VERY important to index using a tensor, otherwise you introduce a copy to the device.\n@@ -1182,7 +1182,7 @@ def update(\n                 The new value states to cache.\n             layer_idx (`int`):\n                 The index of the layer to cache the states for.\n-            cache_kwargs (`Dict[str, Any]`, `optional`):\n+            cache_kwargs (`dict[str, Any]`, `optional`):\n                 Additional arguments for the cache subclass. The `StaticCache` needs the `cache_position` input\n                 to know how where to write in the cache.\n \n@@ -1260,7 +1260,7 @@ class SlidingWindowCache(StaticCache):\n             should pass the `layer_device_map` argument instead.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n+        layer_device_map (`Optional[dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n@@ -1294,7 +1294,7 @@ def __init__(\n         max_cache_len: Optional[int] = None,\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n             raise ValueError(\n@@ -1318,8 +1318,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         if cache_kwargs is None:\n             cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n@@ -1400,7 +1400,7 @@ def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n         for layer_idx in range(len(cross_attention_cache.key_cache)):\n             self.is_updated[layer_idx] = bool(cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n-    def __getitem__(self, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n         sequence length.\n@@ -1422,7 +1422,7 @@ def __len__(self):\n         \"\"\"\n         return len(self.self_attention_cache)\n \n-    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor]]:\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor]]:\n         \"\"\"Converts the `EncoderDecoderCache` instance into its equivalent in the legacy cache format.\"\"\"\n         legacy_cache = ()\n         if len(self.cross_attention_cache) > 0:\n@@ -1436,7 +1436,7 @@ def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor]]:\n \n     @classmethod\n     def from_legacy_cache(\n-        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+        cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n     ) -> \"EncoderDecoderCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `EncoderDecoderCache`.\"\"\"\n         cache = cls(\n@@ -1495,7 +1495,7 @@ def crop(self, maximum_length: int):\n         self.check_dynamic_cache(self.crop.__name__)\n         self.self_attention_cache.crop(maximum_length)\n \n-    def batch_split(self, full_batch_size: int, split_size: int) -> \"List[EncoderDecoderCache]\":\n+    def batch_split(self, full_batch_size: int, split_size: int) -> \"list[EncoderDecoderCache]\":\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         self.check_dynamic_cache(self.batch_split.__name__)\n@@ -1508,7 +1508,7 @@ def batch_split(self, full_batch_size: int, split_size: int) -> \"List[EncoderDec\n         return out\n \n     @classmethod\n-    def from_batch_splits(cls, splits: List[\"EncoderDecoderCache\"]) -> \"EncoderDecoderCache\":\n+    def from_batch_splits(cls, splits: list[\"EncoderDecoderCache\"]) -> \"EncoderDecoderCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n         self_attention_cache = DynamicCache()\n@@ -1569,7 +1569,7 @@ class HybridCache(Cache):\n             should pass the `layer_device_map` argument instead.\n         dtype (torch.dtype, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n+        layer_device_map (`Optional[dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n@@ -1603,7 +1603,7 @@ def __init__(\n         max_cache_len: Optional[int] = None,\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n@@ -1634,8 +1634,8 @@ def __init__(\n         else:\n             self.is_sliding = [False] * config.num_hidden_layers\n \n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n         global_cache_shape = (self.max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n         sliding_cache_shape = (self.max_batch_size, self.num_key_value_heads, self.sliding_window_len, self.head_dim)\n         self.sliding_window = min(config.sliding_window, max_cache_len)\n@@ -1660,8 +1660,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         if cache_kwargs is None:\n             cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n@@ -1757,7 +1757,7 @@ class HybridChunkedCache(Cache):\n             should pass the `layer_device_map` argument instead.\n         dtype (torch.dtype, *optional*, defaults to `torch.bfloat16`):\n             The default `dtype` to use when initializing the layer.\n-        layer_device_map (`Optional[Dict[int, Union[str, torch.device, int]]]]`, *optional*):\n+        layer_device_map (`Optional[dict[int, Union[str, torch.device, int]]]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n@@ -1791,7 +1791,7 @@ def __init__(\n         max_cache_len: Optional[int] = None,\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.bfloat16,\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n@@ -1811,8 +1811,8 @@ def __init__(\n         else:\n             self.is_sliding = [False] * config.num_hidden_layers\n \n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n         self.cumulative_length = [0 for _ in range(config.num_hidden_layers)]\n \n     def initialise_cache_layer(self, layer_idx, key_states):\n@@ -1880,8 +1880,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         if cache_kwargs is None:\n             cache_kwargs = {}\n         cache_position = cache_kwargs.get(\"cache_position\")\n@@ -1968,7 +1968,7 @@ def __init__(\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.bfloat16,\n         offload_device: Union[str, torch.device] = torch.device(\"cpu\"),\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n     ):\n         super().__init__(config, max_batch_size, max_cache_len, device, dtype, layer_device_map)\n \n@@ -2121,8 +2121,8 @@ def __init__(\n         self.ssm_state_size = config.state_size\n         self.conv_kernel_size = config.conv_kernel\n \n-        self.conv_states: List[torch.Tensor] = []\n-        self.ssm_states: List[torch.Tensor] = []\n+        self.conv_states: list[torch.Tensor] = []\n+        self.ssm_states: list[torch.Tensor] = []\n         device = torch.device(device) if device is not None else None\n         for _ in range(config.num_hidden_layers):\n             conv_state: torch.Tensor = torch.zeros(\n@@ -2193,7 +2193,7 @@ class OffloadedStaticCache(StaticCache):\n             The default `dtype` to use when initializing the cache.\n         offload_device (`Union[str, torch.device]`, *optional*, defaults to `cpu`):\n             The device to offload to. Defaults to CPU.\n-        layer_device_map (`Dict[int, Union[str, torch.device, int]]`, *optional*):\n+        layer_device_map (`dict[int, Union[str, torch.device, int]]`, *optional*):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n@@ -2227,7 +2227,7 @@ def __init__(\n         device: Union[str, torch.device],\n         dtype: Optional[torch.dtype] = None,\n         offload_device: Union[str, torch.device] = torch.device(\"cpu\"),\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super(Cache, self).__init__()\n \n@@ -2255,8 +2255,8 @@ def __init__(\n         cache_shape = (max_batch_size, num_key_value_heads, self.max_cache_len, head_dim)\n \n         # Create offloaded CPU tensors.\n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n \n         for i in range(config.num_hidden_layers):\n             # First layer is always on-device.\n@@ -2268,8 +2268,8 @@ def __init__(\n             self.value_cache.append(value_cache)\n \n         # Create device tensors.\n-        self._device_key_cache: List[torch.Tensor] = []\n-        self._device_value_cache: List[torch.Tensor] = []\n+        self._device_key_cache: list[torch.Tensor] = []\n+        self._device_value_cache: list[torch.Tensor] = []\n \n         for i in range(2):\n             key_cache, value_cache = self._create_key_value_cache_tensors(cache_shape, self.device)\n@@ -2289,8 +2289,8 @@ def update(\n         key_states: torch.Tensor,\n         value_states: torch.Tensor,\n         layer_idx: int,\n-        cache_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n         It is VERY important to index using a tensor, otherwise you introduce a copy to the device.\n@@ -2302,7 +2302,7 @@ def update(\n                 The new value states to cache.\n             layer_idx (`int`):\n                 The index of the layer to cache the states for.\n-            cache_kwargs (`Dict[str, Any]`, *optional*):\n+            cache_kwargs (`dict[str, Any]`, *optional*):\n                 Additional arguments for the cache subclass. The `OffloadedStaticCache` needs the\n                 `cache_position` input to know how where to write in the cache.\n \n@@ -2401,13 +2401,13 @@ def seen_tokens(self) -> int:\n         return self._seen_tokens\n \n     def _create_key_value_cache_tensors(\n-        self, shape: Tuple[int, ...], device: torch.device\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        self, shape: tuple[int, ...], device: torch.device\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Creates K/V cache tensors on a device. Pins memory for CPU tensors. Marks them as static\n         addresses for non-CPU tensors.\n \n         Args:\n-            shape (`Tuple[int, ...]`): Shape.\n+            shape (`tuple[int, ...]`): Shape.\n             device (`torch.device`): Device.\n \n         Returns:"
        },
        {
            "sha": "9e10f9e37c7ee17fdadeb243204ca10d9a1558e7",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 35,
            "deletions": 35,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -23,7 +23,7 @@\n from itertools import chain\n from pathlib import Path\n from re import Pattern\n-from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Union\n \n import yaml\n \n@@ -148,15 +148,15 @@ def find_indent(line: str) -> int:\n     return len(search.groups()[0])\n \n \n-def parse_module_content(content: str) -> List[str]:\n+def parse_module_content(content: str) -> list[str]:\n     \"\"\"\n     Parse the content of a module in the list of objects it defines.\n \n     Args:\n         content (`str`): The content to parse\n \n     Returns:\n-        `List[str]`: The list of objects defined in the module.\n+        `list[str]`: The list of objects defined in the module.\n     \"\"\"\n     objects = []\n     current_object = []\n@@ -336,7 +336,7 @@ def add_content_to_file(\n \n def replace_model_patterns(\n     text: str, old_model_patterns: ModelPatterns, new_model_patterns: ModelPatterns\n-) -> Tuple[str, str]:\n+) -> tuple[str, str]:\n     \"\"\"\n     Replace all patterns present in a given text.\n \n@@ -414,10 +414,10 @@ def simplify_replacements(replacements):\n     \"BertConfig->BertNewConfig\" is implied by \"Bert->BertNew\" so not needed.\n \n     Args:\n-        replacements (`List[Tuple[str, str]]`): List of patterns (old, new)\n+        replacements (`list[tuple[str, str]]`): List of patterns (old, new)\n \n     Returns:\n-        `List[Tuple[str, str]]`: The list of patterns simplified.\n+        `list[tuple[str, str]]`: The list of patterns simplified.\n     \"\"\"\n     if len(replacements) <= 1:\n         # Nothing to simplify\n@@ -519,7 +519,7 @@ def duplicate_module(\n     new_model_patterns: ModelPatterns,\n     dest_file: Optional[str] = None,\n     add_copied_from: bool = True,\n-    attrs_to_remove: Optional[List[str]] = None,\n+    attrs_to_remove: Optional[list[str]] = None,\n ):\n     \"\"\"\n     Create a new module from an existing one and adapting all function and classes names from old patterns to new ones.\n@@ -585,17 +585,17 @@ def duplicate_module(\n \n \n def filter_framework_files(\n-    files: List[Union[str, os.PathLike]], frameworks: Optional[List[str]] = None\n-) -> List[Union[str, os.PathLike]]:\n+    files: list[Union[str, os.PathLike]], frameworks: Optional[list[str]] = None\n+) -> list[Union[str, os.PathLike]]:\n     \"\"\"\n     Filter a list of files to only keep the ones corresponding to a list of frameworks.\n \n     Args:\n-        files (`List[Union[str, os.PathLike]]`): The list of files to filter.\n-        frameworks (`List[str]`, *optional*): The list of allowed frameworks.\n+        files (`list[Union[str, os.PathLike]]`): The list of files to filter.\n+        frameworks (`list[str]`, *optional*): The list of allowed frameworks.\n \n     Returns:\n-        `List[Union[str, os.PathLike]]`: The list of filtered files.\n+        `list[Union[str, os.PathLike]]`: The list of filtered files.\n     \"\"\"\n     if frameworks is None:\n         frameworks = get_default_frameworks()\n@@ -617,17 +617,17 @@ def filter_framework_files(\n     return [framework_to_file[f] for f in frameworks if f in framework_to_file] + others\n \n \n-def get_model_files(model_type: str, frameworks: Optional[List[str]] = None) -> Dict[str, Union[Path, List[Path]]]:\n+def get_model_files(model_type: str, frameworks: Optional[list[str]] = None) -> dict[str, Union[Path, list[Path]]]:\n     \"\"\"\n     Retrieves all the files associated to a model.\n \n     Args:\n         model_type (`str`): A valid model type (like \"bert\" or \"gpt2\")\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n             If passed, will only keep the model files corresponding to the passed frameworks.\n \n     Returns:\n-        `Dict[str, Union[Path, List[Path]]]`: A dictionary with the following keys:\n+        `dict[str, Union[Path, list[Path]]]`: A dictionary with the following keys:\n         - **doc_file** -- The documentation file for the model.\n         - **model_files** -- All the files in the model module.\n         - **test_files** -- The test files for the model.\n@@ -663,14 +663,14 @@ def get_model_files(model_type: str, frameworks: Optional[List[str]] = None) ->\n \n \n def find_base_model_checkpoint(\n-    model_type: str, model_files: Optional[Dict[str, Union[Path, List[Path]]]] = None\n+    model_type: str, model_files: Optional[dict[str, Union[Path, list[Path]]]] = None\n ) -> str:\n     \"\"\"\n     Finds the model checkpoint used in the docstrings for a given model.\n \n     Args:\n         model_type (`str`): A valid model type (like \"bert\" or \"gpt2\")\n-        model_files (`Dict[str, Union[Path, List[Path]]`, *optional*):\n+        model_files (`dict[str, Union[Path, list[Path]]`, *optional*):\n             The files associated to `model_type`. Can be passed to speed up the function, otherwise will be computed.\n \n     Returns:\n@@ -713,18 +713,18 @@ def get_default_frameworks():\n _re_model_mapping = re.compile(\"MODEL_([A-Z_]*)MAPPING_NAMES\")\n \n \n-def retrieve_model_classes(model_type: str, frameworks: Optional[List[str]] = None) -> Dict[str, List[str]]:\n+def retrieve_model_classes(model_type: str, frameworks: Optional[list[str]] = None) -> dict[str, list[str]]:\n     \"\"\"\n     Retrieve the model classes associated to a given model.\n \n     Args:\n         model_type (`str`): A valid model type (like \"bert\" or \"gpt2\")\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n             The frameworks to look for. Will default to `[\"pt\", \"tf\", \"flax\"]`, passing a smaller list will restrict\n             the classes returned.\n \n     Returns:\n-        `Dict[str, List[str]]`: A dictionary with one key per framework and the list of model classes associated to\n+        `dict[str, list[str]]`: A dictionary with one key per framework and the list of model classes associated to\n         that framework as values.\n     \"\"\"\n     if frameworks is None:\n@@ -754,20 +754,20 @@ def retrieve_model_classes(model_type: str, frameworks: Optional[List[str]] = No\n     return model_classes\n \n \n-def retrieve_info_for_model(model_type, frameworks: Optional[List[str]] = None):\n+def retrieve_info_for_model(model_type, frameworks: Optional[list[str]] = None):\n     \"\"\"\n     Retrieves all the information from a given model_type.\n \n     Args:\n         model_type (`str`): A valid model type (like \"bert\" or \"gpt2\")\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n             If passed, will only keep the info corresponding to the passed frameworks.\n \n     Returns:\n         `Dict`: A dictionary with the following keys:\n-        - **frameworks** (`List[str]`): The list of frameworks that back this model type.\n-        - **model_classes** (`Dict[str, List[str]]`): The model classes implemented for that model type.\n-        - **model_files** (`Dict[str, Union[Path, List[Path]]]`): The files associated with that model type.\n+        - **frameworks** (`list[str]`): The list of frameworks that back this model type.\n+        - **model_classes** (`dict[str, list[str]]`): The model classes implemented for that model type.\n+        - **model_files** (`dict[str, Union[Path, list[Path]]]`): The files associated with that model type.\n         - **model_patterns** (`ModelPatterns`): The various patterns for the model.\n     \"\"\"\n     if model_type not in auto_module.MODEL_NAMES_MAPPING:\n@@ -833,15 +833,15 @@ def retrieve_info_for_model(model_type, frameworks: Optional[List[str]] = None):\n \n \n def clean_frameworks_in_init(\n-    init_file: Union[str, os.PathLike], frameworks: Optional[List[str]] = None, keep_processing: bool = True\n+    init_file: Union[str, os.PathLike], frameworks: Optional[list[str]] = None, keep_processing: bool = True\n ):\n     \"\"\"\n     Removes all the import lines that don't belong to a given list of frameworks or concern tokenizers/feature\n     extractors/image processors/processors in an init.\n \n     Args:\n         init_file (`str` or `os.PathLike`): The path to the init to treat.\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n            If passed, this will remove all imports that are subject to a framework not in frameworks\n         keep_processing (`bool`, *optional*, defaults to `True`):\n             Whether or not to keep the preprocessing (tokenizer, feature extractor, image processor, processor) imports\n@@ -914,7 +914,7 @@ def clean_frameworks_in_init(\n def add_model_to_main_init(\n     old_model_patterns: ModelPatterns,\n     new_model_patterns: ModelPatterns,\n-    frameworks: Optional[List[str]] = None,\n+    frameworks: Optional[list[str]] = None,\n     with_processing: bool = True,\n ):\n     \"\"\"\n@@ -923,7 +923,7 @@ def add_model_to_main_init(\n     Args:\n         old_model_patterns (`ModelPatterns`): The patterns for the old model.\n         new_model_patterns (`ModelPatterns`): The patterns for the new model.\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n             If specified, only the models implemented in those frameworks will be added.\n         with_processing (`bool`, *optional*, defaults to `True`):\n             Whether the tokenizer/feature extractor/processor of the model should also be added to the init or not.\n@@ -1068,15 +1068,15 @@ def insert_tokenizer_in_auto_module(old_model_patterns: ModelPatterns, new_model\n \n \n def add_model_to_auto_classes(\n-    old_model_patterns: ModelPatterns, new_model_patterns: ModelPatterns, model_classes: Dict[str, List[str]]\n+    old_model_patterns: ModelPatterns, new_model_patterns: ModelPatterns, model_classes: dict[str, list[str]]\n ):\n     \"\"\"\n     Add a model to the relevant mappings in the auto module.\n \n     Args:\n         old_model_patterns (`ModelPatterns`): The patterns for the old model.\n         new_model_patterns (`ModelPatterns`): The patterns for the new model.\n-        model_classes (`Dict[str, List[str]]`): A dictionary framework to list of model classes implemented.\n+        model_classes (`dict[str, list[str]]`): A dictionary framework to list of model classes implemented.\n     \"\"\"\n     for filename in AUTO_CLASSES_PATTERNS:\n         # Extend patterns with all model classes if necessary\n@@ -1169,7 +1169,7 @@ def duplicate_doc_file(\n     old_model_patterns: ModelPatterns,\n     new_model_patterns: ModelPatterns,\n     dest_file: Optional[Union[str, os.PathLike]] = None,\n-    frameworks: Optional[List[str]] = None,\n+    frameworks: Optional[list[str]] = None,\n ):\n     \"\"\"\n     Duplicate a documentation file and adapts it for a new model.\n@@ -1180,7 +1180,7 @@ def duplicate_doc_file(\n         new_model_patterns (`ModelPatterns`): The patterns for the new model.\n         dest_file (`str` or `os.PathLike`, *optional*): Path to the new doc file.\n             Will default to the a file named `{new_model_patterns.model_type}.md` in the same folder as `module_file`.\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n             If passed, will only keep the model classes corresponding to this list of frameworks in the new doc file.\n     \"\"\"\n     with open(doc_file, \"r\", encoding=\"utf-8\") as f:\n@@ -1320,7 +1320,7 @@ def create_new_model_like(\n     model_type: str,\n     new_model_patterns: ModelPatterns,\n     add_copied_from: bool = True,\n-    frameworks: Optional[List[str]] = None,\n+    frameworks: Optional[list[str]] = None,\n     old_checkpoint: Optional[str] = None,\n     create_fast_image_processor: bool = False,\n ):\n@@ -1332,7 +1332,7 @@ def create_new_model_like(\n         new_model_patterns (`ModelPatterns`): The patterns for the new model.\n         add_copied_from (`bool`, *optional*, defaults to `True`):\n             Whether or not to add \"Copied from\" statements to all classes in the new model modeling files.\n-        frameworks (`List[str]`, *optional*):\n+        frameworks (`list[str]`, *optional*):\n             If passed, will limit the duplicate to the frameworks specified.\n         old_checkpoint (`str`, *optional*):\n             The name of the base checkpoint for the old model. Should be passed along when it can't be automatically"
        },
        {
            "sha": "2329f3b7501c258c262a63509aa64693ceb5db0d",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n from argparse import ArgumentParser, Namespace\n-from typing import Any, List, Optional\n+from typing import Any, Optional\n \n from ..pipelines import Pipeline, get_supported_tasks, pipeline\n from ..utils import logging\n@@ -69,8 +69,8 @@ class ServeTokenizeResult(BaseModel):\n     Tokenize result model\n     \"\"\"\n \n-    tokens: List[str]\n-    tokens_ids: Optional[List[int]]\n+    tokens: list[str]\n+    tokens_ids: Optional[list[int]]\n \n \n class ServeDeTokenizeResult(BaseModel):\n@@ -196,7 +196,7 @@ def tokenize(self, text_input: str = Body(None, embed=True), return_ids: bool =\n \n     def detokenize(\n         self,\n-        tokens_ids: List[int] = Body(None, embed=True),\n+        tokens_ids: list[int] = Body(None, embed=True),\n         skip_special_tokens: bool = Body(False, embed=True),\n         cleanup_tokenization_spaces: bool = Body(True, embed=True),\n     ):"
        },
        {
            "sha": "46f1b14414f936ac0ff5715f7dd84d8d6aeec498",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -63,13 +63,13 @@ class PretrainedConfig(PushToHubMixin):\n       Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,\n       (but not necessarily) such as [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`]. They have to be initialized from\n       two or more configs of type [`~transformers.PretrainedConfig`].\n-    - **keys_to_ignore_at_inference** (`List[str]`) -- A list of keys to ignore by default when looking at dictionary\n+    - **keys_to_ignore_at_inference** (`list[str]`) -- A list of keys to ignore by default when looking at dictionary\n       outputs of the model during inference.\n-    - **attribute_map** (`Dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n+    - **attribute_map** (`dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n       naming of attributes.\n-    - **base_model_tp_plan** (`Dict[str, Any]`) -- A dict that maps sub-modules FQNs of a base model to a tensor\n+    - **base_model_tp_plan** (`dict[str, Any]`) -- A dict that maps sub-modules FQNs of a base model to a tensor\n       parallel plan applied to the sub-module when `model.tensor_parallel` is called.\n-    - **base_model_pp_plan** (`Dict[str, Tuple[List[str]]]`) -- A dict that maps child-modules of a base model to a\n+    - **base_model_pp_plan** (`dict[str, tuple[list[str]]]`) -- A dict that maps child-modules of a base model to a\n       pipeline parallel plan that enables users to place the child-module on the appropriate device.\n \n     Common attributes (present in all subclasses):\n@@ -115,7 +115,7 @@ class PretrainedConfig(PushToHubMixin):\n         tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n             Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n             and decoder model to have the exact same parameter names.\n-        prune_heads (`Dict[int, List[int]]`, *optional*, defaults to `{}`):\n+        prune_heads (`dict[int, list[int]]`, *optional*, defaults to `{}`):\n             Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n             heads to prune in said layer.\n \n@@ -128,17 +128,17 @@ class PretrainedConfig(PushToHubMixin):\n \n         > Parameters for fine-tuning tasks\n \n-        architectures (`List[str]`, *optional*):\n+        architectures (`list[str]`, *optional*):\n             Model architectures that can be used with the model pretrained weights.\n         finetuning_task (`str`, *optional*):\n             Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow\n             or PyTorch) checkpoint.\n-        id2label (`Dict[int, str]`, *optional*):\n+        id2label (`dict[int, str]`, *optional*):\n             A map from index (for instance prediction index, or target index) to label.\n-        label2id (`Dict[str, int]`, *optional*): A map from label to index for the model.\n+        label2id (`dict[str, int]`, *optional*): A map from label to index for the model.\n         num_labels (`int`, *optional*):\n             Number of labels to use in the last layer added to the model, typically for a classification task.\n-        task_specific_params (`Dict[str, Any]`, *optional*):\n+        task_specific_params (`dict[str, Any]`, *optional*):\n             Additional keyword arguments to store for the current task.\n         problem_type (`str`, *optional*):\n             Problem type for `XxxForSequenceClassification` models. Can be one of `\"regression\"`,\n@@ -394,7 +394,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                 repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                 namespace).\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n         self._set_token_in_kwargs(kwargs)\n@@ -505,7 +505,7 @@ def from_pretrained(\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or `bool`, *optional*):\n@@ -531,7 +531,7 @@ def from_pretrained(\n             subfolder (`str`, *optional*, defaults to `\"\"`):\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                 specify the folder name here.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n                 values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n                 by the `return_unused_kwargs` keyword parameter.\n@@ -599,7 +599,7 @@ def get_config_dict(\n                 The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n \n         Returns:\n-            `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n+            `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n \n         \"\"\"\n         cls._set_token_in_kwargs(kwargs)\n@@ -723,10 +723,10 @@ def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n         Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n \n         Args:\n-            config_dict (`Dict[str, Any]`):\n+            config_dict (`dict[str, Any]`):\n                 Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n                 retrieved from a pretrained checkpoint by leveraging the [`~PretrainedConfig.get_config_dict`] method.\n-            kwargs (`Dict[str, Any]`):\n+            kwargs (`dict[str, Any]`):\n                 Additional parameters from which to initialize the configuration object.\n \n         Returns:\n@@ -816,7 +816,7 @@ def to_diff_dict(self) -> dict[str, Any]:\n         Python dictionary.\n \n         Returns:\n-            Dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n+            dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         config_dict = self.to_dict()\n \n@@ -874,7 +874,7 @@ def to_dict(self) -> dict[str, Any]:\n         Serializes this instance to a Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n         if hasattr(self.__class__, \"model_type\"):\n@@ -940,7 +940,7 @@ def update(self, config_dict: dict[str, Any]):\n         Updates attributes of this class with attributes from `config_dict`.\n \n         Args:\n-            config_dict (`Dict[str, Any]`): Dictionary of attributes that should be updated for this class.\n+            config_dict (`dict[str, Any]`): Dictionary of attributes that should be updated for this class.\n         \"\"\"\n         for key, value in config_dict.items():\n             setattr(self, key, value)\n@@ -1163,7 +1163,7 @@ def get_configuration_file(configuration_files: list[str]) -> str:\n     Get the configuration file to use for this version of transformers.\n \n     Args:\n-        configuration_files (`List[str]`): The list of available configuration files.\n+        configuration_files (`list[str]`): The list of available configuration files.\n \n     Returns:\n         `str`: The configuration file to use."
        },
        {
            "sha": "fcefc9270efe51a7a2e58d9fd58b8ae5f3df56b0",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n from collections.abc import Mapping\n from dataclasses import dataclass\n from random import randint\n-from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n+from typing import Any, Callable, NewType, Optional, Union\n \n import numpy as np\n \n@@ -33,7 +33,7 @@\n A DataCollator is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary\n of PyTorch/TensorFlow tensors or NumPy arrays.\n \"\"\"\n-DataCollator = NewType(\"DataCollator\", Callable[[List[InputDataClass]], Dict[str, Any]])\n+DataCollator = NewType(\"DataCollator\", Callable[[list[InputDataClass]], dict[str, Any]])\n \n \n class DataCollatorMixin:\n@@ -72,7 +72,7 @@ def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n     return padded\n \n \n-def default_data_collator(features: List[InputDataClass], return_tensors=\"pt\") -> Dict[str, Any]:\n+def default_data_collator(features: list[InputDataClass], return_tensors=\"pt\") -> dict[str, Any]:\n     \"\"\"\n     Very simple data collator that simply collates batches of dict-like objects and performs special handling for\n     potential keys named:\n@@ -119,13 +119,13 @@ class DefaultDataCollator(DataCollatorMixin):\n \n     return_tensors: str = \"pt\"\n \n-    def __call__(self, features: List[Dict[str, Any]], return_tensors=None) -> Dict[str, Any]:\n+    def __call__(self, features: list[dict[str, Any]], return_tensors=None) -> dict[str, Any]:\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n         return default_data_collator(features, return_tensors)\n \n \n-def torch_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n+def torch_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]:\n     import torch\n \n     if not isinstance(features[0], Mapping):\n@@ -161,7 +161,7 @@ def torch_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any\n     return batch\n \n \n-def tf_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n+def tf_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]:\n     import tensorflow as tf\n \n     if not isinstance(features[0], Mapping):\n@@ -202,7 +202,7 @@ def tf_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n     return batch\n \n \n-def numpy_default_data_collator(features: List[InputDataClass]) -> Dict[str, Any]:\n+def numpy_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]:\n     if not isinstance(features[0], Mapping):\n         features = [vars(f) for f in features]\n     first = features[0]\n@@ -268,7 +268,7 @@ class DataCollatorWithPadding:\n     pad_to_multiple_of: Optional[int] = None\n     return_tensors: str = \"pt\"\n \n-    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n+    def __call__(self, features: list[dict[str, Any]]) -> dict[str, Any]:\n         batch = pad_without_fast_tokenizer_warning(\n             self.tokenizer,\n             features,\n@@ -569,7 +569,7 @@ class DataCollatorForMultipleChoice(DataCollatorMixin):\n     pad_to_multiple_of: Optional[int] = None\n     return_tensors: str = \"pt\"\n \n-    def torch_call(self, examples: List[Dict[str, Any]]):  # Refactored implementation from the docs.\n+    def torch_call(self, examples: list[dict[str, Any]]):  # Refactored implementation from the docs.\n         import torch\n \n         # Take labels out of the examples beforehand, because they aren't nested.\n@@ -911,7 +911,7 @@ def tf_bernoulli(shape, probability, generator=None):\n \n     def tf_mask_tokens(\n         self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Optional[Any] = None\n-    ) -> Tuple[Any, Any]:\n+    ) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n         \"\"\"\n@@ -956,7 +956,7 @@ def tf_mask_tokens(\n         # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         import tensorflow as tf\n \n         if self.seed and self.generator is None:\n@@ -1002,7 +1002,7 @@ def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict\n             batch[\"labels\"] = labels\n         return batch\n \n-    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n \n         if self.seed and self.generator is None:\n@@ -1032,7 +1032,7 @@ def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> D\n             batch[\"labels\"] = labels\n         return batch\n \n-    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n+    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n         \"\"\"\n@@ -1081,7 +1081,7 @@ def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n         # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n \n         if self.seed and self.generator is None:\n@@ -1111,7 +1111,7 @@ def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> D\n             batch[\"labels\"] = labels\n         return batch\n \n-    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n+    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n         \"\"\"\n@@ -1193,7 +1193,7 @@ class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):\n \n     </Tip>\"\"\"\n \n-    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if self.seed and self.generator is None:\n             # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n             # If no seed supplied, we will use the global RNG\n@@ -1226,7 +1226,7 @@ def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> D\n         inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n         return {\"input_ids\": inputs, \"labels\": labels}\n \n-    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         import tensorflow as tf\n \n         if self.seed and self.generator is None:\n@@ -1261,7 +1261,7 @@ def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict\n         inputs, labels = self.tf_mask_tokens(tf.cast(batch_input, tf.int64), batch_mask)\n         return {\"input_ids\": inputs, \"labels\": labels}\n \n-    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if self.seed and self.generator is None:\n             # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n             # If no seed supplied, we will use the global RNG\n@@ -1318,7 +1318,7 @@ def _shuffle(self, cand_indexes):\n             self.generator.shuffle(cand_indexes)\n             return cand_indexes\n \n-    def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n+    def _whole_word_mask(self, input_tokens: list[str], max_predictions=512):\n         \"\"\"\n         Get 0/1 labels for masked tokens with whole word mask proxy\n         \"\"\"\n@@ -1358,7 +1358,7 @@ def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n         mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n         return mask_labels\n \n-    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n+    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n         'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n@@ -1414,7 +1414,7 @@ def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n         # The rest of the time ((1-random_replacement_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n+    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n         'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n@@ -1474,7 +1474,7 @@ def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n         # The rest of the time ((1-mask_replace_prob-random_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n+    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set\n         'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.\n@@ -1564,7 +1564,7 @@ def __init__(self, *args, **kwargs):\n             FutureWarning,\n         )\n \n-    def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:\n+    def __call__(self, examples: list[dict[str, Any]]) -> dict[str, Any]:\n         import torch\n         from torch.nn.utils.rnn import pad_sequence\n \n@@ -1587,7 +1587,7 @@ def __call__(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:\n             \"sentence_order_label\": sentence_order_label,\n         }\n \n-    def mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any]:\n+    def mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels/attention_mask for masked language modeling: 80% MASK, 10% random, 10%\n         original. N-gram not applied yet.\n@@ -1645,28 +1645,28 @@ class DataCollatorForPermutationLanguageModeling(DataCollatorMixin):\n     max_span_length: int = 5  # maximum length of a span of masked tokens\n     return_tensors: str = \"pt\"\n \n-    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if isinstance(examples[0], Mapping):\n             examples = [e[\"input_ids\"] for e in examples]\n         batch = _torch_collate_batch(examples, self.tokenizer)\n         inputs, perm_mask, target_mapping, labels = self.torch_mask_tokens(batch)\n         return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n \n-    def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if isinstance(examples[0], Mapping):\n             examples = [e[\"input_ids\"] for e in examples]\n         batch = _tf_collate_batch(examples, self.tokenizer)\n         inputs, perm_mask, target_mapping, labels = self.tf_mask_tokens(batch)\n         return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n \n-    def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n+    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n         if isinstance(examples[0], Mapping):\n             examples = [e[\"input_ids\"] for e in examples]\n         batch = _numpy_collate_batch(examples, self.tokenizer)\n         inputs, perm_mask, target_mapping, labels = self.numpy_mask_tokens(batch)\n         return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n \n-    def torch_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n+    def torch_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]:\n         \"\"\"\n         The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n \n@@ -1765,7 +1765,7 @@ def torch_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n \n         return inputs.long(), perm_mask, target_mapping, labels.long()\n \n-    def tf_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n+    def tf_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]:\n         \"\"\"\n         The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n \n@@ -1872,7 +1872,7 @@ def tf_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n \n         return tf.cast(inputs, tf.int64), tf.cast(perm_mask, tf.float32), target_mapping, tf.cast(labels, tf.int64)\n \n-    def numpy_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n+    def numpy_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]:\n         \"\"\"\n         The masked tokens to be predicted for a particular sequence are determined by the following algorithm:\n "
        },
        {
            "sha": "d8db0dfebac1a8432d18320df4f1f4eba4eb4030",
            "filename": "src/transformers/data/datasets/glue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import warnings\n from dataclasses import dataclass, field\n from enum import Enum\n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n import torch\n from filelock import FileLock\n@@ -75,7 +75,7 @@ class GlueDataset(Dataset):\n \n     args: GlueDataTrainingArguments\n     output_mode: str\n-    features: List[InputFeatures]\n+    features: list[InputFeatures]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "07250ef3cb5402603c75ed2c1a4c2e2200fb3dbe",
            "filename": "src/transformers/data/datasets/language_modeling.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import random\n import time\n import warnings\n-from typing import Dict, List, Optional\n+from typing import Optional\n \n import torch\n from filelock import FileLock\n@@ -139,7 +139,7 @@ def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: i\n     def __len__(self):\n         return len(self.examples)\n \n-    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n+    def __getitem__(self, i) -> dict[str, torch.tensor]:\n         return self.examples[i]\n \n \n@@ -187,7 +187,7 @@ def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: i\n     def __len__(self):\n         return len(self.examples)\n \n-    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n+    def __getitem__(self, i) -> dict[str, torch.tensor]:\n         return self.examples[i]\n \n \n@@ -339,7 +339,7 @@ def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n     def __len__(self):\n         return len(self.examples)\n \n-    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n+    def __getitem__(self, i) -> dict[str, torch.tensor]:\n         return self.examples[i]\n \n \n@@ -433,7 +433,7 @@ def __init__(\n                     f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n                 )\n \n-    def create_examples_from_document(self, document: List[List[int]], doc_index: int, block_size: int):\n+    def create_examples_from_document(self, document: list[list[int]], doc_index: int, block_size: int):\n         \"\"\"Creates examples for a single document.\"\"\"\n \n         max_num_tokens = block_size - self.tokenizer.num_special_tokens_to_add(pair=True)"
        },
        {
            "sha": "fdee571e249ba428dd43d1f4760de37ec094c1b1",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n import time\n from dataclasses import dataclass, field\n from enum import Enum\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import torch\n from filelock import FileLock\n@@ -112,7 +112,7 @@ class SquadDataset(Dataset):\n     \"\"\"\n \n     args: SquadDataTrainingArguments\n-    features: List[SquadFeatures]\n+    features: list[SquadFeatures]\n     mode: Split\n     is_language_sensitive: bool\n \n@@ -195,7 +195,7 @@ def __init__(\n     def __len__(self):\n         return len(self.features)\n \n-    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n+    def __getitem__(self, i) -> dict[str, torch.Tensor]:\n         # Convert to Tensors and build dataset\n         feature = self.features[i]\n "
        },
        {
            "sha": "e005c9bcda13d15bc3aa32a50c79941166d0ba28",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import warnings\n from dataclasses import asdict\n from enum import Enum\n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import is_tf_available, logging\n@@ -39,7 +39,7 @@\n \n \n def glue_convert_examples_to_features(\n-    examples: Union[List[InputExample], \"tf.data.Dataset\"],\n+    examples: Union[list[InputExample], \"tf.data.Dataset\"],\n     tokenizer: PreTrainedTokenizer,\n     max_length: Optional[int] = None,\n     task=None,\n@@ -107,7 +107,7 @@ def gen():\n \n \n def _glue_convert_examples_to_features(\n-    examples: List[InputExample],\n+    examples: list[InputExample],\n     tokenizer: PreTrainedTokenizer,\n     max_length: Optional[int] = None,\n     task=None,"
        },
        {
            "sha": "462156ebac384e08d78a7b42ea06f35a457e5feb",
            "filename": "src/transformers/data/processors/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import dataclasses\n import json\n from dataclasses import dataclass\n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...utils import is_tf_available, is_torch_available, logging\n \n@@ -67,9 +67,9 @@ class InputFeatures:\n             float for regression problems.\n     \"\"\"\n \n-    input_ids: List[int]\n-    attention_mask: Optional[List[int]] = None\n-    token_type_ids: Optional[List[int]] = None\n+    input_ids: list[int]\n+    attention_mask: Optional[list[int]] = None\n+    token_type_ids: Optional[list[int]] = None\n     label: Optional[Union[int, float]] = None\n \n     def to_json_string(self):"
        },
        {
            "sha": "100ee85121c0139cb88f98dfa945dcc09b3d4020",
            "filename": "src/transformers/debug_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdebug_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdebug_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdebug_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -136,7 +136,7 @@ class DebugUnderflowOverflow:\n             The model to debug.\n         max_frames_to_save (`int`, *optional*, defaults to 21):\n             How many frames back to record\n-        trace_batch_nums(`List[int]`, *optional*, defaults to `[]`):\n+        trace_batch_nums(`list[int]`, *optional*, defaults to `[]`):\n             Which batch numbers to trace (turns detection off)\n         abort_after_batch_num  (`int``, *optional*):\n             Whether to abort after a certain batch number has finished"
        },
        {
            "sha": "6a88859e0aa084b782b50a82e8eaab2e3281a123",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -317,7 +317,7 @@ def get_cached_module_file(\n         resume_download:\n             Deprecated and ignored. All downloads are now resumed by default when possible.\n             Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n+        proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n         token (`str` or *bool*, *optional*):\n@@ -507,7 +507,7 @@ def get_class_from_dynamic_module(\n         resume_download:\n             Deprecated and ignored. All downloads are now resumed by default when possible.\n             Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n+        proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n         token (`str` or `bool`, *optional*):\n@@ -593,7 +593,7 @@ def custom_object_save(obj: Any, folder: Union[str, os.PathLike], config: Option\n             A config in which to register the auto_map corresponding to this custom object.\n \n     Returns:\n-        `List[str]`: The list of files saved.\n+        `list[str]`: The list of files saved.\n     \"\"\"\n     if obj.__module__ == \"__main__\":\n         logger.warning(\n@@ -762,7 +762,7 @@ def check_python_requirements(path_or_repo_id, requirements_file=\"requirements.t\n             This can be either:\n             - a string, the *model id* of a model repo on huggingface.co.\n             - a path to a *directory* potentially containing the file.\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional arguments to pass to `cached_file`.\n     \"\"\"\n     failed = []  # error messages regarding requirements"
        },
        {
            "sha": "186f3e311152fa808588ec153c6dc754add1278f",
            "filename": "src/transformers/feature_extraction_sequence_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -81,13 +81,13 @@ def pad(\n         </Tip>\n \n         Args:\n-            processed_features ([`BatchFeature`], list of [`BatchFeature`], `Dict[str, List[float]]`, `Dict[str, List[List[float]]` or `List[Dict[str, List[float]]]`):\n-                Processed inputs. Can represent one input ([`BatchFeature`] or `Dict[str, List[float]]`) or a batch of\n-                input values / vectors (list of [`BatchFeature`], *Dict[str, List[List[float]]]* or *List[Dict[str,\n-                List[float]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n+            processed_features ([`BatchFeature`], list of [`BatchFeature`], `dict[str, list[float]]`, `dict[str, list[list[float]]` or `list[dict[str, list[float]]]`):\n+                Processed inputs. Can represent one input ([`BatchFeature`] or `dict[str, list[float]]`) or a batch of\n+                input values / vectors (list of [`BatchFeature`], *dict[str, list[list[float]]]* or *list[dict[str,\n+                list[float]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n                 collate function.\n \n-                Instead of `List[float]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n+                Instead of `list[float]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),\n                 see the note above for the return type.\n             padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n@@ -235,9 +235,9 @@ def _pad(\n         Pad inputs (on left/right and up to predefined length or max length in the batch)\n \n         Args:\n-            processed_features (`Union[Dict[str, np.ndarray], BatchFeature]`):\n-                Dictionary of input values (`np.ndarray[float]`) / input vectors (`List[np.ndarray[float]]`) or batch\n-                of inputs values (`List[np.ndarray[int]]`) / input vectors (`List[np.ndarray[int]]`)\n+            processed_features (`Union[dict[str, np.ndarray], BatchFeature]`):\n+                Dictionary of input values (`np.ndarray[float]`) / input vectors (`list[np.ndarray[float]]`) or batch\n+                of inputs values (`list[np.ndarray[int]]`) / input vectors (`list[np.ndarray[int]]`)\n             max_length (`int`, *optional*):\n                 Maximum length of the returned list and optionally padding length (see below)\n             padding_strategy (`PaddingStrategy`, *optional*, default to `PaddingStrategy.DO_NOT_PAD`):\n@@ -306,9 +306,9 @@ def _truncate(\n         Truncate inputs to predefined length or max length in the batch\n \n         Args:\n-            processed_features(`Union[Dict[str, np.ndarray], BatchFeature]`):\n-                Dictionary of input values (`np.ndarray[float]`) / input vectors (`List[np.ndarray[float]]`) or batch\n-                of inputs values (`List[np.ndarray[int]]`) / input vectors (`List[np.ndarray[int]]`)\n+            processed_features(`Union[dict[str, np.ndarray], BatchFeature]`):\n+                Dictionary of input values (`np.ndarray[float]`) / input vectors (`list[np.ndarray[float]]`) or batch\n+                of inputs values (`list[np.ndarray[int]]`) / input vectors (`list[np.ndarray[int]]`)\n             max_length (`int`, *optional*):\n                 maximum length of the returned list and optionally padding length (see below)\n             pad_to_multiple_of (`int`, *optional*) :"
        },
        {
            "sha": "0d08f225e5e7da242f57380b73ba66c908e9160c",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -303,7 +303,7 @@ def from_pretrained(\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or `bool`, *optional*):\n@@ -326,7 +326,7 @@ def from_pretrained(\n                 functions returns a `Tuple(feature_extractor, unused_kwargs)` where *unused_kwargs* is a dictionary\n                 consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of\n                 `kwargs` which has not been used to update `feature_extractor` and is otherwise ignored.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are feature extractor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is\n                 controlled by the `return_unused_kwargs` keyword parameter.\n@@ -392,7 +392,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                 repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                 namespace).\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -454,7 +454,7 @@ def get_feature_extractor_dict(\n                 The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n \n         Returns:\n-            `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the feature extractor object.\n+            `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the feature extractor object.\n         \"\"\"\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n@@ -555,11 +555,11 @@ def from_dict(cls, feature_extractor_dict: dict[str, Any], **kwargs) -> PreTrain\n         parameters.\n \n         Args:\n-            feature_extractor_dict (`Dict[str, Any]`):\n+            feature_extractor_dict (`dict[str, Any]`):\n                 Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be\n                 retrieved from a pretrained checkpoint by leveraging the\n                 [`~feature_extraction_utils.FeatureExtractionMixin.to_dict`] method.\n-            kwargs (`Dict[str, Any]`):\n+            kwargs (`dict[str, Any]`):\n                 Additional parameters from which to initialize the feature extractor object.\n \n         Returns:\n@@ -588,7 +588,7 @@ def from_dict(cls, feature_extractor_dict: dict[str, Any], **kwargs) -> PreTrain\n     def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n         output[\"feature_extractor_type\"] = self.__class__.__name__"
        },
        {
            "sha": "498ebffd2072e057beaf6a72bca27b368c2f9a34",
            "filename": "src/transformers/generation/beam_constraints.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fbeam_constraints.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fbeam_constraints.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_constraints.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -1,5 +1,5 @@\n from abc import ABC, abstractmethod\n-from typing import List, Optional\n+from typing import Optional\n \n \n class Constraint(ABC):\n@@ -51,7 +51,7 @@ def advance(self):\n         When called, returns the token(s) that would take this constraint one step closer to being fulfilled.\n \n         Return:\n-            token_ids (Union[int, List[int], None]):\n+            token_ids (Union[int, list[int], None]):\n                 - A single token ID (int) that advances the constraint, or\n                 - A list of token IDs that could advance the constraint\n                 - None if the constraint is completed or cannot be advanced\n@@ -134,11 +134,11 @@ class PhrasalConstraint(Constraint):\n     [`Constraint`] enforcing that an ordered sequence of tokens is included in the output.\n \n     Args:\n-        token_ids (`List[int]`):\n+        token_ids (`list[int]`):\n             The id of the token that must be generated by the output.\n     \"\"\"\n \n-    def __init__(self, token_ids: List[int]):\n+    def __init__(self, token_ids: list[int]):\n         super(Constraint, self).__init__()\n \n         if not isinstance(token_ids, list) or len(token_ids) == 0:\n@@ -205,7 +205,7 @@ def copy(self, stateful=False):\n \n \n class DisjunctiveTrie:\n-    def __init__(self, nested_token_ids: List[List[int]], no_subsets=True):\n+    def __init__(self, nested_token_ids: list[list[int]], no_subsets=True):\n         r\"\"\"\n         A helper class that builds a trie with the words represented in `nested_token_ids`.\n         \"\"\"\n@@ -266,12 +266,12 @@ class DisjunctiveConstraint(Constraint):\n     A special [`Constraint`] that is fulfilled by fulfilling just one of several constraints.\n \n     Args:\n-        nested_token_ids (`List[List[int]]`):\n+        nested_token_ids (`list[list[int]]`):\n             A list of words, where each word is a list of ids. This constraint is fulfilled by generating just one from\n             the list of words.\n     \"\"\"\n \n-    def __init__(self, nested_token_ids: List[List[int]]):\n+    def __init__(self, nested_token_ids: list[list[int]]):\n         super(Constraint, self).__init__()\n \n         if not isinstance(nested_token_ids, list) or len(nested_token_ids) == 0:\n@@ -356,11 +356,11 @@ class ConstraintListState:\n     A class for beam scorers to track its progress through a list of constraints.\n \n     Args:\n-        constraints (`List[Constraint]`):\n+        constraints (`list[Constraint]`):\n             A list of [`Constraint`] objects that must be fulfilled by the beam scorer.\n     \"\"\"\n \n-    def __init__(self, constraints: List[Constraint]):\n+    def __init__(self, constraints: list[Constraint]):\n         self.constraints = constraints\n \n         # max # of steps required to fulfill a given constraint\n@@ -418,7 +418,7 @@ def advance(self):\n         else:\n             return token_list\n \n-    def reset(self, token_ids: Optional[List[int]]):\n+    def reset(self, token_ids: Optional[list[int]]):\n         \"\"\"\n         token_ids: the tokens generated thus far to reset the state of the progress through constraints.\n         \"\"\""
        },
        {
            "sha": "9db8694c2349c7e42b2ca1232ee1dbe3e76f722b",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \n from abc import ABC, abstractmethod\n from collections import UserDict\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -41,7 +41,7 @@\n             Beam indices indicating to which beam hypothesis the `next_tokens` correspond.\n         pad_token_id (`int`, *optional*):\n             The id of the *padding* token.\n-        eos_token_id (`Union[int, List[int]]`, *optional*):\n+        eos_token_id (`Union[int, list[int]]`, *optional*):\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n         beam_indices (`torch.LongTensor`, *optional*):\n             Beam indices indicating to which beam hypothesis each token correspond.\n@@ -77,7 +77,7 @@\n             The beam indices indicating to which beam the `final_beam_tokens` shall be added.\n         pad_token_id (`int`, *optional*):\n             The id of the *padding* token.\n-        eos_token_id (`Union[int, List[int]]`, *optional*):\n+        eos_token_id (`Union[int, list[int]]`, *optional*):\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n \n     Return:\n@@ -103,7 +103,7 @@ def process(\n         next_tokens: torch.LongTensor,\n         next_indices: torch.LongTensor,\n         **kwargs,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         raise NotImplementedError(\"This is an abstract method.\")\n \n     @abstractmethod\n@@ -219,11 +219,11 @@ def process(\n         next_tokens: torch.LongTensor,\n         next_indices: torch.LongTensor,\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n+        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n         group_index: Optional[int] = 0,\n         decoder_prompt_len: Optional[int] = 0,\n-    ) -> Dict[str, torch.Tensor]:\n+    ) -> dict[str, torch.Tensor]:\n         # add up to the length which the next_scores is calculated on (including decoder prompt)\n         cur_len = input_ids.shape[-1] + 1\n         batch_size = len(self._beam_hyps) // self.num_beam_groups\n@@ -325,10 +325,10 @@ def finalize(\n         final_beam_indices: torch.LongTensor,\n         max_length: int,\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n+        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n         decoder_prompt_len: Optional[int] = 0,\n-    ) -> Tuple[torch.LongTensor]:\n+    ) -> tuple[torch.LongTensor]:\n         batch_size = len(self._beam_hyps) // self.num_beam_groups\n \n         if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n@@ -426,7 +426,7 @@ class ConstrainedBeamSearchScorer(BeamScorer):\n             Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n         num_beams (`int`):\n             Number of beams for beam search.\n-        constraints (`List[Constraint]`):\n+        constraints (`list[Constraint]`):\n             A list of positive constraints represented as `Constraint` objects that must be fulfilled in the generation\n             output. For more information, the documentation of [`Constraint`] should be read.\n         device (`torch.device`):\n@@ -457,7 +457,7 @@ def __init__(\n         self,\n         batch_size: int,\n         num_beams: int,\n-        constraints: List[Constraint],\n+        constraints: list[Constraint],\n         device: torch.device,\n         length_penalty: Optional[float] = 1.0,\n         do_early_stopping: Optional[Union[bool, str]] = False,\n@@ -518,10 +518,10 @@ def process(\n         next_indices: torch.LongTensor,\n         scores_for_all_vocab: torch.FloatTensor,\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n+        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n         decoder_prompt_len: Optional[int] = 0,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n@@ -541,7 +541,7 @@ def process(\n                 The scores of all tokens in the vocabulary for each of the beam hypotheses.\n             pad_token_id (`int`, *optional*):\n                 The id of the *padding* token.\n-            eos_token_id (`Union[int, List[int]]`, *optional*):\n+            eos_token_id (`Union[int, list[int]]`, *optional*):\n                 The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n             beam_indices (`torch.LongTensor`, *optional*):\n                 Beam indices indicating to which beam hypothesis each token correspond.\n@@ -818,10 +818,10 @@ def finalize(\n         final_beam_indices: torch.LongTensor,\n         max_length: int,\n         pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n-        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n+        eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None,\n         beam_indices: Optional[torch.LongTensor] = None,\n         decoder_prompt_len: Optional[int] = 0,\n-    ) -> Tuple[torch.LongTensor]:\n+    ) -> tuple[torch.LongTensor]:\n         batch_size = len(self._beam_hyps)\n \n         if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):"
        },
        {
            "sha": "94246b30f5aeffb171d4b69289df4c0fd17afc94",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \n import copy\n import weakref\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple\n+from typing import TYPE_CHECKING, Any, Optional\n \n import numpy as np\n import torch\n@@ -44,7 +44,7 @@\n class CandidateGenerator:\n     \"\"\"Abstract base class for all candidate generators that can be applied during assisted generation.\"\"\"\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -108,7 +108,7 @@ def __init__(\n         input_ids: torch.LongTensor,\n         assistant_model: \"PreTrainedModel\",\n         generation_config: \"GenerationConfig\",\n-        model_kwargs: Dict,\n+        model_kwargs: dict,\n         inputs_tensor: Optional[torch.Tensor] = None,\n         logits_processor: \"LogitsProcessorList\" = None,\n     ):\n@@ -198,7 +198,7 @@ def __init__(\n             self.probs = []\n             self.matches = []\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -281,7 +281,7 @@ def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.F\n \n                 self.assistant_model.generation_config.assistant_confidence_threshold = best_threshold\n \n-    def _calculate_new_tokens(self, input_ids: torch.LongTensor) -> Tuple[int, int]:\n+    def _calculate_new_tokens(self, input_ids: torch.LongTensor) -> tuple[int, int]:\n         \"\"\"Calculate the minimum and maximum number of new tokens to generate.\"\"\"\n         new_cur_len = input_ids.shape[-1]\n         max_new_tokens = min(int(self.num_assistant_tokens), self.generation_config.max_length - new_cur_len - 1)\n@@ -305,7 +305,7 @@ def _update_past_and_masks(\n \n         return has_past_key_values\n \n-    def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int) -> Dict:\n+    def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int) -> dict:\n         \"\"\"Prepare arguments for the generation call.\"\"\"\n         return {\n             self.input_ids_key: input_ids,\n@@ -315,7 +315,7 @@ def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens:\n             \"logits_processor\": self.logits_processor,\n         }\n \n-    def _generate_candidates(self, generation_args: Dict) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def _generate_candidates(self, generation_args: dict) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         \"\"\"Generate candidate sequences using the assistant model.\"\"\"\n         assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n         self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n@@ -374,7 +374,7 @@ def __init__(\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         generation_config: \"GenerationConfig\",\n-        model_kwargs: Dict,\n+        model_kwargs: dict,\n         inputs_tensor: Optional[torch.Tensor] = None,\n         logits_processor: \"LogitsProcessorList\" = None,\n     ):\n@@ -495,7 +495,7 @@ def convert_source_tokens_to_target_tokens(\n         dest_ids = destination_tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n         return dest_ids.to(input_ids.device)\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -537,7 +537,7 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n \n         return new_target_ids, None\n \n-    def _prepare_assistant_input_ids(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, int]:\n+    def _prepare_assistant_input_ids(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, int]:\n         \"\"\"Converts target input IDs to assistant input IDs, handling discrepancies.\"\"\"\n         convert_kwargs = {\n             \"source_tokenizer\": self.target_tokenizer,\n@@ -782,7 +782,7 @@ def _get_assistant_to_target_input_ids(self):\n \n         max_assistant_index = max(assistant_vocab.values())\n         assistant_to_target_input_ids = torch.full((max_assistant_index + 1,), self.SUPPRESS_TOKEN_ID, dtype=int)\n-        target_to_assistant_input_ids: Dict[int, int] = {}\n+        target_to_assistant_input_ids: dict[int, int] = {}\n         for tok, assistant_id in assistant_vocab.items():\n             target_id = target_vocab.get(tok)\n             if target_id is not None:\n@@ -909,7 +909,7 @@ def __init__(\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         generation_config: \"GenerationConfig\",\n-        model_kwargs: Dict,\n+        model_kwargs: dict,\n         atm_translator: AssistantToTargetTranslator,\n         inputs_tensor: Optional[torch.Tensor] = None,\n         logits_processor: \"LogitsProcessorList\" = None,\n@@ -930,7 +930,7 @@ def __init__(\n         self._target_seq_len_with_candidates: int = 0\n         self._prev_assistant_ids: Optional[torch.LongTensor] = None\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Simplified version of get_candidates that uses the translator cache for token conversion.\n         \"\"\"\n@@ -1043,7 +1043,7 @@ def __init__(\n         if self.max_matching_ngram_size <= 0 or self.num_output_tokens <= 0:\n             raise ValueError(\"Invalid max_matching_ngram_size or num_output_tokens\")\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -1153,7 +1153,7 @@ def __init__(\n         input_ids: torch.LongTensor,\n         assistant_model: \"PreTrainedModel\",\n         generation_config: \"GenerationConfig\",\n-        model_kwargs: Dict,\n+        model_kwargs: dict,\n         inputs_tensor: Optional[torch.Tensor] = None,\n         logits_processor: \"LogitsProcessorList\" = None,\n     ):\n@@ -1170,7 +1170,7 @@ def __init__(\n         self.assistant_early_exit = self.generation_config.assistant_early_exit\n         self.generation_config.assistant_early_exit = None\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n         # Temporarily sets the number of hidden layers to the early exit value\n         base_model = getattr(self.assistant_model, self.assistant_model.base_model_prefix)\n         original_num_hidden_layers = base_model.config.num_hidden_layers\n@@ -1221,7 +1221,7 @@ def _crop_past_key_values(model, past_key_values, max_length):\n     return past_key_values\n \n \n-def _prepare_attention_mask(model_kwargs: Dict[str, Any], new_length: int, is_encoder_decoder: bool) -> Dict[str, Any]:\n+def _prepare_attention_mask(model_kwargs: dict[str, Any], new_length: int, is_encoder_decoder: bool) -> dict[str, Any]:\n     \"\"\"Expands or crops the model's mask for decoding purposes, to the defined length\"\"\"\n \n     mask_key = \"decoder_attention_mask\" if is_encoder_decoder else \"attention_mask\"\n@@ -1257,7 +1257,7 @@ def _prepare_attention_mask(model_kwargs: Dict[str, Any], new_length: int, is_en\n     return model_kwargs\n \n \n-def _prepare_token_type_ids(model_kwargs: Dict[str, Any], new_length: int) -> Dict[str, Any]:\n+def _prepare_token_type_ids(model_kwargs: dict[str, Any], new_length: int) -> dict[str, Any]:\n     \"\"\"Expands or crops the model's token_type_ids for decoding purposes, to the defined length\"\"\"\n     if \"token_type_ids\" not in model_kwargs or model_kwargs[\"token_type_ids\"] is None:\n         return model_kwargs"
        },
        {
            "sha": "76587659371b24be8cfcbd3e8b405688b642e010",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -20,7 +20,7 @@\n import warnings\n from abc import ABC, abstractmethod\n from dataclasses import dataclass, is_dataclass\n-from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n \n from .. import __version__\n from ..configuration_utils import PretrainedConfig\n@@ -149,7 +149,7 @@ class GenerationConfig(PushToHubMixin):\n         max_time (`float`, *optional*):\n             The maximum amount of time you allow the computation to run for in seconds. generation will still finish\n             the current pass after allocated time has been passed.\n-        stop_strings (`str or List[str]`, *optional*):\n+        stop_strings (`str or list[str]`, *optional*):\n             A string or a list of strings that should terminate generation if the model outputs them.\n \n         > Parameters that control the generation strategy used\n@@ -163,7 +163,7 @@ class GenerationConfig(PushToHubMixin):\n             [this paper](https://huggingface.co/papers/1610.02424) for more details.\n         penalty_alpha (`float`, *optional*):\n             The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n-        dola_layers (`str` or `List[int]`, *optional*):\n+        dola_layers (`str` or `list[int]`, *optional*):\n             The layers to use for DoLa decoding. If `None`, DoLa decoding is not used. If a string, it must\n             be one of \"low\" or \"high\", which means using the lower part or higher part of the model layers, respectively.\n             \"low\" means the first half of the layers up to the first 20 layers, and \"high\" means the last half of the\n@@ -245,26 +245,26 @@ class GenerationConfig(PushToHubMixin):\n             `length_penalty` < 0.0 encourages shorter sequences.\n         no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n             If set to int > 0, all ngrams of that size can only occur once.\n-        bad_words_ids (`List[List[int]]`, *optional*):\n+        bad_words_ids (`list[list[int]]`, *optional*):\n             List of list of token ids that are not allowed to be generated. Check\n             [`~generation.NoBadWordsLogitsProcessor`] for further documentation and examples.\n-        force_words_ids (`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n-            List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple list of\n-            words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`, this\n+        force_words_ids (`list[list[int]]` or `list[list[list[int]]]`, *optional*):\n+            List of token ids that must be generated. If given a `list[list[int]]`, this is treated as a simple list of\n+            words that must be included, the opposite to `bad_words_ids`. If given `list[list[list[int]]]`, this\n             triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081), where one\n             can allow different forms of each word.\n         renormalize_logits (`bool`, *optional*, defaults to `False`):\n             Whether to renormalize the logits after applying all the logits processors (including the custom\n             ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the score logits\n             are normalized but some logit processors break the normalization.\n-        constraints (`List[Constraint]`, *optional*):\n+        constraints (`list[Constraint]`, *optional*):\n             Custom constraints that can be added to the generation to ensure that the output will contain the use of\n             certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n         forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n             The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful for\n             multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be the target\n             language token.\n-        forced_eos_token_id (`int` or List[int]`, *optional*, defaults to `model.config.forced_eos_token_id`):\n+        forced_eos_token_id (`int` or list[int]`, *optional*, defaults to `model.config.forced_eos_token_id`):\n             The id of the token to force as the last generated token when `max_length` is reached. Optionally, use a\n             list to set multiple *end-of-sequence* tokens.\n         remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n@@ -274,13 +274,13 @@ class GenerationConfig(PushToHubMixin):\n             This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n             generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where\n             penalty starts and `decay_factor` represents the factor of exponential decay\n-        suppress_tokens (`List[int]`, *optional*):\n+        suppress_tokens (`list[int]`, *optional*):\n             A list of tokens that will be suppressed at generation. The `SupressTokens` logit processor will set their\n             log probs to `-inf` so that they are not sampled.\n-        begin_suppress_tokens  (`List[int]`, *optional*):\n+        begin_suppress_tokens  (`list[int]`, *optional*):\n             A list of tokens that will be suppressed at the beginning of the generation. The `SupressBeginTokens` logit\n             processor will set their log probs to `-inf` so that they are not sampled.\n-        sequence_bias (`Dict[Tuple[int], float]`, *optional*)):\n+        sequence_bias (`dict[tuple[int], float]`, *optional*)):\n             Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n             sequence being selected, while negative biases do the opposite. Check\n             [`~generation.SequenceBiasLogitsProcessor`] for further documentation and examples.\n@@ -325,15 +325,15 @@ class GenerationConfig(PushToHubMixin):\n             The id of the *padding* token.\n         bos_token_id (`int`, *optional*):\n             The id of the *beginning-of-sequence* token.\n-        eos_token_id (`Union[int, List[int]]`, *optional*):\n+        eos_token_id (`Union[int, list[int]]`, *optional*):\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n \n         > Generation parameters exclusive to encoder-decoder models\n \n         encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n             If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n             `decoder_input_ids`.\n-        decoder_start_token_id (`int` or `List[int]`, *optional*):\n+        decoder_start_token_id (`int` or `list[int]`, *optional*):\n             If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token or a list of length\n             `batch_size`. Indicating a list enables different start ids for each element in the batch\n             (e.g. multilingual models with different target languages in one batch)\n@@ -846,7 +846,7 @@ def save_pretrained(\n                 Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                 repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                 namespace).\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n \n@@ -933,7 +933,7 @@ def from_pretrained(\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or `bool`, *optional*):\n@@ -959,7 +959,7 @@ def from_pretrained(\n             subfolder (`str`, *optional*, defaults to `\"\"`):\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                 specify the folder name here.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n                 values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n                 by the `return_unused_kwargs` keyword parameter.\n@@ -1090,14 +1090,14 @@ def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n         return json.loads(text)\n \n     @classmethod\n-    def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"GenerationConfig\":\n+    def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"GenerationConfig\":\n         \"\"\"\n         Instantiates a [`GenerationConfig`] from a Python dictionary of parameters.\n \n         Args:\n-            config_dict (`Dict[str, Any]`):\n+            config_dict (`dict[str, Any]`):\n                 Dictionary that will be used to instantiate the configuration object.\n-            kwargs (`Dict[str, Any]`):\n+            kwargs (`dict[str, Any]`):\n                 Additional parameters from which to initialize the configuration object.\n \n         Returns:\n@@ -1123,7 +1123,7 @@ def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"GenerationConfig\":\n         else:\n             return config\n \n-    def dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None:\n+    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n         \"\"\"\n         Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n@@ -1135,13 +1135,13 @@ def dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None:\n             if isinstance(value, dict):\n                 self.dict_torch_dtype_to_str(value)\n \n-    def to_diff_dict(self) -> Dict[str, Any]:\n+    def to_diff_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Removes all attributes from config which correspond to the default config attributes for better readability and\n         serializes to a Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n         \"\"\"\n         config_dict = self.to_dict()\n \n@@ -1158,12 +1158,12 @@ def to_diff_dict(self) -> Dict[str, Any]:\n         self.dict_torch_dtype_to_str(serializable_config_dict)\n         return serializable_config_dict\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n \n@@ -1289,11 +1289,11 @@ def update(self, **kwargs):\n         returning all the unused kwargs.\n \n         Args:\n-            kwargs (`Dict[str, Any]`):\n+            kwargs (`dict[str, Any]`):\n                 Dictionary of attributes to tentatively update this class.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n+            `dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n         \"\"\"\n         to_remove = []\n         for key, value in kwargs.items():\n@@ -1319,7 +1319,7 @@ def from_dict(cls, config_dict, **kwargs):\n         Constructs a BaseWatermarkingConfig instance from a dictionary of parameters.\n \n         Args:\n-            config_dict (Dict[str, Any]): Dictionary containing configuration parameters.\n+            config_dict (dict[str, Any]): Dictionary containing configuration parameters.\n             **kwargs: Additional keyword arguments to override dictionary values.\n \n         Returns:\n@@ -1348,12 +1348,12 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n \n             writer.write(json_string)\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n         Returns:\n-            Dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n+            dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n         return output\n@@ -1479,7 +1479,7 @@ class SynthIDTextWatermarkingConfig(BaseWatermarkingConfig):\n     Args:\n         ngram_len (`int`):\n             Ngram length.\n-        keys (`List[int]`):\n+        keys (`list[int]`):\n             A sequence of watermarking keys, one for each depth.\n         context_history_size (`int`, *optional*, defaults to 1024):\n             Size of the tensor to keep track of seen contexts.\n@@ -1518,7 +1518,7 @@ class SynthIDTextWatermarkingConfig(BaseWatermarkingConfig):\n     def __init__(\n         self,\n         ngram_len: int,\n-        keys: List[int],\n+        keys: list[int],\n         context_history_size: int = 1024,\n         sampling_table_seed: int = 0,\n         sampling_table_size: int = 2**16,\n@@ -1605,6 +1605,6 @@ class CompileConfig:\n     # Used to flag our `generate` call to compile on e.g. CPU. Often not optimal, but useful for testing purposes.\n     _compile_all_devices = None\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n         return copy.deepcopy({key: value for key, value in self.__dict__.items() if key != \"_compile_all_devices\"})"
        },
        {
            "sha": "3149c63a4922c465ff3d2cb06d2fad1f9f544b3a",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 42,
            "deletions": 42,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -23,7 +23,7 @@\n from dataclasses import dataclass, field\n from enum import Enum\n from functools import partial\n-from typing import Deque, Dict, List, Optional, Set, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -59,16 +59,16 @@ class GenerationOutput:\n \n     Attributes:\n         request_id (str): The ID of the generation request.\n-        prompt_ids (List[int]): The IDs of the prompt tokens.\n-        generated_tokens (List[int]): The generated tokens.\n-        logprobs (List[float]): The log probabilities of the generated tokens.\n+        prompt_ids (list[int]): The IDs of the prompt tokens.\n+        generated_tokens (list[int]): The generated tokens.\n+        logprobs (list[float]): The log probabilities of the generated tokens.\n         error (Optional[str]): Any error message associated with the request. When None, the request was successful.\n     \"\"\"\n \n     request_id: str\n-    prompt_ids: List[int] = field(default_factory=list)\n-    generated_tokens: List[int] = field(default_factory=list)\n-    logprobs: List[float] = field(default_factory=list)\n+    prompt_ids: list[int] = field(default_factory=list)\n+    generated_tokens: list[int] = field(default_factory=list)\n+    logprobs: list[float] = field(default_factory=list)\n     error: Optional[str] = None\n     status: RequestStatus = RequestStatus.PENDING\n     created_time: float = field(default_factory=time.time)\n@@ -85,11 +85,11 @@ class RequestState:\n \n     # Required fields\n     request_id: str\n-    prompt_ids: Optional[List[int]] = None  # the one being processed\n-    full_prompt_ids: Optional[List[int]] = None  # the full prompt\n-    remaining_prompt_ids: List[int] = field(default_factory=list)  # For split requests\n-    static_outputs: List[int] = field(default_factory=list)\n-    allocated_blocks: List[int] = field(default_factory=list)\n+    prompt_ids: Optional[list[int]] = None  # the one being processed\n+    full_prompt_ids: Optional[list[int]] = None  # the full prompt\n+    remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests\n+    static_outputs: list[int] = field(default_factory=list)\n+    allocated_blocks: list[int] = field(default_factory=list)\n     position_offset: int = 0  # Current position in the sequence for position_ids\n     status: RequestStatus = RequestStatus.PENDING\n     max_new_tokens: int = 20\n@@ -150,8 +150,8 @@ def __init__(\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n-        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n-        initial_prompt_shapes: Optional[List[List[int]]] = None,\n+        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n+        initial_prompt_shapes: Optional[list[list[int]]] = None,\n     ) -> None:\n         \"\"\"Initialize a paged attention cache for efficient memory usage.\n \n@@ -191,8 +191,8 @@ def __init__(\n         self.dtype = dtype\n         self.device = device\n \n-        self.key_cache: List[torch.Tensor] = []\n-        self.value_cache: List[torch.Tensor] = []\n+        self.key_cache: list[torch.Tensor] = []\n+        self.value_cache: list[torch.Tensor] = []\n         for idx in range(config.num_hidden_layers):\n             layer_device = layer_device_map[idx] if layer_device_map is not None else device\n             new_layer_key_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n@@ -206,10 +206,10 @@ def __init__(\n \n         # Block management data structures\n         self._free_blocks = deque(range(num_blocks))\n-        self._block_tables: Dict[str, List[int]] = {}\n+        self._block_tables: dict[str, list[int]] = {}\n \n     @traced\n-    def allocate_blocks(self, n_blocks: int, request_id: str) -> List[int]:\n+    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]:\n         \"\"\"Allocates n_blocks for a given request_id.\"\"\"\n         if len(self._free_blocks) < n_blocks:\n             return False\n@@ -236,12 +236,12 @@ def get_num_free_blocks(self) -> int:\n         \"\"\"Returns the number of free blocks available.\"\"\"\n         return len(self._free_blocks)\n \n-    def get_block_table(self, request_id: str) -> List[int]:\n+    def get_block_table(self, request_id: str) -> list[int]:\n         \"\"\"Returns the block table for a request.\"\"\"\n         return self._block_tables.get(request_id, [])\n \n     @traced\n-    def _get_physical_indices(self, state: RequestState, logical_indices: List[int]) -> List[int]:\n+    def _get_physical_indices(self, state: RequestState, logical_indices: list[int]) -> list[int]:\n         \"\"\"\n         Maps logical sequence indices to physical cache indices using the block table, using PyTorch.\n \n@@ -289,7 +289,7 @@ def update(\n         read_index,\n         write_index,\n         **kwargs,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         # Reshape cache for easier indexing\n         total_slots = self.num_blocks * self.block_size\n         k_cache_flat = self.key_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n@@ -306,9 +306,9 @@ class Scheduler(ABC):\n     \"\"\"\n \n     def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = False):\n-        self.active_requests: Dict[str, RequestState] = {}\n-        self.waiting_requests: Dict[str, RequestState] = {}\n-        self.waiting_requests_order: Deque[str] = deque()\n+        self.active_requests: dict[str, RequestState] = {}\n+        self.waiting_requests: dict[str, RequestState] = {}\n+        self.waiting_requests_order: deque[str] = deque()\n         self.cache = cache\n         self.retain_cache_on_finish = retain_cache_on_finish\n \n@@ -318,7 +318,7 @@ def add_waiting_request(self, state: RequestState):\n         pass\n \n     @abstractmethod\n-    def schedule_batch(self, token_budget: int) -> List[RequestState]:\n+    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n         pass\n \n     @traced\n@@ -332,7 +332,7 @@ def finish_request(self, request_id: str, evict_from_cache: bool = True):\n         pass\n \n     @traced\n-    def get_active_request_static_outputs(self, request_id: str) -> List[int]:\n+    def get_active_request_static_outputs(self, request_id: str) -> list[int]:\n         if request_id in self.active_requests:\n             return self.active_requests[request_id].static_outputs\n         return []\n@@ -356,7 +356,7 @@ def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n \n     @traced(span_name=\"prepare_request\")\n     def _prepare_request_for_processing(\n-        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: Set[str]\n+        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n     ):\n         \"\"\"Prepare a request for processing in the current batch.\"\"\"\n         request_tokens = (\n@@ -395,9 +395,9 @@ def add_waiting_request(self, state: RequestState):\n         self.waiting_requests_order.append(state.request_id)\n \n     @traced\n-    def schedule_batch(self, token_budget: int) -> List[RequestState]:\n-        priority_states: List[RequestState] = []\n-        second_priority_states: List[RequestState] = []\n+    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n+        priority_states: list[RequestState] = []\n+        second_priority_states: list[RequestState] = []\n         scheduled_requests = []\n \n         for state in self.active_requests.values():\n@@ -475,7 +475,7 @@ def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n \n     @traced(span_name=\"prepare_request\")\n     def _prepare_request_for_processing(\n-        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: Set[str]\n+        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: set[str]\n     ):\n         \"\"\"Prepare a request for processing in the current batch.\"\"\"\n         request_tokens = (\n@@ -514,9 +514,9 @@ def add_waiting_request(self, state: RequestState):\n         self.waiting_requests_order.append(state.request_id)\n \n     @traced\n-    def schedule_batch(self, token_budget: int) -> List[RequestState]:\n-        priority_states: List[RequestState] = []\n-        second_priority_states: List[RequestState] = []\n+    def schedule_batch(self, token_budget: int) -> list[RequestState]:\n+        priority_states: list[RequestState] = []\n+        second_priority_states: list[RequestState] = []\n         scheduled_requests = []\n \n         for state in self.active_requests.values():\n@@ -581,7 +581,7 @@ def compute_optimal_blocks(\n     device: torch.device,\n     config: PretrainedConfig,\n     generation_config: GenerationConfig,\n-    inputs: List[List[int]],\n+    inputs: list[list[int]],\n     dtype: torch.dtype = torch.bfloat16,\n     safety_margin: float = 0.9,\n     median_prefill_length: Optional[int] = None,\n@@ -678,7 +678,7 @@ class PagedAttentionArgs:\n     write_index: torch.Tensor\n     read_index: torch.Tensor\n     logits_indices: torch.Tensor\n-    block_tables: Dict[str, List[int]]\n+    block_tables: dict[str, list[int]]\n     cache: PagedAttentionCache\n     use_cache: bool = False\n \n@@ -754,7 +754,7 @@ def __init__(\n         self.streaming = streaming\n         self.manual_eviction = manual_eviction\n \n-        self.requests_in_batch: List[RequestState] = []\n+        self.requests_in_batch: list[RequestState] = []\n \n         # Get batch size parameters from generation config\n         self._configure_batch_parameters()\n@@ -1152,7 +1152,7 @@ def join(self, timeout: Optional[float] = None):\n                 self._generation_thread = None\n \n     def add_request(\n-        self, input_ids: List[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n+        self, input_ids: list[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n     ) -> str:\n         \"\"\"Add a new generation request to the queue.\n \n@@ -1184,7 +1184,7 @@ def add_request(\n         logger.debug(f\"Added request {request_id} to queue.\")\n         return request_id\n \n-    def add_requests(self, inputs: List[List[int]], **kwargs):\n+    def add_requests(self, inputs: list[list[int]], **kwargs):\n         for i, input_ids in enumerate(inputs):\n             # Assign a predictable request ID for ordering results later\n             req_id = f\"batch_req_{i}\"\n@@ -1428,11 +1428,11 @@ def init_continuous_batching(\n     @torch.inference_mode()\n     def generate_batch(\n         self,\n-        inputs: List[List[int]],\n+        inputs: list[list[int]],\n         generation_config: Optional[GenerationConfig] = None,\n         progress_bar: bool = True,\n         **kwargs,\n-    ) -> List[List[int]]:\n+    ) -> list[list[int]]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n \n         Args:\n@@ -1441,7 +1441,7 @@ def generate_batch(\n             **kwargs: Additional generation parameters\n \n         Returns:\n-            `List[List[int]]`: A list containing the generated sequences (including prompt tokens\n+            `list[list[int]]`: A list containing the generated sequences (including prompt tokens\n                                 if not handled otherwise) for each input prompt, in the same order.\n                                 Returns an empty list `[]` for requests that failed.\n         \"\"\""
        },
        {
            "sha": "08fa411dc6f5761eb52eea585cd6f7abf612edeb",
            "filename": "src/transformers/generation/flax_logits_process.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -39,7 +39,7 @@\n         scores (`jnp.ndarray` of shape `(batch_size, config.vocab_size)`):\n             Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n             search or log softmax for each vocabulary token when using beam search\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional logits processor specific kwargs.\n \n     Return:\n@@ -276,7 +276,7 @@ class FlaxSuppressTokensAtBeginLogitsProcessor(FlaxLogitsProcessor):\n     beginning of the generation.\n \n     Args:\n-        begin_suppress_tokens (`List[int]`):\n+        begin_suppress_tokens (`list[int]`):\n             Tokens to not sample.\n         begin_index (`int`):\n             Index where the tokens are suppressed."
        },
        {
            "sha": "780700233bf4f4a7140bc6496ee83a896971736e",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import inspect\n import warnings\n from functools import partial\n-from typing import Any, Dict, Optional, Union\n+from typing import Any, Optional, Union\n \n import flax\n import jax\n@@ -103,7 +103,7 @@ class GreedyState:\n     sequences: jnp.ndarray\n     running_token: jnp.ndarray\n     is_sent_finished: jnp.ndarray\n-    model_kwargs: Dict[str, jnp.ndarray]\n+    model_kwargs: dict[str, jnp.ndarray]\n \n \n @flax.struct.dataclass\n@@ -113,7 +113,7 @@ class SampleState:\n     running_token: jnp.ndarray\n     is_sent_finished: jnp.ndarray\n     prng_key: jnp.ndarray\n-    model_kwargs: Dict[str, jnp.ndarray]\n+    model_kwargs: dict[str, jnp.ndarray]\n \n \n @flax.struct.dataclass\n@@ -124,7 +124,7 @@ class BeamSearchState:\n     sequences: jnp.ndarray\n     scores: jnp.ndarray\n     is_sent_finished: jnp.ndarray\n-    model_kwargs: Dict[str, jnp.ndarray]\n+    model_kwargs: dict[str, jnp.ndarray]\n \n \n class FlaxGenerationMixin:\n@@ -173,7 +173,7 @@ def _prepare_decoder_input_ids_for_generation(\n         batch_size: int,\n         decoder_start_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = None,\n-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,\n+        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n     ) -> jnp.ndarray:\n         if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n             # Only use this arg if not None, otherwise just remove from model_kwargs\n@@ -249,7 +249,7 @@ def _validate_model_class(self):\n                 exception_message += f\" Please use one of the following classes instead: {generate_compatible_classes}\"\n             raise TypeError(exception_message)\n \n-    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n+    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n         \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n         unused_model_args = []\n         model_args = set(inspect.signature(self.prepare_inputs_for_generation).parameters)\n@@ -273,7 +273,7 @@ def generate(\n         generation_config: Optional[GenerationConfig] = None,\n         prng_key: Optional[jnp.ndarray] = None,\n         trace: bool = True,\n-        params: Optional[Dict[str, jnp.ndarray]] = None,\n+        params: Optional[dict[str, jnp.ndarray]] = None,\n         logits_processor: Optional[FlaxLogitsProcessorList] = None,\n         **kwargs,\n     ):\n@@ -293,13 +293,13 @@ def generate(\n             trace (`bool`, *optional*, defaults to `True`):\n                 Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a\n                 considerably slower runtime.\n-            params (`Dict[str, jnp.ndarray]`, *optional*):\n+            params (`dict[str, jnp.ndarray]`, *optional*):\n                 Optionally the model parameters can be passed. Can be useful for parallelized generation.\n             logits_processor (`FlaxLogitsProcessorList `, *optional*):\n                 Custom logits processors that complement the default logits processors built from arguments and\n                 generation config. If a logit processor is passed that is already created with the arguments or a\n                 generation config an error is thrown. This feature is intended for advanced users.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                 specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n@@ -580,8 +580,8 @@ def _greedy_search(\n         eos_token_id: Optional[int] = None,\n         logits_processor: Optional[FlaxLogitsProcessorList] = None,\n         trace: bool = True,\n-        params: Optional[Dict[str, jnp.ndarray]] = None,\n-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,\n+        params: Optional[dict[str, jnp.ndarray]] = None,\n+        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n     ):\n         # init values\n         max_length = max_length if max_length is not None else self.generation_config.max_length\n@@ -668,8 +668,8 @@ def _sample(\n         logits_processor: Optional[FlaxLogitsProcessorList] = None,\n         logits_warper: Optional[FlaxLogitsProcessorList] = None,\n         trace: bool = True,\n-        params: Optional[Dict[str, jnp.ndarray]] = None,\n-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,\n+        params: Optional[dict[str, jnp.ndarray]] = None,\n+        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n     ):\n         # init values\n         max_length = max_length if max_length is not None else self.generation_config.max_length\n@@ -765,9 +765,9 @@ def _beam_search(\n         early_stopping: Optional[Union[bool, str]] = None,\n         logits_processor: Optional[FlaxLogitsProcessorList] = None,\n         trace: bool = True,\n-        params: Optional[Dict[str, jnp.ndarray]] = None,\n+        params: Optional[dict[str, jnp.ndarray]] = None,\n         num_return_sequences: Optional[int] = None,\n-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,\n+        model_kwargs: Optional[dict[str, jnp.ndarray]] = None,\n     ):\n         \"\"\"\n         This beam search function is heavily inspired by Flax's official example:"
        },
        {
            "sha": "8c72279b6ef400f0b0fc59ad16590a2ec16cb4c4",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n import inspect\n import math\n from collections.abc import Iterable\n-from typing import TYPE_CHECKING, Callable, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -72,7 +72,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n             scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n                 Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n                 beam search or log softmax for each vocabulary token when using beam search\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional kwargs that are specific to a logits processor.\n \n         Return:\n@@ -103,7 +103,7 @@ class MinLengthLogitsProcessor(LogitsProcessor):\n     Args:\n         min_length (`int`):\n             The minimum length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`):\n             The id(s) of the *end-of-sequence* token.\n         device (`str`, *optional*, defaults to `\"cpu\"`):\n             The device to allocate the tensors.\n@@ -134,7 +134,7 @@ class MinLengthLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, min_length: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = \"cpu\"):\n+    def __init__(self, min_length: int, eos_token_id: Union[int, list[int], torch.Tensor], device: str = \"cpu\"):\n         if not isinstance(min_length, int) or min_length < 0:\n             raise ValueError(f\"`min_length` has to be a non-negative integer, but is {min_length}\")\n \n@@ -167,7 +167,7 @@ class MinNewTokensLengthLogitsProcessor(LogitsProcessor):\n             input length.\n         min_new_tokens (`int`):\n             The minimum *new* tokens length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`):\n             The id(s) of the *end-of-sequence* token.\n         device (`str`, *optional*, defaults to `\"cpu\"`):\n             The device to allocate the tensors.\n@@ -197,7 +197,7 @@ def __init__(\n         self,\n         prompt_length_to_skip: int,\n         min_new_tokens: int,\n-        eos_token_id: Union[int, List[int], torch.Tensor],\n+        eos_token_id: Union[int, list[int], torch.Tensor],\n         device: str = \"cpu\",\n     ):\n         for arg_name, arg_value in [\n@@ -917,7 +917,7 @@ def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n \n def _calc_banned_ngram_tokens(\n     ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int\n-) -> List[Iterable[int]]:\n+) -> list[Iterable[int]]:\n     \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n     if cur_len + 1 < ngram_size:\n         # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n@@ -1074,7 +1074,7 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     </Tip>\n \n     Args:\n-        sequence_bias (`List[List[Union[List[int], float]]]`):\n+        sequence_bias (`list[list[Union[list[int], float]]]`):\n             List of lists that maps a sequence of tokens to its bias term (e.g. `[[[10, 45], -2.0],\n             [[64], -7.5]]`). Positive biases increase the odds of the\n             sequence being selected, while negative biases do the opposite. If a sequence has a length of 1, its bias\n@@ -1123,7 +1123,7 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, sequence_bias: List[List[Union[List[int], float]]]):\n+    def __init__(self, sequence_bias: list[list[Union[list[int], float]]]):\n         self.sequence_bias = sequence_bias\n         self._validate_arguments()\n         self._convert_list_arguments_into_dict()\n@@ -1250,9 +1250,9 @@ class NoBadWordsLogitsProcessor(SequenceBiasLogitsProcessor):\n     </Tip>\n \n     Args:\n-        bad_words_ids (`List[List[int]]`):\n+        bad_words_ids (`list[list[int]]`):\n             List of list of token ids that are not allowed to be generated.\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`, *optional*):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`, *optional*):\n             The id(s) of the *end-of-sequence* token.\n \n     Examples:\n@@ -1291,7 +1291,7 @@ class NoBadWordsLogitsProcessor(SequenceBiasLogitsProcessor):\n     \"\"\"\n \n     def __init__(\n-        self, bad_words_ids: List[List[int]], eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None\n+        self, bad_words_ids: list[list[int]], eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None\n     ):\n         self.bad_word_ids = bad_words_ids\n         self._validate_arguments()\n@@ -1332,7 +1332,7 @@ class PrefixConstrainedLogitsProcessor(LogitsProcessor):\n     generation. See [Autoregressive Entity Retrieval](https://huggingface.co/papers/2010.00904) for more information.\n \n     Args:\n-        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`):\n+        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], list[int]]`):\n             This function constraints the beam search to allowed tokens only at each step. This function takes 2\n             arguments `inputs_ids` and the batch ID `batch_id`. It has to return a list with the allowed tokens for the\n             next generation step conditioned on the previously generated tokens `inputs_ids` and the batch ID\n@@ -1373,7 +1373,7 @@ class PrefixConstrainedLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]], num_beams: int):\n+    def __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], list[int]], num_beams: int):\n         self._prefix_allowed_tokens_fn = prefix_allowed_tokens_fn\n         self._num_beams = num_beams\n \n@@ -1586,7 +1586,7 @@ class ForcedEOSTokenLogitsProcessor(LogitsProcessor):\n     Args:\n         max_length (`int`):\n             The maximum length of the sequence to be generated.\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`):\n             The id(s) of the *end-of-sequence* token.\n         device (`str`, *optional*, defaults to `\"cpu\"`):\n             The device to allocate the tensors.\n@@ -1613,7 +1613,7 @@ class ForcedEOSTokenLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, max_length: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = \"cpu\"):\n+    def __init__(self, max_length: int, eos_token_id: Union[int, list[int], torch.Tensor], device: str = \"cpu\"):\n         self.max_length = max_length\n \n         if not isinstance(eos_token_id, torch.Tensor):\n@@ -1666,7 +1666,7 @@ class ExponentialDecayLengthPenalty(LogitsProcessor):\n         exponential_decay_length_penalty (`tuple(int, float)`):\n             This tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where penalty\n             starts and `decay_factor` represents the factor of exponential decay\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`):\n             The id(s) of the *end-of-sequence* token.\n         input_ids_seq_length (`int`):\n             The length of the input sequence.\n@@ -1726,8 +1726,8 @@ class ExponentialDecayLengthPenalty(LogitsProcessor):\n \n     def __init__(\n         self,\n-        exponential_decay_length_penalty: Tuple[int, float],\n-        eos_token_id: Union[int, List[int], torch.Tensor],\n+        exponential_decay_length_penalty: tuple[int, float],\n+        eos_token_id: Union[int, list[int], torch.Tensor],\n         input_ids_seq_length: int,\n     ):\n         self.regulation_start = exponential_decay_length_penalty[0] + input_ids_seq_length\n@@ -2326,13 +2326,13 @@ class BarkEosPrioritizerLogitsProcessor(LogitsProcessor):\n     </Tip>\n \n     Args:\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`):\n             The id(s) of the *end-of-sequence* token.\n         min_eos_p (`float`, *optional*):\n             Minimum end of speech threshold.\n     \"\"\"\n \n-    def __init__(self, eos_token_id: Union[int, List[int], torch.Tensor], min_eos_p: float, device: str = \"cpu\"):\n+    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor], min_eos_p: float, device: str = \"cpu\"):\n         if not isinstance(eos_token_id, torch.Tensor):\n             if isinstance(eos_token_id, int):\n                 eos_token_id = [eos_token_id]\n@@ -2569,7 +2569,7 @@ class SynthIDTextWatermarkLogitsProcessor(LogitsProcessor):\n     Args:\n         ngram_len (`int`):\n             Ngram length.\n-        keys (`List[int]`):\n+        keys (`list[int]`):\n             A sequence of watermarking keys, one for each depth.\n         sampling_table_size (`int`):\n             Size of the sampling table.\n@@ -2610,7 +2610,7 @@ class SynthIDTextWatermarkLogitsProcessor(LogitsProcessor):\n     def __init__(\n         self,\n         ngram_len: int,\n-        keys: List[int],\n+        keys: list[int],\n         sampling_table_size: int,\n         sampling_table_seed: int,\n         context_history_size: int,\n@@ -2808,7 +2808,7 @@ def compute_ngram_keys(self, ngrams: torch.LongTensor) -> torch.LongTensor:\n \n     def _compute_keys(\n         self, n_minus_1_grams: torch.LongTensor, indices: torch.LongTensor\n-    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n+    ) -> tuple[torch.LongTensor, torch.LongTensor]:\n         \"\"\"Computes random keys for each ngram and depth.\n \n         Args:"
        },
        {
            "sha": "2b9e57aacd8d287d7264fdd10b3c0f2f338b5524",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -3,7 +3,7 @@\n from abc import ABC\n from collections import OrderedDict\n from copy import deepcopy\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -33,7 +33,7 @@\n             Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n             or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n             make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional stopping criteria specific kwargs.\n \n     Return:\n@@ -209,7 +209,7 @@ class StopStringCriteria(StoppingCriteria):\n     Args:\n         tokenizer (`PreTrainedTokenizer`):\n             The model's associated tokenizer (necessary to extract vocab and tokenize the termination sequences)\n-        stop_strings (`Union[str, List[str]]`):\n+        stop_strings (`Union[str, list[str]]`):\n             A list of strings that should end generation. If a string is passed, it will be treated like a\n             list with a single element.\n \n@@ -239,10 +239,10 @@ class StopStringCriteria(StoppingCriteria):\n     ```\n     \"\"\"\n \n-    def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_strings: Union[str, List[str]]):\n+    def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_strings: Union[str, list[str]]):\n         if isinstance(stop_strings, str):\n             stop_strings = [stop_strings]\n-        self.stop_strings: Tuple[str, ...] = tuple(stop_strings)\n+        self.stop_strings: tuple[str, ...] = tuple(stop_strings)\n         vocab = tokenizer.get_vocab()\n         token_list, token_indices = tuple(vocab.keys()), tuple(vocab.values())\n         self.embedding_vec, self.max_valid_positions, self.max_valid_end_lens = self.clean_and_embed_tokens_with_cache(\n@@ -298,7 +298,7 @@ def clean_tokenizer_vocab(tokenizer, static_prefix=\"abcdef\"):\n     @staticmethod\n     def _stop_string_get_matching_positions(\n         token_list, token_indices, stop_strings\n-    ) -> Tuple[Dict[str, Dict[str, List[int]]], Dict[str, Dict[str, List[int]]]]:\n+    ) -> tuple[dict[str, dict[str, list[int]]], dict[str, dict[str, list[int]]]]:\n         \"\"\"This function preprocesses stop strings and the tokenizer vocabulary to determine where tokens can\n         validly appear in the stop strings. For each token, it computes a list of positions in the stop string where the\n         token appears, as well as a list of the possible \"end overlaps\" for that token - that is, the number of characters\n@@ -337,7 +337,7 @@ def _stop_string_get_matching_positions(\n         return token_valid_positions, token_end_overlaps\n \n     @staticmethod\n-    def _stop_string_create_embedding_vec(token_list, token_indices, stop_strings) -> Dict[str, torch.tensor]:\n+    def _stop_string_create_embedding_vec(token_list, token_indices, stop_strings) -> dict[str, torch.tensor]:\n         \"\"\"This function precomputes everything needed for the run-time checks in StopStringCriteria, and packs\n         them into an embedding tensor that can be accessed with pure tensor operations. For the specifics of the values\n         that are precomputed and what they are used for, please refer to the StopStringCriteria docstring!\"\"\"\n@@ -455,11 +455,11 @@ class EosTokenCriteria(StoppingCriteria):\n     By default, it uses the `model.generation_config.eos_token_id`.\n \n     Args:\n-        eos_token_id (`Union[int, List[int], torch.Tensor]`):\n+        eos_token_id (`Union[int, list[int], torch.Tensor]`):\n             The id(s) of the *end-of-sequence* token.\n     \"\"\"\n \n-    def __init__(self, eos_token_id: Union[int, List[int], torch.Tensor]):\n+    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor]):\n         if not isinstance(eos_token_id, torch.Tensor):\n             if isinstance(eos_token_id, int):\n                 eos_token_id = [eos_token_id]"
        },
        {
            "sha": "c88ea5afcc9d8cee2d58e33538ac33254df40ce2",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n import inspect\n-from typing import List, Tuple\n \n import numpy as np\n import tensorflow as tf\n@@ -42,7 +41,7 @@\n         cur_len (`int`):\n             The current length of valid input sequence tokens. In the TF implementation, the input_ids' sequence length\n             is the maximum length generate can produce, and we need to know which of its tokens are valid.\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional logits processor specific kwargs.\n \n     Return:\n@@ -290,7 +289,7 @@ class TFNoBadWordsLogitsProcessor(TFLogitsProcessor):\n     [`TFLogitsProcessor`] that enforces that specified sequences will never be sampled.\n \n     Args:\n-        bad_words_ids (`List[List[int]]`):\n+        bad_words_ids (`list[list[int]]`):\n             List of list of token ids that are not allowed to be generated. In order to get the tokens of the words\n             that should not appear in the generated text, make sure to set `add_prefix_space=True` when initializing\n             the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The `add_prefix_space`\n@@ -300,8 +299,8 @@ class TFNoBadWordsLogitsProcessor(TFLogitsProcessor):\n             The id of the *end-of-sequence* token.\n     \"\"\"\n \n-    def __init__(self, bad_words_ids: List[List[int]], eos_token_id: int):\n-        if not isinstance(bad_words_ids, List) or len(bad_words_ids) == 0:\n+    def __init__(self, bad_words_ids: list[list[int]], eos_token_id: int):\n+        if not isinstance(bad_words_ids, list) or len(bad_words_ids) == 0:\n             raise ValueError(f\"`bad_words_ids` has to be a non-empty list, but is {bad_words_ids}.\")\n         if any(not isinstance(bad_word_ids, list) for bad_word_ids in bad_words_ids):\n             raise ValueError(f\"`bad_words_ids` has to be a list of lists, but is {bad_words_ids}.\")\n@@ -370,7 +369,7 @@ def __call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.\n         # To remain simple and XLA-compatible, we work on a per-row fashion.\n         # TODO (Joao): this function might trigger XLA retracing as `cur_len` increases. Fix it if it becomes\n         # a frequent choke point. (make `cur_len` a tensor?)\n-        def _get_row_updated_score(row_inputs: Tuple[tf.Tensor]) -> tf.Tensor:\n+        def _get_row_updated_score(row_inputs: tuple[tf.Tensor]) -> tf.Tensor:\n             row_input_ids, row_score = row_inputs\n             banned_tokens = self._calc_row_banned_bad_tokens(row_input_ids[:cur_len])\n             banned_tokens_mask = tf.scatter_nd(\n@@ -565,7 +564,7 @@ class TFForceTokensLogitsProcessor(TFLogitsProcessor):\n     indices that will be forced before sampling. The processor will set their log probs to `0` and all other tokens to\n     `-inf` so that they are sampled at their corresponding index.\"\"\"\n \n-    def __init__(self, force_token_map: List[List[int]]):\n+    def __init__(self, force_token_map: list[list[int]]):\n         force_token_map = dict(force_token_map)\n         # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the\n         # index of the array corresponds to the index of the token to be forced, for XLA compatibility."
        },
        {
            "sha": "be51c9cd9f439b66a4ee15ed734d3947f43901b9",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 66,
            "deletions": 66,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import inspect\n import warnings\n from dataclasses import dataclass\n-from typing import Any, Dict, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -77,9 +77,9 @@ class TFGreedySearchDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n \n     sequences: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n-    attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n+    attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -116,12 +116,12 @@ class TFGreedySearchEncoderDecoderOutput(ModelOutput):\n     \"\"\"\n \n     sequences: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n-    encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n-    encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n+    encoder_attentions: Optional[tuple[tf.Tensor]] = None\n+    encoder_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -147,9 +147,9 @@ class TFSampleDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n \n     sequences: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n-    attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n+    attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -186,12 +186,12 @@ class TFSampleEncoderDecoderOutput(ModelOutput):\n     \"\"\"\n \n     sequences: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n-    encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n-    encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n+    encoder_attentions: Optional[tuple[tf.Tensor]] = None\n+    encoder_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -223,10 +223,10 @@ class TFBeamSearchDecoderOnlyOutput(ModelOutput):\n \n     sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n-    attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -270,13 +270,13 @@ class TFBeamSearchEncoderDecoderOutput(ModelOutput):\n \n     sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n-    encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n-    encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    encoder_attentions: Optional[tuple[tf.Tensor]] = None\n+    encoder_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -308,10 +308,10 @@ class TFBeamSampleDecoderOnlyOutput(ModelOutput):\n \n     sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n-    attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -354,13 +354,13 @@ class TFBeamSampleEncoderDecoderOutput(ModelOutput):\n \n     sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n-    encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n-    encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    encoder_attentions: Optional[tuple[tf.Tensor]] = None\n+    encoder_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -385,9 +385,9 @@ class TFContrastiveSearchDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n \n     sequences: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n-    attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n+    attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n @dataclass\n@@ -423,12 +423,12 @@ class TFContrastiveSearchEncoderDecoderOutput(ModelOutput):\n     \"\"\"\n \n     sequences: Optional[tf.Tensor] = None\n-    scores: Optional[Tuple[tf.Tensor]] = None\n-    encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n-    encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n+    scores: Optional[tuple[tf.Tensor]] = None\n+    encoder_attentions: Optional[tuple[tf.Tensor]] = None\n+    encoder_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[tf.Tensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[tf.Tensor]]] = None\n \n \n TFGreedySearchOutput = Union[TFGreedySearchEncoderDecoderOutput, TFGreedySearchDecoderOnlyOutput]\n@@ -477,7 +477,7 @@ def prepare_inputs_for_generation(self, *args, **kwargs):\n     def compute_transition_scores(\n         self,\n         sequences: tf.Tensor,\n-        scores: Tuple[tf.Tensor],\n+        scores: tuple[tf.Tensor],\n         beam_indices: Optional[tf.Tensor] = None,\n         normalize_logits: bool = False,\n     ) -> tf.Tensor:\n@@ -619,7 +619,7 @@ def _validate_model_class(self):\n                 exception_message += f\" Please use one of the following classes instead: {generate_compatible_classes}\"\n             raise TypeError(exception_message)\n \n-    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n+    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n         \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n         # Excludes arguments that are handled before calling any model function\n         if self.config.is_encoder_decoder:\n@@ -681,10 +681,10 @@ def generate(\n                 Custom logits processors that complement the default logits processors built from arguments and\n                 generation config. If a logit processor is passed that is already created with the arguments or a\n                 generation config an error is thrown. This feature is intended for advanced users.\n-            seed (`List[int]`, *optional*):\n+            seed (`list[int]`, *optional*):\n                 Random seed to control sampling, containing two integers, used when `do_sample` is `True`. See the\n                 `seed` argument from stateless functions in `tf.random`.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                 specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n@@ -1044,7 +1044,7 @@ def _prepare_attention_mask_for_generation(\n \n     def _prepare_encoder_decoder_kwargs_for_generation(\n         self, inputs_tensor: tf.Tensor, model_kwargs, model_input_name: Optional[str] = None\n-    ) -> Dict[str, Any]:\n+    ) -> dict[str, Any]:\n         # 1. get encoder and store encoder outputs\n         encoder = self.get_encoder()\n \n@@ -1076,10 +1076,10 @@ def _prepare_decoder_input_ids_for_generation(\n         self,\n         batch_size: int,\n         model_input_name: str,\n-        model_kwargs: Dict[str, tf.Tensor],\n+        model_kwargs: dict[str, tf.Tensor],\n         decoder_start_token_id: Optional[int] = None,\n         bos_token_id: Optional[int] = None,\n-    ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n+    ) -> tuple[tf.Tensor, dict[str, tf.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n         # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n         # we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.\n@@ -1138,7 +1138,7 @@ def _expand_inputs_for_generation(\n         input_ids: Optional[tf.Tensor] = None,\n         expand_in_new_axis: bool = False,\n         **model_kwargs,\n-    ) -> Tuple[tf.Tensor, Dict[str, Any]]:\n+    ) -> tuple[tf.Tensor, dict[str, Any]]:\n         \"\"\"\n         Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...] or [batch_size, expand_size, ...],\n         depending on `expand_in_new_axis`. Beam-based approaches expect this function to be used with\n@@ -1174,8 +1174,8 @@ def _prepare_model_inputs(\n         self,\n         inputs: Optional[tf.Tensor] = None,\n         bos_token_id: Optional[int] = None,\n-        model_kwargs: Optional[Dict[str, tf.Tensor]] = None,\n-    ) -> Tuple[tf.Tensor, Optional[str], Dict[str, tf.Tensor]]:\n+        model_kwargs: Optional[dict[str, tf.Tensor]] = None,\n+    ) -> tuple[tf.Tensor, Optional[str], dict[str, tf.Tensor]]:\n         \"\"\"\n         This function extracts the model-specific `inputs` for generation.\n         \"\"\"\n@@ -1240,7 +1240,7 @@ def _maybe_initialize_input_ids_for_generation(\n         self,\n         inputs: Optional[tf.Tensor] = None,\n         bos_token_id: Optional[int] = None,\n-        model_kwargs: Optional[Dict[str, tf.Tensor]] = None,\n+        model_kwargs: Optional[dict[str, tf.Tensor]] = None,\n     ) -> tf.Tensor:\n         \"\"\"Initializes input ids for generation, if necessary.\"\"\"\n         if inputs is not None:\n@@ -1276,8 +1276,8 @@ def _extract_past_from_model_output(outputs: ModelOutput):\n         return past_key_values\n \n     def _update_model_kwargs_for_generation(\n-        self, outputs: ModelOutput, model_kwargs: Dict[str, Any], is_encoder_decoder: bool = False\n-    ) -> Dict[str, Any]:\n+        self, outputs: ModelOutput, model_kwargs: dict[str, Any], is_encoder_decoder: bool = False\n+    ) -> dict[str, Any]:\n         # update past_key_values\n         model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(outputs)\n \n@@ -1294,7 +1294,7 @@ def _update_model_kwargs_for_generation(\n     def _update_model_kwargs_for_xla_generation(\n         self,\n         model_outputs: ModelOutput,\n-        model_kwargs: Dict[str, Any],\n+        model_kwargs: dict[str, Any],\n         cur_len: int,\n         max_length: int,\n         batch_size: int,\n@@ -1550,7 +1550,7 @@ def greedy_search(\n                 The maximum length of the sequence to be generated.\n             pad_token_id (`int`, *optional*):\n                 The id of the *padding* token.\n-            eos_token_id (`Union[int, List[int]]`, *optional*):\n+            eos_token_id (`Union[int, list[int]]`, *optional*):\n                 The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n             output_attentions (`bool`, *optional*, defaults to `False`):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n@@ -1794,7 +1794,7 @@ def sample(\n         max_length: Optional[int] = None,\n         pad_token_id: Optional[int] = None,\n         eos_token_id: Optional[int] = None,\n-        seed: Optional[Tuple[int, int]] = None,\n+        seed: Optional[tuple[int, int]] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_scores: Optional[bool] = None,\n@@ -1818,9 +1818,9 @@ def sample(\n                 The maximum length of the sequence to be generated.\n             pad_token_id (`int`, *optional*):\n                 The id of the *padding* token.\n-            eos_token_id (`Union[int, List[int]]`, *optional*):\n+            eos_token_id (`Union[int, list[int]]`, *optional*):\n                 The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n-            seed (`List[int]`, *optional*):\n+            seed (`list[int]`, *optional*):\n                 Random seed to control sampling, containing two integers, used when `do_sample` is `True`. See the\n                 `seed` argument from stateless functions in `tf.random`.\n             output_attentions (`bool`, *optional*, defaults to `False`):\n@@ -2128,7 +2128,7 @@ def beam_search(\n                 The maximum length of the sequence to be generated.\n             pad_token_id (`int`, *optional*):\n                 The id of the *padding* token.\n-            eos_token_id (`Union[int, List[int]]`, *optional*):\n+            eos_token_id (`Union[int, list[int]]`, *optional*):\n                 The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n             length_penalty (`float`, *optional*, defaults to 1.0):\n                 Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent\n@@ -2719,7 +2719,7 @@ def contrastive_search(\n                 The maximum length of the sequence to be generated.\n             pad_token_id (`int`, *optional*):\n                 The id of the *padding* token.\n-            eos_token_id (`Union[int, List[int]]`, *optional*):\n+            eos_token_id (`Union[int, list[int]]`, *optional*):\n                 The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n             output_attentions (`bool`, *optional*, defaults to `False`):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under"
        },
        {
            "sha": "484b4954cfaad6f874e2e59a70869b152bd4ad5e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 64,
            "deletions": 64,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import os\n import warnings\n from dataclasses import dataclass\n-from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -169,11 +169,11 @@ class GenerateDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n \n     sequences: torch.LongTensor\n-    scores: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None\n+    scores: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n \n \n @dataclass\n@@ -214,14 +214,14 @@ class GenerateEncoderDecoderOutput(ModelOutput):\n     \"\"\"\n \n     sequences: torch.LongTensor\n-    scores: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None\n+    scores: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n \n \n @dataclass\n@@ -260,12 +260,12 @@ class GenerateBeamDecoderOnlyOutput(ModelOutput):\n \n     sequences: torch.LongTensor\n     sequences_scores: Optional[torch.FloatTensor] = None\n-    scores: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n+    scores: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n     beam_indices: Optional[torch.LongTensor] = None\n-    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None\n+    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n \n \n @dataclass\n@@ -314,15 +314,15 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n \n     sequences: torch.LongTensor\n     sequences_scores: Optional[torch.FloatTensor] = None\n-    scores: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n+    scores: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n     beam_indices: Optional[torch.LongTensor] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[tuple[tuple[tuple[torch.FloatTensor]]]] = None\n \n \n # TODO (joao): remove the equivalent classes and typing shortcuts below in v5\n@@ -457,7 +457,7 @@ def _cache_dependant_input_preparation(\n         input_ids: torch.LongTensor,\n         inputs_embeds: Optional[torch.FloatTensor],\n         cache_position: Optional[torch.LongTensor],\n-    ) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n+    ) -> tuple[torch.FloatTensor, torch.LongTensor]:\n         \"\"\"\n         Generic cache-dependent input preparation\n         The code is put in a separate function to allow granular unit testing\n@@ -491,7 +491,7 @@ def _cache_dependant_input_preparation_exporting(\n         input_ids: torch.LongTensor,\n         inputs_embeds: Optional[torch.FloatTensor],\n         cache_position: Optional[torch.LongTensor],\n-    ) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n+    ) -> tuple[torch.FloatTensor, torch.LongTensor]:\n         \"\"\"\n         This method implements method ``_cache_dependant_input_preparation``\n         with :func:`torch.cond` to make it exportable with :func:`torch.export.export`.\n@@ -697,8 +697,8 @@ def _prepare_model_inputs(\n         self,\n         inputs: Optional[torch.Tensor] = None,\n         bos_token_id: Optional[torch.Tensor] = None,\n-        model_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n-    ) -> Tuple[torch.Tensor, Optional[str], Dict[str, torch.Tensor]]:\n+        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n+    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n         \"\"\"\n         This function extracts the model-specific `inputs` for generation.\n         \"\"\"\n@@ -761,7 +761,7 @@ def _maybe_initialize_input_ids_for_generation(\n         self,\n         inputs: Optional[torch.Tensor] = None,\n         bos_token_id: Optional[torch.Tensor] = None,\n-        model_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n+        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n     ) -> torch.LongTensor:\n         \"\"\"Initializes input ids for generation, if necessary.\"\"\"\n         if inputs is not None:\n@@ -793,7 +793,7 @@ def _prepare_attention_mask_for_generation(\n         self,\n         inputs_tensor: torch.Tensor,\n         generation_config: GenerationConfig,\n-        model_kwargs: Dict[str, Any],\n+        model_kwargs: dict[str, Any],\n     ) -> torch.LongTensor:\n         pad_token_id = generation_config._pad_token_tensor\n         eos_token_id = generation_config._eos_token_tensor\n@@ -831,7 +831,7 @@ def _prepare_encoder_decoder_kwargs_for_generation(\n         model_kwargs,\n         model_input_name: Optional[str],\n         generation_config: GenerationConfig,\n-    ) -> Dict[str, Any]:\n+    ) -> dict[str, Any]:\n         # 1. get encoder\n         encoder = self.get_encoder()\n         # Compatibility with Accelerate big model inference: we need the encoder to outputs stuff on the same device\n@@ -870,10 +870,10 @@ def _prepare_decoder_input_ids_for_generation(\n         self,\n         batch_size: int,\n         model_input_name: str,\n-        model_kwargs: Dict[str, torch.Tensor],\n+        model_kwargs: dict[str, torch.Tensor],\n         decoder_start_token_id: torch.Tensor,\n         device: Optional[torch.device] = None,\n-    ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n+    ) -> tuple[torch.LongTensor, dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n         # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n         # we also allow the user to pass it under `input_ids`, if the encoder does not use it as the main input.\n@@ -931,7 +931,7 @@ def _expand_inputs_for_generation(\n         is_encoder_decoder: bool = False,\n         input_ids: Optional[torch.LongTensor] = None,\n         **model_kwargs,\n-    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n+    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n         \"\"\"Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]\"\"\"\n         # Do not call torch.repeat_interleave if expand_size is 1 because it clones\n         # the input tensor and thus requires more memory although no change is applied\n@@ -963,10 +963,10 @@ def _expand_dict_for_generation(dict_to_expand):\n     def _update_model_kwargs_for_generation(\n         self,\n         outputs: ModelOutput,\n-        model_kwargs: Dict[str, Any],\n+        model_kwargs: dict[str, Any],\n         is_encoder_decoder: bool = False,\n         num_new_tokens: int = 1,\n-    ) -> Dict[str, Any]:\n+    ) -> dict[str, Any]:\n         # update past_key_values keeping its naming used in model code\n         for possible_cache_name in ALL_CACHE_NAMES:\n             if possible_cache_name in outputs:\n@@ -1024,7 +1024,7 @@ def _get_candidate_generator(\n         logits_processor: LogitsProcessorList,\n         target_tokenizer: \"PreTrainedTokenizerBase\",\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n-        model_kwargs: Dict,\n+        model_kwargs: dict,\n     ) -> CandidateGenerator:\n         \"\"\"\n         Returns the candidate generator to be used in `assisted_generation`\n@@ -1100,10 +1100,10 @@ def _get_logits_processor(\n         generation_config: GenerationConfig,\n         input_ids_seq_length: Optional[int] = None,\n         encoder_input_ids: torch.LongTensor = None,\n-        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n         logits_processor: Optional[LogitsProcessorList] = None,\n         device: Optional[str] = None,\n-        model_kwargs: Optional[Dict[str, Any]] = None,\n+        model_kwargs: Optional[dict[str, Any]] = None,\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n     ) -> LogitsProcessorList:\n@@ -1403,7 +1403,7 @@ def _merge_criteria_processor_list(\n     def compute_transition_scores(\n         self,\n         sequences: torch.Tensor,\n-        scores: Tuple[torch.Tensor],\n+        scores: tuple[torch.Tensor],\n         beam_indices: Optional[torch.Tensor] = None,\n         normalize_logits: bool = False,\n     ) -> torch.Tensor:\n@@ -1552,7 +1552,7 @@ def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n                     f\"The main and assistant moedels have different tokenizers. Please provide `tokenizer` and `assistant_tokenizer` to `generate()` {doc_reference}.\"\n                 )\n \n-    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n+    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n         \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n         # If a `Cache` instance is passed, checks whether the model is compatible with it\n         if isinstance(model_kwargs.get(\"past_key_values\", None), Cache) and not self._supports_cache_class:\n@@ -1709,8 +1709,8 @@ def _prepare_generated_length(\n         return generation_config\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Dict\n-    ) -> Tuple[GenerationConfig, Dict]:\n+        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n+    ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         Prepares the base generation config, then applies any generation configuration options from kwargs. This\n         function handles retrocompatibility with respect to configuration files.\n@@ -1821,7 +1821,7 @@ def _get_initial_cache_position(self, seq_length, device, model_kwargs):\n         model_kwargs[\"cache_position\"] = cache_position\n         return model_kwargs\n \n-    def _get_layer_device_map_for_cache_init(self) -> Optional[Dict[int, Union[str, int]]]:\n+    def _get_layer_device_map_for_cache_init(self) -> Optional[dict[int, Union[str, int]]]:\n         \"\"\"\n         Returns the device map for each decoder layer, to allocate the cache on the right device.\n         Inspired from `dispatch_model` in accelerate.\n@@ -1982,7 +1982,7 @@ def _supports_default_dynamic_cache(self) -> bool:\n     def _prepare_cache_for_generation(\n         self,\n         generation_config: GenerationConfig,\n-        model_kwargs: Dict,\n+        model_kwargs: dict,\n         assistant_model: \"PreTrainedModel\",\n         batch_size: int,\n         max_cache_length: int,\n@@ -2191,7 +2191,7 @@ def _tensor_or_none(token, device=None):\n         generation_config._pad_token_tensor = pad_token_tensor\n         generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n \n-    def _valid_auto_compile_criteria(self, model_kwargs: Dict, generation_config: GenerationConfig) -> bool:\n+    def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: GenerationConfig) -> bool:\n         \"\"\"\n         Determines whether to trigger auto-compilation of the model's forward pass at generation time.\n         \"\"\"\n@@ -2239,7 +2239,7 @@ def generate(\n         generation_config: Optional[GenerationConfig] = None,\n         logits_processor: Optional[LogitsProcessorList] = None,\n         stopping_criteria: Optional[StoppingCriteriaList] = None,\n-        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n         synced_gpus: Optional[bool] = None,\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         streamer: Optional[\"BaseStreamer\"] = None,\n@@ -2287,7 +2287,7 @@ def generate(\n                 generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n                 sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n                 intended for advanced users.\n-            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n+            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], list[int]]`, *optional*):\n                 If provided, this function constraints the beam search to allowed tokens only at each step. If not\n                 provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n                 `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n@@ -2321,7 +2321,7 @@ def generate(\n                 function defined in that reposity's `custom_generate/generate.py` file will be executed instead of the\n                 standard `generate` method. Note that the logic is for generation is entirely defined in that\n                 repository, and the return type may be different from the standard `generate` method.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n                 specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n@@ -2695,7 +2695,7 @@ def generate(\n \n                 def typeerror():\n                     raise ValueError(\n-                        \"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` \"\n+                        \"`force_words_ids` has to either be a `list[list[list[int]]]` or `list[list[int]]` \"\n                         f\"of positive integers, but is {generation_config.force_words_ids}.\"\n                     )\n \n@@ -2871,7 +2871,7 @@ def heal_tokens(\n     def _dola_decoding(\n         self,\n         input_ids: torch.LongTensor,\n-        dola_layers: Union[str, List[int]],\n+        dola_layers: Union[str, list[int]],\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n@@ -2888,7 +2888,7 @@ def _dola_decoding(\n         Parameters:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                 The sequence used as a prompt for the generation.\n-            dola_layers (`Union[str, List[int]]`):\n+            dola_layers (`Union[str, list[int]]`):\n                 The candidate layers used in contrasting layers of DoLa. It can be either 1) 'low' or 'high', which\n                 means the lower part or higher part of the model layers, respectively, or 2) a list of layer indices\n                 to be used for candidate layers. The 0-th layer is the word embedding layer of the model.\n@@ -3806,7 +3806,7 @@ def _get_top_k_continuations(\n         num_beams: int,\n         vocab_size: int,\n         batch_size: int,\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Get top-K continuations given the accumulated log probs on the next token.\n \n@@ -3855,7 +3855,7 @@ def _get_running_beams_for_next_iteration(\n         topk_running_beam_indices: torch.Tensor,\n         next_token_hits_stopping_criteria: torch.Tensor,\n         num_beams: int,\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Given the top-K continuations, their scores, and whether they hit a stopping criteria, select the\n         best non-finished beams to continue beam search in the next iteration.\n@@ -3886,7 +3886,7 @@ def _update_finished_beams(\n         decoder_prompt_len: int,\n         length_penalty: float,\n         early_stopping: Union[bool, str],\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the finished beams if (and only if) there are new completed sequences that have a higher score than\n         the current finished sequences.\n@@ -5236,8 +5236,8 @@ def _split(data, full_batch_size: int, split_size: int):\n \n \n def _split_model_inputs(\n-    model_input: Union[ModelOutput, Dict], split_size: int, full_batch_size: int, config: PretrainedConfig\n-) -> List[Union[ModelOutput, Dict]]:\n+    model_input: Union[ModelOutput, dict], split_size: int, full_batch_size: int, config: PretrainedConfig\n+) -> list[Union[ModelOutput, dict]]:\n     \"\"\"\n     Split a ModelOutput object (or its subclasses) or Dict into a list of same-class objects based on a specified split\n     size. The input object is dict when it was prepared for forward pass and ModelOutput when it was returned from\n@@ -5292,14 +5292,14 @@ def _split_model_inputs(\n         ]\n \n     # Convert each dictionary in the list to an object of the inferred class\n-    split_model_inputs: List[Union[ModelOutput, Dict]] = [\n+    split_model_inputs: list[Union[ModelOutput, dict]] = [\n         model_output_cls(**data_split, **bool_data) for data_split in data_split_list\n     ]\n \n     return split_model_inputs\n \n \n-def stack_model_outputs(model_outputs: List[ModelOutput], config: PretrainedConfig) -> ModelOutput:\n+def stack_model_outputs(model_outputs: list[ModelOutput], config: PretrainedConfig) -> ModelOutput:\n     \"\"\"\n     Stack a list of ModelOutput objects (or its subclasses) along the batch_size dimension. The function infers the\n     specific ModelOutput subclass from the list provided.\n@@ -5379,8 +5379,8 @@ def _relative_top_filter(\n \n \n def _dola_select_contrast(\n-    candidate_premature_layers: List[int],\n-    candidate_premature_logits: Dict[int, torch.FloatTensor],\n+    candidate_premature_layers: list[int],\n+    candidate_premature_logits: dict[int, torch.FloatTensor],\n     final_logits: torch.FloatTensor,\n ) -> torch.FloatTensor:\n     if len(candidate_premature_layers) == 1:"
        },
        {
            "sha": "16993a949a021a1a375c68bae772fdd9b85ffcb9",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n import collections\n from dataclasses import dataclass\n from functools import lru_cache\n-from typing import Any, Dict, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import torch\n@@ -126,7 +126,7 @@ def __init__(\n         self,\n         model_config: PretrainedConfig,\n         device: str,\n-        watermarking_config: Union[WatermarkingConfig, Dict],\n+        watermarking_config: Union[WatermarkingConfig, dict],\n         ignore_repeated_ngrams: bool = False,\n         max_cache_size: int = 128,\n     ):\n@@ -300,7 +300,7 @@ def __init__(self, watermarking_depth: int):\n         self.beta = torch.nn.Parameter(-2.5 + 0.001 * torch.randn(1, 1, watermarking_depth))\n         self.delta = torch.nn.Parameter(0.001 * torch.randn(1, 1, self.watermarking_depth, watermarking_depth))\n \n-    def _compute_latents(self, g_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n+    def _compute_latents(self, g_values: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Computes the unique token probability distribution given g-values.\n \n         Args:"
        },
        {
            "sha": "e6d92d2baa8f7b8968fb2b5c1970f65b4b5398ff",
            "filename": "src/transformers/hf_argparser.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fhf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fhf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhf_argparser.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -81,7 +81,7 @@ class Args:\n     ```\n \n     Args:\n-        aliases (Union[str, List[str]], optional):\n+        aliases (Union[str, list[str]], optional):\n             Single string or list of strings of aliases to pass on to argparse, e.g. `aliases=[\"--example\", \"-e\"]`.\n             Defaults to None.\n         help (str, optional): Help string to pass on to argparse that can be displayed with --help. Defaults to None.\n@@ -119,7 +119,7 @@ class HfArgumentParser(ArgumentParser):\n     Args:\n         dataclass_types (`DataClassType` or `Iterable[DataClassType]`, *optional*):\n             Dataclass type, or list of dataclass types for which we will \"fill\" instances with the parsed args.\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Passed to `argparse.ArgumentParser()` in the regular way.\n     \"\"\"\n "
        },
        {
            "sha": "4f4597dcff8e4a430055cc9cc245da655e30d872",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -127,7 +127,7 @@ def from_pretrained(\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or `bool`, *optional*):\n@@ -153,7 +153,7 @@ def from_pretrained(\n             subfolder (`str`, *optional*, defaults to `\"\"`):\n                 In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                 specify the folder name here.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are image processor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is\n                 controlled by the `return_unused_kwargs` keyword parameter.\n@@ -219,7 +219,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                 repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                 namespace).\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -286,7 +286,7 @@ def get_image_processor_dict(\n                 The name of the file in the model directory to use for the image processor config.\n \n         Returns:\n-            `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the image processor object.\n+            `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the image processor object.\n         \"\"\"\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n@@ -387,11 +387,11 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         Instantiates a type of [`~image_processing_utils.ImageProcessingMixin`] from a Python dictionary of parameters.\n \n         Args:\n-            image_processor_dict (`Dict[str, Any]`):\n+            image_processor_dict (`dict[str, Any]`):\n                 Dictionary that will be used to instantiate the image processor object. Such a dictionary can be\n                 retrieved from a pretrained checkpoint by leveraging the\n                 [`~image_processing_utils.ImageProcessingMixin.to_dict`] method.\n-            kwargs (`Dict[str, Any]`):\n+            kwargs (`dict[str, Any]`):\n                 Additional parameters from which to initialize the image processor object.\n \n         Returns:\n@@ -431,7 +431,7 @@ def to_dict(self) -> dict[str, Any]:\n         Serializes this instance to a Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this image processor instance.\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this image processor instance.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n         output[\"image_processor_type\"] = self.__class__.__name__"
        },
        {
            "sha": "d3086de1cce410f77f8cf94623e754a3d1829de5",
            "filename": "src/transformers/image_processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -130,7 +130,7 @@ def center_crop(\n         Args:\n             image (`np.ndarray`):\n                 Image to center crop.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format for the output image. If unset, the channel dimension format of the input\n@@ -227,7 +227,7 @@ def get_size_dict(\n       is set, it is added to the dict as `{\"longest_edge\": max_size}`.\n \n     Args:\n-        size (`Union[int, Iterable[int], Dict[str, int]]`, *optional*):\n+        size (`Union[int, Iterable[int], dict[str, int]]`, *optional*):\n             The `size` parameter to be cast into a size dictionary.\n         max_size (`Optional[int]`, *optional*):\n             The `max_size` parameter to be cast into a size dictionary."
        },
        {
            "sha": "4bfd208c9bb65e53b65ad12884e2255d5ed7e85d",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -382,7 +382,7 @@ def center_crop(\n         Args:\n             image (`\"torch.Tensor\"`):\n                 Image to center crop.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n \n         Returns:\n@@ -666,12 +666,12 @@ def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[lis\n         Args:\n             outputs ([`MobileNetV2ForSemanticSegmentation`]):\n                 Raw outputs of the model.\n-            target_sizes (`List[Tuple]` of length `batch_size`, *optional*):\n+            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n                 List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n                 predictions will not be resized.\n \n         Returns:\n-            semantic_segmentation: `List[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n             segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n             specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n         \"\"\""
        },
        {
            "sha": "b936403be7be2177e705555729881abbbf2d0710",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -217,7 +217,7 @@ def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, in\n     Computes the output image size given the input image size and the desired output size.\n \n     Args:\n-        image_size (`Tuple[int, int]`):\n+        image_size (`tuple[int, int]`):\n             The input image size.\n         size (`int`):\n             The desired output size.\n@@ -266,7 +266,7 @@ def get_resize_output_image_size(\n     Args:\n         input_image (`np.ndarray`):\n             The image to resize.\n-        size (`int` or `Tuple[int, int]` or List[int] or `Tuple[int]`):\n+        size (`int` or `tuple[int, int]` or list[int] or `tuple[int]`):\n             The size to use for resizing the image. If `size` is a sequence like (h, w), output size will be matched to\n             this.\n \n@@ -334,7 +334,7 @@ def resize(\n     Args:\n         image (`np.ndarray`):\n             The image to resize.\n-        size (`Tuple[int, int]`):\n+        size (`tuple[int, int]`):\n             The size to use for resizing the image.\n         resample (`int`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n             The filter to user for resampling.\n@@ -464,7 +464,7 @@ def center_crop(\n     Args:\n         image (`np.ndarray`):\n             The image to crop.\n-        size (`Tuple[int, int]`):\n+        size (`tuple[int, int]`):\n             The target size for the cropped image.\n         data_format (`str` or `ChannelDimension`, *optional*):\n             The channel dimension format for the output image. Can be one of:\n@@ -704,7 +704,7 @@ def pad(\n     Args:\n         image (`np.ndarray`):\n             The image to pad.\n-        padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n+        padding (`int` or `tuple[int, int]` or `Iterable[tuple[int, int]]`):\n             Padding to apply to the edges of the height, width axes. Can be one of three formats:\n             - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n             - `((before, after),)` yields same before and after pad for height and width."
        },
        {
            "sha": "f3db6fac44b7224e47c7be63404b06239f82b744",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -218,7 +218,7 @@ def make_flat_list_of_images(\n     Ensure that the output is a flat list of images. If the input is a single image, it is converted to a list of length 1.\n     If the input is a nested list of images, it is converted to a flat list of images.\n     Args:\n-        images (`Union[List[ImageInput], ImageInput]`):\n+        images (`Union[list[ImageInput], ImageInput]`):\n             The input image.\n     Returns:\n         list: A list of images or a 4d array of images.\n@@ -252,7 +252,7 @@ def make_nested_list_of_images(\n     \"\"\"\n     Ensure that the output is a nested list of images.\n     Args:\n-        images (`Union[List[ImageInput], ImageInput]`):\n+        images (`Union[list[ImageInput], ImageInput]`):\n             The input image.\n     Returns:\n         list: A list of list of images or a list of 4d array of images.\n@@ -300,7 +300,7 @@ def infer_channel_dimension_format(\n     Args:\n         image (`np.ndarray`):\n             The image to infer the channel dimension of.\n-        num_channels (`int` or `Tuple[int, ...]`, *optional*, defaults to `(1, 3)`):\n+        num_channels (`int` or `tuple[int, ...]`, *optional*, defaults to `(1, 3)`):\n             The number of channels of the image.\n \n     Returns:\n@@ -393,7 +393,7 @@ def get_image_size_for_max_height_width(\n         - input_size: (100, 200), max_height: 200, max_width: 500 -> output_size: (200, 400)\n \n     Args:\n-        image_size (`Tuple[int, int]`):\n+        image_size (`tuple[int, int]`):\n             The image to resize.\n         max_height (`int`):\n             The maximum allowed height.\n@@ -678,9 +678,9 @@ def normalize(self, image, mean, std, rescale=False):\n         Args:\n             image (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`):\n                 The image to normalize.\n-            mean (`List[float]` or `np.ndarray` or `torch.Tensor`):\n+            mean (`list[float]` or `np.ndarray` or `torch.Tensor`):\n                 The mean (per channel) to use for normalization.\n-            std (`List[float]` or `np.ndarray` or `torch.Tensor`):\n+            std (`list[float]` or `np.ndarray` or `torch.Tensor`):\n                 The standard deviation (per channel) to use for normalization.\n             rescale (`bool`, *optional*, defaults to `False`):\n                 Whether or not to rescale the image to be between 0 and 1. If a PIL image is provided, scaling will\n@@ -729,7 +729,7 @@ def resize(self, image, size, resample=None, default_to_square=True, max_size=No\n         Args:\n             image (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`):\n                 The image to resize.\n-            size (`int` or `Tuple[int, int]`):\n+            size (`int` or `tuple[int, int]`):\n                 The size to use for resizing the image. If `size` is a sequence like (h, w), output size will be\n                 matched to this.\n \n@@ -797,7 +797,7 @@ def center_crop(self, image, size):\n         Args:\n             image (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor` of shape (n_channels, height, width) or (height, width, n_channels)):\n                 The image to resize.\n-            size (`int` or `Tuple[int, int]`):\n+            size (`int` or `tuple[int, int]`):\n                 The size to which crop the image.\n \n         Returns:"
        },
        {
            "sha": "a404af54b24696d85c6b17cb103d380dede94610",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -156,7 +156,7 @@ def find_tied_parameters(model: \"nn.Module\", **kwargs):\n         model (`torch.nn.Module`): The model to inspect.\n \n     Returns:\n-        List[List[str]]: A list of lists of parameter names being all tied together.\n+        list[list[str]]: A list of lists of parameter names being all tied together.\n \n     Example:\n "
        },
        {
            "sha": "c09da6c92e6c8c1a7d4391d03abe65cacc1fde16",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -306,7 +306,7 @@ def _fuse_awq_layernorm(fuse_module_names, module, target_cls):\n     Fuse the LayerNorm layers into a target class using autoawq\n \n     Args:\n-        fuse_module_names (`List[str]`):\n+        fuse_module_names (`list[str]`):\n             The list of module names to fuse\n         module (`nn.Module`):\n             The pytorch parent module that has layernorm modules to fuse\n@@ -333,7 +333,7 @@ def _fuse_awq_mlp(model, current_module_name, fuse_module_names, module, target_\n             The input pretrained model\n         current_module_name (`str`):\n             The current submodule name\n-        fuse_module_names (`List[str]`):\n+        fuse_module_names (`list[str]`):\n             The list of module names to fuse. For the MLP layers it has to be an array\n             of length 3 that consists of the 3 MLP layers in the order (gate (dense layer post-attention) / up / down layers)\n         module (`nn.Module`):\n@@ -374,7 +374,7 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n             The input pretrained model\n         module (`nn.Module`):\n             The pytorch parent module that has layernorm modules to fuse\n-        modules_to_fuse (`List[str]`):\n+        modules_to_fuse (`list[str]`):\n             The module fusing mapping. The dictionary has to contain a field `attention` with attention module names\n             in the correct order: q, k, v, o layer\n         current_module_name (`str`):"
        },
        {
            "sha": "5deb3011211e7accb910eace3a1d8b3960abbd94",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -398,10 +398,10 @@ def replace_with_bitnet_linear(\n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n             Names of the modules to not convert in `BitLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n-        current_key_name (`List[`str`]`, *optional*):\n+        current_key_name (`list[`str`]`, *optional*):\n             An array to track the current key of the recursion. This is used to check whether the current key (part of\n             it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n             `disk`)."
        },
        {
            "sha": "0508477cd99617cf17a7816010dc972534e5ca43",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -243,10 +243,10 @@ def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name\n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n             Names of the modules to not convert in `Linear8bitLt`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n-        current_key_name (`List[`str`]`, *optional*):\n+        current_key_name (`list[`str`]`, *optional*):\n             An array to track the current key of the recursion. This is used to check whether the current key (part of\n             it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n             `disk`)."
        },
        {
            "sha": "45b7548e7e2485d3b32f95824aada618f7a2704c",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -93,10 +93,10 @@ def replace_with_eetq_linear(\n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n             Names of the modules to not convert in `EetqLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n-        current_key_name (`List[`str`]`, *optional*):\n+        current_key_name (`list[`str`]`, *optional*):\n             An array to track the current key of the recursion. This is used to check whether the current key (part of\n             it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n             `disk`)."
        },
        {
            "sha": "c65db3375ca9de264f10dd2e030ba78d48c8f1d2",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -251,10 +251,10 @@ def replace_with_fbgemm_fp8_linear(\n     Parameters:\n         model (`torch.nn.Module`):\n             Input model or `torch.nn.Module` as the function is run recursively.\n-        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n             Names of the modules to not convert in `FP8Linear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n-        current_key_name (`List[`str`]`, *optional*):\n+        current_key_name (`list[`str`]`, *optional*):\n             An array to track the current key of the recursion. This is used to check whether the current key (part of\n             it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n             `disk`)."
        },
        {
            "sha": "8156f1045baa72b9a8b5c4286799c29cdab6c8c9",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n \n@@ -45,7 +45,7 @@ def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):\n     tl.store(s_ptr + pid, s)\n \n \n-def act_quant(x: torch.Tensor, block_size: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:\n+def act_quant(x: torch.Tensor, block_size: int = 128) -> tuple[torch.Tensor, torch.Tensor]:\n     assert x.is_contiguous()\n     assert x.shape[-1] % block_size == 0\n     y = torch.empty_like(x, dtype=torch.float8_e4m3fn)\n@@ -149,7 +149,7 @@ def w8a8_block_fp8_matmul_triton(\n     B: torch.Tensor,\n     As: torch.Tensor,\n     Bs: torch.Tensor,\n-    block_size: List[int],\n+    block_size: list[int],\n     output_dtype: torch.dtype = torch.float32,\n ) -> torch.Tensor:\n     \"\"\"This function performs matrix multiplication with block-wise\n@@ -231,7 +231,7 @@ def w8a8_block_fp8_matmul_compile(\n     weight_q: torch.Tensor,  # [out_features, hidden_dim]\n     input_scale: torch.Tensor,  # [batch * seq_len, num_input_groups]\n     weight_scale: torch.Tensor,  # [num_weight_blocks_m, num_weight_blocks_n]\n-    block_size: Optional[Tuple[int, int]] = None,  # (M=128, N=128) for weights for example\n+    block_size: Optional[tuple[int, int]] = None,  # (M=128, N=128) for weights for example\n     output_dtype: torch.dtype = torch.float32,\n ) -> torch.Tensor:\n     \"\"\"\n@@ -300,7 +300,7 @@ def __init__(\n         out_features: int,\n         bias: bool = False,\n         dtype=None,\n-        block_size: Optional[Tuple[int, int]] = None,\n+        block_size: Optional[tuple[int, int]] = None,\n         device=None,\n         activation_scheme=\"dynamic\",\n     ):"
        },
        {
            "sha": "5a20ba2c8b296a18f1bee2e462902abb5bbf6d8a",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n \n@@ -22,7 +22,7 @@ def flash_attention_forward(\n     sliding_window: Optional[int] = None,\n     softcap: Optional[float] = None,\n     **kwargs,\n-) -> Tuple[torch.Tensor, None]:\n+) -> tuple[torch.Tensor, None]:\n     if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n         logger.warning_once(\n             \"`flash_attention_2` does not support `output_attentions=True` or `head_mask`.\""
        },
        {
            "sha": "fa817f6cb9d6528ee9abdb11e85bd2e9170f2e6d",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -26,7 +26,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from packaging import version\n@@ -106,7 +106,7 @@ def make_flex_block_causal_mask(\n     attention_chunk_size: Optional[int] = None,\n     query_length=None,\n     key_length=None,\n-    offsets: Optional[Tuple[Offset, Offset]] = None,\n+    offsets: Optional[tuple[Offset, Offset]] = None,\n     is_causal: Optional[bool] = True,\n ) -> \"BlockMask\":\n     \"\"\"\n@@ -234,7 +234,7 @@ def flex_attention_forward(\n     softcap: Optional[float] = None,\n     head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n-) -> Tuple[torch.Tensor, torch.Tensor]:\n+) -> tuple[torch.Tensor, torch.Tensor]:\n     if head_mask is not None:\n         logger.warning_once(\n             \"`flex_attention` does not support `head_mask`. Please set your attention to `eager` if you want this feature.\""
        },
        {
            "sha": "d424aa7c6cc96c6edd4300c3e7b5d3cfeb9c6748",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Dict, Union\n+from typing import Union\n \n from ..utils import is_torchdynamo_compiling\n \n@@ -29,7 +29,7 @@\n \n     _hub_kernels_available = True\n \n-    _KERNEL_MAPPING: Dict[str, Dict[Union[Device, str], LayerRepository]] = {\n+    _KERNEL_MAPPING: dict[str, dict[Union[Device, str], LayerRepository]] = {\n         \"MultiScaleDeformableAttention\": {\n             \"cuda\": LayerRepository(\n                 repo_id=\"kernels-community/deformable-detr\","
        },
        {
            "sha": "1f20ed2e7e1c960f5088f522d651f42c0d9b2a67",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -29,7 +29,7 @@\n from dataclasses import asdict, fields\n from enum import Enum\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Dict, Literal, Optional, Union\n+from typing import TYPE_CHECKING, Any, Literal, Optional, Union\n \n import numpy as np\n import packaging.version\n@@ -1692,7 +1692,7 @@ def get_run(cls, trainer):\n \n         raise Exception(\"The trainer doesn't have a NeptuneCallback configured.\")\n \n-    def on_log(self, args, state, control, logs: Optional[Dict[str, float]] = None, **kwargs):\n+    def on_log(self, args, state, control, logs: Optional[dict[str, float]] = None, **kwargs):\n         if not state.is_world_process_zero:\n             return\n "
        },
        {
            "sha": "3fc1fcff28f81926f4d29884f8bd9eac2c10179b",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n import inspect\n import re\n import warnings\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Optional, Union\n \n from packaging import version\n \n@@ -100,11 +100,11 @@ def load_adapter(\n         max_memory: Optional[str] = None,\n         offload_folder: Optional[str] = None,\n         offload_index: Optional[int] = None,\n-        peft_config: Optional[Dict[str, Any]] = None,\n-        adapter_state_dict: Optional[Dict[str, \"torch.Tensor\"]] = None,\n+        peft_config: Optional[dict[str, Any]] = None,\n+        adapter_state_dict: Optional[dict[str, \"torch.Tensor\"]] = None,\n         low_cpu_mem_usage: bool = False,\n         is_trainable: bool = False,\n-        adapter_kwargs: Optional[Dict[str, Any]] = None,\n+        adapter_kwargs: Optional[dict[str, Any]] = None,\n     ) -> None:\n         \"\"\"\n         Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT methods, we\n@@ -133,7 +133,7 @@ def load_adapter(\n                 Whether to use authentication token to load the remote folder. Useful to load private repositories\n                 that are on HuggingFace Hub. You might need to call `huggingface-cli login` and paste your tokens to\n                 cache it.\n-            device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n+            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n                 A map that specifies where each submodule should go. It doesn't need to be refined to each\n                 parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                 same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n@@ -150,10 +150,10 @@ def load_adapter(\n                 If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n             offload_index (`int`, `optional`):\n                 `offload_index` argument to be passed to `accelerate.dispatch_model` method.\n-            peft_config (`Dict[str, Any]`, *optional*):\n+            peft_config (`dict[str, Any]`, *optional*):\n                 The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts\n                 methods. This argument is used in case users directly pass PEFT state dicts\n-            adapter_state_dict (`Dict[str, torch.Tensor]`, *optional*):\n+            adapter_state_dict (`dict[str, torch.Tensor]`, *optional*):\n                 The state dict of the adapter to load. This argument is used in case users directly pass PEFT state\n                 dicts\n             low_cpu_mem_usage (`bool`, *optional*, defaults to `False`):\n@@ -162,7 +162,7 @@ def load_adapter(\n             is_trainable (`bool`, *optional*, defaults to `False`):\n                 Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n                 used for inference.\n-            adapter_kwargs (`Dict[str, Any]`, *optional*):\n+            adapter_kwargs (`dict[str, Any]`, *optional*):\n                 Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n                 `find_adapter_config_file` method.\n         \"\"\"\n@@ -348,15 +348,15 @@ def add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> Non\n \n         self.set_adapter(adapter_name)\n \n-    def set_adapter(self, adapter_name: Union[List[str], str]) -> None:\n+    def set_adapter(self, adapter_name: Union[list[str], str]) -> None:\n         \"\"\"\n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n         official documentation: https://huggingface.co/docs/peft\n \n         Sets a specific adapter by forcing the model to use a that adapter and disable the other adapters.\n \n         Args:\n-            adapter_name (`Union[List[str], str]`):\n+            adapter_name (`Union[list[str], str]`):\n                 The name of the adapter to set. Can be also a list of strings to set multiple adapters.\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n@@ -438,7 +438,7 @@ def enable_adapters(self) -> None:\n                 else:\n                     module.disable_adapters = False\n \n-    def active_adapters(self) -> List[str]:\n+    def active_adapters(self) -> list[str]:\n         \"\"\"\n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n         official documentation: https://huggingface.co/docs/peft\n@@ -518,7 +518,7 @@ def _dispatch_accelerate_model(\n         accelerate (i.e. with `device_map=xxx`)\n \n         Args:\n-            device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n+            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n                 A map that specifies where each submodule should go. It doesn't need to be refined to each\n                 parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                 same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n@@ -562,12 +562,12 @@ def _dispatch_accelerate_model(\n             **dispatch_model_kwargs,\n         )\n \n-    def delete_adapter(self, adapter_names: Union[List[str], str]) -> None:\n+    def delete_adapter(self, adapter_names: Union[list[str], str]) -> None:\n         \"\"\"\n         Delete an adapter's LoRA layers from the underlying model.\n \n         Args:\n-            adapter_names (`Union[List[str], str]`):\n+            adapter_names (`Union[list[str], str]`):\n                 The name(s) of the adapter(s) to delete.\n \n         Example:"
        },
        {
            "sha": "3667832061dfe088e50a5f904127b23753867cff",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n \n@@ -30,7 +30,7 @@ def sdpa_attention_forward(\n     scaling: Optional[float] = None,\n     is_causal: Optional[bool] = None,\n     **kwargs,\n-) -> Tuple[torch.Tensor, None]:\n+) -> tuple[torch.Tensor, None]:\n     if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n         logger.warning_once(\n             \"`sdpa` attention does not support `output_attentions=True` or `head_mask`.\""
        },
        {
            "sha": "558f4a6f7158a575ebf0298d198cb7c96a2d0c02",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n \n@@ -25,7 +25,7 @@ def sdpa_attention_paged_forward(\n     scaling: Optional[float] = None,\n     is_causal: Optional[bool] = None,\n     **kwargs,\n-) -> Tuple[torch.Tensor, None]:\n+) -> tuple[torch.Tensor, None]:\n     cache = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         key, value = cache.update(key, value, module.layer_idx, **kwargs)"
        },
        {
            "sha": "a5e4595ed93f1bc0096d94f80db3b0a00d18f8f4",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import os\n import re\n from functools import partial, reduce\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.distributed as dist\n@@ -93,15 +93,15 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None):\n     return tp_device, device_map, device_mesh\n \n \n-def _blocks_to_block_sizes(total_size: int, blocks: Union[int, List[int]]) -> List[int]:\n+def _blocks_to_block_sizes(total_size: int, blocks: Union[int, list[int]]) -> list[int]:\n     \"\"\"\n     Convert block count or proportions to block sizes.\n \n     This function accepts\n \n     - The number of blocks (int), in which case the block size is\n       total_size//blocks; or\n-    - A list of block sizes (List[int]).\n+    - A list of block sizes (list[int]).\n \n     In the second case, if sum(blocks) < total_size, the ratios between\n     the block sizes will be preserved. For instance, if blocks is\n@@ -608,7 +608,7 @@ def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         if self.use_dtensor:\n             if isinstance(module, nn.Linear):\n                 # rowwise linear runtime sharding requires input tensor shard on last dim\n-                self.desired_input_layouts: Tuple[Placement, ...] = (Shard(-1),)\n+                self.desired_input_layouts: tuple[Placement, ...] = (Shard(-1),)\n             elif isinstance(module, nn.Embedding):\n                 # rowwise embedding runtime sharding requires input tensor replicated\n                 self.desired_input_layouts = (Replicate(),)"
        },
        {
            "sha": "643fa91e652ae4e44c65404044d5af5abe9efb1e",
            "filename": "src/transformers/integrations/vptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fvptq.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -35,7 +35,7 @@ def replace_with_vptq_linear(\n             The model to convert, can be any `torch.nn.Module` instance.\n         quantization_config (`VptqConfig`):\n             The quantization config object that contains the quantization parameters.\n-        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n+        modules_to_not_convert (`list[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n             Names of the modules to not convert in `VQuantLinear`. In practice we keep the `lm_head` in full precision\n             for numerical stability reasons.\n         current_key_name (`list`, *optional*):"
        },
        {
            "sha": "f40590691215599c08eb33d111ab39132b41fae2",
            "filename": "src/transformers/keras_callbacks.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fkeras_callbacks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fkeras_callbacks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkeras_callbacks.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -54,9 +54,9 @@ def rouge_fn(predictions, labels):\n             metric names to numerical values.\n         eval_dataset (`tf.data.Dataset` or `dict` or `tuple` or `np.ndarray` or `tf.Tensor`):\n             Validation data to be used to generate predictions for the `metric_fn`.\n-        output_cols (`List[str], *optional*):\n+        output_cols (`list[str], *optional*):\n             A list of columns to be retained from the model output as the predictions. Defaults to all.\n-        label_cols ('`List[str]`, *optional*'):\n+        label_cols ('`list[str]`, *optional*'):\n             A list of columns to be retained from the input dataset as the labels. Will be autodetected if this is not\n             supplied.\n         batch_size (`int`, *optional*):"
        },
        {
            "sha": "3fdfd9d65e17b9bb022d84b0294504f74e3cd686",
            "filename": "src/transformers/loss/loss_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_d_fine.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -97,7 +97,7 @@ def translate_gt(gt: torch.Tensor, max_num_bins: int, reg_scale: int, up: torch.\n         up (Tensor): Controls the upper bounds of the Weighting Function.\n \n     Returns:\n-        Tuple[Tensor, Tensor, Tensor]:\n+        tuple[Tensor, Tensor, Tensor]:\n             - indices (Tensor): Index of the left bin closest to each GT value, shape (N, ).\n             - weight_right (Tensor): Weight assigned to the right bin, shape (N, ).\n             - weight_left (Tensor): Weight assigned to the left bin, shape (N, ).\n@@ -184,7 +184,7 @@ class DFineLoss(RTDetrLoss):\n         weight_dict (`Dict`):\n             Dictionary relating each loss with its weights. These losses are configured in DFineConf as\n             `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`, `weight_loss_fgl`, `weight_loss_ddf`\n-        losses (`List[str]`):\n+        losses (`list[str]`):\n             List of all the losses to be applied. See `get_loss` for a list of all available losses.\n         alpha (`float`):\n             Parameter alpha used to compute the focal loss."
        },
        {
            "sha": "20b16f58bdd900357c704dd6da34864151f622a1",
            "filename": "src/transformers/loss/loss_for_object_detection.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_for_object_detection.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import List, Optional\n+from typing import Optional\n \n import torch\n import torch.nn as nn\n@@ -104,7 +104,7 @@ class ImageLoss(nn.Module):\n             Number of object categories, omitting the special no-object category.\n         eos_coef (`float`):\n             Relative classification weight applied to the no-object category.\n-        losses (`List[str]`):\n+        losses (`list[str]`):\n             List of all the losses to be applied. See `get_loss` for a list of all available losses.\n     \"\"\"\n \n@@ -243,7 +243,7 @@ def forward(self, outputs, targets):\n         Args:\n              outputs (`dict`, *optional*):\n                 Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n+             targets (`list[dict]`, *optional*):\n                 List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n                 losses applied, see each loss' doc.\n         \"\"\"\n@@ -318,15 +318,15 @@ def forward(self, outputs, targets):\n                 A dictionary that contains at least these entries:\n                 * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                 * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n-            targets (`List[dict]`):\n+            targets (`list[dict]`):\n                 A list of targets (len(targets) = batch_size), where each target is a dict containing:\n                 * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n                   ground-truth\n                  objects in the target) containing the class labels\n                 * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n \n         Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n+            `list[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n             - index_i is the indices of the selected predictions (in order)\n             - index_j is the indices of the corresponding selected targets (in order)\n             For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n@@ -431,7 +431,7 @@ def generalized_box_iou(boxes1, boxes2):\n \n # below: taken from https://github.com/facebookresearch/detr/blob/master/util/misc.py#L306\n def _max_by_axis(the_list):\n-    # type: (List[List[int]]) -> List[int]\n+    # type: (list[list[int]]) -> list[int]\n     maxes = the_list[0]\n     for sublist in the_list[1:]:\n         for index, item in enumerate(sublist):\n@@ -460,7 +460,7 @@ def __repr__(self):\n         return str(self.tensors)\n \n \n-def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n+def nested_tensor_from_tensor_list(tensor_list: list[Tensor]):\n     if tensor_list[0].ndim == 3:\n         max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n         batch_shape = [len(tensor_list)] + max_size"
        },
        {
            "sha": "17ea065e84a3783fe26a9cfe2135951ef1cd17f2",
            "filename": "src/transformers/loss/loss_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_grounding_dino.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -74,15 +74,15 @@ def forward(self, outputs, targets):\n                 * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                 * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n                 * \"label_maps\": Tuple of tensors of dim [num_classes, hidden_dim].\n-            targets (`List[dict]`):\n+            targets (`list[dict]`):\n                 A list of targets (len(targets) = batch_size), where each target is a dict containing:\n                 * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n                   ground-truth\n                  objects in the target) containing the class labels\n                 * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n \n         Returns:\n-            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n+            `list[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n             - index_i is the indices of the selected predictions (in order)\n             - index_j is the indices of the corresponding selected targets (in order)\n             For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n@@ -136,7 +136,7 @@ class GroundingDinoImageLoss(ImageLoss):\n             Module able to compute a matching between targets and proposals.\n         focal_alpha (`float`):\n             Alpha parameter in focal loss.\n-        losses (`List[str]`):\n+        losses (`list[str]`):\n             List of all the losses to be applied. See `get_loss` for a list of all available losses.\n     \"\"\"\n "
        },
        {
            "sha": "eb6e2e65a01a8cb8d485fb60ce1cc7bea96cbe8e",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -136,7 +136,7 @@ class RTDetrLoss(nn.Module):\n         weight_dict (`Dict`):\n             Dictionary relating each loss with its weights. These losses are configured in RTDetrConf as\n             `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`\n-        losses (`List[str]`):\n+        losses (`list[str]`):\n             List of all the losses to be applied. See `get_loss` for a list of all available losses.\n         alpha (`float`):\n             Parameter alpha used to compute the focal loss.\n@@ -374,7 +374,7 @@ def forward(self, outputs, targets):\n         Args:\n              outputs (`dict`, *optional*):\n                 Dictionary of tensors, see the output specification of the model for the format.\n-             targets (`List[dict]`, *optional*):\n+             targets (`list[dict]`, *optional*):\n                 List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n                 losses applied, see each loss' doc.\n         \"\"\""
        },
        {
            "sha": "9f3003150a163dc4b7bd67fdfca8eda8ebf7e325",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -167,7 +167,7 @@ def _repr_to_list(value: torch.Tensor):\n         value (`torch.Tensor`): The tensor to represent.\n \n     Returns:\n-        `List[str]`: List of string lines representing the tensor.\n+        `list[str]`: List of string lines representing the tensor.\n     \"\"\"\n     torch.set_printoptions(sci_mode=True, linewidth=120)\n     with StringIO() as buf, redirect_stdout(buf):"
        },
        {
            "sha": "03e2922b558c1b083d3d198685249e6c03d6e009",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -144,9 +144,9 @@ def _upad_input(\n             Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n         indices_q (`torch.Tensor`):\n             The indices of non-masked tokens from the flattened input target sequence.\n-        (cu_seqlens_q, cu_seqlens_k) (`Tuple[int]`):\n+        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n             The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n-        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`Tuple[int]`):\n+        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n     indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n@@ -216,9 +216,9 @@ def prepare_fa2_from_position_ids(query, key, value, position_ids):\n             Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n         indices_q (`torch.Tensor`):\n             The indices of non-masked tokens from the flattened input target sequence.\n-        (cu_seqlens_q, cu_seqlens_k) (`Tuple[int]`):\n+        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n             The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n-        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`Tuple[int]`):\n+        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n     query = query.view(-1, query.size(-2), query.size(-1))"
        },
        {
            "sha": "5a25a6059a255659c6d900b35d2ffa7cab57f071",
            "filename": "src/transformers/modeling_flax_outputs.py",
            "status": "modified",
            "additions": 60,
            "deletions": 60,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flax_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flax_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_outputs.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Dict, Optional, Tuple\n+from typing import Optional\n \n import flax\n import jax.numpy as jnp\n@@ -41,8 +41,8 @@ class FlaxBaseModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -60,7 +60,7 @@ class FlaxBaseModelOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -81,7 +81,7 @@ class FlaxBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n     pooler_output: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -100,7 +100,7 @@ class FlaxImageClassifierOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -111,7 +111,7 @@ class FlaxBaseModelOutputWithPast(ModelOutput):\n     Args:\n         last_hidden_state (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`):\n             Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`Dict[str, jnp.ndarray]`):\n+        past_key_values (`dict[str, jnp.ndarray]`):\n             Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n             auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n         hidden_states (`tuple(jnp.ndarray)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n@@ -128,9 +128,9 @@ class FlaxBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Dict[str, jnp.ndarray]] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[dict[str, jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -160,8 +160,8 @@ class FlaxBaseModelOutputWithPooling(ModelOutput):\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n     pooler_output: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -207,10 +207,10 @@ class FlaxBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n     pooler_output: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -253,10 +253,10 @@ class FlaxBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -311,13 +311,13 @@ class FlaxSeq2SeqModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n     encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    encoder_attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -355,10 +355,10 @@ class FlaxCausalLMOutputWithCrossAttentions(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -383,8 +383,8 @@ class FlaxMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n FlaxCausalLMOutput = FlaxMaskedLMOutput\n@@ -438,13 +438,13 @@ class FlaxSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n     encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    encoder_attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -470,8 +470,8 @@ class FlaxNextSentencePredictorOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -496,8 +496,8 @@ class FlaxSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -548,13 +548,13 @@ class FlaxSeq2SeqSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n     encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    encoder_attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -581,8 +581,8 @@ class FlaxMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -607,8 +607,8 @@ class FlaxTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -636,8 +636,8 @@ class FlaxQuestionAnsweringModelOutput(ModelOutput):\n \n     start_logits: Optional[jnp.ndarray] = None\n     end_logits: Optional[jnp.ndarray] = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -691,10 +691,10 @@ class FlaxSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n \n     start_logits: Optional[jnp.ndarray] = None\n     end_logits: Optional[jnp.ndarray] = None\n-    past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n-    decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n-    cross_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    past_key_values: Optional[tuple[tuple[jnp.ndarray]]] = None\n+    decoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    decoder_attentions: Optional[tuple[jnp.ndarray]] = None\n+    cross_attentions: Optional[tuple[jnp.ndarray]] = None\n     encoder_last_hidden_state: Optional[jnp.ndarray] = None\n-    encoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    encoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n+    encoder_hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    encoder_attentions: Optional[tuple[jnp.ndarray]] = None"
        },
        {
            "sha": "7d9f18979998d64778dc82c46401386c45c9a9dd",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,6 @@\n \n import os\n from pickle import UnpicklingError\n-from typing import Dict, Tuple\n \n import jax\n import jax.numpy as jnp\n@@ -83,14 +82,14 @@ def load_pytorch_checkpoint_in_flax_state_dict(\n \n \n def rename_key_and_reshape_tensor(\n-    pt_tuple_key: Tuple[str],\n+    pt_tuple_key: tuple[str],\n     pt_tensor: np.ndarray,\n-    random_flax_state_dict: Dict[str, jnp.ndarray],\n+    random_flax_state_dict: dict[str, jnp.ndarray],\n     model_prefix: str,\n-) -> (Tuple[str], np.ndarray):\n+) -> (tuple[str], np.ndarray):\n     \"\"\"Rename PT weight names to corresponding Flax weight names and reshape tensor if necessary\"\"\"\n \n-    def is_key_or_prefix_key_in_dict(key: Tuple[str]) -> bool:\n+    def is_key_or_prefix_key_in_dict(key: tuple[str]) -> bool:\n         \"\"\"Checks if `key` of `(prefix,) + key` is in random_flax_state_dict\"\"\"\n         return len(set(random_flax_state_dict) & {key, (model_prefix,) + key}) > 0\n "
        },
        {
            "sha": "dfc0631abe0efa378a6e0883bb83b9bd32a22d53",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -20,7 +20,7 @@\n import warnings\n from functools import partial\n from pickle import UnpicklingError\n-from typing import Any, Dict, Optional, Set, Tuple, Union\n+from typing import Any, Optional, Union\n \n import flax.linen as nn\n import jax\n@@ -174,7 +174,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         module: nn.Module,\n-        input_shape: Tuple = (1, 1),\n+        input_shape: tuple = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -225,7 +225,7 @@ def __init__(\n         if _do_init:\n             self.params = random_params\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> Dict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> dict:\n         raise NotImplementedError(f\"init method has to be implemented for {self}\")\n \n     def enable_gradient_checkpointing(self):\n@@ -254,7 +254,7 @@ def module(self) -> nn.Module:\n         return self._module\n \n     @property\n-    def params(self) -> Union[Dict, FrozenDict]:\n+    def params(self) -> Union[dict, FrozenDict]:\n         if not self._is_initialized:\n             raise ValueError(\n                 \"`params` cannot be accessed from model when the model is created with `_do_init=False`. \"\n@@ -264,15 +264,15 @@ def params(self) -> Union[Dict, FrozenDict]:\n         return self._params\n \n     @property\n-    def required_params(self) -> Set:\n+    def required_params(self) -> set:\n         return self._required_params\n \n     @property\n-    def params_shape_tree(self) -> Dict:\n+    def params_shape_tree(self) -> dict:\n         return self._params_shape_tree\n \n     @params.setter\n-    def params(self, params: Union[Dict, FrozenDict]):\n+    def params(self, params: Union[dict, FrozenDict]):\n         # don't set params if the model is not initialized\n         if not self._is_initialized:\n             raise ValueError(\n@@ -290,7 +290,7 @@ def params(self, params: Union[Dict, FrozenDict]):\n             )\n         self._params = params\n \n-    def _cast_floating_to(self, params: Union[Dict, FrozenDict], dtype: jnp.dtype, mask: Any = None) -> Any:\n+    def _cast_floating_to(self, params: Union[dict, FrozenDict], dtype: jnp.dtype, mask: Any = None) -> Any:\n         \"\"\"\n         Helper method to cast floating-point values of given parameter `PyTree` to given `dtype`.\n         \"\"\"\n@@ -313,7 +313,7 @@ def conditional_cast(param):\n \n         return unflatten_dict(flat_params)\n \n-    def to_bf16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n+    def to_bf16(self, params: Union[dict, FrozenDict], mask: Any = None):\n         r\"\"\"\n         Cast the floating-point `params` to `jax.numpy.bfloat16`. This returns a new `params` tree and does not cast\n         the `params` in place.\n@@ -352,7 +352,7 @@ def to_bf16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n         ```\"\"\"\n         return self._cast_floating_to(params, jnp.bfloat16, mask)\n \n-    def to_fp32(self, params: Union[Dict, FrozenDict], mask: Any = None):\n+    def to_fp32(self, params: Union[dict, FrozenDict], mask: Any = None):\n         r\"\"\"\n         Cast the floating-point `params` to `jax.numpy.float32`. This method can be used to explicitly convert the\n         model parameters to fp32 precision. This returns a new `params` tree and does not cast the `params` in place.\n@@ -379,7 +379,7 @@ def to_fp32(self, params: Union[Dict, FrozenDict], mask: Any = None):\n         ```\"\"\"\n         return self._cast_floating_to(params, jnp.float32, mask)\n \n-    def to_fp16(self, params: Union[Dict, FrozenDict], mask: Any = None):\n+    def to_fp16(self, params: Union[dict, FrozenDict], mask: Any = None):\n         r\"\"\"\n         Cast the floating-point `params` to `jax.numpy.float16`. This returns a new `params` tree and does not cast the\n         `params` in place.\n@@ -453,7 +453,7 @@ def load_flax_sharded_weights(cls, shard_files):\n         loaded in the model.\n \n         Args:\n-            shard_files (`List[str]`:\n+            shard_files (`list[str]`:\n                 The list of shard files to load.\n \n         Returns:\n@@ -581,7 +581,7 @@ def from_pretrained(\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             local_files_only(`bool`, *optional*, defaults to `False`):\n@@ -1113,7 +1113,7 @@ def save_pretrained(\n             token (`str` or `bool`, *optional*):\n                 The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                 the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n             safe_serialization (`bool`, *optional*, defaults to `False`):\n                 Whether to save the model using `safetensors` or through msgpack."
        },
        {
            "sha": "597e20b28ca8bc8aa84b7f94b1b8033b89e0cd30",
            "filename": "src/transformers/modeling_outputs.py",
            "status": "modified",
            "additions": 126,
            "deletions": 126,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_outputs.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import torch\n \n@@ -44,8 +44,8 @@ class BaseModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -64,7 +64,7 @@ class BaseModelOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -95,8 +95,8 @@ class BaseModelOutputWithPooling(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -118,7 +118,7 @@ class BaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -153,8 +153,8 @@ class BaseModelOutputWithPast(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -185,9 +185,9 @@ class BaseModelOutputWithCrossAttentions(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -230,10 +230,10 @@ class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     past_key_values: Optional[Cache] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -274,9 +274,9 @@ class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -320,11 +320,11 @@ class MoECausalLMOutputWithPast(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     z_loss: Optional[torch.FloatTensor] = None\n     aux_loss: Optional[torch.FloatTensor] = None\n-    router_logits: Optional[Tuple[torch.FloatTensor]] = None\n+    router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -354,9 +354,9 @@ class MoEModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    router_probs: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    router_probs: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -393,9 +393,9 @@ class MoeModelOutputWithPast(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    router_logits: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -441,9 +441,9 @@ class MoeCausalLMOutputWithPast(ModelOutput):\n     aux_loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    router_logits: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -490,10 +490,10 @@ class MoEModelOutputWithPastAndCrossAttentions(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    router_probs: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    router_probs: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -547,12 +547,12 @@ class Seq2SeqModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -615,14 +615,14 @@ class Seq2SeqMoEModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_router_logits: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_router_logits: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -650,8 +650,8 @@ class CausalLMOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -685,8 +685,8 @@ class CausalLMOutputWithPast(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -726,9 +726,9 @@ class CausalLMOutputWithCrossAttentions(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -762,8 +762,8 @@ class SequenceClassifierOutputWithPast(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -791,8 +791,8 @@ class MaskedLMOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -845,12 +845,12 @@ class Seq2SeqLMOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -916,14 +916,14 @@ class Seq2SeqMoEOutput(ModelOutput):\n     encoder_aux_loss: Optional[torch.FloatTensor] = None\n     decoder_aux_loss: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_router_logits: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_router_logits: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -952,8 +952,8 @@ class NextSentencePredictorOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -981,8 +981,8 @@ class SequenceClassifierOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1035,12 +1035,12 @@ class Seq2SeqSequenceClassifierOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1070,8 +1070,8 @@ class MultipleChoiceModelOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1099,8 +1099,8 @@ class TokenClassifierOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1131,8 +1131,8 @@ class QuestionAnsweringModelOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     start_logits: Optional[torch.FloatTensor] = None\n     end_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1188,12 +1188,12 @@ class Seq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     start_logits: Optional[torch.FloatTensor] = None\n     end_logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1230,8 +1230,8 @@ class SemanticSegmenterOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1258,8 +1258,8 @@ class ImageClassifierOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1280,7 +1280,7 @@ class ImageClassifierOutputWithNoAttention(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1309,8 +1309,8 @@ class DepthEstimatorOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     predicted_depth: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1337,8 +1337,8 @@ class ImageSuperResolutionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     reconstruction: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1366,8 +1366,8 @@ class Wav2Vec2BaseModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     extract_features: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1398,8 +1398,8 @@ class XVectorOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1424,9 +1424,9 @@ class BackboneOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    feature_maps: Optional[Tuple[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    feature_maps: Optional[tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1461,9 +1461,9 @@ class BaseModelOutputWithPoolingAndProjection(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    projection_state: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    projection_state: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -1516,12 +1516,12 @@ class Seq2SeqSpectrogramOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     spectrogram: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -1583,12 +1583,12 @@ class Seq2SeqTSModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     loc: Optional[torch.FloatTensor] = None\n     scale: Optional[torch.FloatTensor] = None\n     static_features: Optional[torch.FloatTensor] = None\n@@ -1651,14 +1651,14 @@ class Seq2SeqTSPredictionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    params: Optional[Tuple[torch.FloatTensor]] = None\n+    params: Optional[tuple[torch.FloatTensor]] = None\n     past_key_values: Optional[EncoderDecoderCache] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     loc: Optional[torch.FloatTensor] = None\n     scale: Optional[torch.FloatTensor] = None\n     static_features: Optional[torch.FloatTensor] = None\n@@ -1702,8 +1702,8 @@ class MaskedImageModelingOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     reconstruction: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n     @property\n     def logits(self):"
        },
        {
            "sha": "822c7b486a2230e8df7e2d654120f53353f29dbf",
            "filename": "src/transformers/modeling_tf_outputs.py",
            "status": "modified",
            "additions": 86,
            "deletions": 86,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_outputs.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import tensorflow as tf\n \n@@ -45,8 +45,8 @@ class TFBaseModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -65,7 +65,7 @@ class TFBaseModelOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Optional[Tuple[tf.Tensor, ...]] = None\n+    hidden_states: Optional[tuple[tf.Tensor, ...]] = None\n \n \n @dataclass\n@@ -98,8 +98,8 @@ class TFBaseModelOutputWithPooling(ModelOutput):\n \n     last_hidden_state: Optional[tf.Tensor] = None\n     pooler_output: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -121,7 +121,7 @@ class TFBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n \n     last_hidden_state: Optional[tf.Tensor] = None\n     pooler_output: Optional[tf.Tensor] = None\n-    hidden_states: Optional[Tuple[tf.Tensor, ...]] = None\n+    hidden_states: Optional[tuple[tf.Tensor, ...]] = None\n \n \n @dataclass\n@@ -139,7 +139,7 @@ class TFBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n \n             This output is usually *not* a good summary of the semantic content of the input, you're often better with\n             averaging or pooling the sequence of hidden-states for the whole input sequence.\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -166,10 +166,10 @@ class TFBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n \n     last_hidden_state: Optional[tf.Tensor] = None\n     pooler_output: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -183,7 +183,7 @@ class TFBaseModelOutputWithPast(ModelOutput):\n \n             If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n             hidden_size)` is output.\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -203,9 +203,9 @@ class TFBaseModelOutputWithPast(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -236,9 +236,9 @@ class TFBaseModelOutputWithCrossAttentions(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -252,7 +252,7 @@ class TFBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n \n             If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n             hidden_size)` is output.\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -278,10 +278,10 @@ class TFBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -296,7 +296,7 @@ class TFSeq2SeqModelOutput(ModelOutput):\n \n             If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n             hidden_size)` is output.\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -335,13 +335,13 @@ class TFSeq2SeqModelOutput(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    decoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    decoder_attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    decoder_hidden_states: tuple[tf.Tensor] | None = None\n+    decoder_attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n     encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    encoder_attentions: Tuple[tf.Tensor] | None = None\n+    encoder_hidden_states: tuple[tf.Tensor] | None = None\n+    encoder_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -369,8 +369,8 @@ class TFCausalLMOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -383,7 +383,7 @@ class TFCausalLMOutputWithPast(ModelOutput):\n             Language modeling loss (for next-token prediction).\n         logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n             Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -404,9 +404,9 @@ class TFCausalLMOutputWithPast(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -436,7 +436,7 @@ class TFCausalLMOutputWithCrossAttentions(ModelOutput):\n \n             Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n             weighted average in the cross-attention heads.\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -446,10 +446,10 @@ class TFCausalLMOutputWithCrossAttentions(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -477,8 +477,8 @@ class TFMaskedLMOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -491,7 +491,7 @@ class TFSeq2SeqLMOutput(ModelOutput):\n             Language modeling loss.\n         logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n             Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -531,13 +531,13 @@ class TFSeq2SeqLMOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    decoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    decoder_attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    decoder_hidden_states: tuple[tf.Tensor] | None = None\n+    decoder_attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n     encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    encoder_attentions: Tuple[tf.Tensor] | None = None\n+    encoder_hidden_states: tuple[tf.Tensor] | None = None\n+    encoder_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -566,8 +566,8 @@ class TFNextSentencePredictorOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -595,8 +595,8 @@ class TFSequenceClassifierOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -609,7 +609,7 @@ class TFSeq2SeqSequenceClassifierOutput(ModelOutput):\n             Classification (or regression if config.num_labels==1) loss.\n         logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n             Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -646,13 +646,13 @@ class TFSeq2SeqSequenceClassifierOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    decoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    decoder_attentions: Tuple[tf.Tensor] | None = None\n-    cross_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    decoder_hidden_states: tuple[tf.Tensor] | None = None\n+    decoder_attentions: tuple[tf.Tensor] | None = None\n+    cross_attentions: tuple[tf.Tensor] | None = None\n     encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    encoder_attentions: Tuple[tf.Tensor] | None = None\n+    encoder_hidden_states: tuple[tf.Tensor] | None = None\n+    encoder_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -688,8 +688,8 @@ class TFSemanticSegmenterOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -720,7 +720,7 @@ class TFSemanticSegmenterOutputWithNoAttention(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -746,8 +746,8 @@ class TFImageClassifierOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -777,8 +777,8 @@ class TFMultipleChoiceModelOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -806,8 +806,8 @@ class TFTokenClassifierOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -838,8 +838,8 @@ class TFQuestionAnsweringModelOutput(ModelOutput):\n     loss: tf.Tensor | None = None\n     start_logits: Optional[tf.Tensor] = None\n     end_logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -854,7 +854,7 @@ class TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n             Span-start scores (before SoftMax).\n         end_logits (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n             Span-end scores (before SoftMax).\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -889,12 +889,12 @@ class TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     loss: tf.Tensor | None = None\n     start_logits: Optional[tf.Tensor] = None\n     end_logits: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    decoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    decoder_attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    decoder_hidden_states: tuple[tf.Tensor] | None = None\n+    decoder_attentions: tuple[tf.Tensor] | None = None\n     encoder_last_hidden_state: tf.Tensor | None = None\n-    encoder_hidden_states: Tuple[tf.Tensor] | None = None\n-    encoder_attentions: Tuple[tf.Tensor] | None = None\n+    encoder_hidden_states: tuple[tf.Tensor] | None = None\n+    encoder_attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -907,7 +907,7 @@ class TFSequenceClassifierOutputWithPast(ModelOutput):\n             Classification (or regression if config.num_labels==1) loss.\n         logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n             Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        past_key_values (`List[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        past_key_values (`list[tf.Tensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads,\n             sequence_length, embed_size_per_head)`).\n \n@@ -928,9 +928,9 @@ class TFSequenceClassifierOutputWithPast(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    past_key_values: List[tf.Tensor] | None = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -951,7 +951,7 @@ class TFImageClassifierOutputWithNoAttention(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     logits: Optional[tf.Tensor] = None\n-    hidden_states: Optional[Tuple[tf.Tensor, ...]] = None\n+    hidden_states: Optional[tuple[tf.Tensor, ...]] = None\n \n \n @dataclass\n@@ -978,8 +978,8 @@ class TFMaskedImageModelingOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     reconstruction: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n     @property\n     def logits(self):"
        },
        {
            "sha": "4c758daed87dd643d790a8c899363c1d2800f181",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -27,7 +27,7 @@\n import warnings\n from collections.abc import Mapping\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n \n import h5py\n import numpy as np\n@@ -106,10 +106,10 @@\n tf_logger = tf.get_logger()\n \n TFModelInputType = Union[\n-    List[tf.Tensor],\n-    List[np.ndarray],\n-    Dict[str, tf.Tensor],\n-    Dict[str, np.ndarray],\n+    list[tf.Tensor],\n+    list[np.ndarray],\n+    dict[str, tf.Tensor],\n+    dict[str, np.ndarray],\n     tf.Tensor,\n     np.ndarray,\n ]\n@@ -645,7 +645,7 @@ def tf_shard_checkpoint(weights, max_shard_size=\"10GB\", weights_name: str = TF2_\n     </Tip>\n \n     Args:\n-        weights (`Dict[str, tf.RessourceVariable]`): The list of tf.RessourceVariable of a model to save.\n+        weights (`dict[str, tf.RessourceVariable]`): The list of tf.RessourceVariable of a model to save.\n         max_shard_size (`int` or `str`, *optional*, defaults to `\"10GB\"`):\n             The maximum size of each sub-checkpoint. If expressed as a string, needs to be digits followed by a unit\n             (like `\"5MB\"`).\n@@ -1142,12 +1142,12 @@ class TFPreTrainedModel(keras.Model, TFModelUtilsMixin, TFGenerationMixin, PushT\n     _requires_load_weight_prefix = False\n \n     @property\n-    def dummy_inputs(self) -> Dict[str, tf.Tensor]:\n+    def dummy_inputs(self) -> dict[str, tf.Tensor]:\n         \"\"\"\n         Dummy inputs to build the network.\n \n         Returns:\n-            `Dict[str, tf.Tensor]`: The dummy inputs.\n+            `dict[str, tf.Tensor]`: The dummy inputs.\n         \"\"\"\n         dummies = {}\n         for key, spec in self.input_signature.items():\n@@ -1288,15 +1288,15 @@ def serving(self, inputs):\n         Args:\n         Method used for serving the model. Does not have a specific signature, but will be specialized as concrete\n         functions when saving with `save_pretrained`.\n-            inputs (`Dict[str, tf.Tensor]`):\n+            inputs (`dict[str, tf.Tensor]`):\n                 The input of the saved model as a dictionary of tensors.\n         \"\"\"\n         output = self.call(inputs)\n \n         return self.serving_output(output)\n \n     @property\n-    def input_signature(self) -> Dict[str, tf.TensorSpec]:\n+    def input_signature(self) -> dict[str, tf.TensorSpec]:\n         \"\"\"\n         This property should return a dict mapping input names to tf.TensorSpec objects, representing the expected\n         shape and dtype for model inputs. It is used for both serving and for generating dummy inputs.\n@@ -1414,7 +1414,7 @@ def prepare_tf_dataset(\n         shuffle: bool = True,\n         tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         collate_fn: Optional[Callable] = None,\n-        collate_fn_args: Optional[Dict[str, Any]] = None,\n+        collate_fn_args: Optional[dict[str, Any]] = None,\n         drop_remainder: Optional[bool] = None,\n         prefetch: bool = True,\n     ):\n@@ -1440,7 +1440,7 @@ def prepare_tf_dataset(\n                 A function that collates samples from the dataset into a single batch. Defaults to\n                 `DefaultDataCollator` if no `tokenizer` is supplied or `DataCollatorWithPadding` if a `tokenizer` is\n                 passed.\n-            collate_fn_args (`Dict[str, Any]`, *optional*):\n+            collate_fn_args (`dict[str, Any]`, *optional*):\n                 A dict of arguments to pass to the `collate_fn` alongside the list of samples.\n             drop_remainder (`bool`, *optional*):\n                 Whether to drop the final batch, if the batch_size does not evenly divide the dataset length. Defaults\n@@ -1816,9 +1816,9 @@ def create_model_card(\n         tags: Optional[str] = None,\n         finetuned_from: Optional[str] = None,\n         tasks: Optional[str] = None,\n-        dataset_tags: Optional[Union[str, List[str]]] = None,\n-        dataset: Optional[Union[str, List[str]]] = None,\n-        dataset_args: Optional[Union[str, List[str]]] = None,\n+        dataset_tags: Optional[Union[str, list[str]]] = None,\n+        dataset: Optional[Union[str, list[str]]] = None,\n+        dataset_args: Optional[Union[str, list[str]]] = None,\n     ):\n         \"\"\"\n         Creates a draft of a model card using the information available to the `Trainer`.\n@@ -1833,18 +1833,18 @@ def create_model_card(\n             license (`str`, *optional*):\n                 The license of the model. Will default to the license of the pretrained model used, if the original\n                 model given to the `Trainer` comes from a repo on the Hub.\n-            tags (`str` or `List[str]`, *optional*):\n+            tags (`str` or `list[str]`, *optional*):\n                 Some tags to be included in the metadata of the model card.\n             finetuned_from (`str`, *optional*):\n                 The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n                 of the original model given to the `Trainer` (if it comes from the Hub).\n-            tasks (`str` or `List[str]`, *optional*):\n+            tasks (`str` or `list[str]`, *optional*):\n                 One or several task identifiers, to be included in the metadata of the model card.\n-            dataset_tags (`str` or `List[str]`, *optional*):\n+            dataset_tags (`str` or `list[str]`, *optional*):\n                 One or several dataset tags, to be included in the metadata of the model card.\n-            dataset (`str` or `List[str]`, *optional*):\n+            dataset (`str` or `list[str]`, *optional*):\n                 One or several dataset identifiers, to be included in the metadata of the model card.\n-            dataset_args (`str` or `List[str]`, *optional*):\n+            dataset_args (`str` or `list[str]`, *optional*):\n                One or several dataset arguments, to be included in the metadata of the model card.\n         \"\"\"\n         # Avoids a circular import by doing this when necessary.\n@@ -1947,7 +1947,7 @@ def get_prefix_bias_name(self) -> Union[None, str]:\n         warnings.warn(\"The method get_prefix_bias_name is deprecated. Please use `get_bias` instead.\", FutureWarning)\n         return None\n \n-    def get_bias(self) -> Union[None, Dict[str, tf.Variable]]:\n+    def get_bias(self) -> Union[None, dict[str, tf.Variable]]:\n         \"\"\"\n         Dict of bias attached to an LM head. The key represents the name of the bias attribute.\n \n@@ -1969,7 +1969,7 @@ def set_bias(self, value):\n         Set all the bias in the LM head.\n \n         Args:\n-            value (`Dict[tf.Variable]`):\n+            value (`dict[tf.Variable]`):\n                 All the new bias attached to an LM head.\n         \"\"\"\n         if self.get_lm_head() is not None:\n@@ -2174,14 +2174,14 @@ def _get_resized_lm_head_bias(self, old_lm_head_bias, new_num_tokens):\n         return new_lm_head_bias\n \n     def _v2_get_resized_lm_head_bias(\n-        self, old_lm_head_bias: Dict[str, tf.Variable], new_num_tokens: int\n-    ) -> Dict[str, tf.Tensor]:\n+        self, old_lm_head_bias: dict[str, tf.Variable], new_num_tokens: int\n+    ) -> dict[str, tf.Tensor]:\n         \"\"\"\n         Build a resized bias from the old ones. Increasing the size will add newly initialized vectors at the end.\n         Reducing the size will remove vectors from the end\n \n         Args:\n-            old_lm_head_bias (`Dict[str, tf.Variable]`):\n+            old_lm_head_bias (`dict[str, tf.Variable]`):\n                 Old lm head bias to be resized.\n             new_num_tokens (`int`):\n                 New number of tokens in the linear matrix. Increasing the size will add newly initialized vectors at\n@@ -2332,7 +2332,7 @@ def prune_heads(self, heads_to_prune):\n         Prunes heads of the base model.\n \n         Arguments:\n-            heads_to_prune (`Dict[int, List[int]]`):\n+            heads_to_prune (`dict[int, list[int]]`):\n                 Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads\n                 to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on\n                 layer 1 and heads 2 and 3 on layer 2.\n@@ -2389,7 +2389,7 @@ def save_pretrained(\n             token (`str` or `bool`, *optional*):\n                 The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                 the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -2594,7 +2594,7 @@ def from_pretrained(\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n             proxies:\n-                (`Dict[str, str], `optional`): A dictionary of proxy servers to use by protocol or endpoint, e.g.,\n+                (`dict[str, str], `optional`): A dictionary of proxy servers to use by protocol or endpoint, e.g.,\n                 `{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n                 output_loading_info(`bool`, *optional*, defaults to `False`): Whether ot not to also return a\n                 dictionary containing missing keys, unexpected keys and error messages.\n@@ -3263,7 +3263,7 @@ class TFConv1D(keras.layers.Layer):\n             The number of input features.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation to use to initialize the weights.\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional keyword arguments passed along to the `__init__` of `keras.layers.Layer`.\n     \"\"\"\n \n@@ -3308,7 +3308,7 @@ class TFSharedEmbeddings(keras.layers.Layer):\n         initializer_range (`float`, *optional*):\n             The standard deviation to use when initializing the weights. If no value is provided, it will default to\n             \\\\(1/\\sqrt{hidden\\_size}\\\\).\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional keyword arguments passed along to the `__init__` of `keras.layers.Layer`.\n     \"\"\"\n \n@@ -3423,7 +3423,7 @@ class TFSequenceSummary(keras.layers.Layer):\n             - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n \n         initializer_range (`float`, *optional*, defaults to 0.02): The standard deviation to use to initialize the weights.\n-        kwargs (`Dict[str, Any]`, *optional*):\n+        kwargs (`dict[str, Any]`, *optional*):\n             Additional keyword arguments passed along to the `__init__` of `keras.layers.Layer`.\n     \"\"\"\n "
        },
        {
            "sha": "ae6f194a90b433a7b18476a8b103cab852d2455a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 68,
            "deletions": 68,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -33,7 +33,7 @@\n from enum import Enum\n from functools import partial, wraps\n from threading import Thread\n-from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, TypeVar, Union\n+from typing import Any, Callable, Optional, TypeVar, Union\n from zipfile import is_zipfile\n \n import torch\n@@ -339,7 +339,7 @@ def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n     except StopIteration:\n         # For nn.DataParallel compatibility in PyTorch 1.5\n \n-        def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n+        def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n             tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n             return tuples\n \n@@ -374,7 +374,7 @@ def get_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n         return last_dtype\n \n     # For nn.DataParallel compatibility in PyTorch > 1.5\n-    def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n+    def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n         tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n         return tuples\n \n@@ -644,7 +644,7 @@ def _get_tied_weight_keys(module: nn.Module, prefix=\"\"):\n     return tied_weight_keys\n \n \n-def _find_disjoint(tensors: List[Set[str]], state_dict: Dict[str, torch.Tensor]) -> Tuple[List[Set[str]], List[str]]:\n+def _find_disjoint(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]) -> tuple[list[set[str]], list[str]]:\n     filtered_tensors = []\n     for shared in tensors:\n         if len(shared) < 2:\n@@ -675,7 +675,7 @@ def _find_disjoint(tensors: List[Set[str]], state_dict: Dict[str, torch.Tensor])\n     return shared_tensors, disjoint_tensors\n \n \n-def _find_identical(tensors: List[Set[str]], state_dict: Dict[str, torch.Tensor]) -> Tuple[List[Set[str]], Set[str]]:\n+def _find_identical(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]) -> tuple[list[set[str]], set[str]]:\n     shared_tensors = []\n     identical = []\n     for shared in tensors:\n@@ -738,21 +738,21 @@ def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor\n @torch.no_grad()\n def _load_state_dict_into_meta_model(\n     model: \"PreTrainedModel\",\n-    state_dict: Dict,\n+    state_dict: dict,\n     shard_file: str,\n-    expected_keys: List[str],\n-    reverse_renaming_mapping: Dict[str, str],\n-    device_map: Optional[Dict] = None,\n+    expected_keys: list[str],\n+    reverse_renaming_mapping: dict[str, str],\n+    device_map: Optional[dict] = None,\n     disk_offload_folder: Optional[str] = None,\n-    disk_offload_index: Optional[Dict] = None,\n+    disk_offload_index: Optional[dict] = None,\n     cpu_offload_folder: Optional[str] = None,\n-    cpu_offload_index: Optional[Dict] = None,\n+    cpu_offload_index: Optional[dict] = None,\n     hf_quantizer: Optional[HfQuantizer] = None,\n     is_safetensors: bool = False,\n     keep_in_fp32_regex: Optional[re.Pattern] = None,\n-    unexpected_keys: Optional[List[str]] = None,  # passing `unexpected` for cleanup from quantization items\n+    unexpected_keys: Optional[list[str]] = None,  # passing `unexpected` for cleanup from quantization items\n     device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n-) -> Tuple[Optional[Dict], Optional[Dict]]:\n+) -> tuple[Optional[dict], Optional[dict]]:\n     \"\"\"Load parameters from `meta_state_dict` into the model. The parameters of the `meta_state_dict` are on the meta\n     device in order to easily infer the shapes and dtypes that they will have. Then proper parameters are then loaded\n     from `shard_file`, which is the actual state dict file on disk.\n@@ -998,15 +998,15 @@ def _get_resolved_checkpoint_files(\n     use_safetensors: bool,\n     cache_dir: str,\n     force_download: bool,\n-    proxies: Optional[Dict[str, str]],\n+    proxies: Optional[dict[str, str]],\n     local_files_only: bool,\n     token: Optional[Union[str, bool]],\n     user_agent: dict,\n     revision: str,\n     commit_hash: Optional[str],\n     is_remote_code: bool,  # Because we can't determine this inside this function, we need it to be passed in\n     transformers_explicit_filename: Optional[str] = None,\n-) -> Tuple[Optional[List[str]], Optional[Dict]]:\n+) -> tuple[Optional[list[str]], Optional[dict]]:\n     \"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\n     checkpoints are sharded.\n     This function will download the data if necessary.\n@@ -1315,13 +1315,13 @@ def _get_resolved_checkpoint_files(\n \n def _get_torch_dtype(\n     cls,\n-    torch_dtype: Optional[Union[str, torch.dtype, Dict]],\n-    checkpoint_files: Optional[List[str]],\n+    torch_dtype: Optional[Union[str, torch.dtype, dict]],\n+    checkpoint_files: Optional[list[str]],\n     config: PretrainedConfig,\n-    sharded_metadata: Optional[Dict],\n-    state_dict: Optional[Dict],\n+    sharded_metadata: Optional[dict],\n+    state_dict: Optional[dict],\n     weights_only: bool,\n-) -> Tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n+) -> tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n     \"\"\"Find the correct `torch_dtype` to use based on provided arguments. Also update the `config` based on the\n     inferred dtype. We do the following:\n     1. If torch_dtype is not None, we use that dtype\n@@ -1395,12 +1395,12 @@ def _get_torch_dtype(\n \n def _get_device_map(\n     model: \"PreTrainedModel\",\n-    device_map: Optional[Union[str, Dict]],\n-    max_memory: Optional[Dict],\n+    device_map: Optional[Union[str, dict]],\n+    max_memory: Optional[dict],\n     hf_quantizer: Optional[HfQuantizer],\n     torch_dtype: Optional[torch.dtype],\n     keep_in_fp32_regex: Optional[re.Pattern],\n-) -> Dict:\n+) -> dict:\n     \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n     Otherwise, we check for any device inconsistencies in the device_map.\n     \"\"\"\n@@ -1472,12 +1472,12 @@ def _get_device_map(\n def _find_missing_and_unexpected_keys(\n     cls,\n     model: \"PreTrainedModel\",\n-    original_checkpoint_keys: List[str],\n-    checkpoint_keys: List[str],\n+    original_checkpoint_keys: list[str],\n+    checkpoint_keys: list[str],\n     loading_base_model_from_task_state_dict: bool,\n     hf_quantizer: Optional[HfQuantizer],\n-    device_map: Dict,\n-) -> Tuple[List[str], List[str]]:\n+    device_map: dict,\n+) -> tuple[list[str], list[str]]:\n     \"\"\"Find missing keys (keys that are part of the model parameters but were NOT found in the loaded state dict keys) and unexpected keys\n     (keys found in the loaded state dict keys, but that are NOT part of the model parameters)\n     \"\"\"\n@@ -1531,13 +1531,13 @@ def _find_missing_and_unexpected_keys(\n \n def _find_mismatched_keys(\n     model: \"PreTrainedModel\",\n-    state_dict: Optional[Dict],\n-    checkpoint_files: Optional[List[str]],\n+    state_dict: Optional[dict],\n+    checkpoint_files: Optional[list[str]],\n     ignore_mismatched_sizes: bool,\n-    keys_to_rename_mapping: Dict[str, str],\n+    keys_to_rename_mapping: dict[str, str],\n     is_quantized: bool,\n     weights_only: bool,\n-) -> Tuple[List[str], List[Tuple[int, int]]]:\n+) -> tuple[list[str], list[tuple[int, int]]]:\n     \"\"\"\n     Find potential shape mismatch between the different state dicts and the model parameters, but only if `ignore_mismatched_sizes`\n     is True. Otherwise, return immediately and any shape mismatch that may exist will be raised later on. This avoids checking\n@@ -1710,15 +1710,15 @@ def create_extended_attention_mask_for_decoder(input_shape, attention_mask, devi\n         return extended_attention_mask\n \n     def get_extended_attention_mask(\n-        self, attention_mask: Tensor, input_shape: Tuple[int], device: torch.device = None, dtype: torch.float = None\n+        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n     ) -> Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n \n         Arguments:\n             attention_mask (`torch.Tensor`):\n                 Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n-            input_shape (`Tuple[int]`):\n+            input_shape (`tuple[int]`):\n                 The shape of the input to the model.\n \n         Returns:\n@@ -1853,7 +1853,7 @@ def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool\n \n         return sum(total_numel)\n \n-    def estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> int:\n+    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int:\n         \"\"\"\n         Helper function to estimate the total number of tokens from the model inputs.\n \n@@ -1875,7 +1875,7 @@ def estimate_tokens(self, input_dict: Dict[str, Union[torch.Tensor, Any]]) -> in\n         return 0\n \n     def floating_point_ops(\n-        self, input_dict: Dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n+        self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n     ) -> int:\n         \"\"\"\n         Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n@@ -2003,9 +2003,9 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     _supports_attention_backend = False\n \n     @property\n-    def dummy_inputs(self) -> Dict[str, torch.Tensor]:\n+    def dummy_inputs(self) -> dict[str, torch.Tensor]:\n         \"\"\"\n-        `Dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n+        `dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n         \"\"\"\n         return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n \n@@ -2108,13 +2108,13 @@ def _backward_compatibility_gradient_checkpointing(self):\n             # Remove the attribute now that is has been consumed, so it's no saved in the config.\n             delattr(self.config, \"gradient_checkpointing\")\n \n-    def add_model_tags(self, tags: Union[List[str], str]) -> None:\n+    def add_model_tags(self, tags: Union[list[str], str]) -> None:\n         r\"\"\"\n         Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\n         not overwrite existing tags in the model.\n \n         Args:\n-            tags (`Union[List[str], str]`):\n+            tags (`Union[list[str], str]`):\n                 The desired tags to inject in the model\n \n         Examples:\n@@ -2203,7 +2203,7 @@ def _autoset_attn_implementation(\n         cls,\n         config,\n         torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, Dict[str, int]]] = None,\n+        device_map: Optional[Union[str, dict[str, int]]] = None,\n         check_device_map: bool = True,\n     ):\n         \"\"\"\n@@ -2400,7 +2400,7 @@ def _check_and_enable_flash_attn_2(\n         cls,\n         config,\n         torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, Dict[str, int]]] = None,\n+        device_map: Optional[Union[str, dict[str, int]]] = None,\n         check_device_map: bool = True,\n         hard_check_only: bool = False,\n     ) -> PretrainedConfig:\n@@ -2693,8 +2693,8 @@ def tie_weights(self):\n     def _tie_encoder_decoder_weights(\n         encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, base_encoder_name: str\n     ):\n-        uninitialized_encoder_weights: List[str] = []\n-        tied_weights: List[str] = []\n+        uninitialized_encoder_weights: list[str] = []\n+        tied_weights: list[str] = []\n         if decoder.__class__ != encoder.__class__:\n             logger.info(\n                 f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder\"\n@@ -2706,7 +2706,7 @@ def tie_encoder_to_decoder_recursively(\n             encoder_pointer: nn.Module,\n             module_name: str,\n             base_encoder_name: str,\n-            uninitialized_encoder_weights: List[str],\n+            uninitialized_encoder_weights: list[str],\n             depth=0,\n             total_decoder_name=\"\",\n             total_encoder_name=\"\",\n@@ -2809,7 +2809,7 @@ def _get_no_split_modules(self, device_map: str):\n                 The device map value. Options are [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]\n \n         Returns:\n-            `List[str]`: List of modules that should not be split\n+            `list[str]`: List of modules that should not be split\n         \"\"\"\n         _no_split_modules = set()\n         modules_to_check = [self]\n@@ -3289,7 +3289,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n             f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n         )\n \n-    def get_position_embeddings(self) -> Union[nn.Embedding, Tuple[nn.Embedding]]:\n+    def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]:\n         raise NotImplementedError(\n             f\"`get_position_embeddings` is not implemented for {self.__class__}`. To implement it, you should \"\n             f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n@@ -3312,12 +3312,12 @@ def init_weights(self):\n             # since from_pretrained(...) calls tie weights anyways\n             self.tie_weights()\n \n-    def prune_heads(self, heads_to_prune: Dict[int, List[int]]):\n+    def prune_heads(self, heads_to_prune: dict[int, list[int]]):\n         \"\"\"\n         Prunes heads of the base model.\n \n         Arguments:\n-            heads_to_prune (`Dict[int, List[int]]`):\n+            heads_to_prune (`dict[int, list[int]]`):\n                 Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads\n                 to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on\n                 layer 1 and heads 2 and 3 on layer 2.\n@@ -3486,7 +3486,7 @@ def save_pretrained(\n                 For backward compatibility with PEFT library, in case adapter weights are attached to the model, all\n                 keys of the state dict of adapters needs to be prepended with `base_model.model`. Advanced users can\n                 disable this behaviours by setting `save_peft_format` to `False`.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -3997,7 +3997,7 @@ def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n     @classmethod\n     @restore_default_torch_dtype\n     def from_pretrained(\n-        cls: Type[SpecificPreTrainedModelType],\n+        cls: type[SpecificPreTrainedModelType],\n         pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n         *model_args,\n         config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n@@ -4057,7 +4057,7 @@ def from_pretrained(\n                       save directory.\n                     - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n                       configuration JSON file named *config.json* is found in the directory.\n-            state_dict (`Dict[str, torch.Tensor]`, *optional*):\n+            state_dict (`dict[str, torch.Tensor]`, *optional*):\n                 A state dictionary to use instead of a state dictionary loaded from saved weights file.\n \n                 This option can be used if you want to create a model from a pretrained configuration but load your own\n@@ -4082,7 +4082,7 @@ def from_pretrained(\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             output_loading_info(`bool`, *optional*, defaults to `False`):\n@@ -4131,7 +4131,7 @@ def from_pretrained(\n \n                 </Tip>\n \n-            device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n+            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n                 A map that specifies where each submodule should go. It doesn't need to be refined to each\n                 parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                 same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n@@ -4179,7 +4179,7 @@ def from_pretrained(\n                 Indicates whether unpickler should be restricted to loading only tensors, primitive types,\n                 dictionaries and any types added via torch.serialization.add_safe_globals().\n                 When set to False, we can load wrapper tensor subclass weights.\n-            key_mapping (`Dict[str, str], *optional*):\n+            key_mapping (`dict[str, str], *optional*):\n                 A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers\n                 architecture, but was not converted accordingly.\n             kwargs (remaining dictionary of keyword arguments, *optional*):\n@@ -4799,7 +4799,7 @@ def _assign_original_dtype(module):\n         return model\n \n     @staticmethod\n-    def _fix_state_dict_key_on_load(key: str) -> Tuple[str, bool]:\n+    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n         \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n         # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n         # This rename is logged.\n@@ -4826,8 +4826,8 @@ def _fix_state_dict_key_on_load(key: str) -> Tuple[str, bool]:\n \n     def _get_key_renaming_mapping(\n         self,\n-        checkpoint_keys: List[str],\n-        key_mapping: Optional[Dict[str, str]] = None,\n+        checkpoint_keys: list[str],\n+        key_mapping: Optional[dict[str, str]] = None,\n         loading_base_model_from_task_state_dict: bool = False,\n         loading_task_model_from_base_state_dict: bool = False,\n     ):\n@@ -4885,7 +4885,7 @@ def _get_key_renaming_mapping(\n         return key_renaming_mapping\n \n     @staticmethod\n-    def _fix_state_dict_key_on_save(key) -> Tuple[str, bool]:\n+    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n         \"\"\"\n         Similar to `_fix_state_dict_key_on_load` allows to define hook for state dict key renaming on model save.\n         Do nothing by default, but can be overridden in particular models.\n@@ -4903,19 +4903,19 @@ def _fix_state_dict_keys_on_save(self, state_dict):\n     def _load_pretrained_model(\n         cls,\n         model: \"PreTrainedModel\",\n-        state_dict: Optional[Dict],\n-        checkpoint_files: Optional[List[str]],\n+        state_dict: Optional[dict],\n+        checkpoint_files: Optional[list[str]],\n         pretrained_model_name_or_path: Optional[str],\n         ignore_mismatched_sizes: bool = False,\n-        sharded_metadata: Optional[Dict] = None,\n-        device_map: Optional[Dict] = None,\n+        sharded_metadata: Optional[dict] = None,\n+        device_map: Optional[dict] = None,\n         disk_offload_folder: Optional[str] = None,\n         offload_state_dict: Optional[bool] = None,\n         dtype: Optional[torch.dtype] = None,\n         hf_quantizer: Optional[HfQuantizer] = None,\n         keep_in_fp32_regex: Optional[re.Pattern] = None,\n         device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n-        key_mapping: Optional[Dict[str, str]] = None,\n+        key_mapping: Optional[dict[str, str]] = None,\n         weights_only: bool = True,\n     ):\n         # Useful flags\n@@ -5485,8 +5485,8 @@ def is_backend_compatible(cls):\n \n     def _move_missing_keys_from_meta_to_cpu(\n         self,\n-        missing_keys: List[str],\n-        unexpected_keys: List[str],\n+        missing_keys: list[str],\n+        unexpected_keys: list[str],\n         dtype: Optional[torch.dtype],\n         hf_quantizer: Optional[HfQuantizer],\n     ) -> \"PreTrainedModel\":\n@@ -5520,7 +5520,7 @@ def _move_missing_keys_from_meta_to_cpu(\n \n     def _initialize_missing_keys(\n         self,\n-        loaded_keys: List[str],\n+        loaded_keys: list[str],\n         ignore_mismatched_sizes: bool,\n         is_quantized: bool,\n     ) -> \"PreTrainedModel\":\n@@ -5846,7 +5846,7 @@ def forward(\n         is_impossible: Optional[torch.LongTensor] = None,\n         p_mask: Optional[torch.FloatTensor] = None,\n         return_dict: bool = False,\n-    ) -> Union[SquadHeadOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[SquadHeadOutput, tuple[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n@@ -6090,7 +6090,7 @@ def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n         return torch.device(device).type not in [\"meta\", \"cpu\"]\n \n \n-def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, hf_quantizer: Optional[HfQuantizer]):\n+def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n     the model, which is actually the loading speed bottleneck."
        },
        {
            "sha": "1162c16f61cfbaf31fa978335f9c9fa9cb2743ec",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import os\n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -277,7 +277,7 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    def prune_heads(self, heads: List[int]) -> None:\n+    def prune_heads(self, heads: list[int]) -> None:\n         if len(heads) == 0:\n             return\n         heads, index = find_pruneable_heads_and_indices(\n@@ -301,7 +301,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         mixed_query_layer = self.query(hidden_states)\n         mixed_key_layer = self.key(hidden_states)\n         mixed_value_layer = self.value(hidden_states)\n@@ -366,7 +366,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         if self.position_embedding_type != \"absolute\" or output_attentions:\n             logger.warning(\n                 \"AlbertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n@@ -435,7 +435,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n \n         ffn_output = apply_chunking_to_forward(\n@@ -468,7 +468,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n-    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n+    ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n         layer_hidden_states = ()\n         layer_attentions = ()\n \n@@ -506,7 +506,7 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n-    ) -> Union[BaseModelOutput, Tuple]:\n+    ) -> Union[BaseModelOutput, tuple]:\n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n \n         all_hidden_states = (hidden_states,) if output_hidden_states else None\n@@ -599,8 +599,8 @@ class AlbertForPreTrainingOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     prediction_logits: Optional[torch.FloatTensor] = None\n     sop_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @auto_docstring\n@@ -637,7 +637,7 @@ def get_input_embeddings(self) -> nn.Embedding:\n     def set_input_embeddings(self, value: nn.Embedding) -> None:\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n+    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n         \"\"\"\n         Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} ALBERT has\n         a different architecture in that its layers are shared across groups, which then has inner groups. If an ALBERT\n@@ -666,7 +666,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[BaseModelOutputWithPooling, Tuple]:\n+    ) -> Union[BaseModelOutputWithPooling, tuple]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -784,7 +784,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n+    ) -> Union[AlbertForPreTrainingOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -929,7 +929,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[MaskedLMOutput, Tuple]:\n+    ) -> Union[MaskedLMOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1031,7 +1031,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[SequenceClassifierOutput, Tuple]:\n+    ) -> Union[SequenceClassifierOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1123,7 +1123,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[TokenClassifierOutput, Tuple]:\n+    ) -> Union[TokenClassifierOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1190,7 +1190,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n+    ) -> Union[AlbertForPreTrainingOutput, tuple]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.albert(\n@@ -1267,7 +1267,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n+    ) -> Union[AlbertForPreTrainingOutput, tuple]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary."
        },
        {
            "sha": "f2f19cb27716fb3f8846ef88e870e3eb1188a4bf",
            "filename": "src/transformers/models/albert/modeling_flax_albert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax\n import flax.linen as nn\n@@ -77,8 +77,8 @@ class FlaxAlbertForPreTrainingOutput(ModelOutput):\n \n     prediction_logits: jnp.ndarray = None\n     sop_logits: jnp.ndarray = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n ALBERT_START_DOCSTRING = r\"\"\"\n@@ -518,7 +518,7 @@ class FlaxAlbertPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: AlbertConfig,\n-        input_shape: Tuple = (1, 1),\n+        input_shape: tuple = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -527,7 +527,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         token_type_ids = jnp.zeros_like(input_ids)"
        },
        {
            "sha": "bc2a7e0222b0af55ac7677cb3970bc262a12412e",
            "filename": "src/transformers/models/albert/modeling_tf_albert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -253,7 +253,7 @@ def call(\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         batch_size = shape_list(input_tensor)[0]\n         mixed_query_layer = self.query(inputs=input_tensor)\n         mixed_key_layer = self.key(inputs=input_tensor)\n@@ -350,7 +350,7 @@ def call(\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         attention_outputs = self.attention(\n             input_tensor=hidden_states,\n             attention_mask=attention_mask,\n@@ -403,7 +403,7 @@ def call(\n         output_attentions: bool,\n         output_hidden_states: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n         layer_hidden_states = () if output_hidden_states else None\n         layer_attentions = () if output_attentions else None\n \n@@ -466,7 +466,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n         hidden_states = self.embedding_hidden_mapping_in(inputs=hidden_states)\n         all_attentions = () if output_attentions else None\n         all_hidden_states = (hidden_states,) if output_hidden_states else None\n@@ -563,7 +563,7 @@ def set_output_embeddings(self, value: tf.Variable):\n         self.decoder.weight = value\n         self.decoder.vocab_size = shape_list(value)[0]\n \n-    def get_bias(self) -> Dict[str, tf.Variable]:\n+    def get_bias(self) -> dict[str, tf.Variable]:\n         return {\"bias\": self.bias, \"decoder_bias\": self.decoder_bias}\n \n     def set_bias(self, value: tf.Variable):\n@@ -633,7 +633,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -752,8 +752,8 @@ class TFAlbertForPreTrainingOutput(ModelOutput):\n     loss: Optional[tf.Tensor] = None\n     prediction_logits: Optional[tf.Tensor] = None\n     sop_logits: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor] | None = None\n-    attentions: Tuple[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n \n \n ALBERT_START_DOCSTRING = r\"\"\"\n@@ -883,7 +883,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         outputs = self.albert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -948,7 +948,7 @@ def call(\n         labels: np.ndarray | tf.Tensor | None = None,\n         sentence_order_label: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFAlbertForPreTrainingOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFAlbertForPreTrainingOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         Return:\n \n@@ -1075,7 +1075,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1198,7 +1198,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1295,7 +1295,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1388,7 +1388,7 @@ def call(\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1490,7 +1490,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`"
        },
        {
            "sha": "69de95a2dfe39507ece736ca7a4b315e4d98acad",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import os\n import unicodedata\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -122,7 +122,7 @@ def __init__(\n         pad_token=\"<pad>\",\n         cls_token=\"[CLS]\",\n         mask_token=\"[MASK]\",\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n         # Mask token behave like a normal word, i.e. include the space before it and\n@@ -162,7 +162,7 @@ def __init__(\n     def vocab_size(self) -> int:\n         return len(self.sp_model)\n \n-    def get_vocab(self) -> Dict[str, int]:\n+    def get_vocab(self) -> dict[str, int]:\n         vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n@@ -197,7 +197,7 @@ def preprocess_text(self, inputs):\n \n         return outputs\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Tokenize a string.\"\"\"\n         text = self.preprocess_text(text)\n         pieces = self.sp_model.encode(text, out_type=str)\n@@ -247,8 +247,8 @@ def convert_tokens_to_string(self, tokens):\n         return out_string.strip()\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. An ALBERT sequence has the following format:\n@@ -272,8 +272,8 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n@@ -299,7 +299,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "ed9add51d20743948dc1fe51ad6f5fe0c1ed1543",
            "filename": "src/transformers/models/albert/tokenization_albert_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import AddedToken\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -131,8 +131,8 @@ def __init__(\n         self.vocab_file = vocab_file\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. An ALBERT sequence has the following format:\n@@ -155,7 +155,7 @@ def build_inputs_with_special_tokens(\n             return cls + token_ids_0 + sep\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError(\n                 \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \""
        },
        {
            "sha": "5f6daa5d43c42a642f564a642e00b5de97b96893",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"ALIGN model configuration\"\"\"\n \n-from typing import TYPE_CHECKING, List\n+from typing import TYPE_CHECKING\n \n \n if TYPE_CHECKING:\n@@ -156,19 +156,19 @@ class AlignVisionConfig(PretrainedConfig):\n             Scaling coefficient for network depth at each stage.\n         depth_divisor `int`, *optional*, defaults to 8):\n             A unit of network width.\n-        kernel_sizes (`List[int]`, *optional*, defaults to `[3, 3, 5, 3, 5, 5, 3]`):\n+        kernel_sizes (`list[int]`, *optional*, defaults to `[3, 3, 5, 3, 5, 5, 3]`):\n             List of kernel sizes to be used in each block.\n-        in_channels (`List[int]`, *optional*, defaults to `[32, 16, 24, 40, 80, 112, 192]`):\n+        in_channels (`list[int]`, *optional*, defaults to `[32, 16, 24, 40, 80, 112, 192]`):\n             List of input channel sizes to be used in each block for convolutional layers.\n-        out_channels (`List[int]`, *optional*, defaults to `[16, 24, 40, 80, 112, 192, 320]`):\n+        out_channels (`list[int]`, *optional*, defaults to `[16, 24, 40, 80, 112, 192, 320]`):\n             List of output channel sizes to be used in each block for convolutional layers.\n-        depthwise_padding (`List[int]`, *optional*, defaults to `[]`):\n+        depthwise_padding (`list[int]`, *optional*, defaults to `[]`):\n             List of block indices with square padding.\n-        strides (`List[int]`, *optional*, defaults to `[1, 2, 2, 2, 1, 2, 1]`):\n+        strides (`list[int]`, *optional*, defaults to `[1, 2, 2, 2, 1, 2, 1]`):\n             List of stride sizes to be used in each block for convolutional layers.\n-        num_block_repeats (`List[int]`, *optional*, defaults to `[1, 2, 2, 3, 3, 4, 1]`):\n+        num_block_repeats (`list[int]`, *optional*, defaults to `[1, 2, 2, 3, 3, 4, 1]`):\n             List of the number of times each block is to repeated.\n-        expand_ratios (`List[int]`, *optional*, defaults to `[1, 6, 6, 6, 6, 6, 6]`):\n+        expand_ratios (`list[int]`, *optional*, defaults to `[1, 6, 6, 6, 6, 6, 6]`):\n             List of scaling coefficient of each block.\n         squeeze_expansion_ratio (`float`, *optional*, defaults to 0.25):\n             Squeeze expansion ratio.\n@@ -214,13 +214,13 @@ def __init__(\n         width_coefficient: float = 2.0,\n         depth_coefficient: float = 3.1,\n         depth_divisor: int = 8,\n-        kernel_sizes: List[int] = [3, 3, 5, 3, 5, 5, 3],\n-        in_channels: List[int] = [32, 16, 24, 40, 80, 112, 192],\n-        out_channels: List[int] = [16, 24, 40, 80, 112, 192, 320],\n-        depthwise_padding: List[int] = [],\n-        strides: List[int] = [1, 2, 2, 2, 1, 2, 1],\n-        num_block_repeats: List[int] = [1, 2, 2, 3, 3, 4, 1],\n-        expand_ratios: List[int] = [1, 6, 6, 6, 6, 6, 6],\n+        kernel_sizes: list[int] = [3, 3, 5, 3, 5, 5, 3],\n+        in_channels: list[int] = [32, 16, 24, 40, 80, 112, 192],\n+        out_channels: list[int] = [16, 24, 40, 80, 112, 192, 320],\n+        depthwise_padding: list[int] = [],\n+        strides: list[int] = [1, 2, 2, 2, 1, 2, 1],\n+        num_block_repeats: list[int] = [1, 2, 2, 3, 3, 4, 1],\n+        expand_ratios: list[int] = [1, 6, 6, 6, 6, 6, 6],\n         squeeze_expansion_ratio: float = 0.25,\n         hidden_act: str = \"swish\",\n         hidden_dim: int = 2560,"
        },
        {
            "sha": "952fe0bdc9e465fd3bcea9f6e8b443c82bacff11",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -57,7 +57,7 @@ class AlignVisionModelOutput(ModelOutput):\n \n     image_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -85,8 +85,8 @@ class AlignTextModelOutput(ModelOutput):\n \n     text_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -119,7 +119,7 @@ class AlignOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n     vision_model_output: BaseModelOutputWithPoolingAndNoAttention = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -155,7 +155,7 @@ def round_filters(config: AlignVisionConfig, num_channels: int):\n \n \n # Copied from transformers.models.efficientnet.modeling_efficientnet.correct_pad\n-def correct_pad(kernel_size: Union[int, Tuple], adjust: bool = True):\n+def correct_pad(kernel_size: Union[int, tuple], adjust: bool = True):\n     r\"\"\"\n     Utility function to get the tuple padding value for the depthwise convolution.\n \n@@ -628,9 +628,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -778,9 +778,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -849,9 +849,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -928,12 +928,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -1091,7 +1091,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         Examples:\n \n@@ -1213,7 +1213,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n+    ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         r\"\"\"\n         Examples:\n \n@@ -1410,7 +1410,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, AlignOutput]:\n+    ) -> Union[tuple, AlignOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "33aaca27326212c0f0b821e537f4350f7852a1ad",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Image/Text processor class for ALIGN\n \"\"\"\n \n-from typing import List, Union\n+from typing import Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n@@ -73,7 +73,7 @@ def __init__(self, image_processor, tokenizer):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[AlignProcessorKwargs],\n@@ -86,10 +86,10 @@ def __call__(\n         to the docstring of the above two methods for more information.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`):\n+            text (`str`, `list[str]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences)."
        },
        {
            "sha": "0a07373eddaf466bec304870ef49640038b5b003",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -65,7 +65,7 @@ class AltCLIPTextConfig(PretrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 1): The id of the *padding* token.\n         bos_token_id (`int`, *optional*, defaults to 0): The id of the *beginning-of-sequence* token.\n-        eos_token_id (`Union[int, List[int]]`, *optional*, defaults to 2):\n+        eos_token_id (`Union[int, list[int]]`, *optional*, defaults to 2):\n             The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n         position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n             Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For"
        },
        {
            "sha": "3e917940809c1169550ac86b6bcd5ff4593658e9",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Callable, List, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -82,7 +82,7 @@ class AltCLIPOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -219,9 +219,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -369,9 +369,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -440,9 +440,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -519,12 +519,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -666,7 +666,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -747,7 +747,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -805,7 +805,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -1035,7 +1035,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1091,7 +1091,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n \n@@ -1183,12 +1183,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1317,7 +1317,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPoolingAndProjection]:\n+    ) -> Union[tuple, BaseModelOutputWithPoolingAndProjection]:\n         r\"\"\"\n         Examples:\n \n@@ -1515,7 +1515,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, AltCLIPOutput]:\n+    ) -> Union[tuple, AltCLIPOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "cc7e3c79435740790cb3083fc4495c5df0f556ba",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Image/Text processor class for AltCLIP\n \"\"\"\n \n-from typing import List, Union\n+from typing import Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -59,7 +59,7 @@ def __init__(self, image_processor=None, tokenizer=None):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[AltClipProcessorKwargs],\n@@ -76,7 +76,7 @@ def __call__(\n             images (`ImageInput`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences)."
        },
        {
            "sha": "67f023e1dbf4903d6815f6bdf8abbdaeea2239a4",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Dict, Optional\n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n@@ -104,11 +104,11 @@ class AriaTextConfig(PretrainedConfig):\n                 `beta_slow` (`float`, *optional*):\n                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                     ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n+                `short_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n+                `long_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n@@ -268,7 +268,7 @@ def __init__(\n         vision_config=None,\n         vision_feature_layer: int = -1,\n         text_config: AriaTextConfig = None,\n-        projector_patch_to_query_dict: Optional[Dict] = None,\n+        projector_patch_to_query_dict: Optional[dict] = None,\n         image_token_index: int = 9,\n         initializer_range: float = 0.02,\n         **kwargs,"
        },
        {
            "sha": "d2b6c21a7f13ac4b9590477c42cd05c8fb83d390",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Iterable\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -43,7 +43,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n+def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> list[np.array]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n \n@@ -107,11 +107,11 @@ class AriaImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        image_mean: Optional[List[float]] = None,\n-        image_std: Optional[List[float]] = None,\n+        image_mean: Optional[list[float]] = None,\n+        image_std: Optional[list[float]] = None,\n         max_image_size: int = 980,\n         min_image_size: int = 336,\n-        split_resolutions: Optional[List[Tuple[int, int]]] = None,\n+        split_resolutions: Optional[list[tuple[int, int]]] = None,\n         split_image: Optional[bool] = False,\n         do_convert_rgb: Optional[bool] = True,\n         do_rescale: bool = True,\n@@ -143,9 +143,9 @@ def __init__(\n \n     def preprocess(\n         self,\n-        images: Union[ImageInput, List[ImageInput]],\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        images: Union[ImageInput, list[ImageInput]],\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         max_image_size: Optional[int] = None,\n         min_image_size: Optional[int] = None,\n         split_image: Optional[bool] = None,\n@@ -388,7 +388,7 @@ def _pad_for_patching(\n     def pad(\n         self,\n         image: np.ndarray,\n-        padding: Union[int, Tuple[int, int], Iterable[Tuple[int, int]]],\n+        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n         mode: PaddingMode = PaddingMode.CONSTANT,\n         constant_values: Union[float, Iterable[float]] = 0.0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -402,7 +402,7 @@ def pad(\n         Args:\n             image (`np.ndarray`):\n                 The image to pad.\n-            padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n+            padding (`int` or `tuple[int, int]` or `Iterable[tuple[int, int]]`):\n                 Padding to apply to the edges of the height, width axes. Can be one of three formats:\n                 - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n                 - `((before, after),)` yields same before and after pad for height and width.\n@@ -454,19 +454,19 @@ def pad(\n     def get_image_patches(\n         self,\n         image: np.array,\n-        grid_pinpoints: List[Tuple[int, int]],\n+        grid_pinpoints: list[tuple[int, int]],\n         patch_size: int,\n         resample: PILImageResampling,\n         data_format: ChannelDimension,\n         input_data_format: ChannelDimension,\n-    ) -> List[np.array]:\n+    ) -> list[np.array]:\n         \"\"\"\n         Process an image with variable resolutions by dividing it into patches.\n \n         Args:\n             image (`np.array`):\n                 The input image to be processed.\n-            grid_pinpoints (List[Tuple[int, int]]):\n+            grid_pinpoints (list[tuple[int, int]]):\n                 A list of possible resolutions as tuples.\n             patch_size (`int`):\n                 Size of the patches to divide the image into.\n@@ -478,7 +478,7 @@ def get_image_patches(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            `List[np.array]`: A list of NumPy arrays containing the processed image patches.\n+            `list[np.array]`: A list of NumPy arrays containing the processed image patches.\n         \"\"\"\n         if not isinstance(grid_pinpoints, list):\n             raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")"
        },
        {
            "sha": "f501a17eb46a3ddad29f6cf907b0b74fea0f112d",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from dataclasses import dataclass\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n@@ -527,12 +527,12 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -599,9 +599,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n \n@@ -996,9 +996,9 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[List[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -1070,7 +1070,7 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             pixel_mask (`torch.FloatTensor]`, *optional*):\n                 The tensors corresponding to the input image mask.\n-            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n@@ -1102,15 +1102,15 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, AriaModelOutputWithPast]:\n+    ) -> Union[tuple, AriaModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1259,7 +1259,7 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1269,7 +1269,7 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n+    ) -> Union[tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "b5c18a40b715c15abea7ce17a5f83ede23932800",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 25,
            "deletions": 25,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections.abc import Iterable\n-from typing import Dict, List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -175,11 +175,11 @@ class AriaTextConfig(LlamaConfig):\n                 `beta_slow` (`float`, *optional*):\n                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                     ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n+                `short_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n+                `long_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n@@ -270,7 +270,7 @@ def __init__(\n         vision_config=None,\n         vision_feature_layer: int = -1,\n         text_config: AriaTextConfig = None,\n-        projector_patch_to_query_dict: Optional[Dict] = None,\n+        projector_patch_to_query_dict: Optional[dict] = None,\n         image_token_index: int = 9,\n         initializer_range: float = 0.02,\n         **kwargs,\n@@ -491,11 +491,11 @@ class AriaImageProcessor(BaseImageProcessor):\n \n     def __init__(\n         self,\n-        image_mean: Optional[List[float]] = None,\n-        image_std: Optional[List[float]] = None,\n+        image_mean: Optional[list[float]] = None,\n+        image_std: Optional[list[float]] = None,\n         max_image_size: int = 980,\n         min_image_size: int = 336,\n-        split_resolutions: Optional[List[Tuple[int, int]]] = None,\n+        split_resolutions: Optional[list[tuple[int, int]]] = None,\n         split_image: Optional[bool] = False,\n         do_convert_rgb: Optional[bool] = True,\n         do_rescale: bool = True,\n@@ -527,9 +527,9 @@ def __init__(\n \n     def preprocess(\n         self,\n-        images: Union[ImageInput, List[ImageInput]],\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        images: Union[ImageInput, list[ImageInput]],\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         max_image_size: Optional[int] = None,\n         min_image_size: Optional[int] = None,\n         split_image: Optional[bool] = None,\n@@ -772,7 +772,7 @@ def _pad_for_patching(\n     def pad(\n         self,\n         image: np.ndarray,\n-        padding: Union[int, Tuple[int, int], Iterable[Tuple[int, int]]],\n+        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],\n         mode: PaddingMode = PaddingMode.CONSTANT,\n         constant_values: Union[float, Iterable[float]] = 0.0,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -786,7 +786,7 @@ def pad(\n         Args:\n             image (`np.ndarray`):\n                 The image to pad.\n-            padding (`int` or `Tuple[int, int]` or `Iterable[Tuple[int, int]]`):\n+            padding (`int` or `tuple[int, int]` or `Iterable[tuple[int, int]]`):\n                 Padding to apply to the edges of the height, width axes. Can be one of three formats:\n                 - `((before_height, after_height), (before_width, after_width))` unique pad widths for each axis.\n                 - `((before, after),)` yields same before and after pad for height and width.\n@@ -838,19 +838,19 @@ def pad(\n     def get_image_patches(\n         self,\n         image: np.array,\n-        grid_pinpoints: List[Tuple[int, int]],\n+        grid_pinpoints: list[tuple[int, int]],\n         patch_size: int,\n         resample: PILImageResampling,\n         data_format: ChannelDimension,\n         input_data_format: ChannelDimension,\n-    ) -> List[np.array]:\n+    ) -> list[np.array]:\n         \"\"\"\n         Process an image with variable resolutions by dividing it into patches.\n \n         Args:\n             image (`np.array`):\n                 The input image to be processed.\n-            grid_pinpoints (List[Tuple[int, int]]):\n+            grid_pinpoints (list[tuple[int, int]]):\n                 A list of possible resolutions as tuples.\n             patch_size (`int`):\n                 Size of the patches to divide the image into.\n@@ -862,7 +862,7 @@ def get_image_patches(\n                 The channel dimension format of the input image.\n \n         Returns:\n-            `List[np.array]`: A list of NumPy arrays containing the processed image patches.\n+            `list[np.array]`: A list of NumPy arrays containing the processed image patches.\n         \"\"\"\n         if not isinstance(grid_pinpoints, list):\n             raise TypeError(\"grid_pinpoints must be a list of possible resolutions.\")\n@@ -945,7 +945,7 @@ def __init__(\n         image_processor=None,\n         tokenizer: Union[AutoTokenizer, str] = None,\n         chat_template: Optional[str] = None,\n-        size_conversion: Optional[Dict[Union[float, int], int]] = None,\n+        size_conversion: Optional[dict[Union[float, int], int]] = None,\n     ):\n         if size_conversion is None:\n             size_conversion = {490: 128, 980: 256}\n@@ -960,7 +960,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n         images: Optional[ImageInput] = None,\n         audio=None,\n         videos=None,\n@@ -970,7 +970,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s).\n \n         Args:\n-            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -1030,7 +1030,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         \"\"\"\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n         Args:\n-            image_sizes (`List[List[int]]`, *optional*):\n+            image_sizes (`list[list[int]]`, *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n             `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n@@ -1392,7 +1392,7 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             pixel_mask (`torch.FloatTensor]`, *optional*):\n                 The tensors corresponding to the input image mask.\n-            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n@@ -1422,15 +1422,15 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, AriaModelOutputWithPast]:\n+    ) -> Union[tuple, AriaModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1518,7 +1518,7 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1528,7 +1528,7 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n+    ) -> Union[tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "07a684cd76c468637afcada1c4256bbef96c35a5",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -68,7 +68,7 @@ def __init__(\n         image_processor=None,\n         tokenizer: Union[AutoTokenizer, str] = None,\n         chat_template: Optional[str] = None,\n-        size_conversion: Optional[Dict[Union[float, int], int]] = None,\n+        size_conversion: Optional[dict[Union[float, int], int]] = None,\n     ):\n         if size_conversion is None:\n             size_conversion = {490: 128, 980: 256}\n@@ -83,7 +83,7 @@ def __init__(\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],\n         images: Optional[ImageInput] = None,\n         audio=None,\n         videos=None,\n@@ -93,7 +93,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s).\n \n         Args:\n-            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -153,7 +153,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         \"\"\"\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n         Args:\n-            image_sizes (`List[List[int]]`, *optional*):\n+            image_sizes (`list[list[int]]`, *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n             `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided"
        },
        {
            "sha": "ecd8f4858fe7add5841fc2e4cdf4ed00bc480a92",
            "filename": "src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconfiguration_audio_spectrogram_transformer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Audio Spectogram Transformer (AST) model configuration\"\"\"\n \n-from typing import Any, Dict\n+from typing import Any\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -124,7 +124,7 @@ def __init__(\n     # Overwritten from the parent class: AST is not compatible with `generate`, but has a config parameter sharing the\n     # same name (`max_length`). Sharing the same name triggers checks regarding the config -> generation_config\n     # generative parameters deprecation cycle, overwriting this function prevents this from happening.\n-    def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n+    def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n         return {}\n \n "
        },
        {
            "sha": "f56c0c3213b7e4a920b271df612e7aaad1934a9f",
            "filename": "src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Ffeature_extraction_audio_spectrogram_transformer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Feature extractor class for Audio Spectrogram Transformer.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -160,7 +160,7 @@ def normalize(self, input_values: np.ndarray) -> np.ndarray:\n \n     def __call__(\n         self,\n-        raw_speech: Union[np.ndarray, List[float], List[np.ndarray], List[List[float]]],\n+        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n         sampling_rate: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n@@ -169,7 +169,7 @@ def __call__(\n         Main method to featurize and prepare for the model one or several sequence(s).\n \n         Args:\n-            raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`):\n+            raw_speech (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n                 The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n                 values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not\n                 stereo, i.e. single float per timestep."
        },
        {
            "sha": "d3ccf24153b791d2b95232cf85ea0df2b464eec4",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch Audio Spectrogram Transformer (AST) model.\"\"\"\n \n-from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -155,7 +155,7 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n \n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n         query_layer = self.transpose_for_scores(self.query(hidden_states))\n@@ -216,7 +216,7 @@ def __init__(self, config: ASTConfig) -> None:\n         self.output = ASTSelfOutput(config)\n         self.pruned_heads = set()\n \n-    def prune_heads(self, heads: Set[int]) -> None:\n+    def prune_heads(self, heads: set[int]) -> None:\n         if len(heads) == 0:\n             return\n         heads, index = find_pruneable_heads_and_indices(\n@@ -239,7 +239,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n \n         attention_output = self.output(self_outputs[0], hidden_states)\n@@ -300,7 +300,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in AST, layernorm is applied before self-attention\n             head_mask,\n@@ -423,7 +423,7 @@ def __init__(self, config: ASTConfig) -> None:\n     def get_input_embeddings(self) -> ASTPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n+    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n         \"\"\"\n         Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n         class PreTrainedModel\n@@ -439,11 +439,11 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n             Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n             the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n             [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n             tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]\n@@ -533,7 +533,7 @@ def forward(\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n             Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n+            loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a `numpy.ndarray`, *e.g.* via\n             the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n             [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n             tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]"
        },
        {
            "sha": "eeb959de18655361dedca7ec8635bcdefe53e78e",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -118,7 +118,7 @@\n                       save directory.\n                     - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n                       configuration JSON file named *config.json* is found in the directory.\n-            state_dict (*Dict[str, torch.Tensor]*, *optional*):\n+            state_dict (*dict[str, torch.Tensor]*, *optional*):\n                 A state dictionary to use instead of a state dictionary loaded from saved weights file.\n \n                 This option can be used if you want to create a model from a pretrained configuration but load your own\n@@ -136,7 +136,7 @@\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             output_loading_info(`bool`, *optional*, defaults to `False`):\n@@ -235,7 +235,7 @@\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             output_loading_info(`bool`, *optional*, defaults to `False`):\n@@ -334,7 +334,7 @@\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             output_loading_info(`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "9e9d464953d703b8658a9e1eadbb98fb9ccf845f",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -1096,7 +1096,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             revision (`str`, *optional*, defaults to `\"main\"`):"
        },
        {
            "sha": "e7db1944d3183803277408c0231ccab4a91c5db2",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import warnings\n from collections import OrderedDict\n-from typing import Dict, Optional, Union\n+from typing import Optional, Union\n \n # Build the list of all feature extractors\n from ...configuration_utils import PretrainedConfig\n@@ -148,7 +148,7 @@ def get_feature_extractor_config(\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n@@ -175,7 +175,7 @@ def get_feature_extractor_config(\n         resume_download:\n             Deprecated and ignored. All downloads are now resumed by default when possible.\n             Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n+        proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n         token (`str` or *bool*, *optional*):\n@@ -292,7 +292,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or *bool*, *optional*):\n@@ -311,7 +311,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                 Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n                 should only be set to `True` for repositories you trust and in which you have read the code, as it will\n                 execute code present on the Hub on your local machine.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are feature extractor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is\n                 controlled by the `return_unused_kwargs` keyword parameter."
        },
        {
            "sha": "a3feebced914f0dcb33ccf4d2cd3c61f47269646",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import warnings\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n # Build the list of all image processors\n from ...configuration_utils import PretrainedConfig\n@@ -52,7 +52,7 @@\n if TYPE_CHECKING:\n     # This significantly improves completion suggestion performance when\n     # the transformers package is used with Microsoft's Pylance language server.\n-    IMAGE_PROCESSOR_MAPPING_NAMES: OrderedDict[str, Tuple[Optional[str], Optional[str]]] = OrderedDict()\n+    IMAGE_PROCESSOR_MAPPING_NAMES: OrderedDict[str, tuple[Optional[str], Optional[str]]] = OrderedDict()\n else:\n     IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n@@ -224,7 +224,7 @@ def get_image_processor_config(\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n@@ -251,7 +251,7 @@ def get_image_processor_config(\n         resume_download:\n             Deprecated and ignored. All downloads are now resumed by default when possible.\n             Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n+        proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n         token (`str` or *bool*, *optional*):\n@@ -376,7 +376,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or *bool*, *optional*):\n@@ -401,7 +401,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 execute code present on the Hub on your local machine.\n             image_processor_filename (`str`, *optional*, defaults to `\"config.json\"`):\n                 The name of the file in the model directory to use for the image processor config.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are image processor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is\n                 controlled by the `return_unused_kwargs` keyword parameter."
        },
        {
            "sha": "c81d2ab9055a778e09bb75490a83d18e1c90f1be",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -209,7 +209,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or *bool*, *optional*):\n@@ -228,7 +228,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                 Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n                 should only be set to `True` for repositories you trust and in which you have read the code, as it will\n                 execute code present on the Hub on your local machine.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are feature extractor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* feature extractor attributes is\n                 controlled by the `return_unused_kwargs` keyword parameter."
        },
        {
            "sha": "0e63093f715a731af6bc8a8960b53f54d079875b",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import warnings\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n # Build the list of all video processors\n from ...configuration_utils import PretrainedConfig\n@@ -42,7 +42,7 @@\n if TYPE_CHECKING:\n     # This significantly improves completion suggestion performance when\n     # the transformers package is used with Microsoft's Pylance language server.\n-    VIDEO_PROCESSOR_MAPPING_NAMES: OrderedDict[str, Tuple[Optional[str], Optional[str]]] = OrderedDict()\n+    VIDEO_PROCESSOR_MAPPING_NAMES: OrderedDict[str, tuple[Optional[str], Optional[str]]] = OrderedDict()\n else:\n     VIDEO_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n@@ -101,7 +101,7 @@ def get_video_processor_config(\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n@@ -128,7 +128,7 @@ def get_video_processor_config(\n         resume_download:\n             Deprecated and ignored. All downloads are now resumed by default when possible.\n             Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n+        proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n         token (`str` or *bool*, *optional*):\n@@ -243,7 +243,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n             token (`str` or *bool*, *optional*):\n@@ -262,7 +262,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n                 should only be set to `True` for repositories you trust and in which you have read the code, as it will\n                 execute code present on the Hub on your local machine.\n-            kwargs (`Dict[str, Any]`, *optional*):\n+            kwargs (`dict[str, Any]`, *optional*):\n                 The values in kwargs of any keys which are video processor attributes will be used to override the\n                 loaded values. Behavior concerning key/value pairs whose keys are *not* video processor attributes is\n                 controlled by the `return_unused_kwargs` keyword parameter."
        },
        {
            "sha": "24f0f37a8c8e0b54fec2999d2372d04d8855274a",
            "filename": "src/transformers/models/autoformer/configuration_autoformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fconfiguration_autoformer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Autoformer model configuration\"\"\"\n \n-from typing import List, Optional\n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -142,14 +142,14 @@ def __init__(\n         distribution_output: str = \"student_t\",\n         loss: str = \"nll\",\n         input_size: int = 1,\n-        lags_sequence: List[int] = [1, 2, 3, 4, 5, 6, 7],\n+        lags_sequence: list[int] = [1, 2, 3, 4, 5, 6, 7],\n         scaling: bool = True,\n         num_time_features: int = 0,\n         num_dynamic_real_features: int = 0,\n         num_static_categorical_features: int = 0,\n         num_static_real_features: int = 0,\n-        cardinality: Optional[List[int]] = None,\n-        embedding_dimension: Optional[List[int]] = None,\n+        cardinality: Optional[list[int]] = None,\n+        embedding_dimension: Optional[list[int]] = None,\n         d_model: int = 64,\n         encoder_attention_heads: int = 2,\n         decoder_attention_heads: int = 2,"
        },
        {
            "sha": "6db63b4945f71bc6f10812a4bbf6ed2a655e2f51",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -87,10 +87,10 @@ class AutoFormerDecoderOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     trend: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -155,13 +155,13 @@ class AutoformerModelOutput(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     trend: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n     loc: Optional[torch.FloatTensor] = None\n     scale: Optional[torch.FloatTensor] = None\n     static_features: Optional[torch.FloatTensor] = None\n@@ -179,7 +179,7 @@ class AutoformerFeatureEmbedder(nn.Module):\n             List of embedding dimensions of the categorical features.\n     \"\"\"\n \n-    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) -> None:\n+    def __init__(self, cardinalities: list[int], embedding_dims: list[int]) -> None:\n         super().__init__()\n \n         self.num_features = len(cardinalities)\n@@ -217,7 +217,7 @@ def __init__(self, config: AutoformerConfig):\n \n     def forward(\n         self, data: torch.Tensor, observed_indicator: torch.Tensor\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:\n             data (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`):\n@@ -254,7 +254,7 @@ def __init__(self, config: AutoformerConfig):\n \n     def forward(\n         self, data: torch.Tensor, observed_indicator: torch.Tensor\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:\n             data (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`):\n@@ -306,7 +306,7 @@ def __init__(self, config: AutoformerConfig):\n \n     def forward(\n         self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = None\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:\n             data (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`):\n@@ -491,11 +491,11 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         key_value_states: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -696,7 +696,7 @@ def forward(\n         attention_mask: torch.FloatTensor,\n         layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -796,10 +796,10 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -972,7 +972,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1110,13 +1110,13 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, AutoFormerDecoderOutput]:\n+    ) -> Union[tuple, AutoFormerDecoderOutput]:\n         r\"\"\"\n         Args:\n             trend (`torch.FloatTensor` of shape `(batch_size, prediction_length, feature_size)`, *optional*):\n@@ -1374,7 +1374,7 @@ def create_network_inputs(\n         past_observed_mask: Optional[torch.Tensor] = None,\n         future_values: Optional[torch.Tensor] = None,\n         future_time_features: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Creates the inputs for the network given the past and future values, time features, and static features.\n \n@@ -1485,13 +1485,13 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[AutoformerModelOutput, Tuple]:\n+    ) -> Union[AutoformerModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Past values of the time series, that serve as context in order to predict the future. These values may\n@@ -1751,13 +1751,13 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Seq2SeqTSPredictionOutput, Tuple]:\n+    ) -> Union[Seq2SeqTSPredictionOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Past values of the time series, that serve as context in order to predict the future. These values may"
        },
        {
            "sha": "6430ef425f97045799639738323e863bb543ea6e",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -150,9 +150,9 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[List[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -214,7 +214,7 @@ def set_input_embeddings(self, value):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n@@ -224,7 +224,7 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n@@ -274,17 +274,17 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, AyaVisionModelOutputWithPast]:\n+    ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -397,7 +397,7 @@ def get_decoder(self):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n@@ -429,9 +429,9 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -442,7 +442,7 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n+    ) -> Union[tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "ad5c1e58d4388759ed4015895b8e2896dd6dbd95",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch AyaVision model.\"\"\"\n \n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -121,7 +121,7 @@ class AyaVisionModel(LlavaModel):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         **kwargs,\n     ):\n@@ -131,7 +131,7 @@ def get_image_features(\n         Args:\n             pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n-            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n@@ -181,17 +181,17 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, AyaVisionModelOutputWithPast]:\n+    ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -267,9 +267,9 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -280,7 +280,7 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n+    ) -> Union[tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "8eacb0b5bba66eadafc36c905f02ccd734db1e82",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -139,7 +139,7 @@ def _prompt_split_image(self, num_patches):\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[AyaVisionProcessorKwargs],\n@@ -151,10 +151,10 @@ def __call__(\n         GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -225,7 +225,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n \n         Args:\n-            image_sizes (`List[List[int]]`, *optional*):\n+            image_sizes (`list[list[int]]`, *optional*):\n                 The input sizes formatted as (height, width) per each image.\n \n         Returns:"
        },
        {
            "sha": "eaf387a89271337affc33be1b68305c7b8c611cb",
            "filename": "src/transformers/models/bamba/convert_mamba_ssm_checkpoint.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import re\n from os import path\n-from typing import Dict, Optional, Union\n+from typing import Optional, Union\n \n import torch\n from huggingface_hub import split_torch_state_dict_into_shards\n@@ -31,7 +31,7 @@\n from .configuration_bamba import BambaConfig\n \n \n-def convert_state_dict_from_mamba_ssm(original_sd: Dict) -> Dict[str, torch.Tensor]:\n+def convert_state_dict_from_mamba_ssm(original_sd: dict) -> dict[str, torch.Tensor]:\n     state_dict = {}\n \n     for orig_k, param in original_sd.items():\n@@ -85,7 +85,7 @@ def convert_state_dict_from_mamba_ssm(original_sd: Dict) -> Dict[str, torch.Tens\n \n # Adapted from transformers.models.mamba.convert_mamba_ssm_checkpoint_to_pytorch.py\n def convert_ssm_config_to_hf_config(\n-    config_ssm: Dict,\n+    config_ssm: dict,\n     **kwargs,\n ) -> BambaConfig:\n     \"\"\"Convert a config from mamba_ssm to a BambaConfig from here.\"\"\"\n@@ -129,9 +129,9 @@ def convert_ssm_config_to_hf_config(\n \n \n def save_single_safetensor(\n-    state_dict: Dict,\n+    state_dict: dict,\n     save_directory: str,\n-    metadata: Dict,\n+    metadata: dict,\n ):\n     save_file(\n         state_dict,\n@@ -141,9 +141,9 @@ def save_single_safetensor(\n \n \n def save_sharded_safetensors(\n-    state_dict: Dict,\n+    state_dict: dict,\n     save_directory: str,\n-    metadata: Dict,\n+    metadata: dict,\n     max_shard_size: Union[int, str] = \"5GB\",\n ):\n     filename_pattern = SAFE_WEIGHTS_NAME.replace(\".bin\", \"{suffix}.bin\").replace("
        },
        {
            "sha": "95af1f251369def38682302e3c9459365b334476",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -24,7 +24,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, Tuple, TypedDict, Union\n+from typing import Callable, Optional, TypedDict, Union\n \n import torch\n from torch import nn\n@@ -291,12 +291,12 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -965,9 +965,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -982,7 +982,7 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):"
        },
        {
            "sha": "0d42d6e143299f2278cf5e0863a5e8e1be87ab73",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n # limitations under the License.\n \"\"\"PyTorch Bamba model.\"\"\"\n \n-from typing import Optional, Tuple, TypedDict, Union\n+from typing import Optional, TypedDict, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -738,9 +738,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[BambaFlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -755,7 +755,7 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):"
        },
        {
            "sha": "5bbc0d5a77bd68db17f51222bcbc5800722b99d1",
            "filename": "src/transformers/models/bark/configuration_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconfiguration_bark.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"BARK model configuration\"\"\"\n \n-from typing import Dict, Optional\n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import add_start_docstrings, logging\n@@ -243,10 +243,10 @@ class BarkConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        semantic_config: Optional[Dict] = None,\n-        coarse_acoustics_config: Optional[Dict] = None,\n-        fine_acoustics_config: Optional[Dict] = None,\n-        codec_config: Optional[Dict] = None,\n+        semantic_config: Optional[dict] = None,\n+        coarse_acoustics_config: Optional[dict] = None,\n+        fine_acoustics_config: Optional[dict] = None,\n+        codec_config: Optional[dict] = None,\n         initializer_range=0.02,\n         **kwargs,\n     ):"
        },
        {
            "sha": "0fa68184c88526ccc793a336a64bce798f6d7759",
            "filename": "src/transformers/models/bark/generation_configuration_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fgeneration_configuration_bark.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"BARK model generation configuration\"\"\"\n \n import copy\n-from typing import Dict, Optional\n+from typing import Optional\n \n from ...generation.configuration_utils import GenerationConfig\n from ...utils import logging\n@@ -245,9 +245,9 @@ class BarkGenerationConfig(GenerationConfig):\n \n     def __init__(\n         self,\n-        semantic_config: Optional[Dict] = None,\n-        coarse_acoustics_config: Optional[Dict] = None,\n-        fine_acoustics_config: Optional[Dict] = None,\n+        semantic_config: Optional[dict] = None,\n+        coarse_acoustics_config: Optional[dict] = None,\n+        fine_acoustics_config: Optional[dict] = None,\n         sample_rate=24_000,\n         codebook_size=1024,\n         **kwargs,\n@@ -318,7 +318,7 @@ def to_dict(self):\n         Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n \n         Returns:\n-            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n+            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n "
        },
        {
            "sha": "4ee608d9aec6681a55fac025839aa352842f7491",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n import warnings\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -506,7 +506,7 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwarg\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n+        past_key_values: Optional[tuple[torch.FloatTensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n@@ -516,7 +516,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithPast]:\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n         input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n@@ -659,8 +659,8 @@ def forward(\n \n     @staticmethod\n     def _reorder_cache(\n-        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> Tuple[Tuple[torch.Tensor]]:\n+        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n+    ) -> tuple[tuple[torch.Tensor]]:\n         \"\"\"\n         This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n         [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n@@ -687,7 +687,7 @@ def generate(\n         self,\n         input_ids: torch.Tensor,\n         semantic_generation_config: BarkSemanticGenerationConfig = None,\n-        history_prompt: Optional[Dict[str, torch.Tensor]] = None,\n+        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n@@ -701,7 +701,7 @@ def generate(\n                 long as the longest generation among the batch.\n             semantic_generation_config (`BarkSemanticGenerationConfig`):\n                 Generation config indicating how to generate the semantic tokens.\n-            history_prompt (`Optional[Dict[str,torch.Tensor]]`, *optional*):\n+            history_prompt (`Optional[dict[str,torch.Tensor]]`, *optional*):\n                 Optional `Bark` speaker prompt.\n             attention_mask (`Optional[torch.Tensor]`, *optional*):\n                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n@@ -801,7 +801,7 @@ def preprocess_histories(\n         batch_size: int,\n         semantic_generation_config: int,\n         codebook_size: int,\n-        history_prompt: Optional[Dict[str, torch.Tensor]] = None,\n+        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n     ):\n         \"\"\"\n         Preprocess the optional `Bark` speaker prompts before `self.generate`.\n@@ -817,7 +817,7 @@ def preprocess_histories(\n                 Generation config indicating how to generate the semantic tokens.\n             codebook_size (`int`):\n                 Codebook channel size, i.e. the size of the output vocabulary per codebook channel.\n-            history_prompt (`Optional[Dict[str,torch.Tensor]]`):\n+            history_prompt (`Optional[dict[str,torch.Tensor]]`):\n                 Optional `Bark` speaker prompt.\n         Returns: Returns:\n             `tuple(torch.FloatTensor)`:\n@@ -874,10 +874,10 @@ def generate(\n         semantic_generation_config: BarkSemanticGenerationConfig = None,\n         coarse_generation_config: BarkCoarseGenerationConfig = None,\n         codebook_size: int = 1024,\n-        history_prompt: Optional[Dict[str, torch.Tensor]] = None,\n+        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n         return_output_lengths: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[torch.LongTensor, Tuple[torch.LongTensor, torch.LongTensor]]:\n+    ) -> Union[torch.LongTensor, tuple[torch.LongTensor, torch.LongTensor]]:\n         \"\"\"\n         Generates coarse acoustics tokens from input text semantic tokens and an additional optional `Bark` speaker\n         prompt.\n@@ -891,7 +891,7 @@ def generate(\n                 Generation config indicating how to generate the coarse tokens.\n             codebook_size (`int`, *optional*, defaults to 1024):\n                 Codebook channel size, i.e. the size of the output vocabulary per codebook channel.\n-            history_prompt (`Optional[Dict[str,torch.Tensor]]`, *optional*):\n+            history_prompt (`Optional[dict[str,torch.Tensor]]`, *optional*):\n                 Optional `Bark` speaker prompt.\n             return_output_lengths (`bool`, *optional*):\n                 Whether or not to return the output lengths. Useful when batching.\n@@ -1179,7 +1179,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         codebook_idx (`int`):\n             Index of the codebook that will be predicted.\n@@ -1295,7 +1295,7 @@ def generate(\n         coarse_generation_config: BarkCoarseGenerationConfig = None,\n         fine_generation_config: BarkFineGenerationConfig = None,\n         codebook_size: int = 1024,\n-        history_prompt: Optional[Dict[str, torch.Tensor]] = None,\n+        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n         \"\"\"\n@@ -1313,7 +1313,7 @@ def generate(\n                 Generation config indicating how to generate the fine tokens.\n             codebook_size (`int`, *optional*, defaults to 1024):\n                 Codebook channel size, i.e. the size of the output vocabulary per codebook channel.\n-            history_prompt (`Optional[Dict[str,torch.Tensor]]`, *optional*):\n+            history_prompt (`Optional[dict[str,torch.Tensor]]`, *optional*):\n                 Optional `Bark` speaker prompt.\n         Returns:\n             torch.LongTensor: Output fine acoustics tokens.\n@@ -1563,7 +1563,7 @@ def codec_decode(self, fine_output, output_lengths=None):\n     def generate(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n-        history_prompt: Optional[Dict[str, torch.Tensor]] = None,\n+        history_prompt: Optional[dict[str, torch.Tensor]] = None,\n         return_output_lengths: Optional[bool] = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n@@ -1574,7 +1574,7 @@ def generate(\n             input_ids (`Optional[torch.Tensor]` of shape (batch_size, seq_len), *optional*):\n                 Input ids. Will be truncated up to 256 tokens. Note that the output audios will be as long as the\n                 longest generation among the batch.\n-            history_prompt (`Optional[Dict[str,torch.Tensor]]`, *optional*):\n+            history_prompt (`Optional[dict[str,torch.Tensor]]`, *optional*):\n                 Optional `Bark` speaker prompt. Note that for now, this model takes only one speaker prompt per batch.\n             kwargs (*optional*): Remaining dictionary of keyword arguments. Keyword arguments are of two types:\n \n@@ -1710,7 +1710,7 @@ def _check_and_enable_flash_attn_2(\n         cls,\n         config,\n         torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, Dict[str, int]]] = None,\n+        device_map: Optional[Union[str, dict[str, int]]] = None,\n         hard_check_only: bool = False,\n         check_device_map: bool = False,\n     ):"
        },
        {
            "sha": "e2b47dca6acb4b216853ab983eb192e3ee8509b8",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -39,7 +39,7 @@ class BarkProcessor(ProcessorMixin):\n     Args:\n         tokenizer ([`PreTrainedTokenizer`]):\n             An instance of [`PreTrainedTokenizer`].\n-        speaker_embeddings (`Dict[Dict[str]]`, *optional*):\n+        speaker_embeddings (`dict[dict[str]]`, *optional*):\n             Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g\n             `\"en_speaker_4\"`). The second level contains `\"semantic_prompt\"`, `\"coarse_prompt\"` and `\"fine_prompt\"`\n             embeddings. The values correspond to the path of the corresponding `np.ndarray`. See\n@@ -240,11 +240,11 @@ def __call__(\n         to the tokenizer and to `cached_file` method if `voice_preset` is a valid filename.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            voice_preset (`str`, `Dict[np.ndarray]`):\n+            voice_preset (`str`, `dict[np.ndarray]`):\n                 The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g\n                 `\"en_speaker_1\"`, or directly a dictionary of `np.ndarray` embeddings for each submodel of `Bark`. Or\n                 it can be a valid file name of a local `.npz` single voice preset."
        },
        {
            "sha": "f0adc76924fe737701ca1d11abd0aa7e9e4e4d25",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import copy\n import math\n import warnings\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -198,7 +198,7 @@ def forward(\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -296,7 +296,7 @@ def forward(\n         attention_mask: torch.FloatTensor,\n         layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -385,7 +385,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -782,7 +782,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -956,14 +956,14 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1253,16 +1253,16 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqModelOutput]:\n+    ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1425,8 +1425,8 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1435,7 +1435,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqLMOutput]:\n+    ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1612,7 +1612,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1621,7 +1621,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n+    ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1757,7 +1757,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1767,7 +1767,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n+    ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1925,15 +1925,15 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n             Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:"
        },
        {
            "sha": "818254f3bfa1e95685fdcde8af8f074edb69761f",
            "filename": "src/transformers/models/bart/modeling_flax_bart.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import random\n from functools import partial\n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax.linen as nn\n import jax\n@@ -203,7 +203,7 @@\n         decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n             range `[0, config.max_position_embeddings - 1]`.\n-        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n+        past_key_values (`dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n             Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n             auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n         output_attentions (`bool`, *optional*):\n@@ -309,7 +309,7 @@ def __call__(\n         attention_mask: Optional[jnp.ndarray] = None,\n         init_cache: bool = False,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -429,7 +429,7 @@ def __call__(\n         attention_mask: jnp.ndarray,\n         output_attentions: bool = True,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         residual = hidden_states\n         hidden_states, attn_weights = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask)\n \n@@ -552,7 +552,7 @@ def __call__(\n         init_cache: bool = False,\n         output_attentions: bool = True,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         residual = hidden_states\n \n         # Self Attention\n@@ -910,7 +910,7 @@ class FlaxBartPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: BartConfig,\n-        input_shape: Tuple[int] = (1, 1),\n+        input_shape: tuple[int] = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -919,7 +919,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         # make sure initialization pass will work for FlaxBartForSequenceClassificationModule\n@@ -1742,7 +1742,7 @@ class FlaxBartDecoderPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: BartConfig,\n-        input_shape: Tuple[int] = (1, 1),\n+        input_shape: tuple[int] = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -1753,7 +1753,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         attention_mask = jnp.ones_like(input_ids)"
        },
        {
            "sha": "d0535a57da8761cb764ac9a2d97c129f6560f6ac",
            "filename": "src/transformers/models/bart/modeling_tf_bart.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -181,11 +181,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         key_value_states: tf.Tensor | None = None,\n-        past_key_value: Tuple[Tuple[tf.Tensor]] | None = None,\n+        past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Tuple[tf.Tensor, tf.Tensor | None]:\n+    ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -427,9 +427,9 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_value: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         training: Optional[bool] = False,\n-    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n+    ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -710,7 +710,7 @@ def tf_to_pt_weight_rename(self, tf_weight):\n         encoder_outputs (`tf.FloatTensor`, *optional*):\n             hidden states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n             of shape `(batch_size, sequence_length, hidden_size)` is a sequence of\n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -780,7 +780,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n         \"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -938,13 +938,13 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n         r\"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -988,7 +988,7 @@ def call(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+            past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n                 Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n                 decoding.\n \n@@ -1167,8 +1167,8 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1177,7 +1177,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFSeq2SeqModelOutput, tuple[tf.Tensor]]:\n         # different to other models, Bart automatically creates decoder_input_ids from\n         # input_ids if no decoder_input_ids are provided\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n@@ -1297,8 +1297,8 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1307,7 +1307,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n         **kwargs,\n-    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1430,7 +1430,7 @@ def call(\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1439,7 +1439,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFSeq2SeqLMOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFSeq2SeqLMOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1601,7 +1601,7 @@ def call(\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1610,7 +1610,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFSeq2SeqSequenceClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFSeq2SeqSequenceClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "e5c216346c1e3bc3cd0c4a065ea8261a908f2e9c",
            "filename": "src/transformers/models/bart/tokenization_bart.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n import json\n import os\n from functools import lru_cache\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import regex as re\n \n@@ -279,7 +279,7 @@ def convert_tokens_to_string(self, tokens):\n         text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n         return text\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n@@ -309,8 +309,8 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n         return vocab_file, merge_file\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BART sequence has the following format:\n@@ -319,13 +319,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         if token_ids_1 is None:\n             return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n@@ -334,22 +334,22 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -361,20 +361,20 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. BART does not\n         make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]"
        },
        {
            "sha": "88b002f595299f8d16b605c0ce4fd59330e5c4c4",
            "filename": "src/transformers/models/bart/tokenization_bart_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from tokenizers import processors\n \n@@ -233,7 +233,7 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n \n         return super()._encode_plus(*args, **kwargs)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n \n@@ -245,20 +245,20 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n         return output + [self.eos_token_id] + token_ids_1 + [self.eos_token_id]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. BART does not\n         make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]"
        },
        {
            "sha": "bc583e0cd5dc455fbd91841de386e6bf54ad02b9",
            "filename": "src/transformers/models/barthez/tokenization_barthez.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -117,7 +117,7 @@ def __init__(\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n         # Mask token behave like a normal word, i.e. include the space before it. Will have normalized=False by default this way\n@@ -141,8 +141,8 @@ def __init__(\n         )\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BARThez sequence has the following format:\n@@ -151,13 +151,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n \n         if token_ids_1 is None:\n@@ -167,22 +167,22 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -194,19 +194,19 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]\n@@ -224,7 +224,7 @@ def get_vocab(self):\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         return self.sp_model.encode(text, out_type=str)\n \n     def _convert_token_to_id(self, token):\n@@ -270,7 +270,7 @@ def __setstate__(self, d):\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(self.vocab_file)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "64050ca8848f57c25d272e0bc31a3f878040e14c",
            "filename": "src/transformers/models/barthez/tokenization_barthez_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbarthez%2Ftokenization_barthez_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import AddedToken\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -83,7 +83,7 @@ class BarthezTokenizerFast(PreTrainedTokenizerFast):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        additional_special_tokens (`List[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n+        additional_special_tokens (`list[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n             Additional special tokens used by the tokenizer.\n     \"\"\"\n \n@@ -123,8 +123,8 @@ def __init__(\n         self.vocab_file = vocab_file\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BARThez sequence has the following format:\n@@ -133,13 +133,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n \n         if token_ids_1 is None:\n@@ -149,19 +149,19 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]\n@@ -170,7 +170,7 @@ def create_token_type_ids_from_sequences(\n             return len(cls + token_ids_0 + sep) * [0]\n         return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError(\n                 \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \""
        },
        {
            "sha": "41a122bf913c54526cdd10c14fbbea5896da2d00",
            "filename": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -117,7 +117,7 @@ def __init__(\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n         # Mask token behave like a normal word, i.e. include the space before it\n@@ -177,8 +177,8 @@ def __setstate__(self, d):\n         self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. An BARTPho sequence has the following format:\n@@ -187,13 +187,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n \n         if token_ids_1 is None:\n@@ -203,22 +203,22 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n \n         if already_has_special_tokens:\n@@ -231,20 +231,20 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. BARTPho does not\n         make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n \n         \"\"\"\n \n@@ -264,7 +264,7 @@ def get_vocab(self):\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         return self.sp_model.encode(text, out_type=str)\n \n     def _convert_token_to_id(self, token):\n@@ -283,7 +283,7 @@ def convert_tokens_to_string(self, tokens):\n         out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "4bea72b7bd0b514fbe8d9de83222ab5f3e03ed9f",
            "filename": "src/transformers/models/beit/configuration_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -76,7 +76,7 @@ class BeitConfig(BackboneConfigMixin, PretrainedConfig):\n         use_mean_pooling (`bool`, *optional*, defaults to `True`):\n             Whether to mean pool the final hidden states of the patches instead of using the final hidden state of the\n             CLS token, before applying the classification head.\n-        pool_scales (`Tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`):\n+        pool_scales (`tuple[int]`, *optional*, defaults to `[1, 2, 3, 6]`):\n             Pooling scales used in Pooling Pyramid Module applied on the last feature map.\n         use_auxiliary_head (`bool`, *optional*, defaults to `True`):\n             Whether to use an auxiliary head during training.\n@@ -90,12 +90,12 @@ class BeitConfig(BackboneConfigMixin, PretrainedConfig):\n             Whether to concatenate the output of the auxiliary head with the input before the classification layer.\n         semantic_loss_ignore_index (`int`, *optional*, defaults to 255):\n             The index that is ignored by the loss function of the semantic segmentation model.\n-        out_features (`List[str]`, *optional*):\n+        out_features (`list[str]`, *optional*):\n             If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n             (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n             corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n             same order as defined in the `stage_names` attribute.\n-        out_indices (`List[int]`, *optional*):\n+        out_indices (`list[int]`, *optional*):\n             If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n             many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n             If unset and `out_features` is unset, will default to the last stage. Must be in the"
        },
        {
            "sha": "95e8ec54f19cccf244c7f497d8c3210d640b1120",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Beit.\"\"\"\n \n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n \n@@ -64,7 +64,7 @@ class BeitImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n             `do_resize` parameter in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 256, \"width\": 256}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"height\": 256, \"width\": 256}`):\n             Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n             method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n@@ -74,7 +74,7 @@ class BeitImageProcessor(BaseImageProcessor):\n             Whether to center crop the image. If the input size is smaller than `crop_size` along any edge, the image\n             is padded with 0's and then center cropped. Can be overridden by the `do_center_crop` parameter in the\n             `preprocess` method.\n-        crop_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n+        crop_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 224, \"width\": 224}`):\n             Desired output size when applying center-cropping. Only has an effect if `do_center_crop` is set to `True`.\n             Can be overridden by the `crop_size` parameter in the `preprocess` method.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n@@ -86,10 +86,10 @@ class BeitImageProcessor(BaseImageProcessor):\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n             The mean to use if normalizing the image. This is a float or list of floats of length of the number of\n             channels of the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n             The standard deviation to use if normalizing the image. This is a float or list of floats of length of the\n             number of channels of the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_reduce_labels (`bool`, *optional*, defaults to `False`):\n@@ -106,15 +106,15 @@ class BeitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_rescale: bool = True,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n@@ -136,7 +136,7 @@ def __init__(\n         self.do_reduce_labels = do_reduce_labels\n \n     @classmethod\n-    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n         \"\"\"\n@@ -148,7 +148,7 @@ def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -160,7 +160,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PIL.Image.BICUBIC`):\n                 Resampling filter to use when resiizing the image.\n@@ -194,15 +194,15 @@ def _preprocess(\n         image: ImageInput,\n         do_reduce_labels: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n         if do_reduce_labels:\n@@ -226,15 +226,15 @@ def _preprocess_image(\n         self,\n         image: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n@@ -271,10 +271,10 @@ def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_reduce_labels: Optional[bool] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n@@ -320,15 +320,15 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_reduce_labels: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n@@ -346,14 +346,14 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Size of the image after resizing.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`, Only\n                 has an effect if `do_resize` is set to `True`.\n             do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                 Whether to center crop the image.\n-            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                 Size of the image after center crop. If one edge the image is smaller than `crop_size`, it will be\n                 padded with zeros and then cropped\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n@@ -362,9 +362,9 @@ def preprocess(\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation.\n             do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n                 Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n@@ -470,19 +470,19 @@ def preprocess(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\"\n         Converts the output of [`BeitForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n         Args:\n             outputs ([`BeitForSemanticSegmentation`]):\n                 Raw outputs of the model.\n-            target_sizes (`List[Tuple]` of length `batch_size`, *optional*):\n+            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n                 List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n                 predictions will not be resized.\n \n         Returns:\n-            semantic_segmentation: `List[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n             segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n             specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n         \"\"\""
        },
        {
            "sha": "6a4077008da2816073920227fd9ca20176539ff5",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Beit.\"\"\"\n \n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n from torchvision.transforms import functional as F\n@@ -72,7 +72,7 @@ def __init__(self, **kwargs: Unpack[BeitFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n     @classmethod\n-    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         Overrides the `from_dict` method from the base class to save support of deprecated `reduce_labels` in old configs\n         \"\"\"\n@@ -240,19 +240,19 @@ def preprocess(\n \n         return BatchFeature(data=data)\n \n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\"\n         Converts the output of [`BeitForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n         Args:\n             outputs ([`BeitForSemanticSegmentation`]):\n                 Raw outputs of the model.\n-            target_sizes (`List[Tuple]` of length `batch_size`, *optional*):\n+            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n                 List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n                 predictions will not be resized.\n \n         Returns:\n-            semantic_segmentation: `List[torch.Tensor]` of length `batch_size`, where each item is a semantic\n+            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n             segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n             specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n         \"\"\""
        },
        {
            "sha": "70a6c78e0651d6c4044520a840d42142658282c8",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -291,8 +291,8 @@ def forward(\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int]] = None,\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: Optional[tuple[int]] = None,\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         mixed_query_layer = self.query(hidden_states)\n \n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n@@ -346,8 +346,8 @@ def forward(\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int]] = None,\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: Optional[tuple[int]] = None,\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         if output_attentions or head_mask is not None:\n             logger.warning_once(\n                 \"`BeitSdpaSelfAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not \"\n@@ -456,8 +456,8 @@ def forward(\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int]] = None,\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: Optional[tuple[int]] = None,\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_outputs = self.attention(\n             hidden_states, head_mask, output_attentions, relative_position_bias, interpolate_pos_encoding, resolution\n         )\n@@ -525,8 +525,8 @@ def forward(\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int]] = None,\n-    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        resolution: Optional[tuple[int]] = None,\n+    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in BEiT, layernorm is applied before self-attention\n             head_mask,\n@@ -573,7 +573,7 @@ def __init__(self, config: BeitConfig, window_size: tuple) -> None:\n         # cls to token & token 2 cls & cls to cls\n \n     @compile_compatible_method_lru_cache(maxsize=10)\n-    def generate_relative_position_index(self, window_size: Tuple[int, int]) -> torch.Tensor:\n+    def generate_relative_position_index(self, window_size: tuple[int, int]) -> torch.Tensor:\n         \"\"\"\n         This method creates the relative position index, modified to support arbitrary window sizes,\n         as introduced in [MiDaS v3.1](https://huggingface.co/papers/2307.14460).\n@@ -674,7 +674,7 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int, int]] = None,\n+        resolution: Optional[tuple[int, int]] = None,\n         return_dict: bool = True,\n     ) -> Union[tuple, BaseModelOutput]:\n         all_hidden_states = () if output_hidden_states else None\n@@ -1073,10 +1073,10 @@ def __init__(\n         self,\n         in_channels: int,\n         out_channels: int,\n-        kernel_size: Union[int, Tuple[int, int]],\n-        padding: Union[int, Tuple[int, int], str] = 0,\n+        kernel_size: Union[int, tuple[int, int]],\n+        padding: Union[int, tuple[int, int], str] = 0,\n         bias: bool = False,\n-        dilation: Union[int, Tuple[int, int]] = 1,\n+        dilation: Union[int, tuple[int, int]] = 1,\n     ) -> None:\n         super().__init__()\n         self.conv = nn.Conv2d(\n@@ -1129,7 +1129,7 @@ class BeitPyramidPoolingModule(nn.Module):\n     Based on OpenMMLab's implementation, found in https://github.com/open-mmlab/mmsegmentation.\n     \"\"\"\n \n-    def __init__(self, pool_scales: Tuple[int, ...], in_channels: int, channels: int, align_corners: bool) -> None:\n+    def __init__(self, pool_scales: tuple[int, ...], in_channels: int, channels: int, align_corners: bool) -> None:\n         super().__init__()\n         self.pool_scales = pool_scales\n         self.align_corners = align_corners\n@@ -1141,7 +1141,7 @@ def __init__(self, pool_scales: Tuple[int, ...], in_channels: int, channels: int\n             self.blocks.append(block)\n             self.add_module(str(i), block)\n \n-    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n+    def forward(self, x: torch.Tensor) -> list[torch.Tensor]:\n         ppm_outs = []\n         for ppm in self.blocks:\n             ppm_out = ppm(x)\n@@ -1253,7 +1253,7 @@ class BeitFCNHead(nn.Module):\n     \"\"\"\n \n     def __init__(\n-        self, config: BeitConfig, in_index: int = 2, kernel_size: int = 3, dilation: Union[int, Tuple[int, int]] = 1\n+        self, config: BeitConfig, in_index: int = 2, kernel_size: int = 3, dilation: Union[int, tuple[int, int]] = 1\n     ) -> None:\n         super().__init__()\n         self.in_channels = config.hidden_size"
        },
        {
            "sha": "c80deace6b39dc79968eff060a15573b0e2ea5ad",
            "filename": "src/transformers/models/beit/modeling_flax_beit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_flax_beit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n \n-from typing import Callable, List, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax\n import flax.linen as nn\n@@ -116,7 +116,7 @@ class FlaxBeitModelOutputWithPooling(FlaxBaseModelOutputWithPooling):\n \"\"\"\n \n \n-def relative_position_index_init(window_size: Tuple[int, int]) -> jnp.ndarray:\n+def relative_position_index_init(window_size: tuple[int, int]) -> jnp.ndarray:\n     \"\"\"\n     get pair-wise relative position index for each token inside the window\n     \"\"\"\n@@ -240,7 +240,7 @@ def __call__(self, pixel_values, bool_masked_pos=None, deterministic=True):\n \n class FlaxBeitRelativePositionBias(nn.Module):\n     config: BeitConfig\n-    window_size: Tuple[int, int]\n+    window_size: tuple[int, int]\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n     def setup(self):\n@@ -263,7 +263,7 @@ def __call__(self):\n \n class FlaxBeitSelfAttention(nn.Module):\n     config: BeitConfig\n-    window_size: Tuple[int, int]\n+    window_size: tuple[int, int]\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n     def setup(self):\n@@ -366,7 +366,7 @@ def __call__(self, hidden_states, deterministic: bool = True):\n \n class FlaxBeitAttention(nn.Module):\n     config: BeitConfig\n-    window_size: Tuple[int, int]\n+    window_size: tuple[int, int]\n     dtype: jnp.dtype = jnp.float32\n \n     def setup(self):\n@@ -430,7 +430,7 @@ def __call__(self, hidden_states, deterministic: bool = True):\n \n class FlaxBeitLayer(nn.Module):\n     config: BeitConfig\n-    window_size: Tuple[int, int]\n+    window_size: tuple[int, int]\n     drop_path_rate: float\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n@@ -491,8 +491,8 @@ def __call__(\n \n class FlaxBeitLayerCollection(nn.Module):\n     config: BeitConfig\n-    window_size: Tuple[int, int]\n-    drop_path_rates: List[float]\n+    window_size: tuple[int, int]\n+    drop_path_rates: list[float]\n     relative_position_bias: Callable[[], jnp.ndarray]\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n@@ -546,7 +546,7 @@ def __call__(\n \n class FlaxBeitEncoder(nn.Module):\n     config: BeitConfig\n-    window_size: Tuple[int, int]\n+    window_size: tuple[int, int]\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n     def setup(self):\n@@ -609,7 +609,7 @@ def __init__(\n             input_shape = (1, config.image_size, config.image_size, config.num_channels)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         pixel_values = jnp.zeros(input_shape, dtype=self.dtype)\n "
        },
        {
            "sha": "12080dfff6ff269666283daaae571814025c7f70",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import warnings\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -226,9 +226,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -335,9 +335,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -476,9 +476,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -544,9 +544,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -622,12 +622,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -845,8 +845,8 @@ class BertForPreTrainingOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     prediction_logits: Optional[torch.FloatTensor] = None\n     seq_relationship_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @auto_docstring(\n@@ -908,12 +908,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1080,7 +1080,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n+    ) -> Union[tuple[torch.Tensor], BertForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1185,13 +1185,13 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.Tensor]] = None,\n+        past_key_values: Optional[list[torch.Tensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **loss_kwargs,\n-    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n@@ -1288,7 +1288,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1385,7 +1385,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple[torch.Tensor], NextSentencePredictorOutput]:\n+    ) -> Union[tuple[torch.Tensor], NextSentencePredictorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n@@ -1491,7 +1491,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1579,7 +1579,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1687,7 +1687,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1754,7 +1754,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.bert("
        },
        {
            "sha": "37828642eb4eb025973952ce0750a66e7e7693ea",
            "filename": "src/transformers/models/bert/modeling_flax_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax\n import flax.linen as nn\n@@ -84,8 +84,8 @@ class FlaxBertForPreTrainingOutput(ModelOutput):\n \n     prediction_logits: jnp.ndarray = None\n     seq_relationship_logits: jnp.ndarray = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n BERT_START_DOCSTRING = r\"\"\"\n@@ -770,7 +770,7 @@ class FlaxBertPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: BertConfig,\n-        input_shape: Tuple = (1, 1),\n+        input_shape: tuple = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -792,7 +792,7 @@ def enable_gradient_checkpointing(self):\n             gradient_checkpointing=True,\n         )\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         token_type_ids = jnp.zeros_like(input_ids)"
        },
        {
            "sha": "d4f0b47e24e1ff86080ed04b64620dc27794a38f",
            "filename": "src/transformers/models/bert/modeling_tf_bert.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -20,7 +20,7 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -243,10 +243,10 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor,\n         encoder_attention_mask: tf.Tensor,\n-        past_key_value: Tuple[tf.Tensor],\n+        past_key_value: tuple[tf.Tensor],\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         batch_size = shape_list(hidden_states)[0]\n         mixed_query_layer = self.query(inputs=hidden_states)\n \n@@ -379,10 +379,10 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor,\n         encoder_attention_mask: tf.Tensor,\n-        past_key_value: Tuple[tf.Tensor],\n+        past_key_value: tuple[tf.Tensor],\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         self_outputs = self.self_attention(\n             hidden_states=input_tensor,\n             attention_mask=attention_mask,\n@@ -493,10 +493,10 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n-        past_key_value: Tuple[tf.Tensor] | None,\n+        past_key_value: tuple[tf.Tensor] | None,\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -588,13 +588,13 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] | None,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None,\n         use_cache: Optional[bool],\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -749,7 +749,7 @@ def set_output_embeddings(self, value: tf.Variable):\n         self.input_embeddings.weight = value\n         self.input_embeddings.vocab_size = shape_list(value)[0]\n \n-    def get_bias(self) -> Dict[str, tf.Variable]:\n+    def get_bias(self) -> dict[str, tf.Variable]:\n         return {\"bias\": self.bias}\n \n     def set_bias(self, value: tf.Variable):\n@@ -851,13 +851,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1050,8 +1050,8 @@ class TFBertForPreTrainingOutput(ModelOutput):\n     loss: tf.Tensor | None = None\n     prediction_logits: Optional[tf.Tensor] = None\n     seq_relationship_logits: Optional[tf.Tensor] = None\n-    hidden_states: Optional[Union[Tuple[tf.Tensor], tf.Tensor]] = None\n-    attentions: Optional[Union[Tuple[tf.Tensor], tf.Tensor]] = None\n+    hidden_states: Optional[Union[tuple[tf.Tensor], tf.Tensor]] = None\n+    attentions: Optional[Union[tuple[tf.Tensor], tf.Tensor]] = None\n \n \n BERT_START_DOCSTRING = r\"\"\"\n@@ -1098,7 +1098,7 @@ class TFBertForPreTrainingOutput(ModelOutput):\n \n BERT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        input_ids (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `({0})`):\n+        input_ids (`np.ndarray`, `tf.Tensor`, `list[tf.Tensor]` ``dict[str, tf.Tensor]` or `dict[str, np.ndarray]` and each example must have the shape `({0})`):\n             Indices of input sequence tokens in the vocabulary.\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n@@ -1179,13 +1179,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1197,7 +1197,7 @@ def call(\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -1279,7 +1279,7 @@ def call(\n         labels: np.ndarray | tf.Tensor | None = None,\n         next_sentence_label: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBertForPreTrainingOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBertForPreTrainingOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1291,7 +1291,7 @@ def call(\n \n             - 0 indicates sequence B is a continuation of sequence A,\n             - 1 indicates sequence B is a random sequence.\n-        kwargs (`Dict[str, any]`, *optional*, defaults to `{}`):\n+        kwargs (`dict[str, any]`, *optional*, defaults to `{}`):\n             Used to hide legacy arguments that have been deprecated.\n \n         Return:\n@@ -1410,7 +1410,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1509,15 +1509,15 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n         **kwargs,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1529,7 +1529,7 @@ def call(\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -1622,7 +1622,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         next_sentence_label: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFNextSentencePredictorOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFNextSentencePredictorOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         Return:\n \n@@ -1736,7 +1736,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1825,7 +1825,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1951,7 +1951,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -2051,7 +2051,7 @@ def call(\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "160fd99e993522d4d3aee54699cd7010dae9d54a",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import collections\n import os\n import unicodedata\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n from ...utils import logging\n@@ -184,8 +184,8 @@ def convert_tokens_to_string(self, tokens):\n         return out_string\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BERT sequence has the following format:\n@@ -209,8 +209,8 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n@@ -236,7 +236,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         index = 0\n         if os.path.isdir(save_directory):\n             vocab_file = os.path.join("
        },
        {
            "sha": "2cdc6129881be3f88a32ab1ae653c44db2049ece",
            "filename": "src/transformers/models/bert/tokenization_bert_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Fast Tokenization classes for Bert.\"\"\"\n \n import json\n-from typing import Optional, Tuple\n+from typing import Optional\n \n from tokenizers import normalizers\n \n@@ -138,7 +138,7 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n         return output\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n "
        },
        {
            "sha": "c8fca52c4cbf39d033296429e2034d7b9dd7904e",
            "filename": "src/transformers/models/bert/tokenization_bert_tf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -1,5 +1,5 @@\n import os\n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n import tensorflow as tf\n from tensorflow_text import BertTokenizer as BertTokenizerLayer\n@@ -58,7 +58,7 @@ class instead. BertTokenizer supports some additional options, but is slower and\n \n     def __init__(\n         self,\n-        vocab_list: List,\n+        vocab_list: list,\n         do_lower_case: bool,\n         cls_token_id: Optional[int] = None,\n         sep_token_id: Optional[int] = None,"
        },
        {
            "sha": "959a3cce077da8554e1aee44a7ee77f5088954fa",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch BERT model specific for generation.\"\"\"\n \n import math\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -91,9 +91,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -226,9 +226,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -297,9 +297,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -376,12 +376,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -658,13 +658,13 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -808,13 +808,13 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in"
        },
        {
            "sha": "baeba9e4b38849b6d325fbccd525129cc2955271",
            "filename": "src/transformers/models/bert_generation/tokenization_bert_generation.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Ftokenization_bert_generation.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -73,7 +73,7 @@ class BertGenerationTokenizer(PreTrainedTokenizer):\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    prefix_tokens: List[int] = []\n+    prefix_tokens: list[int] = []\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n \n     def __init__(\n@@ -84,7 +84,7 @@ def __init__(\n         unk_token=\"<unk>\",\n         pad_token=\"<pad>\",\n         sep_token=\"<::::>\",\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n         self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n@@ -129,7 +129,7 @@ def __setstate__(self, d):\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(self.vocab_file)\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n         return self.sp_model.encode(text, out_type=str)\n \n@@ -156,7 +156,7 @@ def convert_tokens_to_string(self, tokens):\n         out_string += self.sp_model.decode(current_sub_tokens)\n         return out_string.strip()\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "decf3dfa5c38caf7c3ae1238ee2036109ae2421b",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import copy\n import os\n import unicodedata\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n from ...utils import is_sentencepiece_available, is_sudachi_projection_available, logging\n@@ -256,8 +256,8 @@ def convert_tokens_to_string(self, tokens):\n \n     # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens\n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BERT sequence has the following format:\n@@ -282,8 +282,8 @@ def build_inputs_with_special_tokens(\n \n     # Copied from transformers.models.bert.tokenization_bert.BertTokenizer.get_special_tokens_mask\n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n@@ -309,7 +309,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if os.path.isdir(save_directory):\n             if self.subword_tokenizer_type == \"sentencepiece\":\n                 vocab_file = os.path.join(\n@@ -892,7 +892,7 @@ def __init__(\n         do_lower_case=False,\n         remove_space=True,\n         keep_accents=True,\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n     ):\n         self.vocab = vocab\n         self.unk_token = unk_token"
        },
        {
            "sha": "3ce1a3182bf9d5b3f960b5a211544612ab3129c3",
            "filename": "src/transformers/models/bertweet/tokenization_bertweet.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbertweet%2Ftokenization_bertweet.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import re\n from shutil import copyfile\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import regex\n \n@@ -165,8 +165,8 @@ def __init__(\n         )\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BERTweet sequence has the following format:\n@@ -175,13 +175,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n \n         if token_ids_1 is None:\n@@ -191,22 +191,22 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n \n         if already_has_special_tokens:\n@@ -219,20 +219,20 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does\n         not make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n \n         sep = [self.sep_token_id]\n@@ -370,7 +370,7 @@ def convert_tokens_to_string(self, tokens):\n         out_string = \" \".join(tokens).replace(\"@@ \", \"\").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "e06c8f87d5f8fcf65f50736005452db3ffc02087",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import os\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -1572,7 +1572,7 @@ def forward(\n         to_mask=None,\n         blocked_encoder_mask=None,\n         return_dict=True,\n-    ) -> Union[BaseModelOutputWithPastAndCrossAttentions, Tuple]:\n+    ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -1788,8 +1788,8 @@ class BigBirdForPreTrainingOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     prediction_logits: Optional[torch.FloatTensor] = None\n     seq_relationship_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -1823,8 +1823,8 @@ class BigBirdForQuestionAnsweringModelOutput(ModelOutput):\n     start_logits: Optional[torch.FloatTensor] = None\n     end_logits: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @auto_docstring\n@@ -1900,13 +1900,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, Tuple[torch.FloatTensor]]:\n+    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, tuple[torch.FloatTensor]]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -2181,7 +2181,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[BigBirdForPreTrainingOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[BigBirdForPreTrainingOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -2290,7 +2290,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[MaskedLMOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -2427,14 +2427,14 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[CausalLMOutputWithCrossAttentions, Tuple[torch.FloatTensor]]:\n+    ) -> Union[CausalLMOutputWithCrossAttentions, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n@@ -2549,7 +2549,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[SequenceClassifierOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -2668,7 +2668,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[MultipleChoiceModelOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[MultipleChoiceModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -2776,7 +2776,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[TokenClassifierOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[TokenClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -2869,7 +2869,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[BigBirdForQuestionAnsweringModelOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[BigBirdForQuestionAnsweringModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         question_lengths (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*):\n             The lengths of the questions in the batch."
        },
        {
            "sha": "11dcb30f3d47e3ecf6d90d7a7305682d687e5828",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax\n import flax.linen as nn\n@@ -81,8 +81,8 @@ class FlaxBigBirdForPreTrainingOutput(ModelOutput):\n \n     prediction_logits: jnp.ndarray = None\n     seq_relationship_logits: jnp.ndarray = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n @flax.struct.dataclass\n@@ -113,8 +113,8 @@ class FlaxBigBirdForQuestionAnsweringModelOutput(ModelOutput):\n     start_logits: jnp.ndarray = None\n     end_logits: jnp.ndarray = None\n     pooled_output: jnp.ndarray = None\n-    hidden_states: Optional[Tuple[jnp.ndarray]] = None\n-    attentions: Optional[Tuple[jnp.ndarray]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray]] = None\n+    attentions: Optional[tuple[jnp.ndarray]] = None\n \n \n BIG_BIRD_START_DOCSTRING = r\"\"\"\n@@ -1647,7 +1647,7 @@ def enable_gradient_checkpointing(self):\n             gradient_checkpointing=True,\n         )\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         token_type_ids = jnp.zeros_like(input_ids)"
        },
        {
            "sha": "56680972a63e6360eaaefc023e57bd6beff71b9d",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import os\n import re\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -81,7 +81,7 @@ class BigBirdTokenizer(PreTrainedTokenizer):\n \n     vocab_files_names = VOCAB_FILES_NAMES\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    prefix_tokens: List[int] = []\n+    prefix_tokens: list[int] = []\n \n     def __init__(\n         self,\n@@ -93,7 +93,7 @@ def __init__(\n         sep_token=\"[SEP]\",\n         mask_token=\"[MASK]\",\n         cls_token=\"[CLS]\",\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n         bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token\n@@ -149,7 +149,7 @@ def __setstate__(self, d):\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(self.vocab_file)\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n         return self.sp_model.encode(text, out_type=str)\n \n@@ -184,7 +184,7 @@ def convert_tokens_to_string(self, tokens):\n \n     def _decode(\n         self,\n-        token_ids: List[int],\n+        token_ids: list[int],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n         spaces_between_special_tokens: bool = True,\n@@ -230,7 +230,7 @@ def _decode(\n         else:\n             return text\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n@@ -248,8 +248,8 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n         return (out_vocab_file,)\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A Big Bird sequence has the following format:\n@@ -273,8 +273,8 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method."
        },
        {
            "sha": "6148585a40b10385098c69714f650e1f42c22b4f",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import AddedToken\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -79,7 +79,7 @@ class BigBirdTokenizerFast(PreTrainedTokenizerFast):\n     vocab_files_names = VOCAB_FILES_NAMES\n     slow_tokenizer_class = BigBirdTokenizer\n     model_input_names = [\"input_ids\", \"attention_mask\"]\n-    prefix_tokens: List[int] = []\n+    prefix_tokens: list[int] = []\n \n     def __init__(\n         self,\n@@ -120,8 +120,8 @@ def __init__(\n         self.vocab_file = vocab_file\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. An BigBird sequence has the following format:\n@@ -145,8 +145,8 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n@@ -175,7 +175,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1]\n         return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError(\n                 \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \""
        },
        {
            "sha": "686277b8e84f29da55b3be36b92d32d3fca50aa9",
            "filename": "src/transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconvert_bigbird_pegasus_tf_to_pytorch.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n import argparse\n-from typing import Dict\n \n import tensorflow as tf\n import torch\n@@ -142,7 +141,7 @@ def convert_bigbird_pegasus(tf_weights: dict, config_update: dict) -> BigBirdPeg\n     return torch_model\n \n \n-def get_tf_weights_as_numpy(path) -> Dict:\n+def get_tf_weights_as_numpy(path) -> dict:\n     init_vars = tf.train.list_variables(path)\n     tf_weights = {}\n     ignore_name = [\"global_step\"]"
        },
        {
            "sha": "bc72d16bf5475960da04c622d32c8b4896ea70ca",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -1261,7 +1261,7 @@ def forward(\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -2120,7 +2120,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2410,16 +2410,16 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqModelOutput]:\n+    ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2572,8 +2572,8 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2582,7 +2582,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqLMOutput]:\n+    ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2730,7 +2730,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2739,7 +2739,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n+    ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -2863,7 +2863,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2873,7 +2873,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n+    ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Provide for translation and summarization training. By default, the model will create this tensor by\n@@ -3015,15 +3015,15 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n             Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:"
        },
        {
            "sha": "a1fba008841f8bc36b34f650bcdb2d239f616260",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -21,7 +21,7 @@\n \n import math\n from functools import partial\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -176,7 +176,7 @@ def forward(\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -283,7 +283,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -538,15 +538,15 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -737,7 +737,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -746,7 +746,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n@@ -828,7 +828,7 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -837,7 +837,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -920,7 +920,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -929,7 +929,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "d639f44ffec540f1eb04a3427854d979914d4315",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from functools import partial\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -110,7 +110,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -365,15 +365,15 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -564,7 +564,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -573,7 +573,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n@@ -655,7 +655,7 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -664,7 +664,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -747,7 +747,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -756,7 +756,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n+    ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "f84403ca7ddc65586528da07e4efc9f8621a0e19",
            "filename": "src/transformers/models/biogpt/tokenization_biogpt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Ftokenization_biogpt.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import json\n import os\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n@@ -231,8 +231,8 @@ def convert_tokens_to_string(self, tokens):\n         return text\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A BioGPT sequence has the following format:\n@@ -255,8 +255,8 @@ def build_inputs_with_special_tokens(\n         return sep + token_ids_0 + sep + token_ids_1\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n@@ -281,7 +281,7 @@ def get_special_tokens_mask(\n             return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1))\n         return [1] + ([0] * len(token_ids_0))\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "2b1f24fa0688fe8a62748e8fac58cf62a2b176d0",
            "filename": "src/transformers/models/bit/configuration_bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fconfiguration_bit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -37,9 +37,9 @@ class BitConfig(BackboneConfigMixin, PretrainedConfig):\n             The number of input channels.\n         embedding_size (`int`, *optional*, defaults to 64):\n             Dimensionality (hidden size) for the embedding layer.\n-        hidden_sizes (`List[int]`, *optional*, defaults to `[256, 512, 1024, 2048]`):\n+        hidden_sizes (`list[int]`, *optional*, defaults to `[256, 512, 1024, 2048]`):\n             Dimensionality (hidden size) at each stage.\n-        depths (`List[int]`, *optional*, defaults to `[3, 4, 6, 3]`):\n+        depths (`list[int]`, *optional*, defaults to `[3, 4, 6, 3]`):\n             Depth (number of layers) for each stage.\n         layer_type (`str`, *optional*, defaults to `\"preactivation\"`):\n             The layer to use, it can be either `\"preactivation\"` or `\"bottleneck\"`.\n@@ -58,12 +58,12 @@ class BitConfig(BackboneConfigMixin, PretrainedConfig):\n             The output stride of the model.\n         width_factor (`int`, *optional*, defaults to 1):\n             The width factor for the model.\n-        out_features (`List[str]`, *optional*):\n+        out_features (`list[str]`, *optional*):\n             If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n             (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n             corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n             same order as defined in the `stage_names` attribute.\n-        out_indices (`List[int]`, *optional*):\n+        out_indices (`list[int]`, *optional*):\n             If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n             many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n             If unset and `out_features` is unset, will default to the last stage. Must be in the"
        },
        {
            "sha": "0bcdb7075a23c242e9a6a9a522e37d7d8999f916",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for BiT.\"\"\"\n \n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -56,7 +56,7 @@ class BitImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n             `do_resize` in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n             Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n             the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n             method.\n@@ -65,7 +65,7 @@ class BitImageProcessor(BaseImageProcessor):\n         do_center_crop (`bool`, *optional*, defaults to `True`):\n             Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n             `preprocess` method.\n-        crop_size (`Dict[str, int]` *optional*, defaults to 224):\n+        crop_size (`dict[str, int]` *optional*, defaults to 224):\n             Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n             method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n@@ -76,10 +76,10 @@ class BitImageProcessor(BaseImageProcessor):\n             method.\n         do_normalize:\n             Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n@@ -92,15 +92,15 @@ class BitImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -126,7 +126,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -139,7 +139,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 Resampling filter to use when resiizing the image.\n@@ -177,15 +177,15 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -200,25 +200,25 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n                 the longest edge resized to keep the input aspect ratio.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n                 has an effect if `do_resize` is set to `True`.\n             do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                 Whether to center crop the image.\n-            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                 Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n                 Whether to rescale the image.\n             rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                 `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):"
        },
        {
            "sha": "d95e3537819dabbb0c93b5e8559a38c5b17d1e5a",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import collections\n import math\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import numpy as np\n import torch\n@@ -40,7 +40,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def get_padding_value(padding=None, kernel_size=7, stride=1, dilation=1) -> Tuple[Tuple, bool]:\n+def get_padding_value(padding=None, kernel_size=7, stride=1, dilation=1) -> tuple[tuple, bool]:\n     r\"\"\"\n     Utility function to get the tuple padding value given the kernel_size and padding.\n "
        },
        {
            "sha": "f526802bfca955a839fcce961e0a44a26600856d",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n \n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -181,12 +181,12 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -244,9 +244,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n "
        },
        {
            "sha": "952d23fc4c98661a2c48c5ff01e492571fb938ac",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n \"\"\"PyTorch BitNet model.\"\"\"\n \n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import torch\n \n@@ -61,12 +61,12 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n "
        },
        {
            "sha": "0d699d01b59c1e258205d801b0cd2a4e72c2497f",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import math\n import os\n import warnings\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -197,7 +197,7 @@ def forward(\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -1209,16 +1209,16 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[Union[Tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1386,8 +1386,8 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[Union[Tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1396,7 +1396,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1577,15 +1577,15 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n             Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:"
        },
        {
            "sha": "8b147211881b778d60236615ecc240812b2865b1",
            "filename": "src/transformers/models/blenderbot/modeling_flax_blenderbot.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import random\n from functools import partial\n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax.linen as nn\n import jax\n@@ -189,7 +189,7 @@\n         decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n             range `[0, config.max_position_embeddings - 1]`.\n-        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n+        past_key_values (`dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n             Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n             auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n         output_attentions (`bool`, *optional*):\n@@ -297,7 +297,7 @@ def __call__(\n         attention_mask: Optional[jnp.ndarray] = None,\n         init_cache: bool = False,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -418,7 +418,7 @@ def __call__(\n         attention_mask: jnp.ndarray,\n         output_attentions: bool = True,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n         hidden_states, attn_weights = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask)\n@@ -543,7 +543,7 @@ def __call__(\n         init_cache: bool = False,\n         output_attentions: bool = True,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n@@ -883,7 +883,7 @@ class FlaxBlenderbotPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: BlenderbotConfig,\n-        input_shape: Tuple[int] = (1, 1),\n+        input_shape: tuple[int] = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -892,7 +892,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         # make sure initialization pass will work for FlaxBlenderbotForSequenceClassificationModule"
        },
        {
            "sha": "86e576c982805f57e78c14ebd2cb3c7b1b535fe3",
            "filename": "src/transformers/models/blenderbot/modeling_tf_blenderbot.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import random\n import warnings\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -176,11 +176,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         key_value_states: tf.Tensor | None = None,\n-        past_key_value: Tuple[Tuple[tf.Tensor]] | None = None,\n+        past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Tuple[tf.Tensor, tf.Tensor | None]:\n+    ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -424,9 +424,9 @@ def call(\n         encoder_attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Tuple[tf.Tensor] | None = None,\n+        past_key_value: tuple[tf.Tensor] | None = None,\n         training: Optional[bool] = False,\n-    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n+    ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape *(batch, seq_len, embed_dim)*\n@@ -651,7 +651,7 @@ class TFBlenderbotPreTrainedModel(TFPreTrainedModel):\n         encoder_outputs (`tf.FloatTensor`, *optional*):\n             hidden states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n             of shape `(batch_size, sequence_length, hidden_size)` is a sequence of\n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -943,7 +943,7 @@ def call(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+            past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n                 Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n                 decoding.\n \n@@ -1128,7 +1128,7 @@ def call(\n         head_mask=None,\n         decoder_head_mask=None,\n         cross_attn_head_mask=None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n         past_key_values=None,\n         inputs_embeds=None,\n         decoder_inputs_embeds=None,\n@@ -1262,8 +1262,8 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: List[tf.Tensor] | None = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1272,7 +1272,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n         **kwargs,\n-    ) -> Union[Tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n+    ) -> Union[tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1414,8 +1414,8 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: List[tf.Tensor] | None = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1424,7 +1424,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[Tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n+    ) -> Union[tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n         r\"\"\"\n         labels (`tf.tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "02be9fd74ae64218c1b0d192fbd34a2190b5a81e",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import json\n import os\n from functools import lru_cache\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import regex as re\n \n@@ -302,7 +302,7 @@ def convert_tokens_to_string(self, tokens):\n         return text\n \n     # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.save_vocabulary with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n@@ -333,22 +333,22 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n \n     # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask with Roberta->Blenderbot, RoBERTa->Blenderbot\n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -361,20 +361,20 @@ def get_special_tokens_mask(\n \n     # Copied from transformers.models.roberta.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences with Roberta->Blenderbot, RoBERTa->Blenderbot\n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. Blenderbot does not\n         make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]\n@@ -390,19 +390,19 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n             text = \" \" + text\n         return (text, kwargs)\n \n-    def build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None):\n+    def build_inputs_with_special_tokens(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None):\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A Blenderbot sequence has the following format:\n         - single sequence: ` X </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Will be ignored\n         Returns:\n-            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         return token_ids_0 + [self.eos_token_id]\n "
        },
        {
            "sha": "0b84200e02d5c7f89141686c4e3c63c8d7fefc14",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Fast Tokenization class for Blenderbot.\"\"\"\n \n import json\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from tokenizers import processors\n \n@@ -236,26 +236,26 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n         return super()._encode_plus(*args, **kwargs)\n \n     # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.save_vocabulary with Roberta->Blenderbot, RoBERTa->Blenderbot\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n \n     # Copied from transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast.create_token_type_ids_from_sequences with Roberta->Blenderbot, RoBERTa->Blenderbot\n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. Blenderbot does not\n         make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]\n@@ -264,19 +264,19 @@ def create_token_type_ids_from_sequences(\n             return len(cls + token_ids_0 + sep) * [0]\n         return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n \n-    def build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None):\n+    def build_inputs_with_special_tokens(self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None):\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A Blenderbot sequence has the following format:\n         - single sequence: ` X </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Will be ignored\n         Returns:\n-            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         return token_ids_0 + [self.eos_token_id]\n "
        },
        {
            "sha": "99666356d09181af8dba98560a6cfd580dd74ed4",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import copy\n import math\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -181,7 +181,7 @@ def forward(\n         # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n         # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -280,7 +280,7 @@ def forward(\n         attention_mask: torch.FloatTensor,\n         layer_head_mask: torch.FloatTensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -370,7 +370,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -1176,16 +1176,16 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[Union[Tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1338,8 +1338,8 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        encoder_outputs: Optional[Union[Tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1348,7 +1348,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n+    ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n@@ -1529,15 +1529,15 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n             Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:"
        },
        {
            "sha": "ac30320bbdb4535d04b36b19cd97cea97a24d673",
            "filename": "src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import math\n import random\n from functools import partial\n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import flax.linen as nn\n import jax\n@@ -200,7 +200,7 @@\n         decoder_position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the\n             range `[0, config.max_position_embeddings - 1]`.\n-        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n+        past_key_values (`dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n             Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n             auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n         output_attentions (`bool`, *optional*):\n@@ -308,7 +308,7 @@ def __call__(\n         attention_mask: Optional[jnp.ndarray] = None,\n         init_cache: bool = False,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -429,7 +429,7 @@ def __call__(\n         attention_mask: jnp.ndarray,\n         output_attentions: bool = True,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         residual = hidden_states\n         hidden_states, attn_weights = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask)\n \n@@ -555,7 +555,7 @@ def __call__(\n         init_cache: bool = False,\n         output_attentions: bool = True,\n         deterministic: bool = True,\n-    ) -> Tuple[jnp.ndarray]:\n+    ) -> tuple[jnp.ndarray]:\n         residual = hidden_states\n \n         # Self Attention\n@@ -880,7 +880,7 @@ class FlaxBlenderbotSmallPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: BlenderbotSmallConfig,\n-        input_shape: Tuple[int] = (1, 1),\n+        input_shape: tuple[int] = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -889,7 +889,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         # make sure initialization pass will work for FlaxBlenderbotSmallForSequenceClassificationModule"
        },
        {
            "sha": "459ef438c53ce9e29e29f05822eb00409a3081b8",
            "filename": "src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n from __future__ import annotations\n \n import random\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -176,11 +176,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         key_value_states: tf.Tensor | None = None,\n-        past_key_value: Tuple[Tuple[tf.Tensor]] | None = None,\n+        past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Tuple[tf.Tensor, tf.Tensor | None]:\n+    ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         # if key_value_states are provided this layer is used as a cross-attention layer\n@@ -424,9 +424,9 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_value: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         training: Optional[bool] = False,\n-    ) -> Tuple[tf.Tensor, tf.Tensor, Tuple[Tuple[tf.Tensor]]]:\n+    ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -655,7 +655,7 @@ class TFBlenderbotSmallPreTrainedModel(TFPreTrainedModel):\n         encoder_outputs (`tf.FloatTensor`, *optional*):\n             hidden states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n             of shape `(batch_size, sequence_length, hidden_size)` is a sequence of\n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -947,7 +947,7 @@ def call(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+            past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n                 Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n                 decoding.\n \n@@ -1129,7 +1129,7 @@ def call(\n         head_mask=None,\n         decoder_head_mask=None,\n         cross_attn_head_mask=None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n         past_key_values=None,\n         inputs_embeds=None,\n         decoder_inputs_embeds=None,\n@@ -1247,8 +1247,8 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[Tuple, TFBaseModelOutput]] = None,\n-        past_key_values: List[tf.Tensor] | None = None,\n+        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1257,7 +1257,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n         **kwargs,\n-    ) -> Union[Tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n+    ) -> Union[tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1384,7 +1384,7 @@ def call(\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n         encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: List[tf.Tensor] | None = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n         use_cache: Optional[bool] = None,\n@@ -1393,7 +1393,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[Tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n+    ) -> Union[tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n         r\"\"\"\n         labels (`tf.tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "adb54025ce23cc2b73eb0179fcc8f57adc95555c",
            "filename": "src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import json\n import os\n-from typing import Dict, List, Optional, Tuple\n+from typing import Optional\n \n import regex as re\n \n@@ -102,7 +102,7 @@ def __init__(\n     def vocab_size(self) -> int:\n         return len(self.encoder)\n \n-    def get_vocab(self) -> Dict:\n+    def get_vocab(self) -> dict:\n         return dict(self.encoder, **self.added_tokens_encoder)\n \n     def bpe(self, token: str) -> str:\n@@ -165,7 +165,7 @@ def bpe(self, token: str) -> str:\n             words.append(word)\n         return \" \".join(words)\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Split a string into tokens using BPE.\"\"\"\n         split_tokens = []\n \n@@ -184,12 +184,12 @@ def _convert_id_to_token(self, index: int) -> str:\n         \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n         return self.decoder.get(index, self.unk_token)\n \n-    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n+    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n         \"\"\"Converts a sequence of tokens in a single string.\"\"\"\n         out_string = \" \".join(tokens).replace(\"@@ \", \"\").strip()\n         return out_string\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "7d905dbbc5b29013823fff3d2232a50c353401ca",
            "filename": "src/transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Ftokenization_blenderbot_small_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Fast tokenization class for BlenderbotSmall.\"\"\"\n \n-from typing import List, Optional\n+from typing import Optional\n \n from tokenizers import ByteLevelBPETokenizer\n \n@@ -77,20 +77,20 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n         return output + [self.eos_token_id] + token_ids_1 + [self.eos_token_id]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. BlenderbotSmall\n         does not make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]"
        },
        {
            "sha": "4c3ec00d26fdc11ae0e8992d83dd2305f8f19d1b",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for BLIP.\"\"\"\n \n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -66,11 +66,11 @@ class BlipImageProcessor(BaseImageProcessor):\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n             overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n@@ -83,13 +83,13 @@ class BlipImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -111,7 +111,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -123,7 +123,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n@@ -161,13 +161,13 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n@@ -182,7 +182,7 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Controls the size of the image after `resize`. The shortest edge of the image is resized to\n                 `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n                 is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n@@ -195,9 +195,9 @@ def preprocess(\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to normalize the image by if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB."
        },
        {
            "sha": "3967f8bce478d9d03884ce6e21d66bf2c428fec8",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -76,12 +76,12 @@ class BlipForConditionalGenerationModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n+    loss: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n     @property\n     def decoder_logits(self):\n@@ -122,8 +122,8 @@ class BlipTextVisionModelOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -163,10 +163,10 @@ class BlipImageTextMatchingModelOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     vision_pooler_output: Optional[torch.FloatTensor] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    question_embeds: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    question_embeds: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -199,7 +199,7 @@ class BlipOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -350,7 +350,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -420,7 +420,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -515,7 +515,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -594,7 +594,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -830,7 +830,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BlipOutput]:\n+    ) -> Union[tuple, BlipOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss.\n@@ -958,7 +958,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BlipForConditionalGenerationModelOutput]:\n+    ) -> Union[tuple, BlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n         Examples:\n \n@@ -1140,7 +1140,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BlipTextVisionModelOutput]:\n+    ) -> Union[tuple, BlipTextVisionModelOutput]:\n         r\"\"\"\n         Examples:\n \n@@ -1381,7 +1381,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BlipTextVisionModelOutput]:\n+    ) -> Union[tuple, BlipTextVisionModelOutput]:\n         r\"\"\"\n         use_itm_head (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the image-text matching head."
        },
        {
            "sha": "151caef0b102d91b80dd7d9a4eb587a487cf26a4",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \n \n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -148,9 +148,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -270,9 +270,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -338,9 +338,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -396,12 +396,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning(\n@@ -592,15 +592,15 @@ class PreTrainedModel\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n     def get_extended_attention_mask(\n-        self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool\n+        self, attention_mask: Tensor, input_shape: tuple[int], device: device, is_decoder: bool\n     ) -> Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n \n         Arguments:\n             attention_mask (`torch.Tensor`):\n                 Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n-            input_shape (`Tuple[int]`):\n+            input_shape (`tuple[int]`):\n                 The shape of the input to the model.\n             device (`torch.device`):\n                 The device of the input to the model.\n@@ -665,13 +665,13 @@ def forward(\n         encoder_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         is_decoder: Optional[bool] = False,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -827,15 +827,15 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.Tensor]] = None,\n+        past_key_values: Optional[list[torch.Tensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         return_logits: Optional[bool] = False,\n         is_decoder: Optional[bool] = True,\n         reduction: Optional[str] = \"mean\",\n-    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states (`torch.FloatTensor`, *optional*): Sequence of\n             hidden-states at the output of the last layer of the encoder. Used in the cross-attention if the model is"
        },
        {
            "sha": "a0bf43892d0e406081102ace5090b75bfe7d70a8",
            "filename": "src/transformers/models/blip/modeling_tf_blip.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import tensorflow as tf\n \n@@ -93,12 +93,12 @@ class TFBlipForConditionalGenerationModelOutput(ModelOutput):\n             heads.`\n     \"\"\"\n \n-    loss: Tuple[tf.Tensor] | None = None\n-    logits: Tuple[tf.Tensor] | None = None\n+    loss: tuple[tf.Tensor] | None = None\n+    logits: tuple[tf.Tensor] | None = None\n     image_embeds: tf.Tensor | None = None\n     last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor, ...] | None = None\n-    attentions: Tuple[tf.Tensor, ...] | None = None\n+    hidden_states: tuple[tf.Tensor, ...] | None = None\n+    attentions: tuple[tf.Tensor, ...] | None = None\n \n     @property\n     def decoder_logits(self):\n@@ -139,8 +139,8 @@ class TFBlipTextVisionModelOutput(ModelOutput):\n     loss: tf.Tensor | None = None\n     image_embeds: tf.Tensor | None = None\n     last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor, ...] | None = None\n-    attentions: Tuple[tf.Tensor, ...] | None = None\n+    hidden_states: tuple[tf.Tensor, ...] | None = None\n+    attentions: tuple[tf.Tensor, ...] | None = None\n \n \n @dataclass\n@@ -180,10 +180,10 @@ class TFBlipImageTextMatchingModelOutput(ModelOutput):\n     loss: tf.Tensor | None = None\n     image_embeds: tf.Tensor | None = None\n     last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Tuple[tf.Tensor, ...] | None = None\n+    hidden_states: tuple[tf.Tensor, ...] | None = None\n     vision_pooler_output: tf.Tensor | None = None\n-    attentions: Tuple[tf.Tensor, ...] | None = None\n-    question_embeds: Tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor, ...] | None = None\n+    question_embeds: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -216,7 +216,7 @@ class TFBlipOutput(ModelOutput):\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -369,7 +369,7 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         output_attentions: Optional[bool] = False,\n         training: Optional[bool] = None,\n-    ) -> Tuple[tf.Tensor, tf.Tensor | None, Tuple[tf.Tensor] | None]:\n+    ) -> tuple[tf.Tensor, tf.Tensor | None, tuple[tf.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = shape_list(hidden_states)\n@@ -472,7 +472,7 @@ def call(\n         attention_mask: tf.Tensor,\n         output_attentions: Optional[bool] = False,\n         training: Optional[bool] = None,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -628,7 +628,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBaseModelOutput]:\n+    ) -> Union[tuple, TFBaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -727,7 +727,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBaseModelOutputWithPooling]:\n+    ) -> Union[tuple, TFBaseModelOutputWithPooling]:\n         r\"\"\"\n         Returns:\n \n@@ -866,7 +866,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBlipOutput]:\n+    ) -> Union[tuple, TFBlipOutput]:\n         # Use BLIP model's config for some fields (if specified) instead of those of vision & text components.\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -959,7 +959,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBlipOutput]:\n+    ) -> Union[tuple, TFBlipOutput]:\n         r\"\"\"\n         Returns:\n \n@@ -1121,7 +1121,7 @@ def call(\n         labels: tf.Tensor | None = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBlipForConditionalGenerationModelOutput]:\n+    ) -> Union[tuple, TFBlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n         Returns:\n \n@@ -1338,7 +1338,7 @@ def call(\n         labels: tf.Tensor | None = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBlipTextVisionModelOutput]:\n+    ) -> Union[tuple, TFBlipTextVisionModelOutput]:\n         r\"\"\"\n         Returns:\n \n@@ -1592,7 +1592,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = None,\n-    ) -> Union[Tuple, TFBlipImageTextMatchingModelOutput]:\n+    ) -> Union[tuple, TFBlipImageTextMatchingModelOutput]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "e85d3e627ce97e26e09ebd3c4a51267865f42309",
            "filename": "src/transformers/models/blip/modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n from __future__ import annotations\n \n import math\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import tensorflow as tf\n \n@@ -337,7 +337,7 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         encoder_hidden_states: tf.Tensor | None = None,\n         encoder_attention_mask: tf.Tensor | None = None,\n-        past_key_value: Tuple[Tuple[tf.Tensor]] | None = None,\n+        past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         output_attentions: Optional[bool] = False,\n         training: Optional[bool] = None,\n     ):\n@@ -751,15 +751,15 @@ def set_input_embeddings(self, value):\n \n     @tf.function\n     def get_extended_attention_mask(\n-        self, attention_mask: tf.Tensor, input_shape: Tuple[int], is_decoder: bool\n+        self, attention_mask: tf.Tensor, input_shape: tuple[int], is_decoder: bool\n     ) -> tf.Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n \n         Arguments:\n             attention_mask (`tf.Tensor`):\n                 Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n-            input_shape (`Tuple[int]`):\n+            input_shape (`tuple[int]`):\n                 The shape of the input to the model.\n             is_decoder (`bool`):\n                 Whether the model is used as a decoder.\n@@ -826,14 +826,14 @@ def call(\n         encoder_embeds: tf.Tensor | None = None,\n         encoder_hidden_states: tf.Tensor | None = None,\n         encoder_attention_mask: tf.Tensor | None = None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] | None = None,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n         use_cache: bool | None = None,\n         output_attentions: bool | None = None,\n         output_hidden_states: bool | None = None,\n         return_dict: bool | None = None,\n         is_decoder: bool = False,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor] | TFBaseModelOutputWithPoolingAndCrossAttentions:\n+    ) -> tuple[tf.Tensor] | TFBaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if"
        },
        {
            "sha": "2dfe4d1f887d6f5ec4fde1f7370d8bd96ae287f0",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for Blip.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -66,7 +66,7 @@ def __init__(self, image_processor, tokenizer, **kwargs):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[BlipProcessorKwargs],\n@@ -80,7 +80,7 @@ def __call__(\n             images (`ImageInput`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences)."
        },
        {
            "sha": "970ee7ad0c44328b47acc6833844481408780579",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Callable, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -62,13 +62,13 @@ class Blip2ForConditionalGenerationModelOutput(ModelOutput):\n             Outputs of the language model.\n     \"\"\"\n \n-    loss: Optional[Tuple[torch.FloatTensor]] = None\n-    logits: Optional[Tuple[torch.FloatTensor]] = None\n+    loss: Optional[tuple[torch.FloatTensor]] = None\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n     vision_outputs: Optional[torch.FloatTensor] = None\n-    qformer_outputs: Optional[Tuple[torch.FloatTensor]] = None\n-    language_model_outputs: Optional[Tuple[torch.FloatTensor]] = None\n+    qformer_outputs: Optional[tuple[torch.FloatTensor]] = None\n+    language_model_outputs: Optional[tuple[torch.FloatTensor]] = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k]\n             if k not in [\"vision_outputs\", \"qformer_outputs\", \"language_model_outputs\"]\n@@ -107,7 +107,7 @@ class Blip2ImageTextMatchingModelOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -140,8 +140,8 @@ class Blip2TextModelOutput(ModelOutput):\n \n     text_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -170,8 +170,8 @@ class Blip2VisionModelOutput(ModelOutput):\n \n     image_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipVisionEmbeddings with Blip->Blip2\n@@ -316,7 +316,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -388,7 +388,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -494,7 +494,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -575,7 +575,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -793,9 +793,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n@@ -1116,7 +1116,7 @@ class PreTrainedModel\n     def get_extended_attention_mask(\n         self,\n         attention_mask: torch.Tensor,\n-        input_shape: Tuple[int],\n+        input_shape: tuple[int],\n         device: torch.device,\n         has_query: bool = False,\n     ) -> torch.Tensor:\n@@ -1126,7 +1126,7 @@ def get_extended_attention_mask(\n         Arguments:\n             attention_mask (`torch.Tensor`):\n                 Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n-            input_shape (`Tuple[int]`):\n+            input_shape (`tuple[int]`):\n                 The shape of the input to the model.\n             device (`torch.device`):\n                 The device of the input to the model.\n@@ -1167,12 +1167,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n             Hidden states to be used in the attention computation. If cross-attention,\n@@ -1531,7 +1531,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n+    ) -> Union[tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n@@ -1697,7 +1697,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Blip2TextModelOutput]:\n+    ) -> Union[tuple, Blip2TextModelOutput]:\n         r\"\"\"\n         Examples:\n \n@@ -1785,7 +1785,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Blip2VisionModelOutput]:\n+    ) -> Union[tuple, Blip2VisionModelOutput]:\n         r\"\"\"\n         Examples:\n \n@@ -2004,7 +2004,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         use_cache: Optional[bool] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n+    ) -> Union[tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n@@ -2308,7 +2308,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Blip2ImageTextMatchingModelOutput]:\n+    ) -> Union[tuple, Blip2ImageTextMatchingModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be"
        },
        {
            "sha": "810311d492a0b7d2376388607639bbbe5e4cf172",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for BLIP-2.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -80,7 +80,7 @@ def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[Blip2ProcessorKwargs],\n@@ -94,7 +94,7 @@ def __call__(\n             images (`ImageInput`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences)."
        },
        {
            "sha": "74748c11304111955f5a5ef038a13256d02d1837",
            "filename": "src/transformers/models/bloom/configuration_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n from collections import OrderedDict\n from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, List, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n \n from packaging import version\n \n@@ -149,7 +149,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: Optional[List[PatchingSpec]] = None,\n+        patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)"
        },
        {
            "sha": "66dfc0c1fa8cb5ca2b4c8dd842d9f5966453d4ec",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n import warnings\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -211,7 +211,7 @@ def __init__(self, config: BloomConfig, layer_idx: Optional[int] = None):\n         self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n \n-    def _reshape(self, fused_qkv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+    def _reshape(self, fused_qkv: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Split the last dimension into (num_heads, head_dim) and reshapes to (bs, heads, len, dim) shape\n         without making any copies, results share same memory storage as `fused_qkv`\n@@ -506,7 +506,7 @@ def set_input_embeddings(self, new_embeddings: torch.Tensor):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n@@ -516,7 +516,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **deprecated_arguments,\n-    ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n@@ -873,7 +873,7 @@ def prepare_inputs_for_generation(\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -884,7 +884,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **deprecated_arguments,\n-    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n@@ -957,8 +957,8 @@ def forward(\n         )\n \n     def _reorder_cache(\n-        self, past: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n-    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:\n+        self, past: tuple[tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n+    ) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]:\n         \"\"\"\n         This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n         [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n@@ -1008,7 +1008,7 @@ def __init__(self, config: BloomConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1018,7 +1018,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **deprecated_arguments,\n-    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n+    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n@@ -1144,7 +1144,7 @@ def __init__(self, config: BloomConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n+        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor, torch.Tensor], ...]]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n@@ -1154,7 +1154,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **deprecated_arguments,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n@@ -1245,7 +1245,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`"
        },
        {
            "sha": "c7bb1cc9c9a5b098f9c03d6313818e744e61951e",
            "filename": "src/transformers/models/bloom/modeling_flax_bloom.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from functools import partial\n-from typing import Optional, Tuple\n+from typing import Optional\n \n import flax.linen as nn\n import jax\n@@ -94,7 +94,7 @@\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n+        past_key_values (`dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n             Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n             auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n         output_attentions (`bool`, *optional*):\n@@ -412,7 +412,7 @@ class FlaxBloomPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: BloomConfig,\n-        input_shape: Tuple = (1, 1),\n+        input_shape: tuple = (1, 1),\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -421,7 +421,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensors\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         attention_mask = jnp.ones_like(input_ids)"
        },
        {
            "sha": "b7a9f7449a4e9a5c2306ba4853ff5c9d99c00a20",
            "filename": "src/transformers/models/bloom/tokenization_bloom_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Ftokenization_bloom_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for Bloom.\"\"\"\n \n import pickle\n-from typing import Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils_base import BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -144,7 +144,7 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n \n         return super()._encode_plus(*args, **kwargs)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n "
        },
        {
            "sha": "98ac0434d361048f88eb6a0826a35ca84f99fd89",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Image processor class for BridgeTower.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n \n@@ -45,7 +45,7 @@\n \n \n # Copied from transformers.models.vilt.image_processing_vilt.max_across_indices\n-def max_across_indices(values: Iterable[Any]) -> List[Any]:\n+def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n     Return the maximum value across all indices of an iterable of values.\n     \"\"\"\n@@ -54,15 +54,15 @@ def max_across_indices(values: Iterable[Any]) -> List[Any]:\n \n # Copied from transformers.models.vilt.image_processing_vilt.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: Tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n \n     Args:\n         image (`np.ndarray`):\n             Image to make the pixel mask for.\n-        output_size (`Tuple[int, int]`):\n+        output_size (`tuple[int, int]`):\n             Output size of the mask.\n     \"\"\"\n     input_height, input_width = get_image_size(image, channel_dim=input_data_format)\n@@ -73,8 +73,8 @@ def make_pixel_mask(\n \n # Copied from transformers.models.vilt.image_processing_vilt.get_max_height_width\n def get_max_height_width(\n-    images: List[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n-) -> List[int]:\n+    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n     \"\"\"\n@@ -97,7 +97,7 @@ def get_resize_output_image_size(\n     longer: int = 1333,\n     size_divisor: int = 32,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-) -> Tuple[int, int]:\n+) -> tuple[int, int]:\n     input_height, input_width = get_image_size(input_image, input_data_format)\n     min_size, max_size = shorter, longer\n \n@@ -130,7 +130,7 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n             `do_resize` parameter in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{'shortest_edge': 288}`):\n+        size (`dict[str, int]` *optional*, defaults to `{'shortest_edge': 288}`):\n             Resize the shorter side of the input to `size[\"shortest_edge\"]`. The longer side will be limited to under\n             `int((1333 / 800) * size[\"shortest_edge\"])` while preserving the aspect ratio. Only has an effect if\n             `do_resize` is set to `True`. Can be overridden by the `size` parameter in the `preprocess` method.\n@@ -149,18 +149,18 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n             overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_center_crop (`bool`, *optional*, defaults to `True`):\n             Whether to center crop the image. Can be overridden by the `do_center_crop` parameter in the `preprocess`\n             method.\n-        crop_size (`Dict[str, int]`, *optional*):\n+        crop_size (`dict[str, int]`, *optional*):\n             Desired output size when applying center-cropping. Only has an effect if `do_center_crop` is set to `True`.\n             Can be overridden by the `crop_size` parameter in the `preprocess` method. If unset defaults to `size`,\n         do_pad (`bool`, *optional*, defaults to `True`):\n@@ -173,16 +173,16 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_center_crop: bool = True,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -210,7 +210,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         size_divisor: int = 32,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -227,7 +227,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Controls the size of the output image. Should be of the form `{\"shortest_edge\": int}`.\n             size_divisor (`int`, *optional*, defaults to 32):\n                 The image is resized to a size that is a multiple of this value.\n@@ -258,7 +258,7 @@ def resize(\n     def center_crop(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         **kwargs,\n@@ -270,7 +270,7 @@ def center_crop(\n         Args:\n             image (`np.ndarray`):\n                 Image to center crop.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image in the form `{\"height\": h, \"width\": w}`.\n             data_format (`str` or `ChannelDimension`, *optional*):\n                 The channel dimension format of the image. If not provided, it will be the same as the input image.\n@@ -291,7 +291,7 @@ def center_crop(\n     def _pad_image(\n         self,\n         image: np.ndarray,\n-        output_size: Tuple[int, int],\n+        output_size: tuple[int, int],\n         constant_values: Union[float, Iterable[float]] = 0,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -318,7 +318,7 @@ def _pad_image(\n     # Copied from transformers.models.vilt.image_processing_vilt.ViltImageProcessor.pad\n     def pad(\n         self,\n-        images: List[np.ndarray],\n+        images: list[np.ndarray],\n         constant_values: Union[float, Iterable[float]] = 0,\n         return_pixel_mask: bool = True,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n@@ -376,17 +376,17 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_pad: Optional[bool] = None,\n         do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -400,7 +400,7 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Controls the size of the image after `resize`. The shortest edge of the image is resized to\n                 `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n                 is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n@@ -415,17 +415,17 @@ def preprocess(\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to normalize the image by if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n             do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n                 Whether to pad the image to the (max_height, max_width) in the batch. If `True`, a pixel mask is also\n                 created and returned.\n             do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                 Whether to center crop the image. If the input size is smaller than `crop_size` along any edge, the\n                 image is padded with 0's and then center cropped.\n-            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                 Size of the image after center crop. If one edge the image is smaller than `crop_size`, it will be\n                 padded with zeros and then cropped\n             return_tensors (`str` or `TensorType`, *optional*):"
        },
        {
            "sha": "9e986f8a2d752b50731212a92ab50fc11df1bd92",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Fast Image processor class for BridgeTower.\"\"\"\n \n from collections.abc import Iterable\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Optional, Union\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -45,15 +45,15 @@\n \n def make_pixel_mask(\n     image: \"torch.Tensor\",\n-    output_size: Tuple[int, int],\n+    output_size: tuple[int, int],\n ) -> \"torch.Tensor\":\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n \n     Args:\n         image (`np.ndarray`):\n             Image to make the pixel mask for.\n-        output_size (`Tuple[int, int]`):\n+        output_size (`tuple[int, int]`):\n             Output size of the mask.\n     \"\"\"\n     input_height, input_width = image.shape[-2:]\n@@ -68,7 +68,7 @@ def get_resize_output_image_size(\n     shorter: int = 800,\n     longer: int = 1333,\n     size_divisor: int = 32,\n-) -> Tuple[int, int]:\n+) -> tuple[int, int]:\n     input_height, input_width = input_image.shape[-2:]\n     min_size, max_size = shorter, longer\n \n@@ -176,7 +176,7 @@ def resize(\n     def center_crop(\n         self,\n         image: \"torch.Tensor\",\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -186,7 +186,7 @@ def center_crop(\n         Args:\n             image (`torch.Tensor`):\n                 Image to center crop.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image in the form `{\"height\": h, \"width\": w}`.\n         \"\"\"\n         output_size = size.shortest_edge\n@@ -199,7 +199,7 @@ def center_crop(\n     def _pad_image(\n         self,\n         image: \"torch.Tensor\",\n-        output_size: Tuple[int, int],\n+        output_size: tuple[int, int],\n         constant_values: Union[float, Iterable[float]] = 0,\n     ) -> \"torch.Tensor\":\n         \"\"\""
        },
        {
            "sha": "10db0bbb62fb6e528c7a29c954f917a40d4a8823",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import math\n from collections import OrderedDict\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -71,8 +71,8 @@ class BridgeTowerModelOutput(ModelOutput):\n     text_features: Optional[torch.FloatTensor] = None\n     image_features: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -102,11 +102,11 @@ class BridgeTowerContrastiveOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    text_embeds: Optional[Tuple[torch.FloatTensor]] = None\n-    image_embeds: Optional[Tuple[torch.FloatTensor]] = None\n-    cross_embeds: Optional[Tuple[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    text_embeds: Optional[tuple[torch.FloatTensor]] = None\n+    image_embeds: Optional[tuple[torch.FloatTensor]] = None\n+    cross_embeds: Optional[tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n class BridgeTowerResidualAttention(nn.Module):\n@@ -448,9 +448,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -583,9 +583,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -684,9 +684,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -763,12 +763,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -1058,12 +1058,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1244,7 +1244,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[Tuple[torch.Tensor], BridgeTowerModelOutput]:\n+    ) -> Union[tuple[torch.Tensor], BridgeTowerModelOutput]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n@@ -1552,7 +1552,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-    ) -> Union[MaskedLMOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n@@ -1654,7 +1654,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-    ) -> Union[SequenceClassifierOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n@@ -1768,7 +1768,7 @@ def forward(\n         output_hidden_states: Optional[bool] = True,\n         return_dict: Optional[bool] = None,\n         return_loss: Optional[bool] = None,\n-    ) -> Union[BridgeTowerContrastiveOutput, Tuple[torch.FloatTensor]]:\n+    ) -> Union[BridgeTowerContrastiveOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n             Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation."
        },
        {
            "sha": "b388c3b22b2338f5645b27b61123ff2737da6097",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for BridgeTower.\n \"\"\"\n \n-from typing import List, Union\n+from typing import Union\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n@@ -67,7 +67,7 @@ def __init__(self, image_processor, tokenizer):\n     def __call__(\n         self,\n         images,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[BridgeTowerProcessorKwargs],"
        },
        {
            "sha": "94d3a9d985d9a89c89027d1da81ca906fd8a2845",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -66,8 +66,8 @@ class BrosSpadeOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     initial_token_logits: Optional[torch.FloatTensor] = None\n     subsequent_token_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n class BrosPositionalEmbedding1D(nn.Module):\n@@ -232,9 +232,9 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[torch.Tensor] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -380,9 +380,9 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states=hidden_states,\n             bbox_pos_emb=bbox_pos_emb,\n@@ -451,9 +451,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -532,12 +532,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -727,12 +727,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n@@ -893,7 +893,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n@@ -1018,7 +1018,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BrosSpadeOutput]:\n+    ) -> Union[tuple[torch.Tensor], BrosSpadeOutput]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n@@ -1159,7 +1159,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n             Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values"
        },
        {
            "sha": "42a638becdfa4bca4fb4550e8e2ada91abbab831",
            "filename": "src/transformers/models/bros/processing_bros.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for Bros.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n@@ -46,7 +46,7 @@ def __init__(self, tokenizer=None, **kwargs):\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n         truncation: Union[bool, str, TruncationStrategy] = None,"
        },
        {
            "sha": "2a9804db1014a963fb2054083f2db2782a41016d",
            "filename": "src/transformers/models/byt5/tokenization_byt5.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbyt5%2Ftokenization_byt5.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization class for model ByT5.\"\"\"\n \n import warnings\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n@@ -53,7 +53,7 @@ class ByT5Tokenizer(PreTrainedTokenizer):\n             indexed from the end of the vocabulary up to beginning (\"<extra_id_0>\" is the last token in the vocabulary\n             like in ByT5 preprocessing see\n             [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)).\n-        additional_special_tokens (`List[str]`, *optional*):\n+        additional_special_tokens (`list[str]`, *optional*):\n             Additional special tokens used by the tokenizer.\n     \"\"\"\n \n@@ -108,22 +108,22 @@ def get_vocab(self):\n         return vocab\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -135,7 +135,7 @@ def get_special_tokens_mask(\n             return ([0] * len(token_ids_0)) + [1]\n         return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n \n-    def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:\n+    def _add_eos_if_not_present(self, token_ids: list[int]) -> list[int]:\n         \"\"\"Do not add eos again if user already added it.\"\"\"\n         if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:\n             warnings.warn(\n@@ -147,20 +147,20 @@ def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:\n             return token_ids + [self.eos_token_id]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. ByT5 does not\n         make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         eos = [self.eos_token_id]\n \n@@ -169,8 +169,8 @@ def create_token_type_ids_from_sequences(\n         return len(token_ids_0 + eos + token_ids_1 + eos) * [0]\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A sequence has the following format:\n@@ -179,13 +179,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `A </s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         token_ids_0 = self._add_eos_if_not_present(token_ids_0)\n         if token_ids_1 is None:\n@@ -194,7 +194,7 @@ def build_inputs_with_special_tokens(\n             token_ids_1 = self._add_eos_if_not_present(token_ids_1)\n             return token_ids_0 + token_ids_1\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Take as input a string and return a list of strings (tokens) for words/sub-words\"\"\"\n         tokens = [chr(i) for i in text.encode(\"utf-8\")]\n         return tokens\n@@ -229,7 +229,7 @@ def convert_tokens_to_string(self, tokens):\n         return string\n \n     # ByT5Tokenizer has no vocab file\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         return ()\n \n "
        },
        {
            "sha": "1b4a52295f285245bcbeeeff606262b59bd1c470",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"PyTorch CamemBERT model.\"\"\"\n \n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -176,9 +176,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -286,9 +286,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n             # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n             logger.warning_once(\n@@ -429,9 +429,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -500,9 +500,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -579,12 +579,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -821,12 +821,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -995,7 +995,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n+    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -1080,7 +1080,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -1175,7 +1175,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1286,7 +1286,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -1365,7 +1365,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n@@ -1467,13 +1467,13 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:"
        },
        {
            "sha": "fae297889f0bcf53783eeb4a59c478411255ed02",
            "filename": "src/transformers/models/camembert/modeling_tf_camembert.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n \n import math\n import warnings\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -343,10 +343,10 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor,\n         encoder_attention_mask: tf.Tensor,\n-        past_key_value: Tuple[tf.Tensor],\n+        past_key_value: tuple[tf.Tensor],\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         batch_size = shape_list(hidden_states)[0]\n         mixed_query_layer = self.query(inputs=hidden_states)\n \n@@ -481,10 +481,10 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor,\n         encoder_attention_mask: tf.Tensor,\n-        past_key_value: Tuple[tf.Tensor],\n+        past_key_value: tuple[tf.Tensor],\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         self_outputs = self.self_attention(\n             hidden_states=input_tensor,\n             attention_mask=attention_mask,\n@@ -598,10 +598,10 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n-        past_key_value: Tuple[tf.Tensor] | None,\n+        past_key_value: tuple[tf.Tensor] | None,\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -694,13 +694,13 @@ def call(\n         head_mask: tf.Tensor,\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n-        past_key_values: Tuple[Tuple[tf.Tensor]] | None,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None,\n         use_cache: Optional[bool],\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -809,13 +809,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1008,13 +1008,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[Tuple, TFBaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple, TFBaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1026,7 +1026,7 @@ def call(\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -1169,7 +1169,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1299,7 +1299,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1395,7 +1395,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1487,7 +1487,7 @@ def call(\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1592,7 +1592,7 @@ def call(\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1706,14 +1706,14 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[Tuple[Tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, Tuple[tf.Tensor]]:\n+    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1725,7 +1725,7 @@ def call(\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n \n-        past_key_values (`Tuple[Tuple[tf.Tensor]]` of length `config.n_layers`)\n+        past_key_values (`tuple[tuple[tf.Tensor]]` of length `config.n_layers`)\n             contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n             If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all"
        },
        {
            "sha": "cd6e399f208df858d647dce23fe0ad74248d80b8",
            "filename": "src/transformers/models/camembert/tokenization_camembert.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -81,7 +81,7 @@ class CamembertTokenizer(PreTrainedTokenizer):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        additional_special_tokens (`List[str]`, *optional*, defaults to `['<s>NOTUSED', '</s>NOTUSED', '<unk>NOTUSED']`):\n+        additional_special_tokens (`list[str]`, *optional*, defaults to `['<s>NOTUSED', '</s>NOTUSED', '<unk>NOTUSED']`):\n             Additional special tokens used by the tokenizer.\n         sp_model_kwargs (`dict`, *optional*):\n             Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n@@ -118,7 +118,7 @@ def __init__(\n         pad_token=\"<pad>\",\n         mask_token=\"<mask>\",\n         additional_special_tokens=[\"<s>NOTUSED\", \"</s>NOTUSED\", \"<unk>NOTUSED\"],\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n     ) -> None:\n         # Mask token behave like a normal word, i.e. include the space before it\n@@ -176,7 +176,7 @@ def get_vocab(self):\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         return self.sp_model.encode(text, out_type=str)\n \n     def _convert_token_to_id(self, token):\n@@ -226,7 +226,7 @@ def __setstate__(self, d):\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(self.vocab_file)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n@@ -244,8 +244,8 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n         return (out_vocab_file,)\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. An CamemBERT sequence has the following format:\n@@ -254,13 +254,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n \n         if token_ids_1 is None:\n@@ -270,22 +270,22 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -297,20 +297,20 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like\n         RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]"
        },
        {
            "sha": "423058ed959aeb3e7122fb3730a6d0f1f57b5982",
            "filename": "src/transformers/models/camembert/tokenization_camembert_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import os\n from shutil import copyfile\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from ...tokenization_utils import AddedToken\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -85,7 +85,7 @@ class CamembertTokenizerFast(PreTrainedTokenizerFast):\n         mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n             The token used for masking values. This is the token used when training this model with masked language\n             modeling. This is the token which the model will try to predict.\n-        additional_special_tokens (`List[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n+        additional_special_tokens (`list[str]`, *optional*, defaults to `[\"<s>NOTUSED\", \"</s>NOTUSED\"]`):\n             Additional special tokens used by the tokenizer.\n     \"\"\"\n \n@@ -126,8 +126,8 @@ def __init__(\n         self.vocab_file = vocab_file\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. An CamemBERT sequence has the following format:\n@@ -136,13 +136,13 @@ def build_inputs_with_special_tokens(\n         - pair of sequences: `<s> A </s></s> B </s>`\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n \n         if token_ids_1 is None:\n@@ -152,20 +152,20 @@ def build_inputs_with_special_tokens(\n         return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like\n         RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         sep = [self.sep_token_id]\n         cls = [self.cls_token_id]\n@@ -174,7 +174,7 @@ def create_token_type_ids_from_sequences(\n             return len(cls + token_ids_0 + sep) * [0]\n         return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError(\n                 \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \""
        },
        {
            "sha": "d55c600d05de2e0464f9324abe43149ab210d077",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import math\n import os\n from dataclasses import dataclass\n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -78,8 +78,8 @@ class CanineModelOutputWithPooling(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n def load_tf_weights_in_canine(model, config, tf_checkpoint_path):\n@@ -419,7 +419,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         mixed_query_layer = self.query(from_tensor)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -492,8 +492,8 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n     def forward(\n-        self, hidden_states: Tuple[torch.FloatTensor], input_tensor: torch.FloatTensor\n-    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n+        self, hidden_states: tuple[torch.FloatTensor], input_tensor: torch.FloatTensor\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n         hidden_states = self.dense(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n         hidden_states = self.LayerNorm(hidden_states + input_tensor)\n@@ -570,11 +570,11 @@ def prune_heads(self, heads):\n \n     def forward(\n         self,\n-        hidden_states: Tuple[torch.FloatTensor],\n+        hidden_states: tuple[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         if not self.local:\n             self_outputs = self.self(hidden_states, hidden_states, attention_mask, head_mask, output_attentions)\n             attention_output = self_outputs[0]\n@@ -665,7 +665,7 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n-    def forward(self, hidden_states: Tuple[torch.FloatTensor], input_tensor: torch.FloatTensor) -> torch.FloatTensor:\n+    def forward(self, hidden_states: tuple[torch.FloatTensor], input_tensor: torch.FloatTensor) -> torch.FloatTensor:\n         hidden_states = self.dense(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n         hidden_states = self.LayerNorm(hidden_states + input_tensor)\n@@ -702,11 +702,11 @@ def __init__(\n \n     def forward(\n         self,\n-        hidden_states: Tuple[torch.FloatTensor],\n+        hidden_states: tuple[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n+    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n@@ -763,13 +763,13 @@ def __init__(\n \n     def forward(\n         self,\n-        hidden_states: Tuple[torch.FloatTensor],\n+        hidden_states: tuple[torch.FloatTensor],\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -812,7 +812,7 @@ def __init__(self, config):\n         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n         self.activation = nn.Tanh()\n \n-    def forward(self, hidden_states: Tuple[torch.FloatTensor]) -> torch.FloatTensor:\n+    def forward(self, hidden_states: tuple[torch.FloatTensor]) -> torch.FloatTensor:\n         # We \"pool\" the model by simply taking the hidden state corresponding\n         # to the first token.\n         first_token_tensor = hidden_states[:, 0]\n@@ -831,7 +831,7 @@ def __init__(self, config):\n             self.transform_act_fn = config.hidden_act\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def forward(self, hidden_states: Tuple[torch.FloatTensor]) -> torch.FloatTensor:\n+    def forward(self, hidden_states: tuple[torch.FloatTensor]) -> torch.FloatTensor:\n         hidden_states = self.dense(hidden_states)\n         hidden_states = self.transform_act_fn(hidden_states)\n         hidden_states = self.LayerNorm(hidden_states)\n@@ -852,7 +852,7 @@ def __init__(self, config):\n         # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n         self.decoder.bias = self.bias\n \n-    def forward(self, hidden_states: Tuple[torch.FloatTensor]) -> torch.FloatTensor:\n+    def forward(self, hidden_states: tuple[torch.FloatTensor]) -> torch.FloatTensor:\n         hidden_states = self.transform(hidden_states)\n         hidden_states = self.decoder(hidden_states)\n         return hidden_states\n@@ -865,8 +865,8 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        sequence_output: Tuple[torch.Tensor],\n-    ) -> Tuple[torch.Tensor]:\n+        sequence_output: tuple[torch.Tensor],\n+    ) -> tuple[torch.Tensor]:\n         prediction_scores = self.predictions(sequence_output)\n         return prediction_scores\n \n@@ -1020,7 +1020,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CanineModelOutputWithPooling]:\n+    ) -> Union[tuple, CanineModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1199,7 +1199,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, SequenceClassifierOutput]:\n+    ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1284,7 +1284,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, MultipleChoiceModelOutput]:\n+    ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1389,7 +1389,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, TokenClassifierOutput]:\n+    ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1486,7 +1486,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.canine("
        },
        {
            "sha": "f6b2a8bfd96efdf6ed87aa1c82571612cfd89a4e",
            "filename": "src/transformers/models/canine/tokenization_canine.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Ftokenization_canine.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for CANINE.\"\"\"\n \n-from typing import Dict, List, Optional\n+from typing import Optional\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n@@ -36,7 +36,7 @@\n RESERVED = 0xE004\n \n # Maps special codepoints to human-readable names.\n-SPECIAL_CODEPOINTS: Dict[int, str] = {\n+SPECIAL_CODEPOINTS: dict[int, str] = {\n     # Special symbols are represented using codepoints values that are valid,\n     # but designated as \"Private Use\", meaning that they will never be assigned\n     # characters by the Unicode Consortium, and are thus safe for use here.\n@@ -52,7 +52,7 @@\n }\n \n # Maps special codepoint human-readable names to their codepoint values.\n-SPECIAL_CODEPOINTS_BY_NAME: Dict[str, int] = {name: codepoint for codepoint, name in SPECIAL_CODEPOINTS.items()}\n+SPECIAL_CODEPOINTS_BY_NAME: dict[str, int] = {name: codepoint for codepoint, name in SPECIAL_CODEPOINTS.items()}\n \n \n class CanineTokenizer(PreTrainedTokenizer):\n@@ -91,12 +91,12 @@ def __init__(\n         mask_token = AddedToken(mask_token, lstrip=True, rstrip=False) if isinstance(mask_token, str) else mask_token\n \n         # Creates a mapping for looking up the IDs of special symbols.\n-        self._special_codepoints: Dict[str, int] = {}\n+        self._special_codepoints: dict[str, int] = {}\n         for codepoint, name in SPECIAL_CODEPOINTS.items():\n             self._special_codepoints[name] = codepoint\n \n         # Creates a mapping for looking up the string forms of special symbol IDs.\n-        self._special_codepoint_strings: Dict[int, str] = {\n+        self._special_codepoint_strings: dict[int, str] = {\n             codepoint: name for name, codepoint in self._special_codepoints.items()\n         }\n \n@@ -124,7 +124,7 @@ def get_vocab(self):\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n-    def _tokenize(self, text: str) -> List[str]:\n+    def _tokenize(self, text: str) -> list[str]:\n         \"\"\"Tokenize a string (i.e. perform character splitting).\"\"\"\n         return list(text)\n \n@@ -151,8 +151,8 @@ def convert_tokens_to_string(self, tokens):\n         return \"\".join(tokens)\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A CANINE sequence has the following format:\n@@ -178,8 +178,8 @@ def build_inputs_with_special_tokens(\n         return result\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method."
        },
        {
            "sha": "34436a5288c8187d893d7ca4775b812b4c4d7961",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"chameleon model configuration\"\"\"\n \n-from typing import List, Optional\n+from typing import Optional\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -47,11 +47,11 @@ class ChameleonVQVAEConfig(PretrainedConfig):\n             Number of input channels.\n         base_channels (`int`, *optional*, defaults to 128):\n             Base channel count.\n-        channel_multiplier (`List[int]`, *optional*, defaults to `[1, 1, 2, 2, 4]`):\n+        channel_multiplier (`list[int]`, *optional*, defaults to `[1, 1, 2, 2, 4]`):\n             Channel multipliers for each resolution.\n         num_res_blocks (`int`, *optional*, defaults to 2):\n             Number of residual blocks.\n-        attn_resolutions (`List[int]`, *optional*):\n+        attn_resolutions (`list[int]`, *optional*):\n             Resolutions to apply attention.\n         dropout (`float`, *optional*, defaults to 0.0):\n             Dropout rate.\n@@ -73,9 +73,9 @@ def __init__(\n         resolution: int = 512,\n         in_channels: int = 3,\n         base_channels: int = 128,\n-        channel_multiplier: List[int] = [1, 1, 2, 2, 4],\n+        channel_multiplier: list[int] = [1, 1, 2, 2, 4],\n         num_res_blocks: int = 2,\n-        attn_resolutions: Optional[List[int]] = None,\n+        attn_resolutions: Optional[list[int]] = None,\n         dropout: float = 0.0,\n         attn_type: str = \"vanilla\",\n         initializer_range=0.02,"
        },
        {
            "sha": "3ff1a9139eb61621c08c8309ae8f61d7d93456d5",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Chameleon.\"\"\"\n \n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -52,7 +52,7 @@ class ChameleonImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n             `do_resize` in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 512}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 512}`):\n             Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n             the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n             method.\n@@ -61,7 +61,7 @@ class ChameleonImageProcessor(BaseImageProcessor):\n         do_center_crop (`bool`, *optional*, defaults to `True`):\n             Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n             `preprocess` method.\n-        crop_size (`Dict[str, int]` *optional*, defaults to {\"height\": 512, \"width\": 512}):\n+        crop_size (`dict[str, int]` *optional*, defaults to {\"height\": 512, \"width\": 512}):\n             Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n             method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n@@ -72,10 +72,10 @@ class ChameleonImageProcessor(BaseImageProcessor):\n             method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `[1.0, 1.0, 1.0]`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `[1.0, 1.0, 1.0]`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `[1.0, 1.0, 1.0]`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `[1.0, 1.0, 1.0]`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n@@ -88,15 +88,15 @@ class ChameleonImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PIL.Image.LANCZOS,\n         do_center_crop: bool = True,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 0.0078,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -122,7 +122,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -135,7 +135,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 Resampling filter to use when resiizing the image.\n@@ -173,15 +173,15 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -196,25 +196,25 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n                 the longest edge resized to keep the input aspect ratio.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n                 has an effect if `do_resize` is set to `True`.\n             do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                 Whether to center crop the image.\n-            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                 Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n                 Whether to rescale the image.\n             rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                 `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):"
        },
        {
            "sha": "d6575a8751d009cea7f6f4a18fca1b315c03cde0",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch Chameleon model.\"\"\"\n \n from functools import cached_property\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.nn.functional as F\n@@ -330,7 +330,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n         query_states = self.q_proj(hidden_states)\n@@ -408,7 +408,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -483,7 +483,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`):\n@@ -949,7 +949,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1252,7 +1252,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "f8dfcff33cf34cba934163cd503c2ea4f5fc77e2",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n Processor class for Chameleon.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -92,7 +92,7 @@ def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, ima\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[ChameleonProcessorKwargs],\n@@ -105,10 +105,10 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -176,7 +176,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n \n         Args:\n-            image_sizes (`List[List[int]]`, *optional*):\n+            image_sizes (`list[list[int]]`, *optional*):\n                 The input sizes formatted as (height, width) per each image.\n \n         Returns:"
        },
        {
            "sha": "701168625c47a4f064a09a236736613a2017d7a9",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for Chinese-CLIP.\"\"\"\n \n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -60,7 +60,7 @@ class ChineseCLIPImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n             `do_resize` in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n             Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n             the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n             method.\n@@ -69,7 +69,7 @@ class ChineseCLIPImageProcessor(BaseImageProcessor):\n         do_center_crop (`bool`, *optional*, defaults to `True`):\n             Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n             `preprocess` method.\n-        crop_size (`Dict[str, int]` *optional*, defaults to 224):\n+        crop_size (`dict[str, int]` *optional*, defaults to 224):\n             Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n             method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n@@ -80,10 +80,10 @@ class ChineseCLIPImageProcessor(BaseImageProcessor):\n             method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n@@ -96,15 +96,15 @@ class ChineseCLIPImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -129,7 +129,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -142,7 +142,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 Resampling filter to use when resiizing the image.\n@@ -170,15 +170,15 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -193,25 +193,25 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n                 the longest edge resized to keep the input aspect ratio.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n                 has an effect if `do_resize` is set to `True`.\n             do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                 Whether to center crop the image.\n-            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                 Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n                 Whether to rescale the image.\n             rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                 `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):"
        },
        {
            "sha": "5de98397cad0a2616b7aa5d38c58cad3b6c72db1",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -82,7 +82,7 @@ class ChineseCLIPOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n     vision_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -278,9 +278,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -428,9 +428,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -474,7 +474,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -599,9 +599,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -676,7 +676,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -791,12 +791,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -891,7 +891,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -966,7 +966,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1063,12 +1063,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1191,7 +1191,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n \n@@ -1374,7 +1374,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, ChineseCLIPOutput]:\n+    ) -> Union[tuple, ChineseCLIPOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "0cc7d2c540945c06bee12ccae19ad62cc6fa50af",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n \"\"\"\n \n import warnings\n-from typing import List, Union\n+from typing import Union\n \n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n@@ -68,7 +68,7 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n \n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         images: ImageInput = None,\n         audio=None,\n         videos=None,\n@@ -82,11 +82,11 @@ def __call__(\n         of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n "
        },
        {
            "sha": "710b4ed551e50bcaf088b5b95fc89e3c8e32b944",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Feature extractor class for CLAP.\"\"\"\n \n import copy\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import torch\n@@ -136,12 +136,12 @@ def __init__(\n             mel_scale=\"slaney\",\n         )\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance, except for the\n+            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance, except for the\n             mel filter banks, which do not need to be saved or printed as they are too long.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)\n@@ -259,7 +259,7 @@ def _get_input_mel(self, waveform: np.array, max_length, truncation, padding) ->\n \n     def __call__(\n         self,\n-        raw_speech: Union[np.ndarray, List[float], List[np.ndarray], List[List[float]]],\n+        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n         truncation: Optional[str] = None,\n         padding: Optional[str] = None,\n         max_length: Optional[int] = None,\n@@ -271,7 +271,7 @@ def __call__(\n         Main method to featurize and prepare for the model one or several sequence(s).\n \n         Args:\n-            raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`):\n+            raw_speech (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n                 The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n                 values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not\n                 stereo, i.e. single float per timestep.\n@@ -349,7 +349,7 @@ def __call__(\n             rand_idx = np.random.randint(0, len(input_mel))\n             is_longer[rand_idx] = True\n \n-        if isinstance(input_mel[0], List):\n+        if isinstance(input_mel[0], list):\n             input_mel = [np.asarray(feature, dtype=np.float64) for feature in input_mel]\n \n         # is_longer is a list of bool"
        },
        {
            "sha": "0f8058ad76acf5a262852ebf88168932293ef09a",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import collections\n import math\n from dataclasses import dataclass\n-from typing import Any, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import torch\n import torch.nn.functional as F\n@@ -146,8 +146,8 @@ class ClapTextModelOutput(ModelOutput):\n \n     text_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -175,8 +175,8 @@ class ClapAudioModelOutput(ModelOutput):\n \n     audio_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -210,7 +210,7 @@ class ClapOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPooling = None\n     audio_model_output: BaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"audio_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -431,7 +431,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         batch_size, dim, num_channels = hidden_states.shape\n         mixed_query_layer = self.query(hidden_states)\n \n@@ -528,7 +528,7 @@ def forward(\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(hidden_states, attention_mask, head_mask, output_attentions)\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n@@ -625,11 +625,11 @@ def maybe_pad(self, hidden_states, height, width):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        input_dimensions: Tuple[int, int],\n+        input_dimensions: tuple[int, int],\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         if not always_partition:\n             self.set_shift_and_window_size(input_dimensions)\n         else:\n@@ -721,11 +721,11 @@ def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, d\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        input_dimensions: Tuple[int, int],\n+        input_dimensions: tuple[int, int],\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         height, width = input_dimensions\n         for i, layer_module in enumerate(self.blocks):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n@@ -757,15 +757,15 @@ class ClapAudioPatchMerging(nn.Module):\n     Patch Merging Layer.\n \n     Args:\n-        input_resolution (`Tuple[int]`):\n+        input_resolution (`tuple[int]`):\n             Resolution of input feature.\n         dim (`int`):\n             Number of input channels.\n         norm_layer (`nn.Module`, *optional*, defaults to `nn.LayerNorm`):\n             Normalization layer class.\n     \"\"\"\n \n-    def __init__(self, input_resolution: Tuple[int], dim: int, norm_layer: nn.Module = nn.LayerNorm) -> None:\n+    def __init__(self, input_resolution: tuple[int], dim: int, norm_layer: nn.Module = nn.LayerNorm) -> None:\n         super().__init__()\n         self.input_resolution = input_resolution\n         self.dim = dim\n@@ -780,7 +780,7 @@ def maybe_pad(self, input_feature, height, width):\n \n         return input_feature\n \n-    def forward(self, input_feature: torch.Tensor, input_dimensions: Tuple[int, int]) -> torch.Tensor:\n+    def forward(self, input_feature: torch.Tensor, input_dimensions: tuple[int, int]) -> torch.Tensor:\n         height, width = input_dimensions\n         # `dim` is height * width\n         batch_size, dim, num_channels = input_feature.shape\n@@ -893,7 +893,7 @@ def forward(\n         output_hidden_states_before_downsampling: Optional[bool] = False,\n         always_partition: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple, ClapAudioModelOutput]:\n+    ) -> Union[tuple, ClapAudioModelOutput]:\n         input_features = input_features.transpose(1, 3)\n         normalized_input_features = self.batch_norm(input_features)\n         normalized_input_features = normalized_input_features.transpose(1, 3)\n@@ -1156,9 +1156,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n \n         # If this is instantiated as a cross-attention module, the keys\n@@ -1306,9 +1306,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         self_outputs = self.self(\n             hidden_states,\n             attention_mask,\n@@ -1377,9 +1377,9 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor]:\n+    ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n         self_attention_outputs = self.attention(\n@@ -1456,12 +1456,12 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -1601,7 +1601,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n             Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n@@ -1692,12 +1692,12 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1942,7 +1942,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, ClapOutput]:\n+    ) -> Union[tuple, ClapOutput]:\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n             Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n@@ -2060,7 +2060,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, ClapTextModelOutput]:\n+    ) -> Union[tuple, ClapTextModelOutput]:\n         r\"\"\"\n         Examples:\n \n@@ -2125,7 +2125,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, ClapAudioModelOutput]:\n+    ) -> Union[tuple, ClapAudioModelOutput]:\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n             Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also"
        },
        {
            "sha": "65c25f7663bc566bc7c0f2ed7569703719a4d48f",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -49,11 +49,11 @@ def __call__(self, text=None, audios=None, return_tensors=None, **kwargs):\n         docstring of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            audios (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            audios (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n                 of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n                 and T the sample length of the audio."
        },
        {
            "sha": "df96f0f64b890fa385b373c819608e5badfffd31",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Image processor class for CLIP.\"\"\"\n \n-from typing import Dict, List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -59,7 +59,7 @@ class CLIPImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n             `do_resize` in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n             Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n             the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n             method.\n@@ -68,7 +68,7 @@ class CLIPImageProcessor(BaseImageProcessor):\n         do_center_crop (`bool`, *optional*, defaults to `True`):\n             Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n             `preprocess` method.\n-        crop_size (`Dict[str, int]` *optional*, defaults to 224):\n+        crop_size (`dict[str, int]` *optional*, defaults to 224):\n             Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n             method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n@@ -79,10 +79,10 @@ class CLIPImageProcessor(BaseImageProcessor):\n             method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n             Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n             channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n@@ -95,15 +95,15 @@ class CLIPImageProcessor(BaseImageProcessor):\n     def __init__(\n         self,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_center_crop: bool = True,\n-        crop_size: Optional[Dict[str, int]] = None,\n+        crop_size: Optional[dict[str, int]] = None,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:\n@@ -153,7 +153,7 @@ def __init__(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -166,7 +166,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the output image.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 Resampling filter to use when resiizing the image.\n@@ -203,15 +203,15 @@ def preprocess(\n         self,\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -227,25 +227,25 @@ def preprocess(\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n                 Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n                 the longest edge resized to keep the input aspect ratio.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n                 has an effect if `do_resize` is set to `True`.\n             do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n                 Whether to center crop the image.\n-            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n                 Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n                 Whether to rescale the image.\n             rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n                 Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n                 Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n                 `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):"
        },
        {
            "sha": "b93f63bcea975e162b33d01d996695dd7757dcf7",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch CLIP model.\"\"\"\n \n from dataclasses import dataclass\n-from typing import Any, Callable, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -80,8 +80,8 @@ class CLIPVisionModelOutput(ModelOutput):\n \n     image_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -109,8 +109,8 @@ class CLIPTextModelOutput(ModelOutput):\n \n     text_embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n@@ -143,7 +143,7 @@ class CLIPOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -326,7 +326,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -408,7 +408,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`"
        },
        {
            "sha": "0394974d06477064c2a151e499a847002106fe45",
            "filename": "src/transformers/models/clip/modeling_flax_clip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_flax_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import flax\n import flax.linen as nn\n@@ -182,8 +182,8 @@ class FlaxCLIPTextModelOutput(ModelOutput):\n \n     text_embeds: jnp.ndarray = None\n     last_hidden_state: jnp.ndarray = None\n-    hidden_states: Optional[Tuple[jnp.ndarray, ...]] = None\n-    attentions: Optional[Tuple[jnp.ndarray, ...]] = None\n+    hidden_states: Optional[tuple[jnp.ndarray, ...]] = None\n+    attentions: Optional[tuple[jnp.ndarray, ...]] = None\n \n \n @flax.struct.dataclass\n@@ -215,7 +215,7 @@ class FlaxCLIPOutput(ModelOutput):\n     text_model_output: FlaxBaseModelOutputWithPooling = None\n     vision_model_output: FlaxBaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -641,7 +641,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensor\n         input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n         position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n@@ -712,7 +712,7 @@ class FlaxCLIPVisionPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: CLIPVisionConfig,\n-        input_shape: Optional[Tuple] = None,\n+        input_shape: Optional[tuple] = None,\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -723,7 +723,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensor\n         pixel_values = jax.random.normal(rng, input_shape)\n \n@@ -783,7 +783,7 @@ class FlaxCLIPPreTrainedModel(FlaxPreTrainedModel):\n     def __init__(\n         self,\n         config: CLIPConfig,\n-        input_shape: Optional[Tuple] = None,\n+        input_shape: Optional[tuple] = None,\n         seed: int = 0,\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n@@ -794,7 +794,7 @@ def __init__(\n         module = self.module_class(config=config, dtype=dtype, **kwargs)\n         super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n \n-    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n+    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = None) -> FrozenDict:\n         # init input tensor\n         input_ids = jnp.zeros(input_shape[0], dtype=\"i4\")\n         position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape[0])"
        },
        {
            "sha": "90ff2652ac31742521a3e10fea218a079c1fbe09",
            "filename": "src/transformers/models/clip/modeling_tf_clip.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -115,7 +115,7 @@ class TFCLIPOutput(ModelOutput):\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -306,7 +306,7 @@ def call(\n         causal_attention_mask: tf.Tensor,\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size = shape_list(hidden_states)[0]\n@@ -423,7 +423,7 @@ def call(\n         causal_attention_mask: tf.Tensor,\n         output_attentions: bool,\n         training: bool = False,\n-    ) -> Tuple[tf.Tensor]:\n+    ) -> tuple[tf.Tensor]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -498,7 +498,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -560,7 +560,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         input_shape = shape_list(input_ids)\n \n         embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n@@ -681,7 +681,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n \n@@ -728,7 +728,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         embedding_output = self.embeddings(pixel_values=pixel_values)\n         embedding_output = self.pre_layernorm(inputs=embedding_output)\n \n@@ -794,7 +794,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -957,7 +957,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFCLIPOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFCLIPOutput, tuple[tf.Tensor]]:\n         if input_ids is None:\n             raise ValueError(\"You have to specify either input_ids\")\n         if pixel_values is None:\n@@ -1077,7 +1077,7 @@ class TFCLIPPreTrainedModel(TFPreTrainedModel):\n \n CLIP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        input_ids (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `({0})`):\n+        input_ids (`np.ndarray`, `tf.Tensor`, `list[tf.Tensor]` ``dict[str, tf.Tensor]` or `dict[str, np.ndarray]` and each example must have the shape `({0})`):\n             Indices of input sequence tokens in the vocabulary.\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n@@ -1114,7 +1114,7 @@ class TFCLIPPreTrainedModel(TFPreTrainedModel):\n \n CLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        pixel_values (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size, num_channels, height, width)`):\n+        pixel_values (`np.ndarray`, `tf.Tensor`, `list[tf.Tensor]` ``dict[str, tf.Tensor]` or `dict[str, np.ndarray]` and each example must have the shape `(batch_size, num_channels, height, width)`):\n             Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n             [`CLIPImageProcessor.__call__`] for details. output_attentions (`bool`, *optional*): Whether or not to\n             return the attentions tensors of all attention layers. See `attentions` under returned tensors for more\n@@ -1134,14 +1134,14 @@ class TFCLIPPreTrainedModel(TFPreTrainedModel):\n \n CLIP_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        input_ids (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `({0})`):\n+        input_ids (`np.ndarray`, `tf.Tensor`, `list[tf.Tensor]` ``dict[str, tf.Tensor]` or `dict[str, np.ndarray]` and each example must have the shape `({0})`):\n             Indices of input sequence tokens in the vocabulary.\n \n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n             [`PreTrainedTokenizer.encode`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size, num_channels, height, width)`):\n+        pixel_values (`np.ndarray`, `tf.Tensor`, `list[tf.Tensor]` `dict[str, tf.Tensor]` or `dict[str, np.ndarray]` and each example must have the shape `(batch_size, num_channels, height, width)`):\n             Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n             [`CLIPImageProcessor.__call__`] for details.\n         attention_mask (`np.ndarray` or `tf.Tensor` of shape `({0})`, *optional*):\n@@ -1195,7 +1195,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         r\"\"\"\n         Returns:\n \n@@ -1254,7 +1254,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:\n+    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n         r\"\"\"\n         Returns:\n \n@@ -1402,7 +1402,7 @@ def call(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         training: bool = False,\n-    ) -> Union[TFCLIPOutput, Tuple[tf.Tensor]]:\n+    ) -> Union[TFCLIPOutput, tuple[tf.Tensor]]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "eb21cc8fc2e450ce009243c0a67d0b71d5acb11e",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -67,11 +67,11 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n "
        },
        {
            "sha": "e4a142f0d3b7bfb748f6455e7ecab7d65bbd2d2d",
            "filename": "src/transformers/models/clip/tokenization_clip.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import os\n import unicodedata\n from functools import lru_cache\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import regex as re\n \n@@ -337,8 +337,8 @@ def get_vocab(self):\n         return dict(self.encoder, **self.added_tokens_encoder)\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A CLIP sequence has the following format:\n@@ -348,13 +348,13 @@ def build_inputs_with_special_tokens(\n         Pairs of sequences are not the expected use case, but they will be handled without a separator.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         bos_token = [self.bos_token_id]\n         eos_token = [self.eos_token_id]\n@@ -364,22 +364,22 @@ def build_inputs_with_special_tokens(\n         return bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n \n         if already_has_special_tokens:\n@@ -392,20 +392,20 @@ def get_special_tokens_mask(\n         return [1] + ([0] * len(token_ids_0)) + [1] + [1] + ([0] * len(token_ids_1)) + [1]\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed. CLIP does not make use of token type ids, therefore a list of\n         zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         bos_token = [self.bos_token_id]\n         eos_token = [self.eos_token_id]\n@@ -486,7 +486,7 @@ def convert_tokens_to_string(self, tokens):\n         text = byte_array.decode(\"utf-8\", errors=self.errors).replace(\"</w>\", \" \").strip()\n         return text\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n             return"
        },
        {
            "sha": "4de1504a806de2fdb0f8542ab5d84d99392b060e",
            "filename": "src/transformers/models/clip/tokenization_clip_fast.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from tokenizers import pre_tokenizers\n \n@@ -107,8 +107,8 @@ def new_decode_method(*args, **kwargs):\n         self.backend_tokenizer.decode = new_decode_method\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. A CLIP sequence has the following format:\n@@ -118,13 +118,13 @@ def build_inputs_with_special_tokens(\n         Pairs of sequences are not the expected use case, but they will be handled without a separator.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         bos_token = [self.bos_token_id]\n         eos_token = [self.eos_token_id]\n@@ -134,20 +134,20 @@ def build_inputs_with_special_tokens(\n         return bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token\n \n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Create a mask from the two sequences passed. CLIP does not make use of token type ids, therefore a list of\n         zeros is returned.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of zeros.\n+            `list[int]`: List of zeros.\n         \"\"\"\n         bos_token = [self.bos_token_id]\n         eos_token = [self.eos_token_id]\n@@ -156,7 +156,7 @@ def create_token_type_ids_from_sequences(\n             return len(bos_token + token_ids_0 + eos_token) * [0]\n         return len(bos_token + token_ids_0 + eos_token + eos_token + token_ids_1 + eos_token) * [0]\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n "
        },
        {
            "sha": "55c29c2b7dd1f750070305b6c405f607dd79e27d",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -223,7 +223,7 @@ class CLIPSegConfig(PretrainedConfig):\n             Dimensionality of text and vision projection layers.\n         logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n             The initial value of the *logit_scale* parameter. Default is used as per the original CLIPSeg implementation.\n-        extract_layers (`List[int]`, *optional*, defaults to `[3, 6, 9]`):\n+        extract_layers (`list[int]`, *optional*, defaults to `[3, 6, 9]`):\n             Layers to extract when forwarding the query image through the frozen visual backbone of CLIP.\n         reduce_dim (`int`, *optional*, defaults to 64):\n             Dimensionality to reduce the CLIP vision embedding."
        },
        {
            "sha": "c68404cb66c8034d2f5fa59211d57b1e5ce51c3f",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import copy\n import math\n from dataclasses import dataclass\n-from typing import Any, Callable, Optional, Tuple, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -78,7 +78,7 @@ class CLIPSegOutput(ModelOutput):\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -101,8 +101,8 @@ class CLIPSegDecoderOutput(ModelOutput):\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -123,7 +123,7 @@ class CLIPSegImageSegmentationOutput(ModelOutput):\n     vision_model_output: BaseModelOutputWithPooling = None\n     decoder_output: CLIPSegDecoderOutput = None\n \n-    def to_tuple(self) -> Tuple[Any]:\n+    def to_tuple(self) -> tuple[Any]:\n         return tuple(\n             self[k] if k not in [\"vision_model_output\", \"decoder_output\"] else getattr(self, k).to_tuple()\n             for k in self.keys()\n@@ -306,7 +306,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         batch_size, seq_length, embed_dim = hidden_states.shape\n@@ -389,7 +389,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -496,7 +496,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -591,7 +591,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -687,7 +687,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n \n@@ -733,7 +733,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = True,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -786,7 +786,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = True,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n \n@@ -962,7 +962,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CLIPSegOutput]:\n+    ) -> Union[tuple, CLIPSegOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss.\n@@ -1067,7 +1067,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         causal_attention_mask: torch.Tensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -1148,7 +1148,7 @@ def __init__(self, config: CLIPSegConfig):\n \n     def forward(\n         self,\n-        hidden_states: Tuple[torch.Tensor],\n+        hidden_states: tuple[torch.Tensor],\n         conditional_embeddings: torch.Tensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1267,7 +1267,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CLIPSegOutput]:\n+    ) -> Union[tuple, CLIPSegOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "e4dc04d30f78f8f16778b364788e116f1f5a3454",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -67,14 +67,14 @@ def __call__(self, text=None, images=None, visual_prompt=None, return_tensors=No\n         the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            visual_prompt (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            visual_prompt (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The visual prompt image or batch of images to be prepared. Each visual prompt image can be a PIL image,\n                 NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape\n                 (C, H, W), where C is a number of channels, H and W are image height and width."
        },
        {
            "sha": "6fe4a52aa5e3227809ac34bac3f392edda981f70",
            "filename": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n Feature extractor class for CLVP\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n import numpy as np\n \n@@ -130,7 +130,7 @@ def _np_extract_fbank_features(self, waveform: np.array) -> np.ndarray:\n \n     def __call__(\n         self,\n-        raw_speech: Union[np.ndarray, List[float], List[np.ndarray], List[List[float]]],\n+        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n         sampling_rate: Optional[int] = None,\n         truncation: bool = True,\n         pad_to_multiple_of: Optional[int] = None,\n@@ -148,7 +148,7 @@ def __call__(\n         seconds long and then the log-mel spectrogram is extracted from it.\n \n         Args:\n-            raw_speech (`np.ndarray`, `List[float]`, `List[np.ndarray]`, `List[List[float]]`):\n+            raw_speech (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n                 The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n                 values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not\n                 stereo, i.e. single float per timestep.\n@@ -230,7 +230,7 @@ def __call__(\n             self._np_extract_fbank_features(waveform).astype(np.float32) for waveform in input_features[0]\n         ]\n \n-        if isinstance(input_features[0], List):\n+        if isinstance(input_features[0], list):\n             padded_inputs[\"input_features\"] = [np.asarray(feature) for feature in input_features]\n         else:\n             padded_inputs[\"input_features\"] = input_features"
        },
        {
            "sha": "a2e42fc2aea829d38831ff8f0e884d16b88362ed",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import copy\n import math\n from dataclasses import dataclass\n-from typing import Callable, Dict, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -169,8 +169,8 @@ class ClvpEncoderOutput(ModelOutput):\n     embeds: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -308,11 +308,11 @@ def forward(\n         rotary_pos_emb: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         use_cache: Optional[bool] = False,\n         head_mask: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[Tuple[torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[tuple[torch.FloatTensor]]]:\n         # Raise error when position_ids is None but rotary_pos_emb is provided, because we need that when applying\n         # rotary_pos_emb to query and key states.\n         if rotary_pos_emb is not None and position_ids is None:\n@@ -451,7 +451,7 @@ def forward(\n         attention_mask: torch.LongTensor,\n         position_ids: torch.LongTensor,\n         output_attentions: Optional[bool] = False,\n-    ) -> Tuple[torch.FloatTensor]:\n+    ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor` of shape `(batch, seq_len, embed_dim)`):\n@@ -605,7 +605,7 @@ def __init__(self, intermediate_size, config):\n         self.act = ACT2FN[config.activation_function]\n         self.dropout = nn.Dropout(config.resid_pdrop)\n \n-    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n+    def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n         hidden_states = self.c_fc(hidden_states)\n         hidden_states = self.act(hidden_states)\n         hidden_states = self.c_proj(hidden_states)\n@@ -627,14 +627,14 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        hidden_states: Optional[tuple[torch.FloatTensor]],\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n-    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n+    ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         attn_outputs = self.attn(\n@@ -884,7 +884,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutput]:\n+    ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n@@ -1042,13 +1042,13 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1205,13 +1205,13 @@ def forward(\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1274,8 +1274,8 @@ def _prepare_model_inputs(\n         self,\n         inputs: Optional[torch.Tensor] = None,\n         bos_token_id: Optional[int] = None,\n-        model_kwargs: Optional[Dict[str, torch.Tensor]] = None,\n-    ) -> Tuple[torch.Tensor, Optional[str], Dict[str, torch.Tensor]]:\n+        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n+    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n         \"\"\"\n         This function extracts the model-specific `inputs` for generation.\n         \"\"\"\n@@ -1394,7 +1394,7 @@ def prepare_inputs_for_generation(\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -1405,7 +1405,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n@@ -1464,8 +1464,8 @@ def forward(\n \n     @staticmethod\n     def _reorder_cache(\n-        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> Tuple[Tuple[torch.Tensor]]:\n+        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n+    ) -> tuple[tuple[torch.Tensor]]:\n         \"\"\"\n         This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n         [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n@@ -1716,7 +1716,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, ClvpOutput]:\n+    ) -> Union[tuple, ClvpOutput]:\n         r\"\"\"\n         input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`):\n             Indicates log mel-spectrogram representations for audio returned by [`ClvpFeatureExtractor`]."
        },
        {
            "sha": "39e7429dfb21c2fb873f3faa2da6c9ddf2ad5a15",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import json\n import os\n from functools import lru_cache\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n import regex as re\n \n@@ -258,22 +258,22 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n     # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.get_special_tokens_mask\n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -334,7 +334,7 @@ def clean_up_tokenization(self, text):\n         return text\n \n     # Copied from transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return"
        },
        {
            "sha": "94d1b4d659851a5f466c8123dd032ab213a90ed4",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n \n import os\n from shutil import copyfile\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n \n import sentencepiece as spm\n \n@@ -108,7 +108,7 @@ class CodeLlamaTokenizer(PreTrainedTokenizer):\n             Whether to add an end of sequence token at the end of sequences.\n         clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n             Whether or not to clean up the tokenization spaces.\n-        additional_special_tokens (`List[str]`, *optional*):\n+        additional_special_tokens (`list[str]`, *optional*):\n             Additional special tokens used by the tokenizer.\n         use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n             Whether or not the default system prompt for Llama should be used.\n@@ -129,7 +129,7 @@ def __init__(\n         eot_token=\"▁<EOT>\",\n         fill_token=\"<FILL_ME>\",\n         suffix_first=False,\n-        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n+        sp_model_kwargs: Optional[dict[str, Any]] = None,\n         add_bos_token=True,\n         add_eos_token=False,\n         clean_up_tokenization_spaces=False,\n@@ -248,7 +248,7 @@ def get_vocab(self):\n         vocab.update(self.added_tokens_encoder)\n         return vocab\n \n-    def tokenize(self, prefix, suffix=None, suffix_first=False, **kwargs) -> List[int]:\n+    def tokenize(self, prefix, suffix=None, suffix_first=False, **kwargs) -> list[int]:\n         # add a prefix space to `prefix`\n         if self.fill_token is not None and self.fill_token in prefix and suffix is None:\n             prefix, suffix = prefix.split(self.fill_token)\n@@ -328,7 +328,7 @@ def convert_tokens_to_string(self, tokens):\n         return out_string\n \n     # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.save_vocabulary\n-    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> tuple[str]:\n         \"\"\"\n         Save the vocabulary and special tokens file to a directory.\n \n@@ -369,22 +369,22 @@ def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n \n     # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.get_special_tokens_mask\n     def get_special_tokens_mask(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n         \"\"\"\n         Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n         special tokens using the tokenizer `prepare_for_model` method.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n             already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n                 Whether or not the token list is already formatted with special tokens for the model.\n \n         Returns:\n-            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n         \"\"\"\n         if already_has_special_tokens:\n             return super().get_special_tokens_mask(\n@@ -407,8 +407,8 @@ def get_special_tokens_mask(\n \n     # Copied from transformers.models.llama.tokenization_llama.LlamaTokenizer.create_token_type_ids_from_sequences\n     def create_token_type_ids_from_sequences(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n         sequence pair mask has the following format:\n@@ -421,13 +421,13 @@ def create_token_type_ids_from_sequences(\n         if token_ids_1 is None, only returns the first portion of the mask (0s).\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of ids.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n+            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n         \"\"\"\n         bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n         eos_token_id = [self.eos_token_id] if self.add_eos_token else []"
        },
        {
            "sha": "b3978587e7f02512a5344f9ad0a33bf86b839757",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n import os\n from shutil import copyfile\n-from typing import List, Optional, Tuple\n+from typing import Optional\n \n from tokenizers import normalizers, processors\n \n@@ -98,7 +98,7 @@ class CodeLlamaTokenizerFast(PreTrainedTokenizerFast):\n             End of text token used for infilling.\n         fill_token (`str`, *optional*, defaults to `\"<FILL_ME>\"`):\n             The token used to split the input between the prefix and suffix.\n-        additional_special_tokens (`List[str]`, *optional*):\n+        additional_special_tokens (`list[str]`, *optional*):\n             Additional special tokens used by the tokenizer.\n         add_bos_token (`bool`, *optional*, defaults to `True`):\n             Whether to add a beginning of sequence token at the start of sequences.\n@@ -323,7 +323,7 @@ def encode_plus(self, text, text_pair=None, suffix_first=False, add_special_toke\n         return tokens\n \n     # Copied from transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast.save_vocabulary\n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not self.can_save_slow_tokenizer:\n             raise ValueError(\n                 \"Your fast tokenizer does not have the necessary information to save the vocabulary for a slow \"\n@@ -343,8 +343,8 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n         return (out_vocab_file,)\n \n     def build_inputs_with_special_tokens(\n-        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n-    ) -> List[int]:\n+        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n+    ) -> list[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n         adding special tokens. The special tokens depend on calling set_lang.\n@@ -358,13 +358,13 @@ def build_inputs_with_special_tokens(\n         separator.\n \n         Args:\n-            token_ids_0 (`List[int]`):\n+            token_ids_0 (`list[int]`):\n                 List of IDs to which the special tokens will be added.\n-            token_ids_1 (`List[int]`, *optional*):\n+            token_ids_1 (`list[int]`, *optional*):\n                 Optional second list of IDs for sequence pairs.\n \n         Returns:\n-            `List[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n+            `list[int]`: list of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n         \"\"\"\n         if token_ids_1 is None:\n             return self.bos_token_id + token_ids_0 + self.eos_token_id"
        },
        {
            "sha": "6a9ab842710cdd67911305ce324fe4c68dec173b",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n \n from collections import OrderedDict\n from collections.abc import Mapping\n-from typing import Any, List, Optional\n+from typing import Any, Optional\n \n from ... import PreTrainedTokenizer, TensorType, is_torch_available\n from ...configuration_utils import PretrainedConfig\n@@ -152,7 +152,7 @@ def __init__(\n         self,\n         config: PretrainedConfig,\n         task: str = \"default\",\n-        patching_specs: Optional[List[PatchingSpec]] = None,\n+        patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)"
        },
        {
            "sha": "6a99d0fa390a4dc884146095a2e9a1a982660dc3",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch CodeGen model.\"\"\"\n \n-from typing import Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -156,8 +156,8 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[\n-        Tuple[torch.Tensor, Tuple[torch.Tensor]],\n-        Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],\n+        tuple[torch.Tensor, tuple[torch.Tensor]],\n+        Optional[tuple[torch.Tensor, tuple[torch.Tensor], tuple[torch.Tensor, ...]]],\n     ]:\n         qkv = self.qkv_proj(hidden_states)\n         # TODO(enijkamp): factor out number of logical TPU-v4 cores or make forward pass agnostic\n@@ -264,7 +264,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n+    ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n         attn_outputs = self.attn(\n@@ -350,7 +350,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -362,7 +362,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n@@ -643,7 +643,7 @@ def set_output_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, Tuple[Tuple[torch.Tensor]]]] = None,\n+        past_key_values: Optional[Union[Cache, tuple[tuple[torch.Tensor]]]] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -656,7 +656,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> Union[tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n@@ -718,8 +718,8 @@ def forward(\n \n     @staticmethod\n     def _reorder_cache(\n-        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> Tuple[Tuple[torch.Tensor]]:\n+        past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor\n+    ) -> tuple[tuple[torch.Tensor]]:\n         \"\"\"\n         This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or\n         [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct"
        },
        {
            "sha": "99f1facb1f8451baba74d740481169790f123e05",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -17,7 +17,7 @@\n import json\n import os\n from functools import lru_cache\n-from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n import regex as re\n@@ -276,7 +276,7 @@ def convert_tokens_to_string(self, tokens):\n         text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n         return text\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         if not os.path.isdir(save_directory):\n             logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n             return\n@@ -313,10 +313,10 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n \n     def decode(\n         self,\n-        token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n+        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n-        truncate_before_pattern: Optional[List[str]] = None,\n+        truncate_before_pattern: Optional[list[str]] = None,\n         **kwargs,\n     ) -> str:\n         \"\"\""
        },
        {
            "sha": "7bac0db7de4e7c548ddea0eb2b3c498919c6196a",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n import re\n-from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n \n@@ -154,16 +154,16 @@ def _encode_plus(self, *args, **kwargs) -> BatchEncoding:\n \n         return super()._encode_plus(*args, **kwargs)\n \n-    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n+    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> tuple[str]:\n         files = self._tokenizer.model.save(save_directory, name=filename_prefix)\n         return tuple(files)\n \n     def decode(\n         self,\n-        token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n+        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: Optional[bool] = None,\n-        truncate_before_pattern: Optional[List[str]] = None,\n+        truncate_before_pattern: Optional[list[str]] = None,\n         **kwargs,\n     ) -> str:\n         \"\"\""
        },
        {
            "sha": "c78d1e9bf8a11c45246a5cb391769566dc69ce50",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -105,11 +105,11 @@ class CohereConfig(PretrainedConfig):\n                 `beta_slow` (`float`, *optional*):\n                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                     ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n+                `short_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n+                `long_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2"
        },
        {
            "sha": "88ca4e31de103a1fec27b299495d88a6f201cdfd",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -27,7 +27,7 @@\n # This file is based on the LLama model definition file in transformers\n \n \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -230,12 +230,12 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -296,9 +296,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -314,7 +314,7 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n         \"\"\"\n@@ -547,7 +547,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "248ada3227f4874e33ceb4a9aa5189bd9fa372d5",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -22,7 +22,7 @@\n \n \"\"\"PyTorch Cohere model.\"\"\"\n \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -153,12 +153,12 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -219,9 +219,9 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -237,7 +237,7 @@ def forward(\n                 (see `past_key_values`).\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n         \"\"\"\n@@ -311,7 +311,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6b1de3dd3171a5df60d3b1ce1ff954a8ab1f48a7",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -16,7 +16,7 @@\n # This file is based on the tokenization_llama_fast.py file in transformers\n \n import pickle\n-from typing import Dict, List, Literal, Union\n+from typing import Literal, Union\n \n from tokenizers import processors\n \n@@ -227,10 +227,10 @@ def add_bos_token(self, value):\n \n     def apply_tool_use_template(\n         self,\n-        conversation: Union[List[Dict[str, str]]],\n-        tools: List[Dict],\n+        conversation: Union[list[dict[str, str]]],\n+        tools: list[dict],\n         **kwargs,\n-    ) -> Union[str, List[int]]:\n+    ) -> Union[str, list[int]]:\n         \"\"\"Create a Command-R tool-use prompt.\n \n         Once rendered, the prompt instructs the model to generate a list of actions to perform on a set of user supplied tools\n@@ -244,16 +244,16 @@ def apply_tool_use_template(\n         You can override the default template using the `tool_use_template` kwarg but the quality of your results may decrease.\n \n         Args:\n-            conversation (Union[List[Dict[str, str]]]): A list of dicts\n+            conversation (Union[list[dict[str, str]]]): A list of dicts\n                 with \"role\" and \"content\" keys, representing the chat history so far.\n-            tools (List[Dict]): a list of tools to render into the prompt for the model to choose from.\n+            tools (list[Dict]): a list of tools to render into the prompt for the model to choose from.\n                 See an example at the bottom of the docstring.\n                 The format should be:\n                    * name (str): The name of the tool to be called. Valid names contain only the characters a-z,\n                         A-Z, 0-9, _ and must not begin with a digit.\n                    * description (str): The description of what the tool does, the model uses the description to\n                         choose when and how to call the function.\n-                   * parameter_definitions (List[Dict]): The input parameters of the tool. Accepts a dictionary\n+                   * parameter_definitions (list[Dict]): The input parameters of the tool. Accepts a dictionary\n                         where the key is the name of the parameter and the value is the parameter spec.\n                         Valid parameter names contain only the characters a-z, A-Z, 0-9, _ and must not begin with a digit.\n                         Parameter specs are as follows:\n@@ -287,7 +287,7 @@ def apply_tool_use_template(\n         Returns:\n             `str`: A rendered prompt string.\n             or if tokenize=True:\n-            `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n+            `list[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n             output is ready to pass to the model, either directly or via methods like `generate()`.\n \n         Examples:\n@@ -336,7 +336,7 @@ def apply_tool_use_template(\n         Here is a list of tools that you have available to you:\n \n         \\\\`\\\\`\\\\`python\n-        def internet_search(query: str) -> List[Dict]:\n+        def internet_search(query: str) -> list[Dict]:\n             \\\"\\\"\\\"Returns a list of relevant document snippets for a textual query retrieved from the internet\n \n             Args:\n@@ -346,7 +346,7 @@ def internet_search(query: str) -> List[Dict]:\n         \\\\`\\\\`\\\\`\n \n         \\\\`\\\\`\\\\`python\n-        def directly_answer() -> List[Dict]:\n+        def directly_answer() -> list[Dict]:\n             \\\"\\\"\\\"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n             \\\"\\\"\\\"\n             pass\n@@ -382,11 +382,11 @@ def directly_answer() -> List[Dict]:\n \n     def apply_grounded_generation_template(\n         self,\n-        conversation: Union[List[Dict[str, str]]],\n-        documents: List[Dict],\n+        conversation: Union[list[dict[str, str]]],\n+        documents: list[dict],\n         citation_mode: Literal[\"fast\", \"accurate\"] = \"accurate\",\n         **kwargs,\n-    ) -> Union[str, List[int]]:\n+    ) -> Union[str, list[int]]:\n         \"\"\"Create a Command-R grounded generation (aka RAG) prompt.\n \n         Once rendered, the prompt instructs the model to generate a response with citations in, based on supplied documents.\n@@ -400,9 +400,9 @@ def apply_grounded_generation_template(\n         You can override the default template using the `grounded_generation_template` kwarg but the quality of your results may decrease.\n \n         Args:\n-            conversation (Union[List[Dict[str, str]]]): A list of dicts\n+            conversation (Union[list[dict[str, str]]]): A list of dicts\n                 with \"role\" and \"content\" keys, representing the chat history so far.\n-            documents (List[Dict[str, str]): A list of dicts, representing documents or tool outputs to ground your\n+            documents (list[dict[str, str]): A list of dicts, representing documents or tool outputs to ground your\n                 generation on. A document is a semistructured dict, with a string to string mapping. Common fields are\n                 `url`, `title`, `snippet` etc but should be descriptive of the key. They will get rendered into the prompt.\n             citation_mode: either \"accurate\" (prompt the model to generate an answer first, then rewrite it with citation\n@@ -435,7 +435,7 @@ def apply_grounded_generation_template(\n         Returns:\n             `str`: A rendered prompt string.\n             or if tokenize=True:\n-            `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n+            `list[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n             output is ready to pass to the model, either directly or via methods like `generate()`.\n \n         Examples:"
        },
        {
            "sha": "747781075a0910ff8e80bad521e4fc3bb41540a5",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -101,11 +101,11 @@ class Cohere2Config(PretrainedConfig):\n                 `beta_slow` (`float`, *optional*):\n                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                     ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n+                `short_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n+                `long_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2"
        },
        {
            "sha": "6999f1632f95053d4ce27446eb9cb591b5d26972",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -199,12 +199,12 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -270,18 +270,18 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`):\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`):\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             attention_mask (`torch.FloatTensor`, *optional*):\n@@ -528,7 +528,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "b32e4d94dd5c36c92ba9c410a7fef6b233754afc",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -13,7 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, Optional, Tuple\n+from typing import Callable, Optional\n \n import torch\n import torch.nn as nn\n@@ -123,11 +123,11 @@ class Cohere2Config(PretrainedConfig):\n                 `beta_slow` (`float`, *optional*):\n                     Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                     ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n+                `short_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n+                `long_factor` (`list[float]`, *optional*):\n                     Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                     `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                     size divided by the number of attention heads divided by 2\n@@ -286,12 +286,12 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -337,18 +337,18 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`):\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`):\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             attention_mask (`torch.FloatTensor`, *optional*):"
        },
        {
            "sha": "b9c55f120d416e454e4f975d5d9e04c4af059d0d",
            "filename": "src/transformers/models/colpali/convert_colpali_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fconvert_colpali_weights_to_hf.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -39,7 +39,7 @@\n import argparse\n import glob\n from pathlib import Path\n-from typing import Any, Dict, Optional\n+from typing import Any, Optional\n \n import torch\n from huggingface_hub import snapshot_download\n@@ -58,7 +58,7 @@\n ORIGINAL_DTYPE = torch.bfloat16\n \n \n-def rename_state_dict_keys(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n+def rename_state_dict_keys(state_dict: dict[str, Any]) -> dict[str, Any]:\n     new_state_dict = {}\n     for key, value in state_dict.items():\n         new_key = key\n@@ -70,7 +70,7 @@ def rename_state_dict_keys(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n     return new_state_dict\n \n \n-def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> Dict[str, torch.Tensor]:\n+def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> dict[str, torch.Tensor]:\n     directory_path = snapshot_download(\n         repo_id=model_id,\n         revision=revision,"
        },
        {
            "sha": "077ec0a88de529b9ee41cfa33a98c4b937adbed2",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -15,7 +15,7 @@\n \"\"\"PyTorch ColPali model\"\"\"\n \n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -84,9 +84,9 @@ class ColPaliForRetrievalOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.Tensor] = None\n-    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n "
        },
        {
            "sha": "a090fb5dfedd2c2f1fbf0f643fefea0a64958241",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from transformers.models.paligemma.processing_paligemma import IMAGE_TOKEN, PaliGemmaProcessor, build_string_from_input\n \n@@ -90,7 +90,7 @@ def query_augmentation_token(self) -> str:\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n@@ -107,11 +107,11 @@ def __call__(\n         Please refer to the docstring of the above two methods for more information.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -197,7 +197,7 @@ def __call__(\n             if suffix is None:\n                 suffix = self.query_augmentation_token * 10\n \n-            texts_query: List[str] = []\n+            texts_query: list[str] = []\n             for query in text:\n                 query = self.tokenizer.bos_token + self.query_prefix + query + suffix + \"\\n\"\n                 texts_query.append(query)\n@@ -224,7 +224,7 @@ def process_images(\n         This method forwards the `images` and `kwargs` arguments to the image processor.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n@@ -249,7 +249,7 @@ def process_images(\n \n     def process_queries(\n         self,\n-        text: Union[TextInput, List[TextInput]],\n+        text: Union[TextInput, list[TextInput]],\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -259,7 +259,7 @@ def process_queries(\n         This method forwards the `text` and `kwargs` arguments to the tokenizer.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -283,8 +283,8 @@ def process_queries(\n \n     def score_retrieval(\n         self,\n-        query_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n-        passage_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n+        query_embeddings: Union[\"torch.Tensor\", list[\"torch.Tensor\"]],\n+        passage_embeddings: Union[\"torch.Tensor\", list[\"torch.Tensor\"]],\n         batch_size: int = 128,\n         output_dtype: Optional[\"torch.dtype\"] = None,\n         output_device: Union[\"torch.device\", str] = \"cpu\",\n@@ -301,8 +301,8 @@ def score_retrieval(\n             obtained by padding the list of tensors.\n \n         Args:\n-            query_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Query embeddings.\n-            passage_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Passage embeddings.\n+            query_embeddings (`Union[torch.Tensor, list[torch.Tensor]`): Query embeddings.\n+            passage_embeddings (`Union[torch.Tensor, list[torch.Tensor]`): Passage embeddings.\n             batch_size (`int`, *optional*, defaults to 128): Batch size for computing scores.\n             output_dtype (`torch.dtype`, *optional*, defaults to `torch.float32`): The dtype of the output tensor.\n                 If `None`, the dtype of the input embeddings is used.\n@@ -327,10 +327,10 @@ def score_retrieval(\n         if output_dtype is None:\n             output_dtype = query_embeddings[0].dtype\n \n-        scores: List[torch.Tensor] = []\n+        scores: list[torch.Tensor] = []\n \n         for i in range(0, len(query_embeddings), batch_size):\n-            batch_scores: List[torch.Tensor] = []\n+            batch_scores: list[torch.Tensor] = []\n             batch_queries = torch.nn.utils.rnn.pad_sequence(\n                 query_embeddings[i : i + batch_size], batch_first=True, padding_value=0\n             )"
        },
        {
            "sha": "759209beacf0a3c82d85824a2c6c95005113544e",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n@@ -63,7 +63,7 @@ def build_string_from_input(prompt, bos_token, image_seq_len, image_token, num_i\n     The output will be:\n     \"<im><im><im><s>Initial str\"\n     Args:\n-        prompt (`List[Union[str, ImageInput]]`): The input prompt.\n+        prompt (`list[Union[str, ImageInput]]`): The input prompt.\n         bos_token (`str`): The beginning of sentence token.\n         image_seq_len (`int`): The length of the image sequence.\n         image_token (`str`): The image token.\n@@ -134,7 +134,7 @@ def __init__(\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n@@ -151,11 +151,11 @@ def __call__(\n         Please refer to the docstring of the above two methods for more information.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -241,7 +241,7 @@ def __call__(\n             if suffix is None:\n                 suffix = self.query_augmentation_token * 10\n \n-            texts_query: List[str] = []\n+            texts_query: list[str] = []\n             for query in text:\n                 query = self.tokenizer.bos_token + self.query_prefix + query + suffix + \"\\n\"\n                 texts_query.append(query)\n@@ -261,10 +261,10 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n \n         Args:\n-            image_sizes (List[List[str]], *optional*):\n+            image_sizes (list[list[str]], *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n-            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            dict[str, list[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n             to a list containing the number of placeholder tokens required. If the model doesn't accept\n             a certain modality or no input sizes are provided, the dict value is set to an empty list.\n         \"\"\"\n@@ -316,7 +316,7 @@ def process_images(\n         This method forwards the `images` and `kwargs` arguments to the image processor.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n@@ -341,7 +341,7 @@ def process_images(\n \n     def process_queries(\n         self,\n-        text: Union[TextInput, List[TextInput]],\n+        text: Union[TextInput, list[TextInput]],\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -351,7 +351,7 @@ def process_queries(\n         This method forwards the `text` and `kwargs` arguments to the tokenizer.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -375,8 +375,8 @@ def process_queries(\n \n     def score_retrieval(\n         self,\n-        query_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n-        passage_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n+        query_embeddings: Union[\"torch.Tensor\", list[\"torch.Tensor\"]],\n+        passage_embeddings: Union[\"torch.Tensor\", list[\"torch.Tensor\"]],\n         batch_size: int = 128,\n         output_dtype: Optional[\"torch.dtype\"] = None,\n         output_device: Union[\"torch.device\", str] = \"cpu\",\n@@ -393,8 +393,8 @@ def score_retrieval(\n             obtained by padding the list of tensors.\n \n         Args:\n-            query_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Query embeddings.\n-            passage_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Passage embeddings.\n+            query_embeddings (`Union[torch.Tensor, list[torch.Tensor]`): Query embeddings.\n+            passage_embeddings (`Union[torch.Tensor, list[torch.Tensor]`): Passage embeddings.\n             batch_size (`int`, *optional*, defaults to 128): Batch size for computing scores.\n             output_dtype (`torch.dtype`, *optional*, defaults to `torch.float32`): The dtype of the output tensor.\n                 If `None`, the dtype of the input embeddings is used.\n@@ -419,10 +419,10 @@ def score_retrieval(\n         if output_dtype is None:\n             output_dtype = query_embeddings[0].dtype\n \n-        scores: List[torch.Tensor] = []\n+        scores: list[torch.Tensor] = []\n \n         for i in range(0, len(query_embeddings), batch_size):\n-            batch_scores: List[torch.Tensor] = []\n+            batch_scores: list[torch.Tensor] = []\n             batch_queries = torch.nn.utils.rnn.pad_sequence(\n                 query_embeddings[i : i + batch_size], batch_first=True, padding_value=0\n             )"
        },
        {
            "sha": "bab31fae74fefc654b48dc3bde22b62125beca30",
            "filename": "src/transformers/models/colqwen2/configuration_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconfiguration_colqwen2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n \n \n from copy import deepcopy\n-from typing import Any, Dict\n+from typing import Any\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n@@ -54,7 +54,7 @@ class ColQwen2Config(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"colqwen2\"\n-    sub_configs: Dict[str, Any] = {\"vlm_config\": PretrainedConfig}\n+    sub_configs: dict[str, Any] = {\"vlm_config\": PretrainedConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "455643b1ac574a8ee17fc5509cd6f8e624c18935",
            "filename": "src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fconvert_colqwen2_weights_to_hf.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -35,7 +35,7 @@\n import argparse\n import glob\n from pathlib import Path\n-from typing import Any, Dict, Optional\n+from typing import Any, Optional\n \n import torch\n from huggingface_hub import snapshot_download\n@@ -54,7 +54,7 @@\n ORIGINAL_DTYPE = torch.bfloat16\n \n \n-def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> Dict[str, torch.Tensor]:\n+def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> dict[str, torch.Tensor]:\n     directory_path = snapshot_download(\n         repo_id=model_id,\n         revision=revision,\n@@ -75,8 +75,8 @@ def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> D\n     return original_state_dict\n \n \n-def rename_state_dict_keys(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n-    new_state_dict: Dict[str, Any] = {}\n+def rename_state_dict_keys(state_dict: dict[str, Any]) -> dict[str, Any]:\n+    new_state_dict: dict[str, Any] = {}\n     for key, value in state_dict.items():\n         if key.startswith(\"custom_text_proj\"):\n             new_key = key.replace(\"custom_text_proj\", \"embedding_proj_layer\")"
        },
        {
            "sha": "a38746fe12ddc5f3ab44a0b7a4129d370572e93c",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n from torch import nn\n \n@@ -92,9 +92,9 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.Tensor] = None\n-    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @auto_docstring(\n@@ -137,7 +137,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "892a2a68700991b9f557566df6c8e9c622f6f800",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Union\n \n from transformers.models.colpali.modeling_colpali import ColPaliForRetrieval, ColPaliPreTrainedModel\n from transformers.models.colpali.processing_colpali import ColPaliProcessor\n@@ -93,7 +93,7 @@ def __init__(\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n@@ -110,11 +110,11 @@ def __call__(\n         Please refer to the doctsring of the above two methods for more information.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -209,7 +209,7 @@ def __call__(\n             if suffix is None:\n                 suffix = self.query_augmentation_token * 10\n \n-            texts_query: List[str] = []\n+            texts_query: list[str] = []\n \n             for query in text:\n                 augmented_query = self.query_prefix + query + suffix\n@@ -261,9 +261,9 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     embeddings: Optional[torch.Tensor] = None\n-    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @auto_docstring(\n@@ -288,7 +288,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         labels: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "e283f57396060a8fa7ca28f64def4e7aff67c1f4",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -19,7 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import List, Optional, Union\n+from typing import Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image\n@@ -93,7 +93,7 @@ def __init__(\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n@@ -110,11 +110,11 @@ def __call__(\n         Please refer to the doctsring of the above two methods for more information.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -209,7 +209,7 @@ def __call__(\n             if suffix is None:\n                 suffix = self.query_augmentation_token * 10\n \n-            texts_query: List[str] = []\n+            texts_query: list[str] = []\n \n             for query in text:\n                 augmented_query = self.query_prefix + query + suffix\n@@ -228,10 +228,10 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n \n         Args:\n-            image_sizes (List[List[str]], *optional*):\n+            image_sizes (list[list[str]], *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n-            Dict[str, List[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n+            dict[str, list[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n             to a list containing the number of placeholder tokens required. If the model doesn't accept\n             a certain modality or no input sizes are provided, the dict value is set to an empty list.\n         \"\"\"\n@@ -283,7 +283,7 @@ def process_images(\n         This method forwards the `images` and `kwargs` arguments to the image processor.\n \n         Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n                 number of channels, H and W are image height and width.\n@@ -308,7 +308,7 @@ def process_images(\n \n     def process_queries(\n         self,\n-        text: Union[TextInput, List[TextInput]],\n+        text: Union[TextInput, list[TextInput]],\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -318,7 +318,7 @@ def process_queries(\n         This method forwards the `text` and `kwargs` arguments to the tokenizer.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+            text (`str`, `list[str]`, `list[list[str]]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n@@ -342,8 +342,8 @@ def process_queries(\n \n     def score_retrieval(\n         self,\n-        query_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n-        passage_embeddings: Union[\"torch.Tensor\", List[\"torch.Tensor\"]],\n+        query_embeddings: Union[\"torch.Tensor\", list[\"torch.Tensor\"]],\n+        passage_embeddings: Union[\"torch.Tensor\", list[\"torch.Tensor\"]],\n         batch_size: int = 128,\n         output_dtype: Optional[\"torch.dtype\"] = None,\n         output_device: Union[\"torch.device\", str] = \"cpu\",\n@@ -360,8 +360,8 @@ def score_retrieval(\n             obtained by padding the list of tensors.\n \n         Args:\n-            query_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Query embeddings.\n-            passage_embeddings (`Union[torch.Tensor, List[torch.Tensor]`): Passage embeddings.\n+            query_embeddings (`Union[torch.Tensor, list[torch.Tensor]`): Query embeddings.\n+            passage_embeddings (`Union[torch.Tensor, list[torch.Tensor]`): Passage embeddings.\n             batch_size (`int`, *optional*, defaults to 128): Batch size for computing scores.\n             output_dtype (`torch.dtype`, *optional*, defaults to `torch.float32`): The dtype of the output tensor.\n                 If `None`, the dtype of the input embeddings is used.\n@@ -386,10 +386,10 @@ def score_retrieval(\n         if output_dtype is None:\n             output_dtype = query_embeddings[0].dtype\n \n-        scores: List[torch.Tensor] = []\n+        scores: list[torch.Tensor] = []\n \n         for i in range(0, len(query_embeddings), batch_size):\n-            batch_scores: List[torch.Tensor] = []\n+            batch_scores: list[torch.Tensor] = []\n             batch_queries = torch.nn.utils.rnn.pad_sequence(\n                 query_embeddings[i : i + batch_size], batch_first=True, padding_value=0\n             )"
        },
        {
            "sha": "c0f41e93e6e108cc570dbd0fc6709387fbdb0400",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 100,
            "deletions": 100,
            "changes": 200,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=508a7040556dc6b45f09174c662a9632284b2445",
            "patch": "@@ -18,7 +18,7 @@\n import pathlib\n from collections import defaultdict\n from collections.abc import Iterable\n-from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n+from typing import Any, Callable, Optional, Union\n \n import numpy as np\n \n@@ -89,12 +89,12 @@\n \n \n # Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> Tuple[int, int]:\n+def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size.\n \n     Args:\n-        image_size (`Tuple[int, int]`):\n+        image_size (`tuple[int, int]`):\n             The input image size.\n         size (`int`):\n             The desired output size.\n@@ -131,10 +131,10 @@ def get_size_with_aspect_ratio(image_size, size, max_size=None) -> Tuple[int, in\n # Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n-    size: Union[int, Tuple[int, int], List[int]],\n+    size: Union[int, tuple[int, int], list[int]],\n     max_size: Optional[int] = None,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-) -> Tuple[int, int]:\n+) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image size and the desired output size. If the desired output size\n     is a tuple or list, the output image size is returned as is. If the desired output size is an integer, the output\n@@ -143,7 +143,7 @@ def get_resize_output_image_size(\n     Args:\n         input_image (`np.ndarray`):\n             The image to resize.\n-        size (`int` or `Tuple[int, int]` or `List[int]`):\n+        size (`int` or `tuple[int, int]` or `list[int]`):\n             The desired output size.\n         max_size (`int`, *optional*):\n             The maximum allowed output size.\n@@ -163,7 +163,7 @@ def get_image_size_for_max_height_width(\n     max_height: int,\n     max_width: int,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-) -> Tuple[int, int]:\n+) -> tuple[int, int]:\n     \"\"\"\n     Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n     Important, even if image_height < max_height and image_width < max_width, the image will be resized\n@@ -233,7 +233,7 @@ def safe_squeeze(arr: np.ndarray, axis: Optional[int] = None) -> np.ndarray:\n \n \n # Copied from transformers.models.detr.image_processing_detr.normalize_annotation\n-def normalize_annotation(annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+def normalize_annotation(annotation: dict, image_size: tuple[int, int]) -> dict:\n     image_height, image_width = image_size\n     norm_annotation = {}\n     for key, value in annotation.items():\n@@ -248,7 +248,7 @@ def normalize_annotation(annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n \n \n # Copied from transformers.models.detr.image_processing_detr.max_across_indices\n-def max_across_indices(values: Iterable[Any]) -> List[Any]:\n+def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n     Return the maximum value across all indices of an iterable of values.\n     \"\"\"\n@@ -257,8 +257,8 @@ def max_across_indices(values: Iterable[Any]) -> List[Any]:\n \n # Copied from transformers.models.detr.image_processing_detr.get_max_height_width\n def get_max_height_width(\n-    images: List[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n-) -> List[int]:\n+    images: list[np.ndarray], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+) -> list[int]:\n     \"\"\"\n     Get the maximum height and width across all images in a batch.\n     \"\"\"\n@@ -276,15 +276,15 @@ def get_max_height_width(\n \n # Copied from transformers.models.detr.image_processing_detr.make_pixel_mask\n def make_pixel_mask(\n-    image: np.ndarray, output_size: Tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n+    image: np.ndarray, output_size: tuple[int, int], input_data_format: Optional[Union[str, ChannelDimension]] = None\n ) -> np.ndarray:\n     \"\"\"\n     Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n \n     Args:\n         image (`np.ndarray`):\n             Image to make the pixel mask for.\n-        output_size (`Tuple[int, int]`):\n+        output_size (`tuple[int, int]`):\n             Output size of the mask.\n     \"\"\"\n     input_height, input_width = get_image_size(image, channel_dim=input_data_format)\n@@ -299,7 +299,7 @@ def convert_coco_poly_to_mask(segmentations, height: int, width: int) -> np.ndar\n     Convert a COCO polygon annotation to a mask.\n \n     Args:\n-        segmentations (`List[List[float]]`):\n+        segmentations (`list[list[float]]`):\n             List of polygons, each polygon represented by a list of x-y coordinates.\n         height (`int`):\n             Height of the mask.\n@@ -427,11 +427,11 @@ def masks_to_boxes(masks: np.ndarray) -> np.ndarray:\n # Copied from transformers.models.detr.image_processing_detr.prepare_coco_panoptic_annotation with DETR->ConditionalDetr\n def prepare_coco_panoptic_annotation(\n     image: np.ndarray,\n-    target: Dict,\n+    target: dict,\n     masks_path: Union[str, pathlib.Path],\n     return_masks: bool = True,\n     input_data_format: Union[ChannelDimension, str] = None,\n-) -> Dict:\n+) -> dict:\n     \"\"\"\n     Prepare a coco panoptic annotation for ConditionalDetr.\n     \"\"\"\n@@ -468,7 +468,7 @@ def prepare_coco_panoptic_annotation(\n \n # Copied from transformers.models.detr.image_processing_detr.get_segmentation_image\n def get_segmentation_image(\n-    masks: np.ndarray, input_size: Tuple, target_size: Tuple, stuff_equiv_classes, deduplicate=False\n+    masks: np.ndarray, input_size: tuple, target_size: tuple, stuff_equiv_classes, deduplicate=False\n ):\n     h, w = input_size\n     final_h, final_w = target_size\n@@ -493,7 +493,7 @@ def get_segmentation_image(\n \n \n # Copied from transformers.models.detr.image_processing_detr.get_mask_area\n-def get_mask_area(seg_img: np.ndarray, target_size: Tuple[int, int], n_classes: int) -> np.ndarray:\n+def get_mask_area(seg_img: np.ndarray, target_size: tuple[int, int], n_classes: int) -> np.ndarray:\n     final_h, final_w = target_size\n     np_seg_img = seg_img.astype(np.uint8)\n     np_seg_img = np_seg_img.reshape(final_h, final_w, 3)\n@@ -503,7 +503,7 @@ def get_mask_area(seg_img: np.ndarray, target_size: Tuple[int, int], n_classes:\n \n \n # Copied from transformers.models.detr.image_processing_detr.score_labels_from_class_probabilities\n-def score_labels_from_class_probabilities(logits: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n+def score_labels_from_class_probabilities(logits: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n     probs = scipy.special.softmax(logits, axis=-1)\n     labels = probs.argmax(-1, keepdims=True)\n     scores = np.take_along_axis(probs, labels, axis=-1)\n@@ -516,11 +516,11 @@ def post_process_panoptic_sample(\n     out_logits: np.ndarray,\n     masks: np.ndarray,\n     boxes: np.ndarray,\n-    processed_size: Tuple[int, int],\n-    target_size: Tuple[int, int],\n-    is_thing_map: Dict,\n+    processed_size: tuple[int, int],\n+    target_size: tuple[int, int],\n+    is_thing_map: dict,\n     threshold=0.85,\n-) -> Dict:\n+) -> dict:\n     \"\"\"\n     Converts the output of [`ConditionalDetrForSegmentation`] into panoptic segmentation predictions for a single sample.\n \n@@ -532,10 +532,10 @@ def post_process_panoptic_sample(\n         boxes (`torch.Tensor`):\n             The prediced bounding boxes for this sample. The boxes are in the normalized format `(center_x, center_y,\n             width, height)` and values between `[0, 1]`, relative to the size the image (disregarding padding).\n-        processed_size (`Tuple[int, int]`):\n+        processed_size (`tuple[int, int]`):\n             The processed size of the image `(height, width)`, as returned by the preprocessing step i.e. the size\n             after data augmentation but before batching.\n-        target_size (`Tuple[int, int]`):\n+        target_size (`tuple[int, int]`):\n             The target size of the image, `(height, width)` corresponding to the requested final size of the\n             prediction.\n         is_thing_map (`Dict`):\n@@ -599,21 +599,21 @@ def post_process_panoptic_sample(\n \n # Copied from transformers.models.detr.image_processing_detr.resize_annotation\n def resize_annotation(\n-    annotation: Dict[str, Any],\n-    orig_size: Tuple[int, int],\n-    target_size: Tuple[int, int],\n+    annotation: dict[str, Any],\n+    orig_size: tuple[int, int],\n+    target_size: tuple[int, int],\n     threshold: float = 0.5,\n     resample: PILImageResampling = PILImageResampling.NEAREST,\n ):\n     \"\"\"\n     Resizes an annotation to a target size.\n \n     Args:\n-        annotation (`Dict[str, Any]`):\n+        annotation (`dict[str, Any]`):\n             The annotation dictionary.\n-        orig_size (`Tuple[int, int]`):\n+        orig_size (`tuple[int, int]`):\n             The original size of the input image.\n-        target_size (`Tuple[int, int]`):\n+        target_size (`tuple[int, int]`):\n             The target size of the image, as returned by the preprocessing `resize` step.\n         threshold (`float`, *optional*, defaults to 0.5):\n             The threshold used to binarize the segmentation masks.\n@@ -681,7 +681,7 @@ def convert_segmentation_to_rle(segmentation):\n         segmentation (`torch.Tensor` or `numpy.array`):\n             A segmentation map of shape `(height, width)` where each value denotes a segment or class id.\n     Returns:\n-        `List[List]`: A list of lists, where each list is the run-length encoding of a segment / class id.\n+        `list[List]`: A list of lists, where each list is the run-length encoding of a segment / class id.\n     \"\"\"\n     segment_ids = torch.unique(segmentation)\n \n@@ -712,7 +712,7 @@ def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_\n     Raises:\n         `ValueError`: Raised when the first dimension doesn't match in all input tensors.\n     Returns:\n-        `Tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`]`: The `masks`, `scores` and `labels` without the region\n+        `tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`]`: The `masks`, `scores` and `labels` without the region\n         < `object_mask_threshold`.\n     \"\"\"\n     if not (masks.shape[0] == scores.shape[0] == labels.shape[0]):\n@@ -749,14 +749,14 @@ def compute_segments(\n     pred_labels,\n     mask_threshold: float = 0.5,\n     overlap_mask_area_threshold: float = 0.8,\n-    label_ids_to_fuse: Optional[Set[int]] = None,\n-    target_size: Optional[Tuple[int, int]] = None,\n+    label_ids_to_fuse: Optional[set[int]] = None,\n+    target_size: Optional[tuple[int, int]] = None,\n ):\n     height = mask_probs.shape[1] if target_size is None else target_size[0]\n     width = mask_probs.shape[2] if target_size is None else target_size[1]\n \n     segmentation = torch.zeros((height, width), dtype=torch.int32, device=mask_probs.device)\n-    segments: List[Dict] = []\n+    segments: list[dict] = []\n \n     if target_size is not None:\n         mask_probs = nn.functional.interpolate(\n@@ -770,7 +770,7 @@ def compute_segments(\n     mask_labels = mask_probs.argmax(0)  # [height, width]\n \n     # Keep track of instances of each class\n-    stuff_memory_list: Dict[str, int] = {}\n+    stuff_memory_list: dict[str, int] = {}\n     for k in range(pred_labels.shape[0]):\n         pred_class = pred_labels[k].item()\n         should_fuse = pred_class in label_ids_to_fuse\n@@ -814,7 +814,7 @@ class ConditionalDetrImageProcessor(BaseImageProcessor):\n         do_resize (`bool`, *optional*, defaults to `True`):\n             Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be\n             overridden by the `do_resize` parameter in the `preprocess` method.\n-        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n+        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n             Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n             in the `preprocess` method. Available options are:\n                 - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n@@ -836,10 +836,10 @@ class ConditionalDetrImageProcessor(BaseImageProcessor):\n         do_normalize:\n             Controls whether to normalize the image. Can be overridden by the `do_normalize` parameter in the\n             `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n             Mean values to use when normalizing the image. Can be a single value or a list of values, one for each\n             channel. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n+        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n             Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n             for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_annotations (`bool`, *optional*, defaults to `True`):\n@@ -851,7 +851,7 @@ class ConditionalDetrImageProcessor(BaseImageProcessor):\n             method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n             If `pad_size` is provided, the image will be padded to the specified dimensions.\n             Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n+        pad_size (`dict[str, int]`, *optional*):\n             The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n             provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n             height and width in the batch.\n@@ -864,16 +864,16 @@ def __init__(\n         self,\n         format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n         do_resize: bool = True,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_annotations: Optional[bool] = None,\n         do_pad: bool = True,\n-        pad_size: Optional[Dict[str, int]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> None:\n         if \"pad_and_return_pixel_mask\" in kwargs:\n@@ -932,7 +932,7 @@ def __init__(\n \n     @classmethod\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.from_dict with Detr->ConditionalDetr\n-    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n         created using from_dict and kwargs e.g. `ConditionalDetrImageProcessor.from_pretrained(checkpoint, size=600,\n@@ -949,12 +949,12 @@ def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n     def prepare_annotation(\n         self,\n         image: np.ndarray,\n-        target: Dict,\n+        target: dict,\n         format: Optional[AnnotationFormat] = None,\n         return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> Dict:\n+    ) -> dict:\n         \"\"\"\n         Prepare an annotation for feeding into ConditionalDetr model.\n         \"\"\"\n@@ -982,7 +982,7 @@ def prepare_annotation(\n     def resize(\n         self,\n         image: np.ndarray,\n-        size: Dict[str, int],\n+        size: dict[str, int],\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -995,7 +995,7 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n-            size (`Dict[str, int]`):\n+            size (`dict[str, int]`):\n                 Size of the image's `(height, width)` dimensions after resizing. Available options are:\n                     - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n                         Do NOT keep the aspect ratio.\n@@ -1054,7 +1054,7 @@ def resize_annotation(\n         orig_size,\n         size,\n         resample: PILImageResampling = PILImageResampling.NEAREST,\n-    ) -> Dict:\n+    ) -> dict:\n         \"\"\"\n         Resize the annotation to match the resized image. If size is an int, smaller edge of the mask will be matched\n         to this number.\n@@ -1091,7 +1091,7 @@ def rescale(\n         return rescale(image, rescale_factor, data_format=data_format, input_data_format=input_data_format)\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.normalize_annotation\n-    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+    def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict:\n         \"\"\"\n         Normalize the boxes in the annotation from `[top_left_x, top_left_y, bottom_right_x, bottom_right_y]` to\n         `[center_x, center_y, width, height]` format and from absolute to relative pixel values.\n@@ -1101,12 +1101,12 @@ def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) ->\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor._update_annotation_for_padded_image\n     def _update_annotation_for_padded_image(\n         self,\n-        annotation: Dict,\n-        input_image_size: Tuple[int, int],\n-        output_image_size: Tuple[int, int],\n+        annotation: dict,\n+        input_image_size: tuple[int, int],\n+        output_image_size: tuple[int, int],\n         padding,\n         update_bboxes,\n-    ) -> Dict:\n+    ) -> dict:\n         \"\"\"\n         Update the annotation for a padded image.\n         \"\"\"\n@@ -1146,8 +1146,8 @@ def _update_annotation_for_padded_image(\n     def _pad_image(\n         self,\n         image: np.ndarray,\n-        output_size: Tuple[int, int],\n-        annotation: Optional[Dict[str, Any]] = None,\n+        output_size: tuple[int, int],\n+        annotation: Optional[dict[str, Any]] = None,\n         constant_values: Union[float, Iterable[float]] = 0,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -1179,24 +1179,24 @@ def _pad_image(\n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.pad\n     def pad(\n         self,\n-        images: List[np.ndarray],\n-        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        images: list[np.ndarray],\n+        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n         constant_values: Union[float, Iterable[float]] = 0,\n         return_pixel_mask: bool = True,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         update_bboxes: bool = True,\n-        pad_size: Optional[Dict[str, int]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n     ) -> BatchFeature:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n         in the batch and optionally returns their corresponding pixel mask.\n \n         Args:\n-            images (List[`np.ndarray`]):\n+            images (list[`np.ndarray`]):\n                 Images to pad.\n-            annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+            annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n                 Annotations to transform according to the padding that is applied to the images.\n             constant_values (`float` or `Iterable[float]`, *optional*):\n                 The value to use for the padding if `mode` is `\"constant\"`.\n@@ -1217,7 +1217,7 @@ def pad(\n                 Whether to update the bounding boxes in the annotations to match the padded images. If the\n                 bounding boxes have not been converted to relative coordinates and `(centre_x, centre_y, width, height)`\n                 format, the bounding boxes will not be updated.\n-            pad_size (`Dict[str, int]`, *optional*):\n+            pad_size (`dict[str, int]`, *optional*):\n                 The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n@@ -1266,24 +1266,24 @@ def pad(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = None,\n         return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n-        size: Optional[Dict[str, int]] = None,\n+        size: Optional[dict[str, int]] = None,\n         resample=None,  # PILImageResampling\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[Union[int, float]] = None,\n         do_normalize: Optional[bool] = None,\n         do_convert_annotations: Optional[bool] = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n         do_pad: Optional[bool] = None,\n         format: Optional[Union[str, AnnotationFormat]] = None,\n         return_tensors: Optional[Union[TensorType, str]] = None,\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        pad_size: Optional[Dict[str, int]] = None,\n+        pad_size: Optional[dict[str, int]] = None,\n         **kwargs,\n     ) -> BatchFeature:\n         \"\"\"\n@@ -1293,15 +1293,15 @@ def preprocess(\n             images (`ImageInput`):\n                 Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n                 from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+            annotations (`AnnotationType` or `list[AnnotationType]`, *optional*):\n                 List of annotations associated with the image or batch of images. If annotation is for object\n                 detection, the annotations should be a dictionary with the following keys:\n                 - \"image_id\" (`int`): The image id.\n-                - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                - \"annotations\" (`list[Dict]`): List of annotations for an image. Each annotation should be a\n                   dictionary. An image can have no annotations, in which case the list should be empty.\n                 If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n                 - \"image_id\" (`int`): The image id.\n-                - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                - \"segments_info\" (`list[Dict]`): List of segments for an image. Each segment should be a dictionary.\n                   An image can have no segments, in which case the list should be empty.\n                 - \"file_name\" (`str`): The file name of the image.\n             return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n@@ -1310,7 +1310,7 @@ def preprocess(\n                 Path to the directory containing the segmentation masks.\n             do_resize (`bool`, *optional*, defaults to self.do_resize):\n                 Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to self.size):\n+            size (`dict[str, int]`, *optional*, defaults to self.size):\n                 Size of the image's `(height, width)` dimensions after resizing. Available options are:\n                     - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n                         Do NOT keep the aspect ratio.\n@@ -1332,9 +1332,9 @@ def preprocess(\n                 Whether to convert the annotations to the format expected by the model. Converts the bounding\n                 boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n                 and in relative coordinates.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to self.image_mean):\n+            image_mean (`float` or `list[float]`, *optional*, defaults to self.image_mean):\n                 Mean to use when normalizing the image.\n-            image_std (`float` or `List[float]`, *optional*, defaults to self.image_std):\n+            image_std (`float` or `list[float]`, *optional*, defaults to self.image_std):\n                 Standard deviation to use when normalizing the image.\n             do_pad (`bool`, *optional*, defaults to self.do_pad):\n                 Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n@@ -1355,7 +1355,7 @@ def preprocess(\n                 - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            pad_size (`Dict[str, int]`, *optional*):\n+            pad_size (`dict[str, int]`, *optional*):\n                 The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n                 provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n                 height and width in the batch.\n@@ -1541,7 +1541,7 @@ def post_process(self, outputs, target_sizes):\n                 image size (before any data augmentation). For visualization, this should be the image size after data\n                 augment, but before padding.\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n             in the batch as predicted by the model.\n         \"\"\"\n         logging.warning_once(\n@@ -1575,7 +1575,7 @@ def post_process(self, outputs, target_sizes):\n \n     # Copied from transformers.models.deformable_detr.image_processing_deformable_detr.DeformableDetrImageProcessor.post_process_object_detection with DeformableDetr->ConditionalDetr\n     def post_process_object_detection(\n-        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, list[tuple]] = None, top_k: int = 100\n     ):\n         \"\"\"\n         Converts the raw output of [`ConditionalDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n@@ -1586,14 +1586,14 @@ def post_process_object_detection(\n                 Raw outputs of the model.\n             threshold (`float`, *optional*):\n                 Score threshold to keep object detection predictions.\n-            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n-                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+            target_sizes (`torch.Tensor` or `list[tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`tuple[int, int]`) containing the target size\n                 (height, width) of each image in the batch. If left to None, predictions will not be resized.\n             top_k (`int`, *optional*, defaults to 100):\n                 Keep only top k bounding boxes before filtering by thresholding.\n \n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n             in the batch as predicted by the model.\n         \"\"\"\n         out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n@@ -1616,7 +1616,7 @@ def post_process_object_detection(\n \n         # and from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n-            if isinstance(target_sizes, List):\n+            if isinstance(target_sizes, list):\n                 img_h = torch.Tensor([i[0] for i in target_sizes])\n                 img_w = torch.Tensor([i[1] for i in target_sizes])\n             else:\n@@ -1634,18 +1634,18 @@ def post_process_object_detection(\n         return results\n \n     # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation with Detr->ConditionalDetr\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[List[Tuple[int, int]]] = None):\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple[int, int]]] = None):\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n \n         Args:\n             outputs ([`ConditionalDetrForSegmentation`]):\n                 Raw outputs of the model.\n-            target_sizes (`List[Tuple[int, int]]`, *optional*):\n-                A list of tuples (`Tuple[int, int]`) containing the target size (height, width) of each image in the\n+            target_sizes (`list[tuple[int, int]]`, *optional*):\n+                A list of tuples (`tuple[int, int]`) containing the target size (height, width) of each image in the\n                 batch. If unset, predictions will not be resized.\n         Returns:\n-            `List[torch.Tensor]`:\n+            `list[torch.Tensor]`:\n                 A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n                 corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n                 `torch.Tensor` correspond to a semantic class id.\n@@ -1688,9 +1688,9 @@ def post_process_instance_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        target_sizes: Optional[List[Tuple[int, int]]] = None,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n         return_coco_annotation: Optional[bool] = False,\n-    ) -> List[Dict]:\n+    ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n \n@@ -1704,16 +1704,16 @@ def post_process_instance_segmentation(\n             overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n                 The overlap mask area threshold to merge or discard small disconnected parts within each binary\n                 instance mask.\n-            target_sizes (`List[Tuple]`, *optional*):\n-                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+            target_sizes (`list[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`tuple[int, int]]`) corresponds to the requested\n                 final size (height, width) of each prediction. If unset, predictions will not be resized.\n             return_coco_annotation (`bool`, *optional*):\n                 Defaults to `False`. If set to `True`, segmentation maps are returned in COCO run-length encoding (RLE)\n                 format.\n         Returns:\n-            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            `list[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n             - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n-              `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n+              `list[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n               `True`. Set to `None` if no mask if found above `threshold`.\n             - **segments_info** -- A dictionary that contains additional information on each segment.\n                 - **id** -- An integer representing the `segment_id`.\n@@ -1732,7 +1732,7 @@ def post_process_instance_segmentation(\n         pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n \n         # Loop over items in batch size\n-        results: List[Dict[str, TensorType]] = []\n+        results: list[dict[str, TensorType]] = []\n \n         for i in range(batch_size):\n             mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n@@ -1772,9 +1772,9 @@ def post_process_panoptic_segmentation(\n         threshold: float = 0.5,\n         mask_threshold: float = 0.5,\n         overlap_mask_area_threshold: float = 0.8,\n-        label_ids_to_fuse: Optional[Set[int]] = None,\n-        target_sizes: Optional[List[Tuple[int, int]]] = None,\n-    ) -> List[Dict]:\n+        label_ids_to_fuse: Optional[set[int]] = None,\n+        target_sizes: Optional[list[tuple[int, int]]] = None,\n+    ) -> list[dict]:\n         \"\"\"\n         Converts the output of [`ConditionalDetrForSegmentation`] into image panoptic segmentation predictions. Only supports\n         PyTorch.\n@@ -1793,11 +1793,11 @@ def post_process_panoptic_segmentation(\n                 The labels in this state will have all their instances be fused together. For instance we could say\n                 there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n                 set, but not the one for person.\n-            target_sizes (`List[Tuple]`, *optional*):\n-                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+            target_sizes (`list[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`tuple[int, int]]`) corresponds to the requested\n                 final size (height, width) of each prediction in batch. If unset, predictions will not be resized.\n         Returns:\n-            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            `list[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n             - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n               `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized to\n               the corresponding `target_sizes` entry.\n@@ -1825,7 +1825,7 @@ def post_process_panoptic_segmentation(\n         pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n \n         # Loop over items in batch size\n-        results: List[Dict[str, TensorType]] = []\n+        results: list[dict[str, TensorType]] = []\n \n         for i in range(batch_size):\n             mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects("
        },
        {
            "sha": "36f87b53921693fad496eef3269758d2dde3748b",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "19b1439302eb0981edb484775c155355842c5fe2",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "a0ab099704ebcf3928d3e7b3f172714b39678423",
            "filename": "src/transformers/models/conditional_detr/modular_conditional_detr.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "1a443c575ab0d3bbc87d5fb608ac1c2458b0911d",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "192b88399279224238201174ef622791ec63360d",
            "filename": "src/transformers/models/convbert/modeling_tf_convbert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "e00bf20b17f16893e033f1dede47c83abdc5b7f5",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "e328e7490f4d2ca78a5d2005c2a00227eba33b78",
            "filename": "src/transformers/models/convbert/tokenization_convbert_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "f54cba58cf296e8c8e3bae70a9f2e2ab21e3c660",
            "filename": "src/transformers/models/convnext/configuration_convnext.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "487913d56ebd86e1867b9212a865b6254f0da3b7",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "ef06dce9710cb674ef745ed840003ebca8d069fa",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "ccb78451031096e22014ef15a3512785f5216cb8",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "d2e5b0c6f4a6a457a8ee3d09d0d5ddd9ec3a3dec",
            "filename": "src/transformers/models/convnext/modeling_tf_convnext.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "53f1825ca57c89249645c98aa54c278d8e7b4a61",
            "filename": "src/transformers/models/convnextv2/configuration_convnextv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconfiguration_convnextv2.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "ef9d282b9ef0665a375d76b36b977100b12f0e44",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "9c2484a1ba75fa9834fab48496495834746eccc2",
            "filename": "src/transformers/models/convnextv2/modeling_tf_convnextv2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "fa367aceb5e634b582622965b687c98874087904",
            "filename": "src/transformers/models/cpm/tokenization_cpm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "adfe804960cc685a772e0f81fb4dde2b49ca6007",
            "filename": "src/transformers/models/cpm/tokenization_cpm_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpm%2Ftokenization_cpm_fast.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "f3ecf4930f6e7999f3ef58a5f2ee0b11445a5cd5",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "98fd4df6addd83665a674c759592e4f9b9a38d72",
            "filename": "src/transformers/models/cpmant/tokenization_cpmant.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Ftokenization_cpmant.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "6e56c5f7686e15b8d83d8eb2afafac7b3dc578c9",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "c33fd10b3d24091a529fc8616a46bfa69aa5922a",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "ad374c86e7a59bae81283667d997a70b3981b87b",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "4bfc68504067d142778c6e8f6ed4374fae45a6bd",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "5cd40dbe559bf95929090cc95c3f5fdf7cb86dfa",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "883803741d7e496c17d3d3ef1cd8b8e7f94a01c7",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "6209ad29cc7f16585ebf236bcdb42a0b84b87f77",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        },
        {
            "sha": "5b7935e6404d153e67f89d5c24c4bfd04936a180",
            "filename": "src/transformers/models/ctrl/tokenization_ctrl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/508a7040556dc6b45f09174c662a9632284b2445/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Ftokenization_ctrl.py?ref=508a7040556dc6b45f09174c662a9632284b2445"
        }
    ],
    "stats": {
        "total": 29847,
        "additions": 14906,
        "deletions": 14941
    }
}