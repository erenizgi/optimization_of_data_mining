{
    "author": "3outeille",
    "message": "test ci training for text model only (#42597)\n\n* begin test ci training\n\n* add better logging\n\n* add better logging + training loop\n\n* fix sentence + grad_norm assert\n\n* create circlci config fort training\n\n* fix ci to detect training_ci job\n\n* add -s for pytest CI\n\n* add generate assert as well\n\n* make training ci trigger for every model change instead\n\n* set eos_token_id to 0 otherwise it will stop generating too soon\n\n* refactor\n\n* moving logging and metrics to proper files\n\n* update marker in pyproject.toml\n\n* linting\n\n* linting again\n\n* reduce pytest worker\n\n* fix deadlock in test\n\n* add license\n\n* dont show logs during test\n\n* loosen threshold a bit\n\n* Initialize cache state in RecurrentGemmaRecurrentBlock based on batch… (#42627)\n\n* Initialize cache state in RecurrentGemmaRecurrentBlock based on batch size\n\n* loosen a bit the threshold\n\n* Revert \"loosen a bit the threshold\"\n\nThis reverts commit d3d42e1e3b632e62eab016d3660bd1172fb326eb.\n\n* skipping BLT for now (until it get fixed)",
    "sha": "afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
    "files": [
        {
            "sha": "7231b0eaad1baa4cdfa6de768ae8d875be994f79",
            "filename": ".circleci/create_circleci_config.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/.circleci%2Fcreate_circleci_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/.circleci%2Fcreate_circleci_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fcreate_circleci_config.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -318,6 +318,14 @@ def job_name(self):\n     parallelism=6,\n )\n \n+training_ci_job = CircleCIJob(\n+    \"training_ci\",\n+    additional_env={\"RUN_TRAINING_TESTS\": True},\n+    docker_image=[{\"image\": \"huggingface/transformers-torch-light\"}],\n+    install_steps=[\"uv pip install .\"],\n+    marker=\"is_training_test\",\n+    parallelism=6,\n+)\n \n # We also include a `dummy.py` file in the files to be doc-tested to prevent edge case failure. Otherwise, the pytest\n # hangs forever during test collection while showing `collecting 0 items / 21 errors`. (To see this, we have to remove\n@@ -348,7 +356,8 @@ def job_name(self):\n PIPELINE_TESTS = [pipelines_torch_job]\n REPO_UTIL_TESTS = [repo_utils_job]\n DOC_TESTS = [doc_test_job]\n-ALL_TESTS = REGULAR_TESTS + EXAMPLES_TESTS + PIPELINE_TESTS + REPO_UTIL_TESTS + DOC_TESTS + [custom_tokenizers_job] + [exotic_models_job]  # fmt: skip\n+TRAINING_CI_TESTS = [training_ci_job]\n+ALL_TESTS = REGULAR_TESTS + EXAMPLES_TESTS + PIPELINE_TESTS + REPO_UTIL_TESTS + DOC_TESTS + [custom_tokenizers_job] + [exotic_models_job] + TRAINING_CI_TESTS  # fmt: skip\n \n \n def create_circleci_config(folder=None):"
        },
        {
            "sha": "4137d0fe7e3d1d61d05ae15e26f4f996e1b16132",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -90,6 +90,7 @@ def pytest_configure(config):\n     config.addinivalue_line(\"markers\", \"torch_export_test: mark test which tests torch export functionality\")\n     config.addinivalue_line(\"markers\", \"flash_attn_test: mark test which tests flash attention functionality\")\n     config.addinivalue_line(\"markers\", \"flash_attn_3_test: mark test which tests flash attention 3 functionality\")\n+    config.addinivalue_line(\"markers\", \"training_ci: mark test for training CI validation\")\n \n     os.environ[\"DISABLE_SAFETENSORS_CONVERSION\"] = \"true\"\n "
        },
        {
            "sha": "ce042caa0cd449658a9cfd43f19e9fb96422f945",
            "filename": "docs/source/en/model_doc/afmoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Fafmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Fafmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fafmoe.md?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -13,7 +13,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on {release_date} and added to Hugging Face Transformers on 2025-11-18.*\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-04.*\n \n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">"
        },
        {
            "sha": "9aa82fe0e22342a98161fcb1db4391f88d340a1d",
            "filename": "docs/source/en/model_doc/fast_vlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -14,7 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-*This model was released on 2025-05-06 and added to Hugging Face Transformers on 2025-10-07.*\n+*This model was released on 2025-05-06 and added to Hugging Face Transformers on 2025-12-04.*\n \n # FastVLM\n "
        },
        {
            "sha": "7e6e53c7d00b62d9ba83b68dd9fe4e0e294037f5",
            "filename": "docs/source/en/model_doc/ministral3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -16,6 +16,7 @@ limitations under the License.\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-04.*\n \n \n # Ministral3"
        },
        {
            "sha": "e57ce048dc43c296f783dd2af55ef49cf74290fe",
            "filename": "docs/source/en/model_doc/t5gemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -14,6 +14,7 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-04.*\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "dc8a22c98c0ee31c9b6509a9e71f6a906f8ac5cc",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -66,7 +66,8 @@ markers = [\n     \"flash_attn_3_test: marks tests related to flash attention 3 (deselect with '-m \\\"not flash_attn_3_test\\\"')\",\n     \"flash_attn_test: marks tests related to flash attention (deselect with '-m \\\"not flash_attn_test\\\"')\",\n     \"bitsandbytes: select (or deselect with `not`) bitsandbytes integration tests\",\n-    \"generate: marks tests that use the GenerationTesterMixin\"\n+    \"generate: marks tests that use the GenerationTesterMixin\",\n+    \"is_training_test: marks tests that use the TrainingTesterMixin (deselect with '-m \\\"not is_training_test\\\"')\",\n ]\n log_cli = 1\n log_cli_level = \"WARNING\""
        },
        {
            "sha": "63573208d36b1bcbb2371224576b775f2905dbd5",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -460,6 +460,7 @@ def forward(\n         use_cache: bool = True,\n     ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:\n         _, seq_len, _ = input_states.shape\n+        batch_size = input_states.shape[0]\n \n         y_branch = self.linear_y(input_states)\n         y_branch = self.act_fn(y_branch)\n@@ -468,6 +469,17 @@ def forward(\n         x_branch = x_branch.transpose(1, 2)\n \n         if use_cache:\n+            # Check if cache needs initialization (None or batch size mismatch)\n+            if self.conv1d_state is None or self.conv1d_state.shape[0] != batch_size:\n+                self.conv1d_state = torch.zeros(\n+                    (batch_size, self.hidden_size, self.conv1d_width - 1),\n+                    device=input_states.device,\n+                    dtype=input_states.dtype,\n+                )\n+                self.rg_lru.recurrent_states = torch.zeros(\n+                    (batch_size, self.lru_width), device=input_states.device, dtype=torch.float32\n+                )\n+\n             if cache_position.shape[0] != 1:  # prefill\n                 self.conv1d_state = nn.functional.pad(x_branch, (self.conv1d_width - x_branch.shape[-1] - 1, 0))\n                 x_branch = self.conv_1d(x_branch)[..., :seq_len]"
        },
        {
            "sha": "8ae930c63182ae38b79b382ad1d57489bec9ced3",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 236,
            "deletions": 0,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -267,6 +267,7 @@ def parse_int_from_env(key, default=None):\n _run_staging = parse_flag_from_env(\"HUGGINGFACE_CO_STAGING\", default=False)\n _run_pipeline_tests = parse_flag_from_env(\"RUN_PIPELINE_TESTS\", default=True)\n _run_agent_tests = parse_flag_from_env(\"RUN_AGENT_TESTS\", default=False)\n+_run_training_tests = parse_flag_from_env(\"RUN_TRAINING_TESTS\", default=True)\n \n \n def is_staging_test(test_case):\n@@ -317,6 +318,22 @@ def is_agent_test(test_case):\n             return pytest.mark.is_agent_test()(test_case)\n \n \n+def is_training_test(test_case):\n+    \"\"\"\n+    Decorator marking a test as a training test. If RUN_TRAINING_TESTS is set to a falsy value, those tests will be\n+    skipped.\n+    \"\"\"\n+    if not _run_training_tests:\n+        return unittest.skip(reason=\"test is training test\")(test_case)\n+    else:\n+        try:\n+            import pytest  # We don't need a hard dependency on pytest in the main library\n+        except ImportError:\n+            return test_case\n+        else:\n+            return pytest.mark.is_training_test()(test_case)\n+\n+\n def slow(test_case):\n     \"\"\"\n     Decorator marking a test as slow.\n@@ -4077,3 +4094,222 @@ def write_file(file, content):\n def read_json_file(file):\n     with open(file, \"r\") as fh:\n         return json.load(fh)\n+\n+\n+# =============================================================================\n+# Training CI Utilities - Logging and Memory Monitoring\n+# =============================================================================\n+\n+\n+# ANSI color codes for terminal output\n+class Colors:\n+    \"\"\"ANSI color codes for terminal output formatting.\"\"\"\n+\n+    RESET = \"\\033[0m\"\n+    BOLD = \"\\033[1m\"\n+    DIM = \"\\033[2m\"\n+\n+    # Foreground colors\n+    RED = \"\\033[31m\"\n+    GREEN = \"\\033[32m\"\n+    YELLOW = \"\\033[33m\"\n+    BLUE = \"\\033[34m\"\n+    MAGENTA = \"\\033[35m\"\n+    CYAN = \"\\033[36m\"\n+    WHITE = \"\\033[37m\"\n+\n+    # Bright variants\n+    BRIGHT_RED = \"\\033[91m\"\n+    BRIGHT_GREEN = \"\\033[92m\"\n+    BRIGHT_YELLOW = \"\\033[93m\"\n+    BRIGHT_BLUE = \"\\033[94m\"\n+    BRIGHT_CYAN = \"\\033[96m\"\n+\n+\n+class ColoredFormatter(logging.Formatter):\n+    \"\"\"Custom formatter that adds colors based on log level.\"\"\"\n+\n+    LEVEL_COLORS = {\n+        logging.DEBUG: Colors.DIM + Colors.CYAN,\n+        logging.INFO: Colors.WHITE,\n+        logging.WARNING: Colors.BRIGHT_YELLOW,\n+        logging.ERROR: Colors.BRIGHT_RED,\n+        logging.CRITICAL: Colors.BOLD + Colors.BRIGHT_RED,\n+    }\n+\n+    # Loggers that should be dimmed (less important/verbose)\n+    DIMMED_LOGGERS = {\"httpx\", \"httpcore\", \"urllib3\", \"requests\"}\n+\n+    def __init__(self, fmt: str | None = None, datefmt: str | None = None):\n+        super().__init__(fmt, datefmt)\n+\n+    def format(self, record: logging.LogRecord) -> str:\n+        # Check if this logger should be dimmed\n+        is_dimmed = record.name in self.DIMMED_LOGGERS\n+\n+        if is_dimmed:\n+            # Dim the entire log line for httpx and similar\n+            timestamp = self.formatTime(record, self.datefmt)\n+            message = record.getMessage()\n+            return f\"{Colors.DIM}{timestamp} - {record.name} - {record.levelname:8} - {message}{Colors.RESET}\"\n+\n+        # Get color for this level\n+        color = self.LEVEL_COLORS.get(record.levelno, Colors.RESET)\n+\n+        # Color the level name\n+        levelname = record.levelname\n+        colored_levelname = f\"{color}{levelname:8}{Colors.RESET}\"\n+\n+        # Color the timestamp\n+        colored_time = f\"{Colors.DIM}{self.formatTime(record, self.datefmt)}{Colors.RESET}\"\n+\n+        # Color the logger name\n+        colored_name = f\"{Colors.BLUE}{record.name}{Colors.RESET}\"\n+\n+        # Get message\n+        message = record.getMessage()\n+\n+        return f\"{colored_time} - {colored_name} - {colored_levelname} - {message}\"\n+\n+\n+_warn_once_logged: set[str] = set()\n+\n+\n+def init_test_logger() -> logging.Logger:\n+    \"\"\"Initialize a test-specific logger with colored stderr handler and INFO level for tests.\n+\n+    Uses a named logger instead of root logger to avoid conflicts with pytest-xdist parallel execution.\n+    Uses stderr instead of stdout to avoid deadlocks with pytest-xdist output capture.\n+    \"\"\"\n+    logger = logging.getLogger(\"transformers.training_test\")\n+    logger.setLevel(logging.INFO)\n+\n+    # Only add handler if not already present (avoid duplicate handlers on repeated calls)\n+    if not logger.handlers:\n+        # Use stderr instead of stdout - pytest-xdist captures stdout which can cause deadlocks\n+        ch = logging.StreamHandler(sys.stderr)\n+        ch.setLevel(logging.INFO)\n+\n+        # Use colored formatter if terminal supports it, plain otherwise\n+        if sys.stderr.isatty():\n+            formatter = ColoredFormatter(datefmt=\"%Y-%m-%d %H:%M:%S\")\n+        else:\n+            formatter = logging.Formatter(\n+                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n+            )\n+\n+        ch.setFormatter(formatter)\n+        logger.addHandler(ch)\n+\n+    logger.propagate = False  # Don't propagate to root logger to avoid duplicate output\n+    return logger\n+\n+\n+def warn_once(logger_instance: logging.Logger, msg: str) -> None:\n+    \"\"\"Log a warning message only once per unique message.\n+\n+    Uses a global set to track messages that have already been logged\n+    to prevent duplicate warning messages from cluttering the output.\n+\n+    Args:\n+        logger_instance: The logger instance to use for warning.\n+        msg: The warning message to log.\n+    \"\"\"\n+    if msg not in _warn_once_logged:\n+        logger_instance.warning(msg)\n+        _warn_once_logged.add(msg)\n+\n+\n+# Named tuple for passing memory stats for logging\n+MemoryStats = collections.namedtuple(\n+    \"MemoryStats\",\n+    [\n+        \"rss_gib\",  # Resident Set Size in GiB\n+        \"rss_pct\",  # RSS as percentage of total memory\n+        \"vms_gib\",  # Virtual Memory Size in GiB\n+        \"peak_rss_gib\",  # Peak RSS in GiB\n+        \"peak_rss_pct\",  # Peak RSS as percentage of total memory\n+        \"available_gib\",  # Available system memory in GiB\n+        \"total_gib\",  # Total system memory in GiB\n+    ],\n+)\n+\n+\n+class CPUMemoryMonitor:\n+    \"\"\"Monitor CPU memory usage for the current process.\"\"\"\n+\n+    def __init__(self):\n+        self.device_name = \"CPU\"\n+        self._peak_rss = 0\n+        self._process = None\n+        self.total_memory = 0\n+        self.total_memory_gib = 0\n+\n+        if is_psutil_available():\n+            import psutil\n+\n+            self._process = psutil.Process(os.getpid())\n+            mem_info = psutil.virtual_memory()\n+            self.total_memory = mem_info.total\n+            self.total_memory_gib = self._to_gib(self.total_memory)\n+\n+    def _to_gib(self, memory_in_bytes: int) -> float:\n+        \"\"\"Convert bytes to GiB.\"\"\"\n+        return memory_in_bytes / (1024 * 1024 * 1024)\n+\n+    def _to_pct(self, memory_in_bytes: int) -> float:\n+        \"\"\"Convert bytes to percentage of total memory.\"\"\"\n+        if self.total_memory == 0:\n+            return 0.0\n+        return 100.0 * memory_in_bytes / self.total_memory\n+\n+    def _update_peak(self) -> None:\n+        \"\"\"Update peak memory tracking.\"\"\"\n+        if self._process is not None:\n+            current_rss = self._process.memory_info().rss\n+            self._peak_rss = max(self._peak_rss, current_rss)\n+\n+    def get_stats(self) -> MemoryStats:\n+        \"\"\"Get current memory statistics.\"\"\"\n+        if not is_psutil_available():\n+            return MemoryStats(0, 0, 0, 0, 0, 0, 0)\n+\n+        import psutil\n+\n+        self._update_peak()\n+\n+        mem_info = self._process.memory_info()\n+        sys_mem = psutil.virtual_memory()\n+\n+        return MemoryStats(\n+            rss_gib=self._to_gib(mem_info.rss),\n+            rss_pct=self._to_pct(mem_info.rss),\n+            vms_gib=self._to_gib(mem_info.vms),\n+            peak_rss_gib=self._to_gib(self._peak_rss),\n+            peak_rss_pct=self._to_pct(self._peak_rss),\n+            available_gib=self._to_gib(sys_mem.available),\n+            total_gib=self._to_gib(sys_mem.total),\n+        )\n+\n+    def reset_peak_stats(self) -> None:\n+        \"\"\"Reset peak memory tracking.\"\"\"\n+        if self._process is not None:\n+            self._peak_rss = self._process.memory_info().rss\n+\n+\n+def build_cpu_memory_monitor(logger_instance: logging.Logger | None = None) -> CPUMemoryMonitor:\n+    \"\"\"Build and initialize a CPU memory monitor.\n+\n+    Args:\n+        logger_instance: Optional logger to log initialization info. If None, no logging is done.\n+\n+    Returns:\n+        CPUMemoryMonitor instance.\n+    \"\"\"\n+    monitor = CPUMemoryMonitor()\n+    if logger_instance is not None:\n+        if is_psutil_available():\n+            logger_instance.info(f\"CPU memory monitor initialized: {monitor.total_memory_gib:.2f} GiB total\")\n+        else:\n+            logger_instance.warning(\"psutil not available, memory monitoring disabled\")\n+    return monitor"
        },
        {
            "sha": "c1f058ff808946a0b1faa832e72d7bc954312c96",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -38,6 +38,7 @@\n     torch_device,\n )\n from .test_pipeline_mixin import PipelineTesterMixin\n+from .test_training_mixin import TrainingTesterMixin\n \n \n if is_torch_available():\n@@ -304,7 +305,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class CausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin):\n+class CausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, TrainingTesterMixin):\n     model_tester_class = None\n     all_model_classes = None\n     pipeline_model_mapping = None"
        },
        {
            "sha": "56ee012aa98c6fd37a884f155ba011630cee8868",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -177,6 +177,10 @@ class BltModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = BltForCausalLM if is_torch_available() else None\n \n+    @unittest.skip(\"BLT model requires special handling for training overfit test\")\n+    def test_training_overfit(self):\n+        pass\n+\n     @pytest.mark.generate\n     @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n     @unittest.skip("
        },
        {
            "sha": "f0b894ca32cbc296051ea6b0c005a0b9969b5dfd",
            "filename": "tests/test_training_mixin.py",
            "status": "added",
            "additions": 413,
            "deletions": 0,
            "changes": 413,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/tests%2Ftest_training_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/tests%2Ftest_training_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_training_mixin.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -0,0 +1,413 @@\n+# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Training overfit tester mixin for model tests.\"\"\"\n+\n+import logging\n+import time\n+from abc import ABC, abstractmethod\n+from typing import Optional\n+\n+import torch\n+\n+from transformers import set_seed\n+from transformers.testing_utils import Colors, build_cpu_memory_monitor, init_test_logger, is_training_test\n+\n+\n+logger = logging.getLogger(\"transformers.training_test\")\n+\n+\n+class TrainingTesterMixin(ABC):\n+    \"\"\"\n+    Mixin for training overfit tests. Add to model test classes alongside ModelTesterMixin.\n+\n+    The model_tester (e.g., CausalLMModelTester) already provides:\n+      - get_config() -> tiny model config\n+      - prepare_config_and_inputs_for_common() -> config + input dict\n+      - causal_lm_class, base_model_class, etc.\n+\n+    This mixin adds training-specific tests using that infrastructure.\n+    \"\"\"\n+\n+    # ============================================================\n+    # Training hyperparameters\n+    # ============================================================\n+    training_overfit_steps: int = 300\n+    training_overfit_batch_size: int = 2\n+    training_overfit_learning_rate: float = 1e-3\n+    training_overfit_seq_length: int = 64\n+    training_overfit_log_freq: int = 10\n+\n+    # Loss reduction and grad norm reduction thresholds for passing the test (i.e 95% reduction)\n+    training_loss_reduction_threshold: float = 0.9\n+    training_grad_norm_reduction_threshold: float = 0.9\n+\n+    @property\n+    @abstractmethod\n+    def model_tester(self):\n+        \"\"\"The model tester instance (e.g., CausalLMModelTester).\"\"\"\n+        ...\n+\n+    # ============================================================\n+    # Modality detection\n+    # ============================================================\n+    def _get_model_modality(self) -> str:\n+        \"\"\"Detect the modality of the model based on its input signature.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        if \"input_ids\" in inputs_dict:\n+            return \"text\"\n+        elif \"pixel_values\" in inputs_dict:\n+            return \"image\"\n+        elif \"input_features\" in inputs_dict or \"input_values\" in inputs_dict:\n+            return \"audio\"\n+        else:\n+            raise ValueError(f\"Unknown modality: {inputs_dict}\")\n+\n+    # ============================================================\n+    # Training data creation for each modality\n+    # ============================================================\n+    def _create_text_training_batch(\n+        self,\n+        batch_size: int,\n+        seq_length: int,\n+        vocab_size: int,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"Create a simple text batch without needing a tokenizer.\"\"\"\n+        # Create a deterministic sequence (not random, so model can learn it)\n+        pattern = list(range(1, min(20, vocab_size)))  # tokens 1-19\n+        num_repeats = (seq_length // len(pattern)) + 1\n+        tokens = (pattern * num_repeats)[:seq_length]\n+        input_ids = torch.tensor([tokens] * batch_size, dtype=torch.long)\n+        return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n+\n+    def _create_image_training_batch(\n+        self,\n+        batch_size: int,\n+        num_channels: int,\n+        height: int,\n+        width: int,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"Create fixed batch for image models using a deterministic pattern.\"\"\"\n+        pass\n+\n+    def _create_audio_training_batch(\n+        self,\n+        batch_size: int,\n+        audio_length: int,\n+        feature_size: Optional[int] = None,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"Create fixed batch for audio models using a deterministic waveform.\"\"\"\n+        pass\n+\n+    def _decode_text_tokens(self, tokens: list[int], max_display: int = 40) -> str:\n+        \"\"\"Decode tokens to readable string (maps token IDs to letters: 1->a, 2->b, etc.).\"\"\"\n+        decoded = \"\".join(chr(ord(\"a\") + (t - 1) % 26) for t in tokens)\n+        if len(decoded) > max_display:\n+            return f\"'{decoded[:max_display]}...'\"\n+        return f\"'{decoded}'\"\n+\n+    def _get_trainable_model_class(self):\n+        \"\"\"Get the model class to use for training (prefers *ForCausalLM, *ForSequenceClassification, etc.).\"\"\"\n+        # Prefer model classes with a head (for computing loss)\n+        if hasattr(self.model_tester, \"causal_lm_class\") and self.model_tester.causal_lm_class is not None:\n+            return self.model_tester.causal_lm_class\n+        if (\n+            hasattr(self.model_tester, \"sequence_classification_class\")\n+            and self.model_tester.sequence_classification_class is not None\n+        ):\n+            return self.model_tester.sequence_classification_class\n+        # Fall back to first model class\n+        return self.all_model_classes[0]\n+\n+    @is_training_test\n+    def test_training_overfit(self):\n+        \"\"\"Test that a tiny model can overfit on a fixed batch.\"\"\"\n+        # Initialize logging and memory monitoring\n+        init_test_logger()\n+        memory_monitor = build_cpu_memory_monitor(logger)\n+\n+        logger.info(\"=\" * 70)\n+        logger.info(f\"Starting test: {self._testMethodName}\")\n+        logger.info(\"=\" * 70)\n+\n+        # Skip if model doesn't support training\n+        if not getattr(self.model_tester, \"is_training\", True):\n+            logger.info(f\"{Colors.YELLOW}Skipping: Model tester not configured for training tests{Colors.RESET}\")\n+            self.skipTest(\"Model tester not configured for training tests\")\n+\n+        # Configuration\n+        logger.info(f\"{Colors.BOLD}Job Configuration:{Colors.RESET}\")\n+        logger.info(f\"  {Colors.CYAN}total_steps:{Colors.RESET} {self.training_overfit_steps}\")\n+        logger.info(f\"  {Colors.CYAN}batch_size:{Colors.RESET} {self.training_overfit_batch_size}\")\n+        logger.info(f\"  {Colors.CYAN}learning_rate:{Colors.RESET} {self.training_overfit_learning_rate}\")\n+        logger.info(f\"  {Colors.CYAN}seq_length:{Colors.RESET} {self.training_overfit_seq_length}\")\n+        logger.info(f\"  {Colors.CYAN}log_freq:{Colors.RESET} {self.training_overfit_log_freq}\")\n+        logger.info(f\"  {Colors.CYAN}device:{Colors.RESET} cpu\")\n+\n+        set_seed(42)\n+\n+        logger.info(\"-\" * 70)\n+        logger.info(f\"{Colors.BOLD}Building model{Colors.RESET}\")\n+        load_start = time.perf_counter()\n+\n+        # Get tiny config from existing infrastructure\n+        config = self.model_tester.get_config()\n+\n+        model_class = self._get_trainable_model_class()\n+        model = model_class(config)\n+        model.train()\n+\n+        load_time = time.perf_counter() - load_start\n+        logger.info(f\"Model loaded in {Colors.GREEN}{load_time:.3f}s{Colors.RESET}\")\n+\n+        # Log model architecture\n+        # TODO(3outeille): make sure if there is other parameters to log\n+        logger.info(f\"{Colors.BOLD}Model Architecture:{Colors.RESET}\")\n+        logger.info(f\"  {Colors.CYAN}model_class:{Colors.RESET} {model_class.__name__}\")\n+        if hasattr(config, \"hidden_size\"):\n+            logger.info(f\"  {Colors.CYAN}hidden_size:{Colors.RESET} {config.hidden_size}\")\n+        if hasattr(config, \"num_hidden_layers\"):\n+            logger.info(f\"  {Colors.CYAN}num_hidden_layers:{Colors.RESET} {config.num_hidden_layers}\")\n+        if hasattr(config, \"num_attention_heads\"):\n+            logger.info(f\"  {Colors.CYAN}num_attention_heads:{Colors.RESET} {config.num_attention_heads}\")\n+        if hasattr(config, \"num_key_value_heads\"):\n+            logger.info(f\"  {Colors.CYAN}num_key_value_heads:{Colors.RESET} {config.num_key_value_heads}\")\n+        if hasattr(config, \"intermediate_size\"):\n+            logger.info(f\"  {Colors.CYAN}intermediate_size:{Colors.RESET} {config.intermediate_size}\")\n+        if hasattr(config, \"vocab_size\"):\n+            logger.info(f\"  {Colors.CYAN}vocab_size:{Colors.RESET} {config.vocab_size}\")\n+        if hasattr(config, \"num_experts\"):\n+            logger.info(f\"  {Colors.CYAN}num_experts:{Colors.RESET} {config.num_experts}\")\n+        if hasattr(config, \"num_experts_per_tok\"):\n+            logger.info(f\"  {Colors.CYAN}num_experts_per_tok:{Colors.RESET} {config.num_experts_per_tok}\")\n+\n+        # Count parameters\n+        total_params = sum(p.numel() for p in model.parameters())\n+        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n+        logger.info(\n+            f\"{Colors.CYAN}Model size:{Colors.RESET} {Colors.BRIGHT_GREEN}{total_params:,}{Colors.RESET} total parameters\"\n+        )\n+        logger.info(\n+            f\"{Colors.CYAN}Trainable parameters:{Colors.RESET} {Colors.BRIGHT_GREEN}{trainable_params:,}{Colors.RESET}\"\n+        )\n+\n+        # Memory after model load\n+        mem_stats = memory_monitor.get_stats()\n+        logger.info(\n+            f\"{Colors.MAGENTA}Memory after model load:{Colors.RESET} {mem_stats.rss_gib:.2f} GiB ({mem_stats.rss_pct:.1f}%)\"\n+        )\n+\n+        logger.info(\"-\" * 70)\n+        logger.info(f\"{Colors.BOLD}Creating fixed batch{Colors.RESET}\")\n+\n+        modality = self._get_model_modality()\n+        logger.info(f\"{Colors.CYAN}Detected modality:{Colors.RESET} {modality}\")\n+        _, sample_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        if modality == \"text\":\n+            # For text models, we need a tokenizer - use a simple one or create fake tokens\n+            batch = self._create_text_training_batch(\n+                batch_size=self.training_overfit_batch_size,\n+                seq_length=self.training_overfit_seq_length,\n+                vocab_size=config.vocab_size,\n+            )\n+            logger.info(f\"{Colors.CYAN}Training pattern:{Colors.RESET} Repeating token sequence (1-19)\")\n+        else:\n+            raise ValueError(f\"Modality {modality} not supported yet for training overfit\")\n+\n+        tokens_per_batch = self.training_overfit_batch_size * self.training_overfit_seq_length\n+        logger.info(f\"  {Colors.CYAN}batch_size:{Colors.RESET} {self.training_overfit_batch_size}\")\n+        logger.info(f\"  {Colors.CYAN}seq_length:{Colors.RESET} {self.training_overfit_seq_length}\")\n+        logger.info(f\"  {Colors.CYAN}tokens_per_batch:{Colors.RESET} {tokens_per_batch:,}\")\n+        logger.info(f\"{Colors.DIM}Using same fixed batch every step (deterministic overfitting){Colors.RESET}\")\n+\n+        logger.info(\"-\" * 70)\n+        logger.info(f\"{Colors.BOLD}Building optimizer{Colors.RESET}\")\n+\n+        optimizer = torch.optim.Adam(\n+            model.parameters(), lr=self.training_overfit_learning_rate, weight_decay=0.0, betas=(0.9, 0.999)\n+        )\n+        logger.info(f\"{Colors.CYAN}Optimizer:{Colors.RESET} Adam\")\n+        logger.info(f\"  {Colors.CYAN}learning_rate:{Colors.RESET} {self.training_overfit_learning_rate}\")\n+        logger.info(f\"  {Colors.CYAN}weight_decay:{Colors.RESET} 0.0\")\n+        logger.info(f\"  {Colors.CYAN}betas:{Colors.RESET} (0.9, 0.999)\")\n+\n+        # Training Loop\n+        logger.info(\"-\" * 70)\n+        logger.info(\"Training starts at step 1\")\n+\n+        initial_loss = None\n+        final_loss = None\n+        initial_grad_norm = None\n+        final_grad_norm = None\n+        training_start = time.perf_counter()\n+        memory_monitor.reset_peak_stats()\n+\n+        for step in range(1, self.training_overfit_steps + 1):\n+            step_start = time.perf_counter()\n+\n+            optimizer.zero_grad()\n+            outputs = model(**batch)\n+            loss = outputs.loss\n+\n+            if initial_loss is None:\n+                initial_loss = loss.item()\n+            final_loss = loss.item()\n+\n+            loss.backward()\n+\n+            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n+\n+            if initial_grad_norm is None:\n+                initial_grad_norm = grad_norm.item()\n+            final_grad_norm = grad_norm.item()\n+\n+            optimizer.step()\n+\n+            step_time = time.perf_counter() - step_start\n+\n+            # Log at frequency\n+            if step == 1 or step % self.training_overfit_log_freq == 0 or step == self.training_overfit_steps:\n+                tokens_per_sec = tokens_per_batch / step_time\n+                mem_stats = memory_monitor.get_stats()\n+                logger.info(\n+                    f\"{Colors.CYAN}step:{Colors.RESET} {step}  \"\n+                    f\"{Colors.GREEN}loss:{Colors.RESET} {loss.item():7.4f}  \"\n+                    f\"{Colors.YELLOW}grad_norm:{Colors.RESET} {grad_norm.item():6.4f}  \"\n+                    f\"{Colors.MAGENTA}memory:{Colors.RESET} {mem_stats.rss_gib:.2f}GiB({mem_stats.rss_pct:.1f}%)  \"\n+                    f\"{Colors.BLUE}tok/s:{Colors.RESET} {tokens_per_sec:,.0f}  \"\n+                    f\"{Colors.DIM}step_time:{Colors.RESET} {step_time:.3f}s\"\n+                )\n+\n+        training_time = time.perf_counter() - training_start\n+\n+        # Training Summary\n+        total_tokens = self.training_overfit_steps * tokens_per_batch\n+        logger.info(\"-\" * 70)\n+        logger.info(f\"{Colors.BOLD}Training completed{Colors.RESET}\")\n+        logger.info(f\"Total training time: {training_time:.2f}s\")\n+        logger.info(f\"Total steps: {self.training_overfit_steps}\")\n+        logger.info(f\"Total tokens seen: {total_tokens:,}\")\n+        logger.info(f\"Average tokens/sec: {total_tokens / training_time:,.0f}\")\n+\n+        # Memory summary\n+        mem_stats = memory_monitor.get_stats()\n+        logger.info(f\"{Colors.BOLD}Memory usage:{Colors.RESET}\")\n+        logger.info(\n+            f\"  {Colors.CYAN}current_rss:{Colors.RESET} {mem_stats.rss_gib:.2f} GiB ({mem_stats.rss_pct:.1f}%)\"\n+        )\n+        logger.info(\n+            f\"  {Colors.CYAN}peak_rss:{Colors.RESET} {mem_stats.peak_rss_gib:.2f} GiB ({mem_stats.peak_rss_pct:.1f}%)\"\n+        )\n+        logger.info(\n+            f\"  {Colors.CYAN}available:{Colors.RESET} {mem_stats.available_gib:.2f} GiB / {mem_stats.total_gib:.2f} GiB\"\n+        )\n+\n+        # Loss analysis\n+        loss_reduction = (initial_loss - final_loss) / initial_loss * 100\n+        logger.info(f\"{Colors.BOLD}Loss metrics:{Colors.RESET}\")\n+        logger.info(f\"  {Colors.CYAN}initial_loss:{Colors.RESET} {initial_loss:.4f}\")\n+        logger.info(f\"  {Colors.CYAN}final_loss:{Colors.RESET} {final_loss:.4f}\")\n+        logger.info(f\"  {Colors.CYAN}loss_reduction:{Colors.RESET} {loss_reduction:.1f}%\")\n+\n+        # Grad norm analysis\n+        grad_norm_reduction = (initial_grad_norm - final_grad_norm) / initial_grad_norm * 100\n+        logger.info(f\"{Colors.BOLD}Grad norm metrics:{Colors.RESET}\")\n+        logger.info(f\"  {Colors.CYAN}initial_grad_norm:{Colors.RESET} {initial_grad_norm:.4f}\")\n+        logger.info(f\"  {Colors.CYAN}final_grad_norm:{Colors.RESET} {final_grad_norm:.4f}\")\n+        logger.info(f\"  {Colors.CYAN}grad_norm_reduction:{Colors.RESET} {grad_norm_reduction:.1f}%\")\n+\n+        # Generation Test (only for text/causal LM models)\n+        # TODO(3outeille): handle audio and generate\n+        generation_matches = None\n+        if modality == \"text\" and hasattr(model, \"generate\"):\n+            logger.info(\"-\" * 70)\n+            logger.info(f\"{Colors.BOLD}Testing generation{Colors.RESET}\")\n+\n+            model.eval()\n+\n+            # Get the expected token sequence (same pattern used in training)\n+            expected_tokens = batch[\"input_ids\"][0].tolist()\n+\n+            # Use first token as prompt\n+            prompt_ids = torch.tensor([[expected_tokens[0]]], dtype=torch.long)\n+            num_tokens_to_generate = len(expected_tokens) - 1\n+\n+            logger.info(f\"Prompt: {self._decode_text_tokens([expected_tokens[0]])}\")\n+\n+            with torch.no_grad():\n+                generated_ids = model.generate(\n+                    prompt_ids,\n+                    max_new_tokens=num_tokens_to_generate,\n+                    do_sample=False,\n+                    pad_token_id=config.pad_token_id if hasattr(config, \"pad_token_id\") else 0,\n+                    eos_token_id=0,\n+                )\n+\n+            generated_tokens = generated_ids[0].tolist()\n+\n+            # Compare generated tokens with expected tokens\n+            generation_matches = generated_tokens == expected_tokens\n+\n+            # TODO(3outeille): handle audio and image generation\n+            if generation_matches:\n+                logger.info(f\"Expected:  {Colors.GREEN}{self._decode_text_tokens(expected_tokens)}{Colors.RESET}\")\n+                logger.info(f\"Generated: {Colors.GREEN}{self._decode_text_tokens(generated_tokens)}{Colors.RESET}\")\n+                logger.info(f\"{Colors.GREEN}✓ Generation matches training sequence!{Colors.RESET}\")\n+            else:\n+                logger.info(f\"Expected:  {Colors.GREEN}{self._decode_text_tokens(expected_tokens)}{Colors.RESET}\")\n+                logger.info(f\"Generated: {Colors.RED}{self._decode_text_tokens(generated_tokens)}{Colors.RESET}\")\n+                # Count matching tokens\n+                matches = sum(1 for g, e in zip(generated_tokens, expected_tokens) if g == e)\n+                logger.info(\n+                    f\"{Colors.YELLOW}✗ Generation mismatch: {matches}/{len(expected_tokens)} tokens match{Colors.RESET}\"\n+                )\n+\n+        # Assertions\n+        logger.info(\"-\" * 70)\n+        logger.info(f\"{Colors.BOLD}Running assertions{Colors.RESET}\")\n+\n+        # Assert loss decreased significantly\n+        loss_reduction_ratio = (initial_loss - final_loss) / initial_loss\n+        self.assertGreater(\n+            loss_reduction_ratio,\n+            self.training_loss_reduction_threshold,\n+            f\"Expected loss to decrease by at least {self.training_loss_reduction_threshold * 100:.0f}%, \"\n+            f\"got {loss_reduction:.1f}%\",\n+        )\n+        logger.info(\n+            f\"{Colors.GREEN}✓ Loss decreased by more than {self.training_loss_reduction_threshold * 100:.0f}%{Colors.RESET}\"\n+        )\n+\n+        # Assert grad_norm decreased significantly\n+        grad_norm_reduction_ratio = (initial_grad_norm - final_grad_norm) / initial_grad_norm\n+        self.assertGreater(\n+            grad_norm_reduction_ratio,\n+            self.training_grad_norm_reduction_threshold,\n+            f\"Expected grad_norm to decrease by at least {self.training_grad_norm_reduction_threshold * 100:.0f}%, \"\n+            f\"got {grad_norm_reduction:.1f}%\",\n+        )\n+        logger.info(\n+            f\"{Colors.GREEN}✓ Grad norm decreased by more than {self.training_grad_norm_reduction_threshold * 100:.0f}%{Colors.RESET}\"\n+        )\n+\n+        # Assert generation matches (if applicable)\n+        if generation_matches is not None:\n+            self.assertTrue(generation_matches, \"Expected model to generate the training sequence after overfitting\")\n+            logger.info(f\"{Colors.GREEN}✓ Generated sequence matches training sequence{Colors.RESET}\")\n+\n+        logger.info(\"=\" * 70)\n+        logger.info(f\"Finished test: {self._testMethodName}\")\n+        logger.info(\"=\" * 70)"
        },
        {
            "sha": "c7a9578f5192afd420dfd688235eae22b85cefdb",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afa43c73a2cea3c44d78585e424fc2b4ad2ba068/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=afa43c73a2cea3c44d78585e424fc2b4ad2ba068",
            "patch": "@@ -1100,6 +1100,7 @@ def parse_commit_message(commit_message: str) -> dict[str, bool]:\n     \"pipelines_torch\": r\"tests/models/.*/test_modeling_.*\",\n     \"tests_hub\": r\"tests/.*\",\n     \"tests_non_model\": r\"tests/[^/]*?/test_.*\\.py\",\n+    \"tests_training_ci\": r\"tests/models/.*/test_modeling_.*\",\n }\n \n "
        }
    ],
    "stats": {
        "total": 690,
        "additions": 685,
        "deletions": 5
    }
}