{
    "author": "itazap",
    "message": "handle flash slow tests (#41072)\n\n* handle flash slow tests\n\n* update patch mask to 1/0 for flash\n\n* don't skip flash\n\n* flash\n\n* raise tols\n\n* rm flash support :(\n\n* nits\n\n---------\n\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-173-7.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-171-230.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-95.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-214.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-163-147.ec2.internal>",
    "sha": "071eb5334f5a9ac2c7a13515219be8a272388ec6",
    "files": [
        {
            "sha": "1e677dda4a98116bacde1e578538ed1a7e221995",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/071eb5334f5a9ac2c7a13515219be8a272388ec6/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071eb5334f5a9ac2c7a13515219be8a272388ec6/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=071eb5334f5a9ac2c7a13515219be8a272388ec6",
            "patch": "@@ -426,8 +426,8 @@ class BltPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"BltTransformerLayer\"]\n     _can_compile_fullgraph = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n+    _supports_flash_attn = False\n+    _supports_flex_attn = False\n     _supports_attention_backend = False\n     _can_record_outputs = {\n         \"hidden_states\": OutputRecorder(BltTransformerLayer, index=0, layer_name=\"local_decoder\"),\n@@ -1116,7 +1116,12 @@ def forward(\n         )\n \n         cross_attn_mask_enc = _prepare_patch_cross_attention_mask(\n-            patch_ids, patch_lengths.shape[1], sequence_length, True, self.config.cross_attn_k, encoder_embeds.dtype\n+            patch_ids=patch_ids,\n+            num_patches=patch_lengths.shape[1],\n+            sequence_length=sequence_length,\n+            patches_as_queries=True,\n+            cross_attn_k=self.config.cross_attn_k,\n+            dtype=encoder_embeds.dtype,\n         )\n         encoder_hidden_states, encoder_cross_states = self.local_encoder(\n             input_ids=input_ids,\n@@ -1148,12 +1153,12 @@ def forward(\n         )\n         decoder_patch_ids = self._patch_ids_from_lengths(patch_lengths[:, 1:], sequence_length)\n         cross_attn_mask_dec = _prepare_patch_cross_attention_mask(\n-            decoder_patch_ids,\n-            patch_lengths.shape[1],\n-            sequence_length,\n-            False,\n-            self.config.cross_attn_k,\n-            encoder_embeds.dtype,\n+            patch_ids=decoder_patch_ids,\n+            num_patches=patch_lengths.shape[1],\n+            sequence_length=sequence_length,\n+            patches_as_queries=False,\n+            cross_attn_k=self.config.cross_attn_k,\n+            dtype=encoder_embeds.dtype,\n         )\n         output = self.local_decoder(\n             input_ids=input_ids,"
        },
        {
            "sha": "00b1211fdb088aff4e05b2d67ca51d00cc606da1",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/071eb5334f5a9ac2c7a13515219be8a272388ec6/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071eb5334f5a9ac2c7a13515219be8a272388ec6/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=071eb5334f5a9ac2c7a13515219be8a272388ec6",
            "patch": "@@ -376,6 +376,8 @@ def forward(\n class BltPreTrainedModel(MllamaPreTrainedModel):\n     config: BltConfig\n     _supports_attention_backend = False\n+    _supports_flash_attn = False\n+    _supports_flex_attn = False\n     _no_split_modules = [\"BltTransformerLayer\"]\n     _can_record_outputs = {\n         \"hidden_states\": OutputRecorder(BltTransformerLayer, index=0, layer_name=\"local_decoder\"),\n@@ -866,7 +868,12 @@ def forward(\n         )\n \n         cross_attn_mask_enc = _prepare_patch_cross_attention_mask(\n-            patch_ids, patch_lengths.shape[1], sequence_length, True, self.config.cross_attn_k, encoder_embeds.dtype\n+            patch_ids=patch_ids,\n+            num_patches=patch_lengths.shape[1],\n+            sequence_length=sequence_length,\n+            patches_as_queries=True,\n+            cross_attn_k=self.config.cross_attn_k,\n+            dtype=encoder_embeds.dtype,\n         )\n         encoder_hidden_states, encoder_cross_states = self.local_encoder(\n             input_ids=input_ids,\n@@ -898,12 +905,12 @@ def forward(\n         )\n         decoder_patch_ids = self._patch_ids_from_lengths(patch_lengths[:, 1:], sequence_length)\n         cross_attn_mask_dec = _prepare_patch_cross_attention_mask(\n-            decoder_patch_ids,\n-            patch_lengths.shape[1],\n-            sequence_length,\n-            False,\n-            self.config.cross_attn_k,\n-            encoder_embeds.dtype,\n+            patch_ids=decoder_patch_ids,\n+            num_patches=patch_lengths.shape[1],\n+            sequence_length=sequence_length,\n+            patches_as_queries=False,\n+            cross_attn_k=self.config.cross_attn_k,\n+            dtype=encoder_embeds.dtype,\n         )\n         output = self.local_decoder(\n             input_ids=input_ids,"
        },
        {
            "sha": "5c3451ca41b48b4225039d2d17a54fe94f627580",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/071eb5334f5a9ac2c7a13515219be8a272388ec6/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/071eb5334f5a9ac2c7a13515219be8a272388ec6/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=071eb5334f5a9ac2c7a13515219be8a272388ec6",
            "patch": "@@ -284,10 +284,6 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n \n         self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n \n-    @unittest.skip(reason=\"Decoder cannot keep gradients\")\n-    def test_flex_attention_with_grads():\n-        pass\n-\n \n @require_torch_accelerator\n class BltIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 28,
        "deletions": 20
    }
}