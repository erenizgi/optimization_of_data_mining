{
    "author": "tanmay2004",
    "message": "Fix shapes in modular_gpt_oss.py (#42737)\n\n* Fix shapes in modular_gpt_oss.py\n\n* Run make fix-copies",
    "sha": "68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434",
    "files": [
        {
            "sha": "3ccfc723448dbab4aa907c77fb36dc3bbecf205b",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434",
            "patch": "@@ -88,8 +88,8 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n \n         Args:\n             hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)\n-            selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n-            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n+            selected_experts (torch.Tensor): (batch_size * seq_len, top_k)\n+            routing_weights (torch.Tensor): (batch_size * seq_len, top_k)\n         Returns:\n             torch.Tensor\n         \"\"\"\n@@ -159,8 +159,8 @@ def __init__(self, config):\n \n     def forward(self, hidden_states):\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n-        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n-        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n+        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (num_tokens, num_experts)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (num_tokens, top_k)\n         router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n         router_scores = router_top_value\n         return router_logits, router_scores, router_indices"
        },
        {
            "sha": "93346ce2aec393f978acedb8281fec414f697634",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=68dcd13bfb67bb5b2b12a2f9502d31ab7ecbc434",
            "patch": "@@ -86,8 +86,8 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n \n         Args:\n             hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)\n-            selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n-            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n+            selected_experts (torch.Tensor): (batch_size * seq_len, top_k)\n+            routing_weights (torch.Tensor): (batch_size * seq_len, top_k)\n         Returns:\n             torch.Tensor\n         \"\"\"\n@@ -157,8 +157,8 @@ def __init__(self, config):\n \n     def forward(self, hidden_states):\n         hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n-        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n-        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n+        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (num_tokens, num_experts)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (num_tokens, top_k)\n         router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n         router_scores = router_top_value\n         return router_logits, router_scores, router_indices"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}