{
    "author": "salmanmohammadi",
    "message": "Fixing flex attention for torch=2.6.0 (#37285)\n\n* adding compile kwarg for torch 2.6\n\n* fixing dynamic\n\n* addressing comment\n\n* typo\n\n* Update src/transformers/integrations/flex_attention.py\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "794fde7b1c3d041519fc28ea3e1461b0cfcad4e7",
    "files": [
        {
            "sha": "3dac6d8e4816fd8deb7e1f906f7c10af6965a1cf",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/794fde7b1c3d041519fc28ea3e1461b0cfcad4e7/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/794fde7b1c3d041519fc28ea3e1461b0cfcad4e7/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=794fde7b1c3d041519fc28ea3e1461b0cfcad4e7",
            "patch": "@@ -31,6 +31,7 @@\n import torch\n \n from ..utils import is_torch_flex_attn_available\n+from ..utils.import_utils import _torch_version\n \n \n if is_torch_flex_attn_available():\n@@ -60,8 +61,16 @@ def __init__(self):\n         \"\"\"\n         Initialize or update the singleton instance.\n         \"\"\"\n-        if self._is_flex_compiled is False:\n-            self._compiled_flex_attention = torch.compile(flex_attention, backend=\"inductor\")\n+        if not self._is_flex_compiled:\n+            # In PyTorch 2.6.0, there's a known issue with flex attention compilation which may\n+            # cause errors. The suggested fix is to compile with \"max-autotune-no-cudagraphs\"\n+            # see https://github.com/pytorch/pytorch/issues/146260 for training\n+            if _torch_version == \"2.6.0\":\n+                self._compiled_flex_attention = torch.compile(\n+                    flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\"\n+                )\n+            else:\n+                self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)\n             self._is_flex_compiled = True\n \n     def __call__(self):"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 11,
        "deletions": 2
    }
}