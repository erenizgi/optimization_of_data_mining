{
    "author": "McPatate",
    "message": "docs: add continuous batching to serving (#40758)\n\n* docs: tmp\n\n* docs: add continuous batching to serving\n\n* docs: reword after @lysandrejik review",
    "sha": "a9b313a0c2264865631876e09e7f5fcebebf375c",
    "files": [
        {
            "sha": "f421a284950a085a15ee4be01b316f43c7baa22d",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 36,
            "deletions": 2,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9b313a0c2264865631876e09e7f5fcebebf375c/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9b313a0c2264865631876e09e7f5fcebebf375c/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=a9b313a0c2264865631876e09e7f5fcebebf375c",
            "patch": "@@ -21,7 +21,7 @@ Transformer models can be efficiently deployed using libraries such as vLLM, Tex\n > [!TIP]\n > Responses API is now supported as an experimental API! Read more about it [here](#responses-api).\n \n-Apart from that you can also serve transformer models easily using the `transformers serve` CLI. This is ideal for experimentation purposes, or to run models locally for personal and private use.\n+You can also serve transformer models with the `transformers serve` CLI. With Continuous Batching, `serve` now delivers solid throughput and latency well suited for evaluation, experimentation, and moderate-load local or self-hosted deployments. While vLLM, SGLang, or other inference engines remain our recommendations for large-scale production, `serve` avoids the extra runtime and operational overhead, and is on track to gain more production-oriented features.\n \n In this document, we dive into the different supported endpoints and modalities; we also cover the setup of several user interfaces that can be used on top of `transformers serve` in the following guides:\n - [Jan (text and MCP user interface)](./jan.md)\n@@ -58,7 +58,7 @@ or by sending an HTTP request, like we'll see below.\n \n ## Chat Completions - text-based\n \n-See below for examples for text-based requests. Both LLMs and VLMs should handle \n+See below for examples for text-based requests. Both LLMs and VLMs should handle\n \n <hfoptions id=\"chat-completion-http\">\n <hfoption id=\"curl\">\n@@ -366,6 +366,40 @@ The `transformers serve` server is also an MCP client, so it can interact with M\n \n <!-- TODO: example with a minimal python example, and explain that it is possible to pass a full generation config in the request -->\n \n+## Continuous Batching\n \n+Continuous Batching (CB) lets the server dynamically group and interleave requests so they can share forward passes on the GPU. Instead of processing each request sequentially, `serve` adds new requests as others progress (prefill) and drops finished ones during decode. The result is significantly higher GPU utilization and better throughput without sacrificing latency for most workloads.\n+\n+Thanks to this, evaluation, experimentation, and moderate-load local/self-hosted use can now be handled comfortably by `transformers serve` without introducing an extra runtime to operate.\n+\n+### Enable CB in serve\n+\n+CB is opt-in and currently applies to chat completions.\n+\n+```sh\n+transformers serve \\\n+  --continuous-batching\n+  --attn_implementation sdpa_paged\n+```\n+\n+\n+### Performance tips\n+\n+- Use an efficient attention backend when available:\n+\n+```sh\n+transformers serve \\\n+  --continuous_batching \\\n+  --attn_implementation paged_attention\n+```\n+\n+> [!TIP]\n+> If you choose `paged_attention`, you must install `flash-attn` separately: `pip install flash-attn --no-build-isolation`\n+\n+- `--dtype {bfloat16|float16}` typically improve throughput and memory use vs. `float32`\n+\n+- `--load_in_4bit`/`--load_in_8bit` can reduce memory footprint for LoRA setups\n+\n+- `--force-model <repo_id>` avoids per-request model hints and helps produce stable, repeatable runs\n \n "
        }
    ],
    "stats": {
        "total": 38,
        "additions": 36,
        "deletions": 2
    }
}