{
    "author": "gante",
    "message": "[t5gemma] fix `get_text_config` and related fixes (#40939)\n\n* tmp commit\n\n* t5gemma fixes",
    "sha": "01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
    "files": [
        {
            "sha": "16d116849f5139d0d150100eeaae5c45948a68c1",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
            "patch": "@@ -395,7 +395,12 @@ def update(\n         if not self.is_initialized:\n             self.lazy_initialization(key_states)\n \n-        cache_position = cache_kwargs.get(\"cache_position\")\n+        # Some old models give None for `cache_position` or even omit passing `cache_kwargs` when used as cross-attention,\n+        # in which case we should copy the whole Layer (key_states.shape[-2] == self.max_cache_len)\n+        cache_position = cache_kwargs.get(\"cache_position\") if cache_kwargs is not None else None\n+        cache_position = (\n+            cache_position if cache_position is not None else torch.arange(key_states.shape[-2], device=self.device)\n+        )\n \n         cumulative_length = self.cumulative_length\n         is_full = cumulative_length >= self.max_cache_len\n@@ -955,17 +960,19 @@ def __init__(\n         layers = []\n         # If a config is passed, use it to infer the layer types and initialize accordingly\n         if config is not None:\n-            config = config.get_text_config(decoder=True)\n-            sliding_window = getattr(config, \"sliding_window\", None) or getattr(config, \"attention_chunk_size\", None)\n-            layer_types = getattr(config, \"layer_types\", None)\n+            decoder_config = config.get_text_config(decoder=True)\n+            sliding_window = getattr(decoder_config, \"sliding_window\", None) or getattr(\n+                decoder_config, \"attention_chunk_size\", None\n+            )\n+            layer_types = getattr(decoder_config, \"layer_types\", None)\n             if layer_types is None:\n                 layer_types = [\n                     \"sliding_attention\" if sliding_window is not None else \"full_attention\"\n-                    for _ in range(config.num_hidden_layers)\n+                    for _ in range(decoder_config.num_hidden_layers)\n                 ]\n             # Some models have shared layers thus no cache is needed for them (e.g. Gemma3n)\n-            if hasattr(config, \"num_kv_shared_layers\"):\n-                layer_types = layer_types[: -config.num_kv_shared_layers]\n+            if hasattr(decoder_config, \"num_kv_shared_layers\"):\n+                layer_types = layer_types[: -decoder_config.num_kv_shared_layers]\n \n             for layer_type in layer_types:\n                 # From a cache point of view, both sliding and chunked are the same in how they should behave and how many"
        },
        {
            "sha": "d998ae18b6e622b03db18c1c3a8a9990afd35a35",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
            "patch": "@@ -324,9 +324,5 @@ def __setattr__(self, key, value):\n             setattr(self.decoder, key, value)\n         super().__setattr__(key, value)\n \n-    def get_text_config(self, *args, **kwargs):\n-        # Always return self, regardless of the decoder option.\n-        return self\n-\n \n __all__ = [\"T5GemmaConfig\", \"T5GemmaModuleConfig\"]"
        },
        {
            "sha": "9ed3abf34007c26375704517fbbb0f2ab4128382",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
            "patch": "@@ -339,10 +339,6 @@ def __setattr__(self, key, value):\n             setattr(self.decoder, key, value)\n         super().__setattr__(key, value)\n \n-    def get_text_config(self, *args, **kwargs):\n-        # Always return self, regardless of the decoder option.\n-        return self\n-\n \n class T5GemmaRMSNorm(Gemma2RMSNorm):\n     pass"
        },
        {
            "sha": "c5baabee3389171848ecf9abe392408a0ebc1064",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
            "patch": "@@ -1209,7 +1209,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n \n             # This test is for decoder-only models (encoder-decoder models have native input embeddings support in the\n             # decoder)\n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 continue\n             config.is_decoder = True\n \n@@ -1288,7 +1288,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             model = model_class(config).to(torch_device).eval()\n@@ -1439,7 +1439,7 @@ def test_generate_continue_from_inputs_embeds(self):\n             if \"token_type_ids\" in inputs_dict:\n                 del inputs_dict[\"token_type_ids\"]\n \n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder\")\n             # TODO (joao, raushan): the correct line below is `if not hasattr(config.get_text_config(), \"use_cache\")`,\n             # but it breaks a few models. Fix and then apply `has_similar_generate_outputs` pattern\n@@ -1512,7 +1512,7 @@ def test_generate_with_static_cache(self):\n             set_config_for_less_flaky_test(config)\n             main_input = inputs_dict[model_class.main_input_name]\n \n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             config.is_decoder = True\n@@ -1567,10 +1567,7 @@ def test_generate_with_quant_cache(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n-            if (\n-                config.get_text_config(decoder=True).is_encoder_decoder\n-                or not model_class._supports_default_dynamic_cache()\n-            ):\n+            if config.is_encoder_decoder or not model_class._supports_default_dynamic_cache():\n                 self.skipTest(reason=\"This model does not support the quantized cache format\")\n \n             config.is_decoder = True\n@@ -1670,7 +1667,7 @@ def test_generate_compile_model_forward_fullgraph(self):\n                     if not has_defined_cache_implementation:\n                         decoder_cache = (\n                             gen_out.past_key_values.self_attention_cache\n-                            if config.get_text_config(decoder=True).is_encoder_decoder\n+                            if config.is_encoder_decoder\n                             else gen_out.past_key_values\n                         )\n                         self.assertTrue(isinstance(decoder_cache, DynamicCache))\n@@ -1696,7 +1693,7 @@ def test_generate_compile_model_forward_fullgraph(self):\n                         # sanity checks\n                         decoder_cache = (\n                             gen_out.past_key_values.self_attention_cache\n-                            if config.get_text_config(decoder=True).is_encoder_decoder\n+                            if config.is_encoder_decoder\n                             else gen_out.past_key_values\n                         )\n                         self.assertFalse(isinstance(decoder_cache, DynamicCache))"
        },
        {
            "sha": "7f7a1d38915af6d933be6e7ff5593cc7e4378a95",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
            "patch": "@@ -448,7 +448,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             model = model_class(config).to(torch_device).eval()\n@@ -509,7 +509,7 @@ def test_generate_with_static_cache(self):\n             set_config_for_less_flaky_test(config)\n             main_input = inputs_dict[model_class.main_input_name]\n \n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             config.is_decoder = True"
        },
        {
            "sha": "a6b4c63def967ec9556417522f62237e7b6c2790",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01c9e1ba683b3e50d7c76bf92f2d470759fd5e81/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=01c9e1ba683b3e50d7c76bf92f2d470759fd5e81",
            "patch": "@@ -4145,7 +4145,7 @@ def test_flex_attention_with_grads(self):\n                 if key in inputs_dict:\n                     dummy_inputs[key] = inputs_dict[key].to(torch_device)\n \n-            if config.get_text_config(decoder=True).is_encoder_decoder:\n+            if config.is_encoder_decoder:\n                 dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"].to(torch_device)\n                 dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"].to(torch_device)\n "
        }
    ],
    "stats": {
        "total": 52,
        "additions": 24,
        "deletions": 28
    }
}