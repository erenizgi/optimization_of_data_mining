{
    "author": "stevhliu",
    "message": "[docs] Deploying (#42263)\n\n* update\n\n* monitoring\n\n* revert\n\n* revert\n\n* toctree\n\n* fix",
    "sha": "2f5cc5df90e0b5e99b47460729bc1ab11b816dd1",
    "files": [
        {
            "sha": "9b02a16652625af51f3cb176ec5dd90364e3d9d2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=2f5cc5df90e0b5e99b47460729bc1ab11b816dd1",
            "patch": "@@ -19,6 +19,8 @@\n       title: Customizing model components\n     - local: model_sharing\n       title: Sharing\n+    - local: serialization\n+      title: Exporting to production\n     - local: modular_transformers\n       title: Contributing a new model to Transformers\n     - local: add_new_model\n@@ -226,13 +228,6 @@\n   - local: quantization/contribute\n     title: Contribute\n   title: Quantization\n-- isExpanded: false\n-  sections:\n-  - local: serialization\n-    title: ONNX\n-  - local: executorch\n-    title: ExecuTorch\n-  title: Export to production\n - isExpanded: false\n   sections:\n   - sections:"
        },
        {
            "sha": "ce54640e6cdf85e51a56d0411d2aacbf978e0e8e",
            "filename": "docs/source/en/executorch.md",
            "status": "removed",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/88a5623361e1b3d844daef3c6c95535d12e70056/docs%2Fsource%2Fen%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/88a5623361e1b3d844daef3c6c95535d12e70056/docs%2Fsource%2Fen%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexecutorch.md?ref=88a5623361e1b3d844daef3c6c95535d12e70056",
            "patch": "@@ -1,33 +0,0 @@\n-<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# ExecuTorch\n-\n-[ExecuTorch](https://pytorch.org/executorch/stable/index.html) runs PyTorch models on mobile and edge devices. Export your Transformers models to the ExecuTorch format with [Optimum ExecuTorch](https://github.com/huggingface/optimum-executorch) with the command below.\n-\n-```bash\n-optimum-cli export executorch \\\n-    --model \"HuggingFaceTB/SmolLM2-135M-Instruct\" \\\n-    --task \"text-generation\" \\\n-    --recipe \"xnnpack\" \\\n-    --use_custom_sdpa \\\n-    --use_custom_kv_cache \\\n-    --qlinear 8da4w \\\n-    --qembedding 8w \\\n-    --output_dir=\"hf_smollm2\"\n-```\n-\n-Run `optimum-cli export executorch --help` to see all export options. For detailed export instructions, check the [README](optimum/exporters/executorch/README.md)."
        },
        {
            "sha": "0031c9b0b3b8c8f13ddcb5817148d8933017db1b",
            "filename": "docs/source/en/model_doc/audioflamingo3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md?ref=2f5cc5df90e0b5e99b47460729bc1ab11b816dd1",
            "patch": "@@ -14,7 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-*This model was released on 2025-07-10 and added to Hugging Face Transformers on 2025-11-11.*\n+*This model was released on 2025-07-10 and added to Hugging Face Transformers on 2025-11-17.*\n \n # Audio Flamingo 3\n "
        },
        {
            "sha": "f4c5bb804e65d1f7eb8e577ebeb2ad39e65c6d93",
            "filename": "docs/source/en/serialization.md",
            "status": "modified",
            "additions": 65,
            "deletions": 54,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f5cc5df90e0b5e99b47460729bc1ab11b816dd1/docs%2Fsource%2Fen%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserialization.md?ref=2f5cc5df90e0b5e99b47460729bc1ab11b816dd1",
            "patch": "@@ -14,87 +14,98 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# ONNX\n+# Exporting to production\n \n-[ONNX](http://onnx.ai) is an open standard that defines a common set of operators and a file format to represent deep learning models in different frameworks, including PyTorch and TensorFlow. When a model is exported to ONNX, the operators construct a computational graph (or *intermediate representation*) which represents the flow of data through the model. Standardized operators and data types makes it easy to switch between frameworks.\n+Export Transformers' models to different formats for optimized runtimes and devices. Deploy the same model to cloud providers or run it on mobile and edge devices. You don't need to rewrite the model from scratch for each deployment environment. Freely deploy across any inference ecosystem.\n \n-The [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to ONNX with configuration objects which are supported for [many architectures](https://huggingface.co/docs/optimum/exporters/onnx/overview) and can be easily extended. If a model isn't supported, feel free to make a [contribution](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute) to Optimum.\n+## ExecuTorch\n \n-The benefits of exporting to ONNX include the following.\n+[ExecuTorch](https://pytorch.org/executorch/stable/index.html) runs PyTorch models on mobile and edge devices. It exports a model into a graph of standardized operators, compiles the graph into an ExecuTorch program, and executes it on the target device. The runtime is lightweight and calculates the execution plan ahead of time.\n \n-- [Graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization) for improving inference.\n-- Use the [`~optimum.onnxruntime.ORTModel`] API to run a model with [ONNX Runtime](https://onnxruntime.ai/).\n-- Use [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines) for ONNX models.\n+Install [Optimum ExecuTorch](https://huggingface.co/docs/optimum-executorch/en/index) from source.\n \n-Export a Transformers model to ONNX with the Optimum CLI or the `optimum.onnxruntime` module.\n+```bash\n+git clone https://github.com/huggingface/optimum-executorch.git\n+cd optimum-executorch\n+pip install '.[dev]'\n+```\n \n-## Optimum CLI\n+Export a Transformers model to ExecuTorch with the CLI tool.\n+\n+```bash\n+optimum-cli export executorch \\\n+    --model \"Qwen/Qwen3-8B\" \\\n+    --task \"text-generation\" \\\n+    --recipe \"xnnpack\" \\\n+    --use_custom_sdpa \\\n+    --use_custom_kv_cache \\\n+    --qlinear 8da4w \\\n+    --qembedding 8w \\\n+    --output_dir=\"hf_smollm2\"\n+```\n \n-Run the command below to install Optimum and the [exporters](https://huggingface.co/docs/optimum/exporters/overview) module.\n+Run the following command to view all export options.\n \n ```bash\n-pip install optimum-onnx\n+optimum-cli export executorch --help\n ```\n \n-> [!TIP]\n-> Refer to the [Export a model to ONNX with optimum.exporters.onnx](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) guide for all available arguments or with the command below.\n->\n-> ```bash\n-> optimum-cli export onnx --help\n-> ```\n+## ONNX\n+\n+[ONNX](http://onnx.ai) is a shared language for describing models from different frameworks. It represents models as a graph of standardized operators with well-defined types, shapes, and metadata. Models serialize into compact protobuf files that you can deploy across optimized runtimes and engines.\n \n-Set the `--model` argument to export a PyTorch model from the Hub.\n+[Optimum ONNX](https://huggingface.co/docs/optimum-onnx/index) exports models to ONNX with configuration objects. It supports many [architectures](https://huggingface.co/docs/optimum-onnx/onnx/overview) and is easily extendable. Export models through the CLI tool or programmatically.\n+\n+Install [Optimum ONNX](https://huggingface.co/docs/optimum-onnx/index).\n \n ```bash\n-optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/\n+uv pip install optimum-onnx\n ```\n \n-You should see logs indicating the progress and showing where the resulting `model.onnx` is saved.\n-\n-```text\n-Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n-\t-[✓] ONNX model output names match reference model (start_logits, end_logits)\n-\t- Validating ONNX Model output \"start_logits\":\n-\t\t-[✓] (2, 16) matches (2, 16)\n-\t\t-[✓] all values close (atol: 0.0001)\n-\t- Validating ONNX Model output \"end_logits\":\n-\t\t-[✓] (2, 16) matches (2, 16)\n-\t\t-[✓] all values close (atol: 0.0001)\n-The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx\n-```\n+### optimum-cli\n \n-For local models, make sure the model weights and tokenizer files are saved in the same directory, for example `local_path`. Pass the directory to the `--model` argument and use `--task` to indicate the [task](https://huggingface.co/docs/optimum/exporters/task_manager) a model can perform. If `--task` isn't provided, the model architecture without a task-specific head is used.\n+Specify a model to export and the output directory with the `--model` argument.\n \n ```bash\n-optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/\n+optimum-cli export onnx --model Qwen/Qwen3-8B Qwen/Qwen3-8b-onnx/\n ```\n \n-The `model.onnx` file can be deployed with any [accelerator](https://onnx.ai/supported-tools.html#deployModel) that supports ONNX. The example below demonstrates loading and running a model with ONNX Runtime.\n+Run the following command to view all available arguments or refer to the [Export a model to ONNX with optimum.exporters.onnx](https://huggingface.co/docs/optimum-onnx/onnx/usage_guides/export_a_model) guide for more details.\n \n-```python\n->>> from transformers import AutoTokenizer\n->>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n+```bash\n+optimum cli export onnx --help\n+```\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n->>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n->>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n->>> outputs = model(**inputs)\n+To export a local model, save the weights and tokenizer files in the same directory. Pass the directory path to the `--model` argument and use the `--task` argument to specify the [task](https://huggingface.co/docs/optimum/exporters/task_manager#transformers). If you don't provide `--task`, the system auto-infers it from the model or uses an architecture without a task-specific head.\n+\n+```bash\n+optimum-cli export onnx --model path/to/local/model --task text-generation Qwen/Qwen3-8b-onnx/\n ```\n \n-## optimum.onnxruntime\n+Deploy the model with any [runtime](https://onnx.ai/supported-tools.html#deployModel) that supports ONNX, including ONNX Runtime.\n \n-The `optimum.onnxruntime` module supports programmatically exporting a Transformers model. Instantiate a [`~optimum.onnxruntime.ORTModel`] for a task and set `export=True`. Use [`~OptimizedModel.save_pretrained`] to save the ONNX model.\n+```py\n+from transformers import AutoTokenizer\n+from optimum.onnxruntime import ORTModelForCausalLM\n \n-```python\n->>> from optimum.onnxruntime import ORTModelForSequenceClassification\n->>> from transformers import AutoTokenizer\n+tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8b-onnx\")\n+model = ORTModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8b-onnx\")\n+inputs = tokenizer(\"Plants generate energy through a process known as \", return_tensors=\"pt\")\n+outputs = model.generate(**inputs)\n+print(tokenizer.batch_decode(outputs))\n+```\n \n->>> model_checkpoint = \"distilbert/distilbert-base-uncased-distilled-squad\"\n->>> save_directory = \"onnx/\"\n+### optimum.onnxruntime\n \n->>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n+Export Transformers' models programmatically with Optimum ONNX. Instantiate a [`~optimum.onnxruntime.ORTModel`] with a model and set `export=True`. Save the ONNX model with [`~optimum.onnxruntime.ORTModel.save_pretrained`].\n \n->>> ort_model.save_pretrained(save_directory)\n->>> tokenizer.save_pretrained(save_directory)\n-```\n+```py\n+from optimum.onnxruntime import ORTModelForCausalLM\n+from transformers import AutoTokenizer\n+\n+ort_model = ORTModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8b\", export=True)\n+tokenizer = AutoTokenizer.from_pretrained(\"onnx/\")\n+\n+ort_model.save_pretrained(\"onnx/\")\n+tokenizer.save_pretrained(\"onnx/\")\n+```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 163,
        "additions": 68,
        "deletions": 95
    }
}