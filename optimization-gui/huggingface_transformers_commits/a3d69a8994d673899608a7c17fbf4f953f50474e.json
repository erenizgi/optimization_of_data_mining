{
    "author": "faaany",
    "message": "[docs] add xpu device check  (#34684)\n\n* add XPU path\r\n\r\n* use accelerate API\r\n\r\n* Update docs/source/en/tasks/semantic_segmentation.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* update more places with accelerate API\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "a3d69a8994d673899608a7c17fbf4f953f50474e",
    "files": [
        {
            "sha": "621edeb20e8ea36f52328045814df877e295dca9",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -508,10 +508,11 @@ See the following examples for DoLa decoding with the 32-layer LLaMA-7B model.\n ```python\n >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n >>> import torch\n+>>> from accelerate.test_utils.testing import get_backend\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n >>> model = AutoModelForCausalLM.from_pretrained(\"huggyllama/llama-7b\", torch_dtype=torch.float16)\n->>> device = 'cuda' if torch.cuda.is_available() else 'cpu'\n+>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> model.to(device)\n >>> set_seed(42)\n "
        },
        {
            "sha": "7e3335762ea43b442668b0b1ba16261fb1beb107",
            "filename": "docs/source/en/tasks/idefics.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -386,9 +386,9 @@ The use and prompting for the conversational use is very similar to using the ba\n ```py\n >>> import torch\n >>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n+>>> from accelerate.test_utils.testing import get_backend\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-\n+>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n >>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n >>> processor = AutoProcessor.from_pretrained(checkpoint)"
        },
        {
            "sha": "9a78967cb5198d7cb4a499dc03387bcfb2d77d30",
            "filename": "docs/source/en/tasks/image_captioning.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -256,8 +256,9 @@ image\n Prepare image for the model.\n \n ```python\n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-\n+from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device, _, _ = get_backend()\n inputs = processor(images=image, return_tensors=\"pt\").to(device)\n pixel_values = inputs.pixel_values\n ```"
        },
        {
            "sha": "80b701588b26b4c9cd91fa0104641c8d2cc169ff",
            "filename": "docs/source/en/tasks/image_feature_extraction.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -43,8 +43,9 @@ Let's see the pipeline in action. First, initialize the pipeline. If you don't p\n ```python\n import torch\n from transformers import pipeline\n-\n-DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n+from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+DEVICE, _, _ = get_backend()\n pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n ```\n "
        },
        {
            "sha": "f1c62e47aebf248e4e2960c94005cbe168b91e54",
            "filename": "docs/source/en/tasks/image_to_image.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -37,8 +37,9 @@ We can now initialize the pipeline with a [Swin2SR model](https://huggingface.co\n ```python\n from transformers import pipeline\n import torch\n-\n-device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n+from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device, _, _ = get_backend()\n pipe = pipeline(task=\"image-to-image\", model=\"caidas/swin2SR-lightweight-x2-64\", device=device)\n ```\n "
        },
        {
            "sha": "17fb363df8e2a0bdb1fba661ab6e3271b5199003",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -58,15 +58,15 @@ from transformers import TrainingArguments, Trainer\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-\n+from accelerate.test_utils.testing import get_backend\n \n class ImageDistilTrainer(Trainer):\n     def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n         super().__init__(model=student_model, *args, **kwargs)\n         self.teacher = teacher_model\n         self.student = student_model\n         self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n-        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n+        device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n         self.teacher.to(device)\n         self.teacher.eval()\n         self.temperature = temperature"
        },
        {
            "sha": "db16e035e303e0fe3b168bf5f1caf4e2b13759de",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -125,9 +125,9 @@ the processor.\n ```python\n from transformers import SamModel, SamProcessor\n import torch\n-\n-device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n-\n+from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device, _, _ = get_backend()\n model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n ```"
        },
        {
            "sha": "edd22122f32bd6832633889ab8b1df357e85c90e",
            "filename": "docs/source/en/tasks/monocular_depth_estimation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -53,8 +53,9 @@ Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggi\n ```py\n >>> from transformers import pipeline\n >>> import torch\n-\n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+>>> device, _, _ = get_backend()\n >>> checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n >>> pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n ```"
        },
        {
            "sha": "c307dd3334fe920e720248b0c486b1a3eb4974b4",
            "filename": "docs/source/en/tasks/object_detection.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -1488,7 +1488,9 @@ Now that you have finetuned a model, evaluated it, and uploaded it to the Huggin\n \n Load model and image processor from the Hugging Face Hub (skip to use already trained in this session):\n ```py\n->>> device = \"cuda\"\n+>>> from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+>>> device, _, _ = get_backend()\n >>> model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n \n >>> image_processor = AutoImageProcessor.from_pretrained(model_repo)"
        },
        {
            "sha": "a21ff62edf1a563f5fbdeeb5c2d55ca81d8cbf72",
            "filename": "docs/source/en/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -689,7 +689,9 @@ Reload the dataset and load an image for inference.\n We will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel_values` on a GPU:\n \n ```py\n->>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\n+>>> from accelerate.test_utils.testing import get_backend\n+# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+>>> device, _, _ = get_backend()\n >>> encoding = image_processor(image, return_tensors=\"pt\")\n >>> pixel_values = encoding.pixel_values.to(device)\n ```"
        },
        {
            "sha": "e25da4e19efeaac3666194224fcc7423220ad89f",
            "filename": "docs/source/en/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -282,10 +282,10 @@ containing the corresponding speaker embedding.\n >>> import os\n >>> import torch\n >>> from speechbrain.inference.classifiers import EncoderClassifier\n+>>> from accelerate.test_utils.testing import get_backend\n \n >>> spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n-\n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> speaker_model = EncoderClassifier.from_hparams(\n ...     source=spk_model_name,\n ...     run_opts={\"device\": device},"
        },
        {
            "sha": "87dbfb751bfa9872fa75bdd7834a1ae23b27a698",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a3d69a8994d673899608a7c17fbf4f953f50474e/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=a3d69a8994d673899608a7c17fbf4f953f50474e",
            "patch": "@@ -363,10 +363,11 @@ GPU, if available, which we didn't need to do earlier when training, as [`Traine\n ```py\n >>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n >>> import torch\n+>>> from accelerate.test_utils.testing import get_backend\n \n >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n >>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> model.to(device)\n ```\n "
        }
    ],
    "stats": {
        "total": 52,
        "additions": 31,
        "deletions": 21
    }
}