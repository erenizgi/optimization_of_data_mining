{
    "author": "zucchini-nlp",
    "message": "Add in-out modalities as class attribute per model (#41366)\n\n* update all models\n\n* fix copies\n\n* explanation comment\n\n* better notation in omni model\n\n* style\n\n* fix copies\n\n* output_modalities under generation mixin\n\n* fix copies\n\n* oh, glm4v also needs conversion",
    "sha": "1c36d407d5ce0712c702e4314c62705c012bb700",
    "files": [
        {
            "sha": "6ae8ff48ca8b12aa560b304f051cf30be25397b8",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -366,6 +366,9 @@ class GenerationMixin(ContinuousMixin):\n     To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n     \"\"\"\n \n+    # Should be overwritten by models that can generate non-text output\n+    output_modalities = \"text\"\n+\n     def adjust_generation_fn(\n         self,\n         generation_config,"
        },
        {
            "sha": "f61519c063c60932cb7dc43be52bed8e0f700d67",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1734,6 +1734,10 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     _supports_attention_backend = False\n     _can_record_outputs = None\n \n+    # Attributes used mainly in multimodal LLMs, though all models contain a valid field for these\n+    # Possible values are: text, image, video, audio and time\n+    input_modalities: Union[str, list[str]] = \"text\"  # most models are text\n+\n     @property\n     @torch._dynamo.allow_in_graph\n     def can_record_outputs(self) -> dict[str, OutputRecorder]:"
        },
        {
            "sha": "f44879e37b02f97e55340bd4f75481cec6269645",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -394,6 +394,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n \n     config: Aimv2Config\n     base_model_prefix = \"aimv2\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Aimv2EncoderLayer\","
        },
        {
            "sha": "a7ea96f8f2c206113dd98d9aa6c8abb6d3d7d0b1",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -437,6 +437,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n \n     config: Aimv2Config\n     base_model_prefix = \"aimv2\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Aimv2EncoderLayer\","
        },
        {
            "sha": "57b73d38ab48131f07f9ceba1cdb541e81b30623",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -820,6 +820,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class AlignPreTrainedModel(PreTrainedModel):\n     config: AlignConfig\n     base_model_prefix = \"align\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):\n@@ -849,6 +850,7 @@ def _init_weights(self, module: nn.Module):\n )\n class AlignTextModel(AlignPreTrainedModel):\n     config: AlignTextConfig\n+    input_modalities = \"text\"\n     _no_split_modules = [\"AlignTextEmbeddings\"]\n \n     def __init__(self, config: AlignTextConfig, add_pooling_layer: bool = True):\n@@ -969,6 +971,7 @@ def forward(\n class AlignVisionModel(AlignPreTrainedModel):\n     config: AlignVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n \n     def __init__(self, config: AlignVisionConfig):"
        },
        {
            "sha": "be84fb62b66d29d533975da1a3998b326990345f",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -766,6 +766,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n class AltCLIPPreTrainedModel(PreTrainedModel):\n     config: AltCLIPConfig\n     base_model_prefix = \"altclip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_module = []\n \n@@ -870,6 +871,7 @@ def forward(\n class AltCLIPVisionModel(AltCLIPPreTrainedModel):\n     config: AltCLIPVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: AltCLIPVisionConfig):\n         super().__init__(config)\n@@ -1028,6 +1030,7 @@ def forward(\n \n class AltCLIPTextModel(AltCLIPPreTrainedModel):\n     config: AltCLIPTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "b02da35ae91af40bb0958fb6b6a2fd02dd2ae393",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -572,6 +572,7 @@ def forward(\n class AriaTextPreTrainedModel(PreTrainedModel):\n     config: AriaTextConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "691a881e973b70e2228a3674f8b944528fe45e4b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1215,6 +1215,7 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n class AriaTextPreTrainedModel(PreTrainedModel):\n     config: AriaTextConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "0a918edd188613df8bcc5bd788e89dbdb0c1ca4e",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -288,6 +288,7 @@ def forward(self, hidden_states: torch.Tensor) -> BaseModelOutput:\n class ASTPreTrainedModel(PreTrainedModel):\n     config: ASTConfig\n     base_model_prefix = \"audio_spectrogram_transformer\"\n+    input_modalities = \"audio\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True"
        },
        {
            "sha": "14b93fb1b66ec03f612f6586ca51f91c83dac476",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -822,6 +822,7 @@ def forward(\n class AutoformerPreTrainedModel(PreTrainedModel):\n     config: AutoformerConfig\n     base_model_prefix = \"model\"\n+    input_modalities = \"time\"\n     main_input_name = \"past_values\"\n     supports_gradient_checkpointing = True\n "
        },
        {
            "sha": "39f9d70fcc7b3f1d260c4a05bb9ff35c26710b13",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -91,6 +91,7 @@ def pixel_shuffle(self, image_features):  # B, S, D\n class AyaVisionPreTrainedModel(PreTrainedModel):\n     config: AyaVisionConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "7954972d8ee899fb41a51225387cf3be330c0545",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -370,6 +370,7 @@ def device(self) -> torch.device:\n # GPT2-like autoregressive model\n class BarkCausalModel(BarkPreTrainedModel, GenerationMixin):\n     config: BarkSubModelConfig\n+    output_modalities = \"audio\"\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "fff3158ab38712f5105c4d009d0672c7c874b2d6",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -685,6 +685,7 @@ def forward(\n class BeitPreTrainedModel(PreTrainedModel):\n     config: BeitConfig\n     base_model_prefix = \"beit\"\n+    input_modalities = \"image\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BeitLayer\"]"
        },
        {
            "sha": "916f99a1556effd6e380d048b0011914ac672ebe",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -624,6 +624,7 @@ def forward(\n class BitPreTrainedModel(PreTrainedModel):\n     config: BitConfig\n     base_model_prefix = \"bit\"\n+    input_modalities = \"image\"\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"BitEmbeddings\"]\n "
        },
        {
            "sha": "abde4b5dba0a8209d7c1b1daa840698d4bd8266f",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -414,6 +414,7 @@ def forward(\n class BlipPreTrainedModel(PreTrainedModel):\n     config: BlipConfig\n     base_model_prefix = \"blip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BlipEncoderLayer\", \"BlipTextEmbeddings\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n@@ -482,6 +483,7 @@ def forward(\n \n class BlipVisionModel(BlipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     config: BlipVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": BlipEncoderLayer,"
        },
        {
            "sha": "806b08469f6f0195b9f4e446d707583b3a410cb8",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -392,6 +392,7 @@ def forward(\n class Blip2PreTrainedModel(PreTrainedModel):\n     config: Blip2Config\n     base_model_prefix = \"blip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True\n@@ -474,6 +475,7 @@ def forward(\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->Blip2, BLIP->BLIP_2\n class Blip2VisionModel(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     config: Blip2VisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": Blip2EncoderLayer,\n@@ -1489,6 +1491,7 @@ def forward(\n @auto_docstring\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n     _supports_flash_attn = False  # because self.qformer does not support FA2\n \n@@ -1960,6 +1963,7 @@ def generate(\n )\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n     _supports_flash_attn = False  # because self.qformer does not support FA2\n "
        },
        {
            "sha": "b698a246b61c6ecd92c8a926c5f752c7764d76f8",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -419,6 +419,7 @@ def forward(\n class BltPreTrainedModel(PreTrainedModel):\n     config: BltConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BltTransformerLayer\"]\n     _can_compile_fullgraph = False  # static cache cannot have different shapes for each layer"
        },
        {
            "sha": "9647f8bb38f855fbc694cf67d102a13ad82895f0",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -914,6 +914,7 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n class BridgeTowerPreTrainedModel(PreTrainedModel):\n     config: BridgeTowerConfig\n     base_model_prefix = \"bridgetower\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"BridgeTowerSelfAttention\", \"BridgeTowerResidualAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -947,6 +948,7 @@ def _init_weights(self, module: nn.Module):\n \n class BridgeTowerVisionModel(BridgeTowerPreTrainedModel):\n     config: BridgeTowerVisionConfig\n+    input_modalities = \"image\"\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -976,6 +978,7 @@ def forward(self, image, image_mask=None, interpolate_pos_encoding=False):\n )\n class BridgeTowerTextModel(BridgeTowerPreTrainedModel):\n     config: BridgeTowerTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\""
        },
        {
            "sha": "8b350e9a55148959547a913be1b99c83303a1016",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -796,6 +796,7 @@ def convert_img2bpe(self, img_batch: torch.Tensor) -> torch.Tensor:\n class ChameleonPreTrainedModel(PreTrainedModel):\n     config: ChameleonConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ChameleonDecoderLayer\", \"ChameleonSwinDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]"
        },
        {
            "sha": "6e254f9bb3a7a1a068e798fd7160f1dbd40f0407",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -559,6 +559,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class ChineseCLIPPreTrainedModel(PreTrainedModel):\n     config: ChineseCLIPConfig\n     base_model_prefix = \"chinese_clip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):\n@@ -795,6 +796,7 @@ class ChineseCLIPTextModel(ChineseCLIPPreTrainedModel):\n     \"\"\"\n \n     config: ChineseCLIPTextConfig\n+    input_modalities = \"text\"\n     _no_split_modules = [\"ChineseCLIPTextEmbeddings\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n@@ -902,6 +904,7 @@ def forward(\n class ChineseCLIPVisionModel(ChineseCLIPPreTrainedModel):\n     config: ChineseCLIPVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"ChineseCLIPVisionEmbeddings\", \"ChineseCLIPVisionAttention\"]\n \n     def __init__(self, config: ChineseCLIPVisionConfig):"
        },
        {
            "sha": "89ad2ec26a613a74dffda605c3a1424887f4a575",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1305,6 +1305,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class ClapPreTrainedModel(PreTrainedModel):\n     config: ClapConfig\n     base_model_prefix = \"clap\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = False\n \n     def _init_weights(self, module: nn.Module):\n@@ -1334,6 +1335,7 @@ def _init_weights(self, module: nn.Module):\n class ClapAudioModel(ClapPreTrainedModel):\n     config: ClapAudioConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: ClapAudioConfig):\n         super().__init__(config)\n@@ -1406,6 +1408,7 @@ def forward(\n )\n class ClapTextModel(ClapPreTrainedModel):\n     config: ClapTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\"\n@@ -1710,6 +1713,7 @@ def forward(\n @auto_docstring\n class ClapTextModelWithProjection(ClapPreTrainedModel):\n     config: ClapTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: ClapTextConfig):\n         super().__init__(config)\n@@ -1776,6 +1780,7 @@ def forward(\n class ClapAudioModelWithProjection(ClapPreTrainedModel):\n     config: ClapAudioConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: ClapAudioConfig):\n         super().__init__(config)"
        },
        {
            "sha": "3ed174ed11aad5057ec98b87eddcf7707477fad3",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -420,6 +420,7 @@ def forward(\n class CLIPPreTrainedModel(PreTrainedModel):\n     config: CLIPConfig\n     base_model_prefix = \"clip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n@@ -662,6 +663,7 @@ def forward(\n )\n class CLIPTextModel(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n+    input_modalities = \"text\"\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n     _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n@@ -769,6 +771,7 @@ def forward(\n class CLIPVisionModel(CLIPPreTrainedModel):\n     config: CLIPVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPVisionConfig):\n@@ -1029,6 +1032,7 @@ def forward(\n @auto_docstring\n class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n+    input_modalities = \"text\"\n \n     _supports_flash_attn = False\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n@@ -1099,6 +1103,7 @@ def forward(\n class CLIPVisionModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: CLIPVisionConfig):\n         super().__init__(config)\n@@ -1169,6 +1174,7 @@ def forward(\n )\n class CLIPForImageClassification(CLIPPreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: CLIPConfig) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "be00e0e7038109637714148b154a49d24a5e03f4",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -424,6 +424,7 @@ def forward(\n class CLIPSegPreTrainedModel(PreTrainedModel):\n     config: CLIPSegConfig\n     base_model_prefix = \"clip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):\n@@ -648,6 +649,7 @@ def forward(\n \n class CLIPSegTextModel(CLIPSegPreTrainedModel):\n     config: CLIPSegTextConfig\n+    input_modalities = \"text\"\n \n     _no_split_modules = [\"CLIPSegTextEmbeddings\", \"CLIPSegEncoderLayer\"]\n \n@@ -753,6 +755,7 @@ def forward(\n class CLIPSegVisionModel(CLIPSegPreTrainedModel):\n     config: CLIPSegVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: CLIPSegVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c041ce831fe5dc91439abfb492a2b33424e81fdf",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -130,6 +130,7 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n class Cohere2VisionPreTrainedModel(PreTrainedModel):\n     config: Cohere2VisionConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "16ced722841ca76c8133b805cbb7466d1c4eb76e",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -32,6 +32,7 @@\n class ColPaliPreTrainedModel(PreTrainedModel):\n     config: ColPaliConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "0c22fb99c8873394b32ed32069845cfefb1f4326",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -40,6 +40,7 @@\n class ColQwen2PreTrainedModel(PreTrainedModel):\n     config: ColQwen2Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "1b9660c8b22a2608267c4ee9cf5eddb7b4533329",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -967,6 +967,7 @@ class ConditionalDetrPreTrainedModel(PreTrainedModel):\n     config: ConditionalDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [r\"ConditionalDetrConvEncoder\", r\"ConditionalDetrEncoderLayer\", r\"ConditionalDetrDecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "bcdca46a84e6e715e74d17cb6a6163692a458ddc",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -236,6 +236,7 @@ class ConvNextPreTrainedModel(PreTrainedModel):\n     config: ConvNextConfig\n     base_model_prefix = \"convnext\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"ConvNextLayer\"]\n     _can_record_outputs = {}  # hidden states are collected explicitly\n "
        },
        {
            "sha": "d206ededf0eed87c5874405edd7e77ca447c1e24",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -257,6 +257,7 @@ class ConvNextV2PreTrainedModel(PreTrainedModel):\n     config: ConvNextV2Config\n     base_model_prefix = \"convnextv2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"ConvNextV2Layer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "f0371983a5986b84a8cacae23e63b8d89b8976f0",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -364,6 +364,7 @@ def forward(\n class CsmPreTrainedModel(PreTrainedModel):\n     config: CsmConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "bf919e04c25075b934f906b139f757a753572484",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -124,6 +124,7 @@ class CsmDecoderLayer(LlamaDecoderLayer):\n class CsmPreTrainedModel(PreTrainedModel):\n     config: CsmConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "b2ca4aa4c181a888a9235fa00b56a68d355138bb",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -441,6 +441,7 @@ class DFinePreTrainedModel(PreTrainedModel):\n     config: DFineConfig\n     base_model_prefix = \"d_fine\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [r\"DFineHybridEncoder\", r\"DFineDecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "891a9010566741d7d0f21862fac079b7b3e83e67",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -812,6 +812,7 @@ class DabDetrPreTrainedModel(PreTrainedModel):\n     config: DabDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [r\"DabDetrConvEncoder\", r\"DabDetrEncoderLayer\", r\"DabDetrDecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "81cfcbb931d4a2320539661a96d9f583f4f121a3",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -557,6 +557,8 @@ def remove_weight_norm(self):\n     \"\"\"\n )\n class DacModel(DacPreTrainedModel):\n+    input_modalities = \"audio\"\n+\n     def __init__(self, config: DacConfig):\n         super().__init__(config)\n         self.config = config"
        },
        {
            "sha": "2559a29abca13594fa1a6b8df970b6543b13fb9a",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -474,6 +474,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     config: Data2VecAudioConfig\n     base_model_prefix = \"data2vec_audio\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "b51d7ed0f5d546b4da4a565bf6a9acf13f7c6a6c",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -699,6 +699,7 @@ def forward(\n class Data2VecVisionPreTrainedModel(PreTrainedModel):\n     config: Data2VecVisionConfig\n     base_model_prefix = \"data2vec_vision\"\n+    input_modalities = \"image\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Data2VecVisionLayer\"]"
        },
        {
            "sha": "142bf7a5e783e3fccf6db80b533f0309e4a917aa",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -138,6 +138,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     config: Data2VecAudioConfig\n     base_model_prefix = \"data2vec_audio\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "41b6460e12bc435c7df2aec2fbb777487b8d0791",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -122,6 +122,7 @@ def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n class DeepseekVLPreTrainedModel(PreTrainedModel):\n     config: DeepseekVLConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -243,6 +244,7 @@ def forward(\n \n class DeepseekVLForConditionalGeneration(DeepseekVLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    output_modalities = \"text\"\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: DeepseekVLConfig):\n@@ -260,9 +262,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.language_model.set_input_embeddings(value)\n \n-    def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n-        raise AttributeError(\"Not needed for DeepseekVL\")\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "9b894b7f750576bf4313529eddc0724aee9a3be7",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -165,6 +165,8 @@ def __init__(self, config):\n \n \n class DeepseekVLForConditionalGeneration(JanusForConditionalGeneration):\n+    output_modalities = \"text\"\n+\n     def prepare_embeddings_for_image_generation(self):\n         raise AttributeError(\"Not needed for DeepseekVL\")\n "
        },
        {
            "sha": "531da23a5c51d5dbd416ec0016108b4943a2eecd",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -204,6 +204,7 @@ def forward(\n class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n     config: DeepseekVLHybridConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -388,6 +389,7 @@ def get_high_res_image_features(self, pixel_values):\n \n class DeepseekVLHybridForConditionalGeneration(DeepseekVLHybridPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    output_modalities = \"text\"\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: DeepseekVLHybridConfig):\n@@ -405,9 +407,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.language_model.set_input_embeddings(value)\n \n-    def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n-        raise AttributeError(\"Not needed for DeepseekVLHybrid\")\n-\n     @can_return_tuple\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward("
        },
        {
            "sha": "04a45b413c73f1d7ab0e905c65ccdeb9f5bac46d",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -923,6 +923,7 @@ class DeformableDetrPreTrainedModel(PreTrainedModel):\n     config: DeformableDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         r\"DeformableDetrConvEncoder\","
        },
        {
            "sha": "4d6a16c0a4388fb27cb06be9a2f0742da7a6b5d4",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -354,6 +354,7 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     config: DeiTConfig\n     base_model_prefix = \"deit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeiTLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "862b77807d3ae68a13e41a397c2dc670fe97a25e",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -213,6 +213,7 @@ class DepthAnythingPreTrainedModel(PreTrainedModel):\n     config: DepthAnythingConfig\n     base_model_prefix = \"depth_anything\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "c8a90eaaef023eb0efa136f022e491c3e72a2948",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -602,6 +602,7 @@ class DepthProPreTrainedModel(PreTrainedModel):\n     config: DepthProConfig\n     base_model_prefix = \"depth_pro\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _no_split_modules = [\"DepthProPreActResidualLayer\"]"
        },
        {
            "sha": "f0378c25a381f63f4a0aff1fea676b0a49069f62",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -724,6 +724,7 @@ class DetrPreTrainedModel(PreTrainedModel):\n     config: DetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [r\"DetrConvEncoder\", r\"DetrEncoderLayer\", r\"DetrDecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "c8d785ceec63dbd97a5cdb03d85f33926b2f6c1e",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -774,6 +774,7 @@ def forward(\n )\n class DiaForConditionalGeneration(DiaPreTrainedModel, DiaGenerationMixin):\n     base_model_prefix = \"model\"\n+    output_modalities = \"audio\"\n \n     def __init__(self, config: DiaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "ade2183164a5ad87ffbf5952ecb5edb0a024a524",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -595,6 +595,7 @@ def forward(\n )\n class DiaForConditionalGeneration(DiaPreTrainedModel, DiaGenerationMixin):\n     base_model_prefix = \"model\"\n+    output_modalities = \"audio\"\n \n     def __init__(self, config: DiaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "8f3220cfa1e96f00b7248da9f8a4d19ba93b4ffa",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -559,6 +559,7 @@ class DinatPreTrainedModel(PreTrainedModel):\n     config: DinatConfig\n     base_model_prefix = \"dinat\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "fa1887588020f5ab228e3d7282f5d1a0027166e4",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -403,6 +403,7 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     config: Dinov2Config\n     base_model_prefix = \"dinov2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2Layer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "bf16e8eadc40a40533c569b6674de5d96cbd4cb7",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -420,6 +420,7 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     config: Dinov2WithRegistersConfig\n     base_model_prefix = \"dinov2_with_registers\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2WithRegistersLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "bc6720ebfe73965ead43055cb631593f613b48ec",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -188,6 +188,7 @@ class DINOv3ConvNextPreTrainedModel(PreTrainedModel):\n     config: DINOv3ConvNextConfig\n     base_model_prefix = \"dinov3_convnext\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"DINOv3ConvNextLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "49e75dcd35bfd158c2fec4a34846d097dbb44bcb",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -435,6 +435,7 @@ class DINOv3ViTPreTrainedModel(PreTrainedModel):\n     config: DINOv3ViTConfig\n     base_model_prefix = \"dinov3_vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DINOv3ViTLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "eac5d7449604beb39d1dadb49020b6f03e408b3c",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -785,6 +785,7 @@ class DonutSwinPreTrainedModel(PreTrainedModel):\n     config: DonutSwinConfig\n     base_model_prefix = \"donut\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DonutSwinStage\"]\n "
        },
        {
            "sha": "6185ab3a45d0c46e2fed6fb701be00c6824eba89",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -722,6 +722,7 @@ class DPTPreTrainedModel(PreTrainedModel):\n     config: DPTConfig\n     base_model_prefix = \"dpt\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "417583b4a18edb08130c3b3f9f0a4bf47fadd06a",
            "filename": "src/transformers/models/edgetam/modeling_edgetam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -303,6 +303,7 @@ class EdgeTamPreTrainedModel(PreTrainedModel):\n     config_class = EdgeTamConfig\n     base_model_prefix = \"edgetam\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -919,6 +920,7 @@ def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n     \"\"\"\n )\n class EdgeTamModel(EdgeTamPreTrainedModel):\n+    input_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]"
        },
        {
            "sha": "1e5f1290c8c1c3ec61c7cdbdec8b21a5ee5c3e45",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -773,6 +773,7 @@ class EdgeTamVideoPreTrainedModel(PreTrainedModel):\n     config_class = EdgeTamVideoConfig\n     base_model_prefix = \"edgetam_video\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"video\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -1975,6 +1976,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class EdgeTamVideoModel(EdgeTamVideoPreTrainedModel):\n+    input_modalities = [\"video\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]"
        },
        {
            "sha": "53ca4e8a72baf484d10f785174b59b2ccd03bdf6",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -629,6 +629,7 @@ class EfficientLoFTRPreTrainedModel(PreTrainedModel):\n     config_class = EfficientLoFTRConfig\n     base_model_prefix = \"efficientloftr\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "0e35f791f9d23da8aca440c8846bebda39991096",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -433,6 +433,7 @@ class EfficientNetPreTrainedModel(PreTrainedModel):\n     config: EfficientNetConfig\n     base_model_prefix = \"efficientnet\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = []\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "0ed76ce93b3090955da59a9c1c9d07d8a5736c90",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -926,6 +926,7 @@ class Emu3VQVAE(PreTrainedModel):\n     config: Emu3VQVAEConfig\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -1091,6 +1092,7 @@ def convert_bpe2img(self, img_batch: torch.Tensor) -> torch.Tensor:\n class Emu3PreTrainedModel(PreTrainedModel):\n     config: Emu3Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Emu3DecoderLayer\",\n@@ -1457,6 +1459,7 @@ def forward(\n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"\"\n+    output_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _checkpoint_conversion_mapping = {\n         \"^text_model.model\": \"model.text_model\","
        },
        {
            "sha": "3d15090bb67d922a7022b2e544033e98959b4d72",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -676,6 +676,7 @@ class Emu3VQVAE(PreTrainedModel):\n     config: Emu3VQVAEConfig\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -1041,6 +1042,7 @@ def forward(\n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"\"\n+    output_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _checkpoint_conversion_mapping = {\n         \"^text_model.model\": \"model.text_model\","
        },
        {
            "sha": "8579e1b7a443b7a3b57b1051832e683caeb89e7b",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -987,6 +987,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     config: EomtConfig\n     base_model_prefix = \"eomt\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "be66a7b7598dcdf2caf80b8eba18e08988d72c54",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -392,6 +392,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     config: EomtConfig\n     base_model_prefix = \"eomt\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "8a19b90ac2cf4efa20df2530dccb45d7b8bc620e",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -662,6 +662,7 @@ def forward(self, hidden_states: torch.Tensor):\n class FlavaPreTrainedModel(PreTrainedModel):\n     config: FlavaConfig\n     base_model_prefix = \"flava\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n@@ -697,6 +698,7 @@ class FlavaImageModel(FlavaPreTrainedModel):\n     # This override allows us to load FlavaImageModel from FlavaModel/FlavaForPreTraining checkpoints.\n     base_model_prefix = \"flava.image_model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: FlavaImageConfig, add_pooling_layer: bool = True):\n         r\"\"\"\n@@ -776,6 +778,7 @@ class FlavaTextModel(FlavaPreTrainedModel):\n     config: FlavaTextConfig\n     # This override allows us to load FlavaTextModel from FlavaModel/FlavaForPreTraining checkpoints.\n     base_model_prefix = \"flava.text_model\"\n+    input_modalities = \"text\"\n \n     def __init__(self, config: FlavaTextConfig, add_pooling_layer: bool = True):\n         r\"\"\"\n@@ -1306,6 +1309,7 @@ class FlavaImageCodebook(FlavaPreTrainedModel):\n     base_model_prefix = \"\"\n     config: FlavaImageCodebookConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n \n     def __init__("
        },
        {
            "sha": "d34ce8a86dfdb3b4d9263c6b18d940aacc755092",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -485,6 +485,7 @@ def forward(self, hidden_states: torch.Tensor):\n class Florence2VisionPreTrainedModel(PreTrainedModel):\n     config_class = Florence2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -615,6 +616,7 @@ class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n class Florence2PreTrainedModel(PreTrainedModel):\n     config: Florence2Config\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "06e07846902361c9f6ad710251ab8cceba4bd74f",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1370,6 +1370,7 @@ def forward(self, hidden_states: torch.Tensor):\n class Florence2VisionPreTrainedModel(PreTrainedModel):\n     config_class = Florence2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "fdacd7409615294dc7b0a4948fb62fb81bad4770",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -35,6 +35,7 @@\n class FuyuPreTrainedModel(PreTrainedModel):\n     config: FuyuConfig\n     base_model_prefix = \"fuyu\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "ee72079834ab4f1bb46e463c5e448c211ff3c5d0",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -426,6 +426,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n         \"hidden_states\": Gemma3DecoderLayer,\n         \"attentions\": Gemma3Attention,\n     }\n+    input_modalities = [\"image\", \"text\"]\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n@@ -452,6 +453,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n @auto_docstring\n class Gemma3TextModel(Gemma3PreTrainedModel):\n     config: Gemma3TextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)\n@@ -1335,6 +1337,7 @@ class Gemma3TextForSequenceClassification(GenericForSequenceClassification, Gemm\n     \"\"\"\n \n     config: Gemma3TextConfig\n+    input_modalities = \"text\"\n \n \n __all__ = ["
        },
        {
            "sha": "c7c6589073d3500e78cab774d7ac7d24a4da25cd",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -515,6 +515,7 @@ def forward(\n \n class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     _no_split_modules = [\n         \"Gemma3DecoderLayer\",\n         \"SiglipVisionEmbeddings\",\n@@ -546,6 +547,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n \n class Gemma3TextModel(Gemma2Model):\n     config: Gemma3TextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)\n@@ -1154,6 +1156,7 @@ class Gemma3TextForSequenceClassification(GenericForSequenceClassification, Gemm\n     \"\"\"\n \n     config: Gemma3TextConfig\n+    input_modalities = \"text\"\n \n \n __all__ = ["
        },
        {
            "sha": "be2be9607616e0305bac49b232071f3017cc393a",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -911,6 +911,7 @@ class Gemma3nAudioEncoder(PreTrainedModel):\n     config: Gemma3nAudioConfig\n \n     main_input_name = \"audio_mel\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: Gemma3nAudioConfig):\n         super().__init__(config)\n@@ -1491,6 +1492,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n         \"hidden_states\": Gemma3nTextDecoderLayer,\n         \"attentions\": Gemma3nTextAttention,\n     }\n+    input_modalities = [\"image\", \"text\", \"audio\"]\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n@@ -1505,6 +1507,7 @@ def _init_weights(self, module):\n @auto_docstring(custom_intro=\"The base Gemma 3n language model without a language modeling head.\")\n class Gemma3nTextModel(Gemma3nPreTrainedModel):\n     config: Gemma3nTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Gemma3nTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "41554ede4981797fa1de5e394ce16f7817730158",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1482,6 +1482,7 @@ class Gemma3nAudioEncoder(PreTrainedModel):\n     config: Gemma3nAudioConfig\n \n     main_input_name = \"audio_mel\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: Gemma3nAudioConfig):\n         super().__init__(config)\n@@ -1919,6 +1920,7 @@ def forward(\n class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     config: Gemma3nConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\", \"audio\"]\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "5cc3195b4c38a6e32acbed74bcd7849ab7fc23ee",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -385,6 +385,7 @@ def forward(\n class GitPreTrainedModel(PreTrainedModel):\n     config: GitConfig\n     base_model_prefix = \"git\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):\n@@ -803,6 +804,7 @@ def forward(\n class GitVisionModel(GitPreTrainedModel):\n     config: GitVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.__init__ with CLIP->Git\n     def __init__(self, config: GitVisionConfig):"
        },
        {
            "sha": "1decc6a3442547403fb169dc5c7297533f3d0b15",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -661,6 +661,7 @@ class Glm4vModelOutputWithPast(ModelOutput):\n class Glm4vPreTrainedModel(PreTrainedModel):\n     config: Glm4vConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Glm4vTextDecoderLayer\", \"Glm4vVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -677,6 +678,7 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig\n+    input_modalities = [\"image\", \"video\"]\n     _no_split_modules = [\"Glm4vVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -788,6 +790,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n @auto_docstring\n class Glm4vTextModel(Glm4vPreTrainedModel):\n     config: Glm4vTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Glm4vTextConfig):\n         super().__init__(config)\n@@ -1393,6 +1396,8 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1401,8 +1406,6 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "c1e2030cdab963f76918af080eb8be2966806652",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -751,6 +751,7 @@ class Glm4vPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig\n+    input_modalities = [\"image\", \"video\"]\n     _no_split_modules = [\"Glm4vVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -1354,6 +1355,8 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1362,8 +1365,6 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "5591c4733c002520a841b291199e97bbe2ebbf2d",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -514,6 +514,7 @@ class Glm4vMoePreTrainedModel(PreTrainedModel):\n         \"attentions\": Glm4vMoeTextAttention,\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n     }\n+    input_modalities = [\"text\", \"image\", \"video\"]\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n@@ -554,6 +555,7 @@ class Glm4vMoeCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class Glm4vMoeTextModel(Glm4vMoePreTrainedModel):\n     config: Glm4vMoeTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Glm4vMoeTextConfig):\n         super().__init__(config)\n@@ -937,6 +939,7 @@ def forward(\n \n class Glm4vMoeVisionModel(Glm4vMoePreTrainedModel):\n     config: Glm4vMoeVisionConfig\n+    input_modalities = [\"image\", \"video\"]\n     _no_split_modules = [\"Glm4vMoeVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -1597,6 +1600,8 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vMoeCausalLMOutputWithPast]:\n         r\"\"\"\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1605,8 +1610,6 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "e50d3085986dd60b0195b732d48161dce5525edb",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -458,6 +458,7 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: int):\n class Glm4vMoePreTrainedModel(Glm4MoePreTrainedModel):\n     config: Glm4vMoeConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"text\", \"image\", \"video\"]\n     _no_split_modules = [\"Glm4vMoeTextDecoderLayer\", \"Glm4vMoeVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "17d6f5565edb9eaf711e8949ab0d86894aa32e5e",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -386,6 +386,7 @@ class GLPNPreTrainedModel(PreTrainedModel):\n     config: GLPNConfig\n     base_model_prefix = \"glpn\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = []\n \n     # Copied from transformers.models.segformer.modeling_segformer.SegformerPreTrainedModel._init_weights"
        },
        {
            "sha": "809926990d414ba03383f3bf2821aa1e90598df4",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -277,6 +277,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.FloatTensor]:\n class GotOcr2PreTrainedModel(PreTrainedModel):\n     config: GotOcr2Config\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = False\n@@ -399,6 +400,7 @@ def forward(self, hidden_states):\n \n class GotOcr2VisionEncoder(GotOcr2PreTrainedModel):\n     _can_record_outputs = {\"hidden_states\": GotOcr2VisionLayer, \"attentions\": GotOcr2VisionAttention}\n+    input_modalities = \"image\"\n \n     def __init__(self, config: GotOcr2VisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c85bbf78ba01ff636811f04219e179e09e04da85",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -248,11 +248,11 @@ def __init__(self, config, window_size):\n \n \n class GotOcr2PreTrainedModel(SamPreTrainedModel):\n-    pass\n+    input_modalities = [\"image\", \"text\"]\n \n \n class GotOcr2VisionEncoder(SamVisionEncoder, GotOcr2PreTrainedModel):\n-    pass\n+    input_modalities = \"image\"\n \n \n class GotOcr2MultiModalProjector(nn.Module):"
        },
        {
            "sha": "6973124fb51f949a96d71cf3e9caab5e07eae253",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -281,6 +281,7 @@ def forward(self, hidden_states: torch.Tensor):\n @auto_docstring\n class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     config: GraniteSpeechConfig\n+    input_modalities = [\"audio\", \"text\"]\n \n     _supports_flash_attn = False  # `blip_2_qformer` dependency does not allow for this\n     _supports_sdpa = True"
        },
        {
            "sha": "6c53d3ba21f239f0f6c252548c29df9e374e39e5",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1367,6 +1367,7 @@ class GroundingDinoPreTrainedModel(PreTrainedModel):\n     config: GroundingDinoConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = [\"image\", \"text\"]\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "4c852db4668c7525def7c77d6fb74444cebf3bf7",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -745,6 +745,7 @@ def forward(\n class GroupViTPreTrainedModel(PreTrainedModel):\n     config: GroupViTConfig\n     base_model_prefix = \"groupvit\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):\n@@ -1019,6 +1020,7 @@ def forward(\n \n class GroupViTTextModel(GroupViTPreTrainedModel):\n     config: GroupViTTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: GroupViTTextConfig):\n         super().__init__(config)\n@@ -1123,6 +1125,7 @@ def forward(\n class GroupViTVisionModel(GroupViTPreTrainedModel):\n     config: GroupViTVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: GroupViTVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "2b412c8fa1dd349d923780169b463da3cf8c9e45",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -42,6 +42,7 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     config: HGNetV2Config\n     base_model_prefix = \"hgnetv2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n "
        },
        {
            "sha": "d07e3008da030ef317e21e545cad9869cf721f14",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -167,6 +167,7 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     config: HGNetV2Config\n     base_model_prefix = \"hgnetv2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n "
        },
        {
            "sha": "0316d87ab6c35c3185f9320d73acf8447adc5555",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -773,6 +773,7 @@ class HieraPreTrainedModel(PreTrainedModel):\n     config: HieraConfig\n     base_model_prefix = \"hiera\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module) -> None:"
        },
        {
            "sha": "9729e481f402927f82b0543afad3e762991e34d8",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -632,6 +632,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     config: HubertConfig\n     base_model_prefix = \"hubert\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "a0a7d805c973f1977434fc10db6386299a55c03f",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -128,6 +128,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     config: HubertConfig\n     base_model_prefix = \"hubert\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "5cc389b79344f0bbb579f94c8c67efc3f616910a",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -817,6 +817,7 @@ def forward(\n class IdeficsPreTrainedModel(PreTrainedModel):\n     config: IdeficsConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "0ee1ca8bac68e9b05711e03b6180d1e1d45fa2ba",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -407,6 +407,7 @@ def forward(\n class Idefics2PreTrainedModel(PreTrainedModel):\n     config: Idefics2Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Idefics2VisionAttention\", \"Idefics2MLP\", \"Idefics2PerceiverLayer\", \"Idefics2DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -447,6 +448,7 @@ def _init_weights(self, module):\n )\n class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n     config: Idefics2VisionConfig\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -703,6 +705,7 @@ def forward(\n )\n class Idefics2PerceiverResampler(Idefics2PreTrainedModel):\n     config: Idefics2PerceiverConfig\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attention_2 = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "1fe99f4e685587af9225b0986c5b160b308392e4",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -423,6 +423,7 @@ def forward(self, image_hidden_states):\n class Idefics3PreTrainedModel(PreTrainedModel):\n     config: Idefics3Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Idefics3VisionAttention\", \"Idefics3DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -457,6 +458,7 @@ def _init_weights(self, module):\n )\n class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     config: Idefics3VisionConfig\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "0a0c8fbb032149f960071b5f87ba761905d2d4c0",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -312,6 +312,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     config: IJepaConfig\n     base_model_prefix = \"ijepa\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "f1ae9ee0c92615c8f519453bfbaf944b9329626a",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -362,6 +362,7 @@ class ImageGPTPreTrainedModel(PreTrainedModel):\n     config: ImageGPTConfig\n     base_model_prefix = \"transformer\"\n     main_input_name = \"input_ids\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ImageGPTBlock\"]\n "
        },
        {
            "sha": "901685a074ecb98fedea275e7af2c155cd02e385",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -247,6 +247,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     config: InformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "16d2f2d40105f8f5b7aead277a71d2c1d350b6ad",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -83,6 +83,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     config: InformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "ceec6a15f6acc7f67d70137504ec9dc4c617ce24",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -308,6 +308,7 @@ def forward(\n class InstructBlipPreTrainedModel(PreTrainedModel):\n     config: InstructBlipConfig\n     base_model_prefix = \"blip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True\n@@ -379,6 +380,7 @@ def forward(\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlip, BLIP->INSTRUCTBLIP\n class InstructBlipVisionModel(InstructBlipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     config: InstructBlipVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": InstructBlipEncoderLayer,"
        },
        {
            "sha": "f2ec0fc9dbf01f0c50d60fca6d1667ff2dc2b8d5",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -131,6 +131,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     config: InstructBlipVideoConfig\n     base_model_prefix = \"blip\"\n+    input_modalities = [\"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True\n@@ -346,6 +347,7 @@ def forward(\n \n class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"video\"\n     config: InstructBlipVideoVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": InstructBlipVideoEncoderLayer,"
        },
        {
            "sha": "fe265f4e8aab3f944308ab9ca451515fa6370fc1",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -160,11 +160,11 @@ def __init__(\n \n \n class InstructBlipVideoPreTrainedModel(InstructBlipPreTrainedModel):\n-    pass\n+    input_modalities = [\"video\", \"text\"]\n \n \n class InstructBlipVideoVisionModel(InstructBlipVisionModel):\n-    pass\n+    input_modalities = \"video\"\n \n \n class InstructBlipVideoQFormerModel(InstructBlipQFormerModel):"
        },
        {
            "sha": "308bd851103841a0fd2764b5eea693cc777562bf",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -398,6 +398,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     config: InternVLVisionConfig\n     base_model_prefix = \"internvl_vision\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = [\"image\", \"video\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n@@ -471,6 +472,7 @@ def forward(\n class InternVLPreTrainedModel(PreTrainedModel):\n     config: InternVLConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\", \"video\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "213c4a2dd81dd40b90e0ffb9e0bb93263fe6cf14",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -355,6 +355,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     config: InternVLVisionConfig\n     base_model_prefix = \"internvl_vision\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = [\"image\", \"video\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n@@ -425,7 +426,7 @@ def forward(\n \n \n class InternVLPreTrainedModel(LlavaPreTrainedModel):\n-    pass\n+    input_modalities = [\"image\", \"text\", \"video\"]\n \n \n INTERNVL_INPUTS_DOCSTRING = None"
        },
        {
            "sha": "5dd4508257d347af0fbac7210e66d06c47c333a5",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -49,6 +49,7 @@\n class JanusPreTrainedModel(PreTrainedModel):\n     config: JanusConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -545,6 +546,7 @@ def forward(\n @auto_docstring\n class JanusVisionModel(JanusPreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     config: JanusVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": JanusEncoderLayer,\n@@ -1163,6 +1165,7 @@ def forward(\n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    output_modalities = [\"image\", \"text\"]\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: JanusConfig):"
        },
        {
            "sha": "eb2a90a2bcc80493dfc379a60fec746e6e9fc4d9",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -382,6 +382,7 @@ def __init__(\n class JanusPreTrainedModel(PreTrainedModel):\n     config: JanusConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -980,6 +981,7 @@ def forward(\n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.language_model.embed_tokens.weight\", \"lm_head.weight\"]\n+    output_modalities = [\"image\", \"text\"]\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: JanusConfig):"
        },
        {
            "sha": "62aeb8d1d1ad81c0359d59106617c13200275aca",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1113,6 +1113,7 @@ def forward(\n @auto_docstring\n class Kosmos2PreTrainedModel(PreTrainedModel):\n     config: Kosmos2Config\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Kosmos2VisionEncoderLayer\", \"Kosmos2TextBlock\"]\n     _supports_attention_backend = True\n@@ -1175,6 +1176,7 @@ def _init_weights(self, module: nn.Module):\n class Kosmos2VisionModel(Kosmos2PreTrainedModel):\n     config: Kosmos2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.__init__ with CLIP_VISION->KOSMOS2_VISION,CLIP->Kosmos2,self.vision_model->self.model\n     def __init__(self, config: Kosmos2VisionConfig):\n@@ -1207,6 +1209,7 @@ def forward(\n \n class Kosmos2TextModel(Kosmos2PreTrainedModel):\n     config: Kosmos2TextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Kosmos2TextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "f8756aa9b000910347a7b94edcad2feb81f5aaae",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1219,6 +1219,7 @@ class Kosmos2_5PreTrainedModel(PreTrainedModel):\n     \"\"\"\n \n     config_class = Kosmos2_5Config\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Kosmos2_5VisionLayer\", \"Kosmos2_5TextBlock\"]\n     _supports_flash_attn_2 = True\n@@ -1253,6 +1254,7 @@ def _init_weights(self, module):\n \n class Kosmos2_5VisionModel(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5VisionConfig\n+    input_modalities = \"text\"\n \n     # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionModel.__init__ with Pix2Struct->Kosmos2_5\n     def __init__(self, config: Kosmos2_5VisionConfig):\n@@ -1314,6 +1316,7 @@ def forward(\n # Adapted from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextModel with KOSMOS2->KOSMOS2_5\n class Kosmos2_5TextModel(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5TextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Kosmos2_5TextConfig):\n         super().__init__(config)\n@@ -1499,6 +1502,7 @@ def forward(\n )\n class Kosmos2_5TextForCausalLM(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5TextConfig\n+    input_modalities = \"text\"\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: Kosmos2_5TextConfig):"
        },
        {
            "sha": "8f61034fdf4587cc0d57cc8a5bac1cd4b228033d",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -115,6 +115,7 @@ def forward(self, x, layer_idx=None):\n class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n     config: KyutaiSpeechToTextConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"KyutaiSpeechToTextDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn = True\n@@ -1064,6 +1065,7 @@ class KyutaiSpeechToTextForConditionalGeneration(KyutaiSpeechToTextPreTrainedMod\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     _keep_in_fp32_modules_strict = [\"codec_model\"]\n+    output_modalities = [\"audio\", \"text\"]\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "5b2b409001027d7734213410c7f3c0811696ad31",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -252,6 +252,7 @@ def __init__(self, config):\n \n class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin):\n     _keep_in_fp32_modules_strict = [\"codec_model\"]\n+    output_modalities = [\"audio\", \"text\"]\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "faf3979d1edb5b3e6af932d93c61b0325a2f6d88",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -456,6 +456,7 @@ def forward(\n class LayoutLMv2PreTrainedModel(PreTrainedModel):\n     config: LayoutLMv2Config\n     base_model_prefix = \"layoutlmv2\"\n+    input_modalities = [\"image\", \"text\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "3aa97051f85502fa9a25184d392a7fe6d89965c3",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -201,6 +201,7 @@ def forward(\n class LayoutLMv3PreTrainedModel(PreTrainedModel):\n     config: LayoutLMv3Config\n     base_model_prefix = \"layoutlmv3\"\n+    input_modalities = [\"image\", \"text\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "5d331081721c24e244116b92b3bcf458e9b05cf9",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -469,6 +469,7 @@ class LevitPreTrainedModel(PreTrainedModel):\n     config: LevitConfig\n     base_model_prefix = \"levit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"LevitResidualLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "317786625ba848f40e6f221c09d26d7e8d9f9a61",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -77,6 +77,7 @@ def pixel_unshuffle(self, hidden_states: torch.Tensor):\n class Lfm2VlPreTrainedModel(PreTrainedModel):\n     config: Lfm2VlConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "5db33105b97698e0265635efc8d8d48983b634f2",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -423,6 +423,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     config: LightGlueConfig\n     base_model_prefix = \"lightglue\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "cfb0b35a395fe95dd75fb533ac09ff7fd250aab6",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -583,6 +583,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     config: LightGlueConfig\n     base_model_prefix = \"lightglue\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "325ff5031532a5d78feedd9c0da37d0e7f6863ef",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -426,6 +426,7 @@ def forward(\n @auto_docstring\n class Llama4PreTrainedModel(PreTrainedModel):\n     config: Llama4Config\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = False\n@@ -466,6 +467,7 @@ def _init_weights(self, module):\n class Llama4TextModel(Llama4PreTrainedModel):\n     _no_split_modules = [\"Llama4TextDecoderLayer\"]\n     base_model_prefix = \"model\"\n+    input_modalities = \"text\"\n     config: Llama4TextConfig\n     _can_record_outputs = {\n         \"attentions\": Llama4TextAttention,\n@@ -1002,6 +1004,7 @@ def forward(self, hidden_states):\n \n class Llama4VisionModel(Llama4PreTrainedModel):\n     base_model_prefix = \"vision_model\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"Llama4VisionEncoderLayer\"]\n     config: Llama4VisionConfig\n "
        },
        {
            "sha": "0ee351b03b54b96231490a96dd6524c44ae12870",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -111,6 +111,7 @@ def forward(self, image_features):\n class LlavaPreTrainedModel(PreTrainedModel):\n     config: LlavaConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "7e01bbb385f8498495d09b527933d188333ab256",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -223,6 +223,7 @@ def forward(self, image_features):\n class LlavaNextPreTrainedModel(PreTrainedModel):\n     config: LlavaNextConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "98b46e13f5875ef13219db039ea31e137681d4f6",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -164,6 +164,7 @@ def forward(self, image_features):\n class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     config: LlavaNextVideoConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "92a3f51f8a71d167d0c8dbc6f7751e94861c7bcd",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -25,6 +25,7 @@\n     LlavaNextModel,\n     LlavaNextModelOutputWithPast,\n     LlavaNextMultiModalProjector,\n+    LlavaNextPreTrainedModel,\n     TransformersKwargs,\n     image_size_to_num_patches,\n )\n@@ -258,6 +259,10 @@ class LlavaNextVideoMultiModalProjector(LlavaNextMultiModalProjector):\n     pass\n \n \n+class LlavaNextVideoPreTrainedModel(LlavaNextPreTrainedModel):\n+    input_modalities = [\"image\", \"video\", \"text\"]\n+\n+\n class LlavaNextVideoModel(LlavaNextModel):\n     def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         super().__init__(config, **super_kwargs)\n@@ -713,5 +718,5 @@ def prepare_inputs_for_generation(\n     \"LlavaNextVideoConfig\",\n     \"LlavaNextVideoForConditionalGeneration\",\n     \"LlavaNextVideoModel\",\n-    \"LlavaNextVideoPreTrainedModel\",  # noqa: F822\n+    \"LlavaNextVideoPreTrainedModel\",\n ]"
        },
        {
            "sha": "4484d4647da194a5a0f32063e49faa85211bb1c8",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -105,6 +105,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     config: LlavaOnevisionConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "08be81ae3c0ea482e4eb839c3fa7ba032493fc40",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -679,6 +679,7 @@ def forward(self, sequence_output, pooled_output):\n class LxmertPreTrainedModel(PreTrainedModel):\n     config: LxmertConfig\n     base_model_prefix = \"lxmert\"\n+    input_modalities = [\"image\", \"text\"]\n     _supports_param_buffer_assignment = False\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "278f977320edfa99cae95641ee51118e73e11386",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -2100,6 +2100,7 @@ class Mask2FormerPreTrainedModel(PreTrainedModel):\n     config: Mask2FormerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def _init_weights(self, module: nn.Module):\n         xavier_std = self.config.init_xavier_std"
        },
        {
            "sha": "bc961d2eb0ecb5097aead63c2f30596ed584bf57",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1434,6 +1434,7 @@ class MaskFormerPreTrainedModel(PreTrainedModel):\n     config: MaskFormerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def _init_weights(self, module: nn.Module):\n         xavier_std = self.config.init_xavier_std"
        },
        {
            "sha": "f0d5d1dc3dd8924edae7a309d64f9c880dfe10ba",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -697,6 +697,7 @@ class MaskFormerSwinPreTrainedModel(PreTrainedModel):\n     config: MaskFormerSwinConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MaskFormerSwinStage\"]\n "
        },
        {
            "sha": "c01b97d70bf6420949c0245e298aca43a4dbad6f",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -270,6 +270,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class MetaClip2PreTrainedModel(PreTrainedModel):\n     config: MetaClip2Config\n     base_model_prefix = \"metaclip_2\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n@@ -568,6 +569,7 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     ```\"\"\"\n \n     config: MetaClip2TextConfig\n+    input_modalities = \"text\"\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n     _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n@@ -670,6 +672,7 @@ class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     ```\"\"\"\n \n     config: MetaClip2TextConfig\n+    input_modalities = \"text\"\n \n     _supports_flash_attn = False\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n@@ -1129,6 +1132,7 @@ class MetaClip2VisionModel(MetaClip2PreTrainedModel):\n \n     config: MetaClip2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2VisionConfig):\n@@ -1235,6 +1239,7 @@ class MetaClip2VisionModelWithProjection(MetaClip2PreTrainedModel):\n \n     config: MetaClip2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: MetaClip2VisionConfig):\n         super().__init__(config)\n@@ -1304,6 +1309,7 @@ def forward(\n )\n class MetaClip2ForImageClassification(MetaClip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: MetaClip2Config) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "3ab8becc35fbd9d8172d967c23444e335b0a7ce4",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -223,6 +223,7 @@ class MetaClip2MLP(CLIPMLP):\n class MetaClip2PreTrainedModel(PreTrainedModel):\n     config: MetaClip2Config\n     base_model_prefix = \"metaclip_2\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "8d3aa4d560ef753934075b28f42fe9274da219ae",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1356,6 +1356,7 @@ class MimiPreTrainedModel(PreTrainedModel):\n     config: MimiConfig\n     base_model_prefix = \"mimi\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MimiDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "b98efd38e824538f51e6f72967ff25ec0eb5d0ae",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -177,6 +177,7 @@ class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n class Mistral3PreTrainedModel(PreTrainedModel):\n     config: Mistral3Config\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "76414a21225f3315c8bd4329e06154c4dca36d03",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -554,6 +554,7 @@ def _init_weights(self, module):\n class MLCDVisionModel(MLCDPreTrainedModel):\n     config: MLCDVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"MLCDEncoderLayer\"]\n \n     def __init__(self, config: MLCDVisionConfig):"
        },
        {
            "sha": "c62e28312aebf8f1257386692db5d847be316b51",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -755,6 +755,7 @@ def forward(self, x, position_ids):\n class MllamaPreTrainedModel(PreTrainedModel):\n     config: MllamaConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"MllamaVisionEncoderLayer\",\n@@ -940,6 +941,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class MllamaVisionModel(MllamaPreTrainedModel):\n     config: MllamaVisionConfig\n     base_model_prefix = \"vision_model\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: MllamaVisionConfig):\n         super().__init__(config)\n@@ -1137,6 +1139,7 @@ def forward(\n class MllamaTextModel(MllamaPreTrainedModel):\n     config: MllamaTextConfig\n     base_model_prefix = \"language_model.model\"\n+    input_modalities = \"text\"\n \n     def __init__(self, config: MllamaTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "3af9608e0b244237211d143abc7ade77754e0be1",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -504,6 +504,7 @@ class MMGroundingDinoPreTrainedModel(PreTrainedModel):\n     config: MMGroundingDinoConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = [\"image\", \"text\"]\n \n     def _init_weights(self, module):\n         std = self.config.init_std"
        },
        {
            "sha": "25f8a826437cba4dc8998ed120473ac3b397ff3f",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -128,6 +128,7 @@ class MobileNetV1PreTrainedModel(PreTrainedModel):\n     config: MobileNetV1Config\n     base_model_prefix = \"mobilenet_v1\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     _no_split_modules = []\n "
        },
        {
            "sha": "0a92fb2f109326db4fd20fbf01c8d0fdbc130f9a",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -254,6 +254,7 @@ class MobileNetV2PreTrainedModel(PreTrainedModel):\n     config: MobileNetV2Config\n     base_model_prefix = \"mobilenet_v2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     _no_split_modules = []\n "
        },
        {
            "sha": "f7f30b7faf1d55c681a05d21e23ad9acffeacfca",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -603,6 +603,7 @@ class MobileViTPreTrainedModel(PreTrainedModel):\n     config: MobileViTConfig\n     base_model_prefix = \"mobilevit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MobileViTLayer\"]\n "
        },
        {
            "sha": "c637273f03950251fd4af758c49974083806f3e6",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -570,6 +570,7 @@ class MobileViTV2PreTrainedModel(PreTrainedModel):\n     config: MobileViTV2Config\n     base_model_prefix = \"mobilevitv2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MobileViTV2Layer\"]\n "
        },
        {
            "sha": "afb1347474b3a2aa5ed0e895d481a5901b13e55e",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -460,6 +460,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     config: MoonshineConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n     _supports_flash_attn = True"
        },
        {
            "sha": "b40177195d521d41f4aebaa00dae072a9cb5ea3c",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -493,6 +493,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     config: MoonshineConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n     _supports_flash_attn = True"
        },
        {
            "sha": "77645cd25010ccc26ec656514da244d59a1aadb6",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -800,6 +800,7 @@ def forward(\n class MoshiPreTrainedModel(PreTrainedModel):\n     config: MoshiConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoshiDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn = True\n@@ -1454,6 +1455,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class MoshiForCausalLM(MoshiPreTrainedModel, GenerationMixin):\n+    input_modalities = \"text\"\n     _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n \n     # Copied from transformers.models.gemma.modeling_gemma.GemmaForCausalLM.__init__ with Gemma->Moshi\n@@ -1573,6 +1575,7 @@ def forward(\n class MoshiForConditionalGeneration(MoshiPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"decoder.model.embed_tokens.weight\", \"decoder.lm_head.weight\"]\n     config: MoshiConfig\n+    output_modalities = [\"audio\", \"text\"]\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "7386318895f1a50b7bd0549a236121a6046da147",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -785,6 +785,8 @@ def forward(\n     \"\"\"\n )\n class MusicgenForCausalLM(MusicgenPreTrainedModel, GenerationMixin):\n+    output_modalities = \"audio\"\n+\n     def __init__(self, config: MusicgenDecoderConfig):\n         super().__init__(config)\n \n@@ -1285,6 +1287,7 @@ def generate(\n )\n class MusicgenForConditionalGeneration(MusicgenPreTrainedModel, GenerationMixin):\n     config: MusicgenConfig\n+    output_modalities = \"audio\"\n     base_model_prefix = \"encoder_decoder\"\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True"
        },
        {
            "sha": "74632ec86c81d7d3e2e437e90f08ece2f7ed3fa4",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -739,6 +739,8 @@ def forward(\n )\n # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody,MusicGen->Musicgen Melody\n class MusicgenMelodyForCausalLM(MusicgenMelodyPreTrainedModel, GenerationMixin):\n+    output_modalities = \"audio\"\n+\n     def __init__(self, config: MusicgenMelodyDecoderConfig):\n         super().__init__(config)\n \n@@ -1229,6 +1231,7 @@ def generate(\n class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     config: MusicgenMelodyConfig\n     main_input_name = \"input_ids\"\n+    output_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "3c552a4b5cb54601160d579942e48b61f0b3a89c",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -985,6 +985,7 @@ class OmDetTurboPreTrainedModel(PreTrainedModel):\n     config: OmDetTurboConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = [\"image\", \"text\"]\n \n     def _init_weights(self, module):\n         def linear_init_(module_to_init):"
        },
        {
            "sha": "458128f2d083673deb447e164d35ee1f4d196747",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -2768,6 +2768,7 @@ class OneFormerPreTrainedModel(PreTrainedModel):\n     config: OneFormerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def _init_weights(self, module: nn.Module):\n         xavier_std = self.config.init_xavier_std"
        },
        {
            "sha": "02a8af5d5865e55ed649c2c65a1cc59986c136c2",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -418,6 +418,7 @@ def forward(self, visual_tokens: torch.Tensor) -> torch.Tensor:\n class Ovis2PreTrainedModel(PreTrainedModel):\n     config: Ovis2Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Ovis2VisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "f6277dc91a0a4cdf61b410e4946feced274e107c",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -147,6 +147,7 @@ def forward(self, visual_tokens: torch.Tensor) -> torch.Tensor:\n class Ovis2PreTrainedModel(PreTrainedModel):\n     config: Ovis2Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Ovis2VisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "391470ccb1de9a6b307bca4e2f18d06dd204dee8",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -563,6 +563,7 @@ def forward(\n class Owlv2PreTrainedModel(PreTrainedModel):\n     config: Owlv2Config\n     base_model_prefix = \"owlv2\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Owlv2EncoderLayer\"]\n \n@@ -768,6 +769,7 @@ def forward(\n # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextModel with google/owlvit-base-patch32->google/owlv2-base-patch16, OWLVIT->OWLV2,OwlViT->Owlv2\n class Owlv2TextModel(Owlv2PreTrainedModel):\n     config: Owlv2TextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Owlv2TextConfig):\n         super().__init__(config)\n@@ -880,6 +882,7 @@ def forward(\n class Owlv2VisionModel(Owlv2PreTrainedModel):\n     config: Owlv2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: Owlv2VisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "0eb4ddbcd445ee91520423ecda4a272747d8ea9c",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -550,6 +550,7 @@ def forward(\n class OwlViTPreTrainedModel(PreTrainedModel):\n     config: OwlViTConfig\n     base_model_prefix = \"owlvit\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OwlViTEncoderLayer\"]\n \n@@ -752,6 +753,7 @@ def forward(\n \n class OwlViTTextModel(OwlViTPreTrainedModel):\n     config: OwlViTTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: OwlViTTextConfig):\n         super().__init__(config)\n@@ -862,6 +864,7 @@ def forward(\n class OwlViTVisionModel(OwlViTPreTrainedModel):\n     config: OwlViTVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: OwlViTVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "b3887a002c8fdbbfad4962e45373dcb855076dc3",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -207,6 +207,7 @@ def create_causal_mask_mapping(\n class PaliGemmaPreTrainedModel(PreTrainedModel):\n     config: PaliGemmaConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "7f74cf33418ea7f6d63a759379e9463230f0ee98",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -418,6 +418,7 @@ class ParakeetPreTrainedModel(PreTrainedModel):\n     config: ParakeetCTCConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ParakeetEncoderBlock\"]\n     _supports_flat_attention_mask = True"
        },
        {
            "sha": "b8366c8cd086d8f9b31e28619463c146cbdfed33",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -304,6 +304,7 @@ class ParakeetPreTrainedModel(PreTrainedModel):\n     config: ParakeetCTCConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ParakeetEncoderBlock\"]\n     _supports_flat_attention_mask = True"
        },
        {
            "sha": "8cd4ec0594738022f4577827e0e27d115cd2b15a",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -682,6 +682,7 @@ class PatchTSMixerPreTrainedModel(PreTrainedModel):\n     config: PatchTSMixerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     supports_gradient_checkpointing = False\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "6411b89567430b5bed1ecd1f263072aee4845445",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -552,6 +552,7 @@ class PatchTSTPreTrainedModel(PreTrainedModel):\n     config: PatchTSTConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     supports_gradient_checkpointing = False\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "0b734c0714ee87883847d19ab8403e984abba92b",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -529,6 +529,7 @@ class PerceiverPreTrainedModel(PreTrainedModel):\n     config: PerceiverConfig\n     base_model_prefix = \"perceiver\"\n     main_input_name = \"inputs\"\n+    input_modalities = \"image\"  # techinically can be anything but HF impl has only image processor\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "9fb7ede3e9f86aeace67d44a27161e966101ac90",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -90,6 +90,7 @@ def forward(self, features):\n class PerceptionLMPreTrainedModel(PreTrainedModel):\n     config: PerceptionLMConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "3e045e518560f72800824b33cc2ee31e49a8ef02",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -308,6 +308,7 @@ def default_flax_embed_init(tensor):\n class Phi4MultimodalVisionPreTrainedModel(PreTrainedModel):\n     config: Phi4MultimodalVisionConfig\n     base_model_prefix = \"phi4_vision\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n@@ -931,6 +932,7 @@ def forward(self, x):\n @auto_docstring\n class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     config: Phi4MultimodalAudioConfig\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n     _supports_flash_attn = True\n@@ -1529,6 +1531,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n         \"attentions\": Phi4MultimodalAttention,\n     }\n     _version = \"0.0.5\"\n+    input_modalities = [\"image\", \"audio\", \"text\"]\n \n     def _init_weights(self, module):\n         super()._init_weights(module)"
        },
        {
            "sha": "d745217fe0b3cb7dbf605ca07ff55d613b6aa9cb",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -538,6 +538,7 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n class Phi4MultimodalVisionPreTrainedModel(SiglipPreTrainedModel):\n     config: Phi4MultimodalVisionConfig\n     base_model_prefix = \"phi4_vision\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n@@ -1116,6 +1117,7 @@ def forward(self, x):\n @auto_docstring\n class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     config: Phi4MultimodalAudioConfig\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n     _supports_flash_attn = True\n@@ -1446,6 +1448,8 @@ class Phi4MultimodalRotaryEmbedding(Phi3RotaryEmbedding):\n \n \n class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n+    input_modalities = [\"image\", \"audio\", \"text\"]\n+\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Phi4MultimodalImageEmbedding):"
        },
        {
            "sha": "e38176b3f1ed82e5ffde4eb6d05d97c7c498cca0",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -337,6 +337,7 @@ def forward(\n @auto_docstring\n class Pix2StructPreTrainedModel(PreTrainedModel):\n     config: Pix2StructConfig\n+    input_modalities = [\"image\", \"text\"]\n \n     _can_compile_fullgraph = False\n \n@@ -461,6 +462,7 @@ def _shift_right(self, input_ids):\n class Pix2StructVisionModel(Pix2StructPreTrainedModel):\n     config: Pix2StructVisionConfig\n     main_input_name = \"flattened_patches\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Pix2StructVisionLayer\"]\n \n@@ -962,6 +964,7 @@ def forward(\n )\n class Pix2StructTextModel(Pix2StructPreTrainedModel):\n     config: Pix2StructTextConfig\n+    input_modalities = \"text\"\n     _no_split_modules = [\"Pix2StructTextBlock\"]\n     _tied_weights_keys = [\"lm_head.weight\"]\n     supports_gradient_checkpointing = True"
        },
        {
            "sha": "817d1202f32eb46421e190b228422e29223eaa35",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -398,6 +398,7 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     config: PixtralVisionConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "a32b6dde21b5a8bff575f64bd0a796154c8cb3e3",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -242,6 +242,7 @@ class PoolFormerPreTrainedModel(PreTrainedModel):\n     config: PoolFormerConfig\n     base_model_prefix = \"poolformer\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"PoolFormerLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "20f34c8d4d80736053b13253d2b578685ca36613",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -538,6 +538,7 @@ def forward(\n class Pop2PianoPreTrainedModel(PreTrainedModel):\n     config: Pop2PianoConfig\n     base_model_prefix = \"transformer\"\n+    output_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     _can_compile_fullgraph = False"
        },
        {
            "sha": "cf9b260a0fa39a0b0213956d5fc3471fafaba0c7",
            "filename": "src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -243,6 +243,7 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n     config: PromptDepthAnythingConfig\n     base_model_prefix = \"prompt_depth_anything\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n "
        },
        {
            "sha": "33dfab832e71dfdabaf6707d7dad982eed2c9527",
            "filename": "src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -162,6 +162,7 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n     config: PromptDepthAnythingConfig\n     base_model_prefix = \"prompt_depth_anything\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n "
        },
        {
            "sha": "4abde5266d1183ac4daa06a1f55e3a36f33d92c0",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -418,6 +418,7 @@ class PvtPreTrainedModel(PreTrainedModel):\n     config: PvtConfig\n     base_model_prefix = \"pvt\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = []\n \n     def _init_weights(self, module: nn.Module) -> None:"
        },
        {
            "sha": "113a4a14bd95389a934d6b6a219cfc0f5d93b79a",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -365,6 +365,7 @@ class PvtV2PreTrainedModel(PreTrainedModel):\n     config: PvtV2Config\n     base_model_prefix = \"pvt_v2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:"
        },
        {
            "sha": "e4bc84ea4f66e9e3f86426e76642ac57587c75ff",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -64,6 +64,7 @@\n class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     config: Qwen2_5OmniConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\", \"Qwen2_5OmniVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -74,6 +75,8 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         self,\n         attention_mask: torch.Tensor,\n@@ -705,6 +708,7 @@ def forward(self, seqlen: int):\n class Qwen2_5OmniAudioEncoder(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniAudioEncoderConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"Qwen2_5OmniAudioEncoderLayer\"]\n     _supports_sdpa = True\n \n@@ -1071,6 +1075,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n class Qwen2_5OmniVisionEncoder(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n     _no_split_modules = [\"Qwen2_5OmniVisionBlock\"]\n+    input_modalities = [\"image\", \"video\"]\n \n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None:\n         super().__init__(config, *inputs, **kwargs)\n@@ -1498,6 +1503,7 @@ def forward(\n @auto_docstring\n class Qwen2_5OmniThinkerTextModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniTextConfig\n+    input_modalities = \"text\"\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\"]\n \n     def __init__(self, config: Qwen2_5OmniTextConfig):\n@@ -2074,6 +2080,8 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class Qwen2_5OmniTalkerModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniTalkerConfig\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+\n     _no_split_modules = [\"Qwen2_5OmniTalkerDecoderLayer\"]\n \n     def __init__(self, config: Qwen2_5OmniTalkerConfig):\n@@ -2232,6 +2240,7 @@ def forward(\n class Qwen2_5OmniTalkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config: Qwen2_5OmniTalkerConfig\n     base_model_prefix = \"talker\"\n+    output_modalities = \"audio\"\n \n     def __init__(self, config: Qwen2_5OmniTalkerConfig):\n         super().__init__(config)\n@@ -3336,6 +3345,7 @@ def forward(self, hidden_states):\n )\n class Qwen2_5OmniToken2WavBigVGANModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniBigVGANConfig\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: Qwen2_5OmniBigVGANConfig):\n         super().__init__(config)\n@@ -3471,6 +3481,7 @@ def integrate(self, time_points):\n )\n class Qwen2_5OmniToken2WavDiTModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniDiTConfig\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"DiTDecoderLayer\"]\n \n     def __init__(self, config: Qwen2_5OmniDiTConfig):\n@@ -3627,6 +3638,7 @@ def ode_function(time_step, hidden_states):\n class Qwen2_5OmniToken2WavModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniToken2WavConfig\n     base_model_prefix = \"model\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"Qwen2_5OmniToken2WavDiTModel\", \"Qwen2_5OmniToken2WavBigVGANModel\"]\n \n     def __init__(self, config: Qwen2_5OmniToken2WavConfig):\n@@ -3694,6 +3706,7 @@ def forward(\n )\n class Qwen2_5OmniForConditionalGeneration(Qwen2_5OmniPreTrainedModel, GenerationMixin):\n     config: Qwen2_5OmniConfig\n+    output_modalities = [\"audio\", \"text\"]\n     _no_split_modules = [\n         \"Qwen2_5OmniTalkerForConditionalGeneration\",\n         \"Qwen2_5OmniToken2WavModel\","
        },
        {
            "sha": "e9899ad752f0c8c2df32665e3065032e6a3bb686",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1125,10 +1125,13 @@ def get_text_config(self, *args, **kwargs):\n \n class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5OmniConfig\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n     _can_compile_fullgraph = False\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1705,6 +1708,7 @@ def forward(self, seqlen: int):\n class Qwen2_5OmniAudioEncoder(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniAudioEncoderConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"Qwen2_5OmniAudioEncoderLayer\"]\n     _supports_sdpa = True\n \n@@ -1990,6 +1994,7 @@ def forward(\n \n class Qwen2_5OmniVisionEncoder(Qwen2_5_VisionTransformerPretrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n+    input_modalities = [\"image\", \"video\"]\n     _no_split_modules = [\"Qwen2_5OmniVisionBlock\"]\n \n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None:\n@@ -2525,6 +2530,8 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n \n class Qwen2_5OmniTalkerModel(Qwen2_5_VLTextModel):\n     config: Qwen2_5OmniTalkerConfig\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+\n     _no_split_modules = [\"Qwen2_5OmniTalkerDecoderLayer\"]\n \n     def __init__(self, config: Qwen2_5OmniTalkerConfig):\n@@ -2535,6 +2542,7 @@ def __init__(self, config: Qwen2_5OmniTalkerConfig):\n class Qwen2_5OmniTalkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config: Qwen2_5OmniTalkerConfig\n     base_model_prefix = \"talker\"\n+    output_modalities = \"audio\"\n \n     def __init__(self, config: Qwen2_5OmniTalkerConfig):\n         super().__init__(config)\n@@ -3639,6 +3647,7 @@ def forward(self, hidden_states):\n )\n class Qwen2_5OmniToken2WavBigVGANModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniBigVGANConfig\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: Qwen2_5OmniBigVGANConfig):\n         super().__init__(config)\n@@ -3774,6 +3783,7 @@ def integrate(self, time_points):\n )\n class Qwen2_5OmniToken2WavDiTModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniDiTConfig\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"DiTDecoderLayer\"]\n \n     def __init__(self, config: Qwen2_5OmniDiTConfig):\n@@ -3930,6 +3940,7 @@ def ode_function(time_step, hidden_states):\n class Qwen2_5OmniToken2WavModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniToken2WavConfig\n     base_model_prefix = \"model\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"Qwen2_5OmniToken2WavDiTModel\", \"Qwen2_5OmniToken2WavBigVGANModel\"]\n \n     def __init__(self, config: Qwen2_5OmniToken2WavConfig):\n@@ -3997,6 +4008,7 @@ def forward(\n )\n class Qwen2_5OmniForConditionalGeneration(Qwen2_5OmniPreTrainedModel, GenerationMixin):\n     config: Qwen2_5OmniConfig\n+    output_modalities = [\"audio\", \"text\"]\n     _no_split_modules = [\n         \"Qwen2_5OmniTalkerForConditionalGeneration\",\n         \"Qwen2_5OmniToken2WavModel\","
        },
        {
            "sha": "d7b642dd3117a19e97d37144c1099ca26547e0b4",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -293,6 +293,7 @@ def forward(\n class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     config: Qwen2_5_VLConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -766,6 +767,7 @@ def forward(\n @auto_docstring\n class Qwen2_5_VLTextModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5_VLTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Qwen2_5_VLTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "23fc12e8142712eb6c552cdd2cd9847304fbe814",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -249,6 +249,7 @@ def forward(\n class Qwen2AudioPreTrainedModel(PreTrainedModel):\n     config: Qwen2AudioConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2AudioAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -295,6 +296,7 @@ class Qwen2AudioEncoder(Qwen2AudioPreTrainedModel):\n     # Ignore copy\n     config: Qwen2AudioEncoderConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"Qwen2AudioEncoderLayer\"]\n \n     def __init__(self, config: Qwen2AudioEncoderConfig):"
        },
        {
            "sha": "58da069cef84812f48017fb299b92e86a3b11086",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -627,6 +627,7 @@ def forward(\n class Qwen2VLPreTrainedModel(PreTrainedModel):\n     config: Qwen2VLConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2VLDecoderLayer\", \"Qwen2VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -640,6 +641,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n @auto_docstring\n class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):\n     config: Qwen2VLVisionConfig\n+    input_modalities = [\"image\", \"video\"]\n     _no_split_modules = [\"Qwen2VLVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -737,6 +739,7 @@ def forward(\n @auto_docstring\n class Qwen2VLTextModel(Qwen2VLPreTrainedModel):\n     config: Qwen2VLTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Qwen2VLTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "34b97ead4777c61417e90f9a84578f464bd2fa73",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -67,6 +67,7 @@\n class Qwen3OmniMoePreTrainedModel(PreTrainedModel):\n     config: Qwen3OmniMoeConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3OmniMoeDecoderLayer\", \"Qwen3OmniMoeVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -88,6 +89,8 @@ def _get_feat_extract_output_lengths(input_lengths):\n \n \n class Qwen3OmniMoePreTrainedModelForConditionalGeneration(Qwen3OmniMoePreTrainedModel):\n+    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         self,\n         attention_mask: torch.Tensor,\n@@ -634,6 +637,7 @@ def forward(self, seqlen: int):\n class Qwen3OmniMoeAudioEncoder(Qwen3OmniMoePreTrainedModel):\n     config: Qwen3OmniMoeAudioEncoderConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"Qwen3OmniMoeAudioEncoderLayer\"]\n     _supports_sdpa = True\n \n@@ -3638,6 +3642,8 @@ def forward(self, hidden):\n \n \n class Qwen3OmniMoeCode2Wav(Qwen3OmniMoePreTrainedModel):\n+    input_modalities = \"audio\"\n+\n     def __init__(self, config: Qwen3OmniMoeCode2WavConfig):\n         super().__init__(config)\n         self.total_upsample = np.prod(config.upsample_rates + config.upsampling_ratios)\n@@ -3700,6 +3706,7 @@ def chunked_decode(self, codes, chunk_size=300, left_context_size=25):\n \n class Qwen3OmniMoeForConditionalGeneration(Qwen3OmniMoePreTrainedModel, GenerationMixin):\n     config_class = Qwen3OmniMoeConfig\n+    output_modalities = [\"text\", \"audio\"]\n \n     def __init__(self, config: Qwen3OmniMoeConfig):\n         super().__init__(config)"
        },
        {
            "sha": "e41832e2b520b82f92f81254bf96cbb2c511ac29",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -2220,6 +2220,8 @@ def forward(self, hidden):\n \n \n class Qwen3OmniMoeCode2Wav(Qwen3OmniMoePreTrainedModel):\n+    input_modalities = \"audio\"\n+\n     def __init__(self, config: Qwen3OmniMoeCode2WavConfig):\n         super().__init__(config)\n         self.total_upsample = np.prod(config.upsample_rates + config.upsampling_ratios)\n@@ -2282,6 +2284,7 @@ def chunked_decode(self, codes, chunk_size=300, left_context_size=25):\n \n class Qwen3OmniMoeForConditionalGeneration(Qwen3OmniMoePreTrainedModel, GenerationMixin):\n     config_class = Qwen3OmniMoeConfig\n+    output_modalities = [\"text\", \"audio\"]\n \n     def __init__(self, config: Qwen3OmniMoeConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c82e1f96b821fb4eea3c752da3b3446c5f3c8182",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -545,6 +545,7 @@ class Qwen3VLModelOutputWithPast(ModelOutput):\n class Qwen3VLPreTrainedModel(PreTrainedModel):\n     config: Qwen3VLConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "801907aa1e632304c07a6d784eab854223b359a1",
            "filename": "src/transformers/models/resnet/modeling_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -247,6 +247,7 @@ class ResNetPreTrainedModel(PreTrainedModel):\n     config: ResNetConfig\n     base_model_prefix = \"resnet\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"ResNetConvLayer\", \"ResNetShortCut\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "9f7418ef7f2a86fd58dad6c790de31dc939cb030",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1007,6 +1007,7 @@ class RTDetrPreTrainedModel(PreTrainedModel):\n     config: RTDetrConfig\n     base_model_prefix = \"rt_detr\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [r\"RTDetrHybridEncoder\", r\"RTDetrDecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "b7e56abc170c774179741bac7da9bba85e966aa9",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -300,6 +300,7 @@ class RTDetrResNetPreTrainedModel(PreTrainedModel):\n     config: RTDetrResNetConfig\n     base_model_prefix = \"resnet\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"RTDetrResNetConvLayer\", \"RTDetrResNetShortCut\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "5092a77e56146ea8a41c7d40ce0c40d052444dcf",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -454,6 +454,7 @@ class RTDetrV2PreTrainedModel(PreTrainedModel):\n     config: RTDetrV2Config\n     base_model_prefix = \"rt_detr_v2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [r\"RTDetrV2HybridEncoder\", r\"RTDetrV2DecoderLayer\"]\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "cd59721180ba4963a99b2b73c4099e0500b8fe40",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1009,6 +1009,7 @@ class SamPreTrainedModel(PreTrainedModel):\n     config: SamConfig\n     base_model_prefix = \"sam\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"SamVisionAttention\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n@@ -1111,6 +1112,7 @@ def forward(\n     \"\"\"\n )\n class SamModel(SamPreTrainedModel):\n+    input_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]"
        },
        {
            "sha": "f7ec0da2d319fd0aee231cd15dcf84d8ac683f86",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -551,6 +551,7 @@ class Sam2PreTrainedModel(PreTrainedModel):\n     config_class = Sam2Config\n     base_model_prefix = \"sam2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -1276,6 +1277,7 @@ def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n     \"\"\"\n )\n class Sam2Model(Sam2PreTrainedModel):\n+    input_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]"
        },
        {
            "sha": "1e6bb7f006bead585957d44f66a4438d60876ba4",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -672,6 +672,7 @@ class Sam2PreTrainedModel(PreTrainedModel):\n     config_class = Sam2Config\n     base_model_prefix = \"sam2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "751e9c0445cb27cc4e7b2389ec71d8020d7351fa",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -661,6 +661,7 @@ class Sam2VideoPreTrainedModel(PreTrainedModel):\n     config_class = Sam2VideoConfig\n     base_model_prefix = \"sam2_video\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"video\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -1558,6 +1559,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class Sam2VideoModel(Sam2VideoPreTrainedModel):\n+    input_modalities = [\"video\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]"
        },
        {
            "sha": "6caef802aa2013706bc6c662982beb51284dbdcf",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -986,6 +986,7 @@ class Sam2VideoPreTrainedModel(PreTrainedModel):\n     config_class = Sam2VideoConfig\n     base_model_prefix = \"sam2_video\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"video\"\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -1447,6 +1448,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class Sam2VideoModel(Sam2Model):\n+    input_modalities = [\"video\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     # need to be ignored, as it's a buffer and will not be correctly detected as tied weight\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]"
        },
        {
            "sha": "5dee354b2600c8d319954d0f2547ebfe1e47439f",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -424,6 +424,7 @@ class SamHQPreTrainedModel(PreTrainedModel):\n     config: SamHQConfig\n     base_model_prefix = \"sam_hq\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\"SamHQVisionAttention\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n@@ -1234,6 +1235,7 @@ def forward(\n     \"\"\"\n )\n class SamHQModel(SamHQPreTrainedModel):\n+    input_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(SamHQTwoWayAttentionBlock, index=2)}"
        },
        {
            "sha": "43209d44b372d1a87e215f6a54d0d5836331f3f6",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1449,6 +1449,7 @@ def compute_last_hidden_states_per_sample(\n )\n class SeamlessM4TSpeechEncoder(SeamlessM4TPreTrainedModel):\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: SeamlessM4TConfig):\n         super().__init__(config)\n@@ -2281,6 +2282,7 @@ def forward(self, input_embeds: torch.FloatTensor) -> torch.FloatTensor:\n class SeamlessM4TCodeHifiGan(PreTrainedModel):\n     config: SeamlessM4TConfig\n     main_input_name = \"input_embeds\"\n+    input_modalities = \"audio\"\n     _no_split_modules = []\n \n     def __init__(self, config):\n@@ -2705,6 +2707,7 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4TForSpeechToText(SeamlessM4TPreTrainedModel, GenerationMixin):\n+    input_modalities = \"audio\"\n     _keys_to_ignore_on_load_missing = [\"text_encoder\", \"t2u_model\", \"vocoder\"]\n     main_input_name = \"input_features\"\n \n@@ -2966,6 +2969,7 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4TForTextToSpeech(SeamlessM4TPreTrainedModel, GenerationMixin):\n+    output_modalities = \"audio\"\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n     main_input_name = \"input_ids\"\n \n@@ -3289,6 +3293,8 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4TForSpeechToSpeech(SeamlessM4TPreTrainedModel, GenerationMixin):\n+    input_modalities = \"audio\"\n+    output_modalities = \"audio\"\n     _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3620,6 +3626,8 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4TModel(SeamlessM4TPreTrainedModel, GenerationMixin):\n+    input_modalities = [\"audio\", \"text\"]\n+    output_modalities = [\"audio\", \"text\"]\n     _tied_weights_keys = [\n         \"lm_head.weight\",\n         \"text_encoder.embed_tokens.weight\","
        },
        {
            "sha": "9be5d8b1c5655ad41ad1a3de3a455f9e3c0cbce7",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1489,6 +1489,7 @@ def _hard_upsample(self, hidden_states, durations):\n # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TSpeechEncoder with SeamlessM4T->SeamlessM4Tv2\n class SeamlessM4Tv2SpeechEncoder(SeamlessM4Tv2PreTrainedModel):\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: SeamlessM4Tv2Config):\n         super().__init__(config)\n@@ -2478,6 +2479,7 @@ def forward(self, input_embeds: torch.FloatTensor) -> torch.FloatTensor:\n class SeamlessM4Tv2CodeHifiGan(PreTrainedModel):\n     config: SeamlessM4Tv2Config\n     main_input_name = \"input_embeds\"\n+    input_modalities = \"audio\"\n     _no_split_modules = []\n \n     def __init__(self, config):\n@@ -2912,6 +2914,7 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4Tv2ForSpeechToText(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n+    input_modalities = \"audio\"\n     _keys_to_ignore_on_load_missing = [\"text_encoder\", \"t2u_model\", \"vocoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3181,6 +3184,7 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4Tv2ForTextToSpeech(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n+    output_modalities = \"audio\"\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n     main_input_name = \"input_ids\"\n \n@@ -3542,6 +3546,8 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4Tv2ForSpeechToSpeech(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n+    input_modalities = \"audio\"\n+    output_modalities = \"audio\"\n     _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3910,6 +3916,8 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4Tv2Model(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n+    input_modalities = [\"audio\", \"text\"]\n+    output_modalities = [\"audio\", \"text\"]\n     _tied_weights_keys = [\n         \"lm_head.weight\",\n         \"text_encoder.embed_tokens.weight\","
        },
        {
            "sha": "99382806bedd91b9cd94dacab26660a5b95e9f69",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -412,6 +412,7 @@ class SegformerPreTrainedModel(PreTrainedModel):\n     config: SegformerConfig\n     base_model_prefix = \"segformer\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "9de5ad3a0729735d6e86b00ef0d7801201bde99b",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -591,6 +591,7 @@ class SegGptPreTrainedModel(PreTrainedModel):\n     config: SegGptConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SegGptEmbeddings\", \"SegGptLayer\"]\n "
        },
        {
            "sha": "8cf3e2d24036f9806b1ff6448f8f7a09c364f92a",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -512,6 +512,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     config: SEWConfig\n     base_model_prefix = \"sew\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "8a2cfc3a2689698bf788df2ccc2f6792669ac312",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -249,6 +249,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     config: SEWConfig\n     base_model_prefix = \"sew\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "7dda405146632f5d637bedca6d37417f427afccb",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1184,6 +1184,7 @@ class SEWDPreTrainedModel(PreTrainedModel):\n     config: SEWDConfig\n     base_model_prefix = \"sew-d\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "36fd972de1404672b6ab05c24464a7761ceafe6a",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -44,6 +44,7 @@ class ShieldGemma2ImageClassifierOutputWithNoAttention(ImageClassifierOutputWith\n @auto_docstring\n class ShieldGemma2ForImageClassification(PreTrainedModel):\n     config: ShieldGemma2Config\n+    input_modalities = [\"image\", \"text\"]\n     _checkpoint_conversion_mapping = {\n         \"model.language_model.model\": \"model.model.language_model\",\n         \"model.vision_tower\": \"model.model.vision_tower\","
        },
        {
            "sha": "9fbfb286a2a03c6df57ce3f318fb684552aee6dd",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -466,6 +466,7 @@ def forward(\n class SiglipPreTrainedModel(PreTrainedModel):\n     config: SiglipConfig\n     base_model_prefix = \"siglip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\n@@ -629,6 +630,7 @@ def forward(\n )\n class SiglipTextModel(SiglipPreTrainedModel):\n     config: SiglipTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: SiglipTextConfig):\n         super().__init__(config)\n@@ -746,6 +748,7 @@ def forward(self, hidden_state):\n class SiglipVisionModel(SiglipPreTrainedModel):\n     config: SiglipVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: SiglipVisionConfig):\n         super().__init__(config)\n@@ -1005,6 +1008,7 @@ def forward(\n )\n class SiglipForImageClassification(SiglipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: SiglipConfig) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "a50e13329e83ed6d3c062f6fd7628424930dbe54",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -541,6 +541,7 @@ def default_flax_embed_init(tensor):\n class Siglip2PreTrainedModel(PreTrainedModel):\n     config: Siglip2Config\n     base_model_prefix = \"siglip2\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\n@@ -709,6 +710,7 @@ def forward(\n )\n class Siglip2TextModel(Siglip2PreTrainedModel):\n     config: Siglip2TextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__(config)\n@@ -795,6 +797,7 @@ def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Ten\n class Siglip2VisionModel(Siglip2PreTrainedModel):\n     config: Siglip2VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n@@ -1084,6 +1087,7 @@ def forward(\n )\n class Siglip2ForImageClassification(Siglip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: Siglip2Config) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "e7b120369a7bd215ac297cc6f875056c78dbde9c",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -73,6 +73,7 @@ def extra_repr(self):\n class SmolVLMPreTrainedModel(PreTrainedModel):\n     config: SmolVLMConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SmolVLMVisionAttention\", \"SmolVLMDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -349,6 +350,7 @@ def forward(\n )\n class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):\n     config: SmolVLMVisionConfig\n+    input_modalities = \"image\"\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "5db0e2ff2605078dcdfe08bd9c587f6af7af0251",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -65,6 +65,7 @@ class SpeechEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     config: SpeechEncoderDecoderConfig\n     base_model_prefix = \"speech_encoder_decoder\"\n     main_input_name = \"inputs\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n     _supports_flash_attn = True"
        },
        {
            "sha": "090bd25316f303fb9dd38e7fff37dc59ec506ba0",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1019,6 +1019,7 @@ def forward(\n     \"\"\"\n )\n class Speech2TextForConditionalGeneration(Speech2TextPreTrainedModel, GenerationMixin):\n+    input_modalities = [\"audio\", \"text\"]\n     base_model_prefix = \"model\"\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "72c63fb86d437d876c2456e6f818c1217d29c5a1",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1167,6 +1167,7 @@ class SpeechT5PreTrainedModel(PreTrainedModel):\n     config: SpeechT5Config\n     base_model_prefix = \"speecht5\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):\n@@ -2314,6 +2315,7 @@ def _generate_speech(\n     \"\"\"\n )\n class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n+    input_modalities = \"text\"\n     main_input_name = \"input_ids\"\n \n     def __init__(self, config: SpeechT5Config):"
        },
        {
            "sha": "61495fc31164542301ec0d934905b1be3b01fd0a",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -467,6 +467,7 @@ class SuperGluePreTrainedModel(PreTrainedModel):\n     config: SuperGlueConfig\n     base_model_prefix = \"superglue\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "c211705aaefd9a0026a6a06b3a4126a08e343f74",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -325,6 +325,7 @@ class SuperPointPreTrainedModel(PreTrainedModel):\n     config: SuperPointConfig\n     base_model_prefix = \"superpoint\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:"
        },
        {
            "sha": "9eed87cd4166cf88e46272afdf7327361fa913ce",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -384,6 +384,7 @@ class SwiftFormerPreTrainedModel(PreTrainedModel):\n     config: SwiftFormerConfig\n     base_model_prefix = \"swiftformer\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SwiftFormerEncoderBlock\"]\n "
        },
        {
            "sha": "9835a395e936a00004fcda53444f746b5e317897",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -807,6 +807,7 @@ class SwinPreTrainedModel(PreTrainedModel):\n     config: SwinConfig\n     base_model_prefix = \"swin\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SwinStage\"]\n "
        },
        {
            "sha": "534b2f2102b58ac55711517e8a01e8d3f1bc1a05",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -688,6 +688,7 @@ class Swin2SRPreTrainedModel(PreTrainedModel):\n     config: Swin2SRConfig\n     base_model_prefix = \"swin2sr\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "afdcf3396b4ca582d260b52d56351ded9f0bcba0",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -882,6 +882,7 @@ class Swinv2PreTrainedModel(PreTrainedModel):\n     config: Swinv2Config\n     base_model_prefix = \"swinv2\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Swinv2Stage\"]\n "
        },
        {
            "sha": "90e687b14ffd56f9b94df9f1b13e23d12e858cdb",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -687,6 +687,7 @@ class TableTransformerPreTrainedModel(PreTrainedModel):\n     config: TableTransformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = [\n         r\"TableTransformerConvEncoder\",\n         r\"TableTransformerEncoderLayer\","
        },
        {
            "sha": "c5c9b94a7d971df3097b4c7f340bbcba480bd765",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -607,6 +607,7 @@ class TimeSeriesTransformerPreTrainedModel(PreTrainedModel):\n     config: TimeSeriesTransformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     supports_gradient_checkpointing = True\n     # TODO: tests would need a rewrite to check for correct implementation\n     # Current tests always assume certain inputs to be passed"
        },
        {
            "sha": "814f045c61b826c97d947d48b06cb4dc0a6b5283",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -303,6 +303,7 @@ class TimesFmPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"timesfm\"\n     _no_split_modules = [\"TimesFmDecoderLayer\"]\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "f88973c420e99905f35463d500ef9aeac2e7ad76",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -259,6 +259,7 @@ class TimesFmPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"timesfm\"\n     _no_split_modules = [\"TimesFmDecoderLayer\"]\n     main_input_name = \"past_values\"\n+    input_modalities = \"time\"\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "556bbe4ade09a99d988de0ef6928395c83dede7e",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -451,6 +451,7 @@ class TimesformerPreTrainedModel(PreTrainedModel):\n     config: TimesformerConfig\n     base_model_prefix = \"timesformer\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"TimesformerLayer\"]\n "
        },
        {
            "sha": "d446fc96f71bde565973d4ade1b3ba8b156da71d",
            "filename": "src/transformers/models/timm_backbone/modeling_timm_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -39,6 +39,7 @@ class TimmBackbone(PreTrainedModel, BackboneMixin):\n     \"\"\"\n \n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = False\n     config: TimmBackboneConfig\n "
        },
        {
            "sha": "970349054697e500d2052961f1c8d8f1f2faebed",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -80,6 +80,7 @@ def _create_timm_model_with_error_handling(config: \"TimmWrapperConfig\", **model_\n @auto_docstring\n class TimmWrapperPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     config: TimmWrapperConfig\n     _no_split_modules = []\n     model_tags = [\"timm\"]"
        },
        {
            "sha": "9e6a038197fbd672e09c5dc17ad4245c19402be9",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -519,6 +519,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class TvpPreTrainedModel(PreTrainedModel):\n     config: TvpConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"video\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "de9ec3539238c43a3804120ef795d8ca7837cc62",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -251,6 +251,7 @@ def forward(self, pixel_values):\n class UdopPreTrainedModel(PreTrainedModel):\n     config: UdopConfig\n     base_model_prefix = \"transformer\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     _can_compile_fullgraph = False"
        },
        {
            "sha": "8bdec6b3cae883c40b5d798bba418dd73f31240b",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -734,6 +734,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     config: UniSpeechConfig\n     base_model_prefix = \"unispeech\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "534490235db13a309d7e55d1616d6b4f6aba2420",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -141,6 +141,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     config: UniSpeechConfig\n     base_model_prefix = \"unispeech\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "57e5d3cdbcc01b4ad6be98a77f19f9ba401d6d72",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -739,6 +739,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     config: UniSpeechSatConfig\n     base_model_prefix = \"unispeech_sat\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "e209c7c18ea3e0102fe4b32ffd4fdf7d89906edd",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -153,6 +153,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     config: UniSpeechSatConfig\n     base_model_prefix = \"unispeech_sat\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "048d68e7276a27b151cc0775e2f129b9c3f0c5f5",
            "filename": "src/transformers/models/univnet/modeling_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -427,6 +427,7 @@ def remove_weight_norm(self):\n class UnivNetModel(PreTrainedModel):\n     config: UnivNetConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n \n     def __init__(self, config: UnivNetConfig):\n         super().__init__(config)"
        },
        {
            "sha": "5c9521766379466065061c2e3466df56203f83a4",
            "filename": "src/transformers/models/upernet/modeling_upernet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -269,6 +269,7 @@ def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor:\n class UperNetPreTrainedModel(PreTrainedModel):\n     config: UperNetConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _no_split_modules = []\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "6454da2a73c48478d6a615efa9b3f6d31bdfd27a",
            "filename": "src/transformers/models/video_llama_3/modeling_video_llama_3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -299,14 +299,11 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.Tensor`):\n-                Input to the layer of shape `(seq_len, embed_dim)`.\n-            cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n-                The cumulative sequence lengths of each image or video feature.\n-            position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n-                The cosine and sine position embeddings for vision attention.\n+        r\"\"\"\n+        cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+            The cumulative sequence lengths of each image or video feature.\n+        position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+            The cosine and sine position embeddings for vision attention.\n         \"\"\"\n         residual = hidden_states\n \n@@ -373,6 +370,7 @@ def forward(\n class VideoLlama3PreTrainedModel(PreTrainedModel):\n     config: VideoLlama3Config\n     base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlama3VisionEncoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -386,6 +384,7 @@ class VideoLlama3PreTrainedModel(PreTrainedModel):\n class VideoLlama3VisionModel(VideoLlama3PreTrainedModel):\n     config: VideoLlama3VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _can_record_outputs = {\n         \"hidden_states\": VideoLlama3VisionEncoderLayer,\n         \"attentions\": VideoLlama3VisionAttention,"
        },
        {
            "sha": "6ec9dc80506713ee1e1bfbfcfa285b52b2bb0051",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -374,14 +374,11 @@ def forward(\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.Tensor`):\n-                Input to the layer of shape `(seq_len, embed_dim)`.\n-            cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n-                The cumulative sequence lengths of each image or video feature.\n-            position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n-                The cosine and sine position embeddings for vision attention.\n+        r\"\"\"\n+        cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+            The cumulative sequence lengths of each image or video feature.\n+        position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+            The cosine and sine position embeddings for vision attention.\n         \"\"\"\n         residual = hidden_states\n \n@@ -441,6 +438,7 @@ class VideoLlama3PreTrainedModel(Qwen2VLPreTrainedModel):\n class VideoLlama3VisionModel(VideoLlama3PreTrainedModel):\n     config: VideoLlama3VisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     _can_record_outputs = {\n         \"hidden_states\": VideoLlama3VisionEncoderLayer,\n         \"attentions\": VideoLlama3VisionAttention,"
        },
        {
            "sha": "3f874c2e9353d701492a29656cff7137863e1234",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -125,6 +125,7 @@ def forward(self, image_features):\n class VideoLlavaPreTrainedModel(PreTrainedModel):\n     config: VideoLlavaConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "95163da0311f2687dbcfed120da8b3b182aa963c",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -380,6 +380,7 @@ class VideoMAEPreTrainedModel(PreTrainedModel):\n     config: VideoMAEConfig\n     base_model_prefix = \"videomae\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"video\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoMAEEmbeddings\", \"VideoMAELayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "9a32ee12be13c0bb1dcb8478590197cf33279a53",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -512,6 +512,7 @@ def forward(\n class ViltPreTrainedModel(PreTrainedModel):\n     config: ViltConfig\n     base_model_prefix = \"vilt\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViltEmbeddings\", \"ViltSelfAttention\"]\n "
        },
        {
            "sha": "16606f8ccf4d5af1f142685b8097bc1517b1f4ea",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -114,6 +114,7 @@ def forward(self, hidden_states):\n class VipLlavaPreTrainedModel(PreTrainedModel):\n     config: VipLlavaConfig\n     base_model_prefix = \"\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "796580aaa0c64abdbb586cb95b2f4be873dd44d7",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -65,6 +65,7 @@ class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     config: VisionEncoderDecoderConfig\n     base_model_prefix = \"vision_encoder_decoder\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n     _supports_flash_attn = True"
        },
        {
            "sha": "0f7f86bb14588c57ed11bd27fd319252ae109164",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -47,6 +47,7 @@ def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n class VisionTextDualEncoderModel(PreTrainedModel):\n     config: VisionTextDualEncoderConfig\n     base_model_prefix = \"vision_text_dual_encoder\"\n+    input_modalities = [\"image\", \"text\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "b8a68cd257ae87a01e565d9bc128c633fb6a76fd",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -464,6 +464,7 @@ def forward(self, sequence_output, pooled_output):\n class VisualBertPreTrainedModel(PreTrainedModel):\n     config: VisualBertConfig\n     base_model_prefix = \"visual_bert\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "7923264d7e016ff6c3f24b07ab3cf05653a8ea67",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -353,6 +353,7 @@ class ViTPreTrainedModel(PreTrainedModel):\n     config: ViTConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTEmbeddings\", \"ViTLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "159fca54943eb1e4ab254715d18be5a6b9c385ba",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -519,6 +519,7 @@ class ViTMAEPreTrainedModel(PreTrainedModel):\n     config: ViTMAEConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "1ed50e9da579dac88be0d8a44e27db978f7ba6dd",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -356,6 +356,7 @@ class ViTMSNPreTrainedModel(PreTrainedModel):\n     config: ViTMSNConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTMSNAttention\", \"ViTMSNSdpaAttention\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "b02b66f4d52cc1a601b859ede9d1fcb8a505d30a",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -589,6 +589,7 @@ class VitDetPreTrainedModel(PreTrainedModel):\n     config: VitDetConfig\n     base_model_prefix = \"vitdet\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n "
        },
        {
            "sha": "8863056c51902052e3e40ba08e1884b82fcf3c84",
            "filename": "src/transformers/models/vitmatte/modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -54,6 +54,7 @@ class ImageMattingOutput(ModelOutput):\n class VitMattePreTrainedModel(PreTrainedModel):\n     config: VitMatteConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n "
        },
        {
            "sha": "247e7b47ccec80b7e20664d8255c0ad1f3e41f6a",
            "filename": "src/transformers/models/vitpose/modeling_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -63,6 +63,7 @@ class VitPosePreTrainedModel(PreTrainedModel):\n     config: VitPoseConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]):"
        },
        {
            "sha": "e4fb4276a3138ec47497c6d58a73ee4eb2381159",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -348,6 +348,7 @@ class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n     config: VitPoseBackboneConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "098c891922e2bf7d1573a1e2cd8fa914cd668dde",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -363,6 +363,7 @@ class VivitPreTrainedModel(PreTrainedModel):\n     config: VivitConfig\n     base_model_prefix = \"vivit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"video\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True"
        },
        {
            "sha": "86d002ede4beafde4695b926560c6c61b9376a4d",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -930,6 +930,7 @@ class VJEPA2PreTrainedModel(PreTrainedModel):\n     config: VJEPA2Config\n     base_model_prefix = \"vjepa2\"\n     main_input_name = \"pixel_values_videos\"\n+    input_modalities = \"video\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"VJEPA2Layer\","
        },
        {
            "sha": "bc309bddf00698d657e68b95a2c0e40f5ca38030",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -220,6 +220,7 @@ def forward(\n class VoxtralPreTrainedModel(PreTrainedModel):\n     config: VoxtralConfig\n     base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = None\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -269,6 +270,7 @@ class VoxtralEncoder(VoxtralPreTrainedModel):\n     # Ignore copy\n     config: VoxtralEncoderConfig\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     _no_split_modules = [\"VoxtralEncoderLayer\"]\n     _can_record_outputs = {\n         \"attentions\": VoxtralAttention,"
        },
        {
            "sha": "2ebbf28e11bb994e1904e7ce6ea36f1d721b91f4",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -974,6 +974,7 @@ class Wav2Vec2PreTrainedModel(PreTrainedModel):\n     config: Wav2Vec2Config\n     base_model_prefix = \"wav2vec2\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "c8593d38d131f1d3ba83ddff73bd27afc1661e55",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -708,6 +708,7 @@ class Wav2Vec2BertPreTrainedModel(PreTrainedModel):\n     config: Wav2Vec2BertConfig\n     base_model_prefix = \"wav2vec2_bert\"\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "3bce99771f559604479d3211ef23283bbc8622c3",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -580,6 +580,7 @@ class Wav2Vec2BertPreTrainedModel(PreTrainedModel):\n     config: Wav2Vec2BertConfig\n     base_model_prefix = \"wav2vec2_bert\"\n     main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "9fddc1ce224ff97bec15a45030483d3f34e5feee",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -848,6 +848,7 @@ class Wav2Vec2ConformerPreTrainedModel(PreTrainedModel):\n     config: Wav2Vec2ConformerConfig\n     base_model_prefix = \"wav2vec2_conformer\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "7a0e757a8496a5915b59336a2553d80b86472de0",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -547,6 +547,7 @@ class Wav2Vec2ConformerPreTrainedModel(PreTrainedModel):\n     config: Wav2Vec2ConformerConfig\n     base_model_prefix = \"wav2vec2_conformer\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "274d83fa8914afe93ceebda9e1314e1b7dea7019",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -597,6 +597,7 @@ class WavLMPreTrainedModel(PreTrainedModel):\n     config: WavLMConfig\n     base_model_prefix = \"wavlm\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = False\n     _supports_sdpa = False"
        },
        {
            "sha": "4020f0b3335bf2bde8082ad6e71ba34380da87cb",
            "filename": "src/transformers/models/wavlm/modular_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -507,6 +507,7 @@ class WavLMPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     config: WavLMConfig\n     base_model_prefix = \"wavlm\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = False\n     _supports_sdpa = False"
        },
        {
            "sha": "3fc03b3d54d5e50634d0205b394911cca17f6bf5",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -529,6 +529,7 @@ class WhisperPreTrainedModel(PreTrainedModel):\n     config: WhisperConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"input_features\"\n+    input_modalities = [\"audio\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"WhisperEncoderLayer\", \"WhisperDecoderLayer\"]\n     _supports_flash_attn = True"
        },
        {
            "sha": "7d59d57341e8015d87fe3d1f0a072f6e2d90892a",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -501,6 +501,7 @@ def forward(\n class XCLIPPreTrainedModel(PreTrainedModel):\n     config: XCLIPConfig\n     base_model_prefix = \"x_clip\"\n+    input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):\n@@ -711,6 +712,7 @@ def forward(\n \n class XCLIPTextModel(XCLIPPreTrainedModel):\n     config: XCLIPTextConfig\n+    input_modalities = \"text\"\n \n     def __init__(self, config: XCLIPTextConfig):\n         super().__init__(config)\n@@ -905,6 +907,7 @@ def forward(\n class XCLIPVisionModel(XCLIPPreTrainedModel):\n     config: XCLIPVisionConfig\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n \n     def __init__(self, config: XCLIPVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "774f9c74b8de766162f9d686bcd99fc57ce32e5e",
            "filename": "src/transformers/models/xcodec/modeling_xcodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxcodec%2Fmodeling_xcodec.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -325,6 +325,7 @@ class XcodecPreTrainedModel(PreTrainedAudioTokenizerBase):\n     config_class = XcodecConfig\n     base_model_prefix = \"xcodec\"\n     main_input_name = \"input_values\"\n+    input_modalities = \"audio\"\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "443423435c7b61e1c27a9db63b05895c8c09a4dd",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -433,6 +433,7 @@ class YolosPreTrainedModel(PreTrainedModel):\n     config: YolosConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True"
        },
        {
            "sha": "eb2cc630c021b4a7d26ff623e6f695451be9c07c",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c36d407d5ce0712c702e4314c62705c012bb700/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=1c36d407d5ce0712c702e4314c62705c012bb700",
            "patch": "@@ -1208,6 +1208,7 @@ class ZoeDepthPreTrainedModel(PreTrainedModel):\n     config: ZoeDepthConfig\n     base_model_prefix = \"zoedepth\"\n     main_input_name = \"pixel_values\"\n+    input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n     def _init_weights(self, module):"
        }
    ],
    "stats": {
        "total": 464,
        "additions": 430,
        "deletions": 34
    }
}