{
    "author": "cyyever",
    "message": "Improve spacing of markdown files (#42984)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "e9f0f8e0cb40be6c3addc88f1282723f6932813c",
    "files": [
        {
            "sha": "2a9e05e30c4a4923b6c6446a76e149883254a233",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -193,7 +193,7 @@ The library has 400+ models with many established patterns:\n - Search for similar models (e.g., other vision-language models)\n - Reuse attention mechanisms, layer implementations, and processing patterns\n - Check models like LLaVA, Idefics2, Fuyu for vision-language patterns\n-- Use provided decorators like (`auto_docstring`, `can_return_tuple`, `check_model_inputs` and `_can_record_outputs`) where relevant. \n+- Use provided decorators like (`auto_docstring`, `can_return_tuple`, `check_model_inputs` and `_can_record_outputs`) where relevant.\n - Don't reinvent the wheel\n \n â˜ **7. Run quality checks and read the output**\n@@ -206,6 +206,7 @@ make fixup\n ```\n \n **Important**: Take time to read the output of `make fixup`. It will:\n+\n - Lint and format your code automatically\n - Run consistency checks (imports, docstrings, etc.)\n - Show any remaining issues that need manual fixes"
        },
        {
            "sha": "9b7cc2793744848f86a9f134b41a238237293667",
            "filename": "docs/source/en/assisted_decoding.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fassisted_decoding.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fassisted_decoding.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fassisted_decoding.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -164,4 +164,4 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n ## Resources\n \n-- Read the [Assisted Generation: a new direction toward low-latency text generation](https://huggingface.co/blog/assisted-generation) blog post for more context about text generation latency and assisted generation.\n\\ No newline at end of file\n+- Read the [Assisted Generation: a new direction toward low-latency text generation](https://huggingface.co/blog/assisted-generation) blog post for more context about text generation latency and assisted generation."
        },
        {
            "sha": "a0f730faf99a2b15a02afeafe9dade1d001af2e3",
            "filename": "docs/source/en/chat_content_patterns.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fchat_content_patterns.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fchat_content_patterns.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_content_patterns.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -147,7 +147,7 @@ message = [\n \n ## Batched\n \n-Batched inference processes multiple conversations in a single forward pass to improve throughput and efficiency. Wrap each conversation in its own list, then pass them together as a list of lists. \n+Batched inference processes multiple conversations in a single forward pass to improve throughput and efficiency. Wrap each conversation in its own list, then pass them together as a list of lists.\n \n ```py\n messages = [\n@@ -195,4 +195,4 @@ message = [\n         ]\n     }\n ]\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "95b8c32955d5e83677ebdb44799d81c6bcf0ab56",
            "filename": "docs/source/en/continuous_batching.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fcontinuous_batching.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fcontinuous_batching.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcontinuous_batching.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -171,4 +171,4 @@ The [`Scheduler`] selects requests for processing at each step based on the toke\n \n ## Resources\n \n-The [Continuous batching](https://huggingface.co/blog/continuous_batching) blog post explains KV caching, chunked prefill, and ragged batching with dynamic scheduling in more detail.\n\\ No newline at end of file\n+The [Continuous batching](https://huggingface.co/blog/continuous_batching) blog post explains KV caching, chunked prefill, and ragged batching with dynamic scheduling in more detail."
        },
        {
            "sha": "fdbb52c3d89ff738988f2e218b3413b279593d36",
            "filename": "docs/source/en/internal/import_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -98,4 +98,4 @@ You can specify the following operators: `==`, `>`, `>=`, `<`, `<=`, `!=`.\n \n [[autodoc]] utils.import_utils.requires\n \n-[[autodoc]] utils.import_utils.requires_backends\n\\ No newline at end of file\n+[[autodoc]] utils.import_utils.requires_backends"
        },
        {
            "sha": "af3db1bb5b338fd68fcd03be18bd12d7972897d9",
            "filename": "docs/source/en/internal/tokenization_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Finternal%2Ftokenization_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Finternal%2Ftokenization_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Ftokenization_utils.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -28,7 +28,6 @@ Most of those are only useful if you are studying the code of the tokenizers in\n     - __call__\n     - all\n \n-\n ## Enums and namedtuples\n \n [[autodoc]] tokenization_utils_base.TruncationStrategy"
        },
        {
            "sha": "f44f0dbf4ad1204fc2eecc8af3410e3a9b2fd225",
            "filename": "docs/source/en/model_doc/bloom.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbloom.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -63,7 +63,6 @@ See also:\n [[autodoc]] BloomConfig\n     - all\n \n-\n ## BloomModel\n \n [[autodoc]] BloomModel"
        },
        {
            "sha": "782d8d8bc744e903dd65911d51aa3ca081dfc5b3",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -133,7 +133,6 @@ visualizer(\"Plants create energy through a process known as\")\n \n [[autodoc]] CohereTokenizer\n \n-\n ## CohereModel\n \n [[autodoc]] CohereModel"
        },
        {
            "sha": "dc6d477c7c486567234938be0b0503012bde7c95",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -169,7 +169,8 @@ print(\"Pooled output shape:\", pooled_output.shape)\n [[autodoc]] DINOv3ViTModel\n     - forward\n \n-## DINOv3ViTBackbone    \n+## DINOv3ViTBackbone\n+\n [[autodoc]] DINOv3ViTBackbone\n \n ## DINOv3ConvNextModel"
        },
        {
            "sha": "baf66c393e4d96477ab9ad34e248f935920b8c55",
            "filename": "docs/source/en/model_doc/ernie4_5_vl_moe.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_vl_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_vl_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_vl_moe.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -45,7 +45,6 @@ This architecture has the advantage to enhance multimodal understanding without\n \n Other models from the family can be found at [Ernie 4.5](./ernie4_5) and at [Ernie 4.5 MoE](./ernie4_5_moe.md).\n \n-\n ## Usage\n \n The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n@@ -172,7 +171,6 @@ output_text = processor.batch_decode(\n print(output_text)\n ```\n \n-\n ## Ernie4_5_VL_MoeConfig\n \n [[autodoc]] Ernie4_5_VL_MoeConfig"
        },
        {
            "sha": "acb1aeb7c0c60bda6d03277e536284907950707a",
            "filename": "docs/source/en/model_doc/fast_vlm.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -43,7 +43,7 @@ The original code can be found [here](https://github.com/apple/ml-fastvlm).\n \n - Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n \n-**Important: **\n+**Important:**\n \n Hugging Face models use SDPA by default; however, this modelâ€™s visual backbone supports only eager attention, so it automatically falls back to `\"eager\"`.\n \n@@ -62,6 +62,7 @@ will result in an error.\n Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processorâ€™s `apply_chat_template` method.\n \n **Important:**\n+\n - You must construct a conversation history â€” passing a plain string won't work.\n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.\n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.\n@@ -70,7 +71,6 @@ Each **checkpoint** is trained with a specific prompt format, depending on the u\n \n ### Single input inference\n \n-\n ```python\n import torch\n from transformers import AutoProcessor, FastVlmForConditionalGeneration\n@@ -102,7 +102,6 @@ generate_ids = model.generate(**inputs, max_new_tokens=30)\n processor.batch_decode(generate_ids, skip_special_tokens=True)\n ```\n \n-\n ### Batched inference\n \n FastVLM also supports batched inference. Here is how you can do it:\n@@ -152,7 +151,6 @@ generate_ids = model.generate(**inputs, max_new_tokens=30)\n processor.batch_decode(generate_ids, skip_special_tokens=True)\n ```\n \n-\n ## Note regarding reproducing original implementation\n \n In order to match the logits of the [original implementation](https://github.com/apple/ml-fastvlm), one needs to use float32. In half precision the logit difference is higher due to tiny differences in how some ops are implemented in timm."
        },
        {
            "sha": "f6dea062afda488b1588bf9c97ce2a2c8675a22b",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -170,7 +170,6 @@ print(output_text)\n \n [[autodoc]] Glm4vConfig\n \n-\n ## Glm4vVisionConfig\n \n [[autodoc]] Glm4vVisionConfig"
        },
        {
            "sha": "ffb6a3d85cb2909f79bbcbc2294362d342e2e26a",
            "filename": "docs/source/en/model_doc/glm4v_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v_moe.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -48,7 +48,6 @@ The model also introduces a **Thinking Mode** switch, allowing users to balance\n \n [[autodoc]] Glm4vMoeConfig\n \n-\n ## Glm4vMoeVisionConfig\n \n [[autodoc]] Glm4vMoeVisionConfig\n@@ -75,4 +74,4 @@ The model also introduces a **Thinking Mode** switch, allowing users to balance\n ## Glm4vMoeForConditionalGeneration\n \n [[autodoc]] Glm4vMoeForConditionalGeneration\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "62cb2994467f84deefb64158f4773d65a1b98f40",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -149,7 +149,6 @@ for i, transcription in enumerate(transcriptions):\n     print(f\"Audio {i+1}: {transcription}\")\n ```\n \n-\n ## GraniteSpeechConfig\n \n [[autodoc]] GraniteSpeechConfig"
        },
        {
            "sha": "9096a31af4b3110b0182e01ec827ef6ed45358eb",
            "filename": "docs/source/en/model_doc/lasr.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Flasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Flasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flasr.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -106,4 +106,3 @@ TODO\n ## LasrForCTC\n \n [[autodoc]] LasrForCTC\n-"
        },
        {
            "sha": "ce2c2f7ad73ad39d55e79fc26109d24bd38f72d5",
            "filename": "docs/source/en/model_doc/layoutxlm.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -70,12 +70,10 @@ data for the model.\n As LayoutXLM's architecture is equivalent to that of LayoutLMv2, one can refer to [LayoutLMv2's documentation page](layoutlmv2) for all tips, code examples and notebooks.\n </Tip>\n \n-\n ## LayoutXLMConfig\n \n [[autodoc]] LayoutXLMConfig\n \n-\n ## LayoutXLMTokenizer\n \n [[autodoc]] LayoutXLMTokenizer"
        },
        {
            "sha": "c55d725bd3509af2c37ac6154bd66dacd5161f66",
            "filename": "docs/source/en/model_doc/ministral3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral3.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -12,13 +12,11 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n *This model was released on 2025-12-01 and added to Hugging Face Transformers on 2025-12-01.*\n \n-\n # Ministral3\n \n ## Overview\n@@ -30,6 +28,7 @@ This model is the instruct post-trained version, fine-tuned for instruction task\n The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.\n \n Key features:\n+\n - Vision: Enables the model to analyze images and provide insights based on visual content, in addition to text.\n - Multilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic.\n - System Prompt: Maintains strong adherence and support for system prompts.\n@@ -83,7 +82,6 @@ decoded_output = tokenizer.decode(output[len(tokenized[\"input_ids\"][0]):])\n print(decoded_output)\n ```\n \n-\n ## Ministral3Config\n \n [[autodoc]] Ministral3Config"
        },
        {
            "sha": "bdbfc4560ad58ce9e1b0c048a32195463fc996dc",
            "filename": "docs/source/en/model_doc/nanochat.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fnanochat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fnanochat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnanochat.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n     </div>\n </div>\n \n-[NanoChat](https://huggingface.co/karpathy/nanochat-d32) is a compact decoder-only transformer model designed for educational purposes and efficient training. The model features several fundamental architectural innovations which are common in modern transformer models. Therefore, it is a good model to use as a starting point to understand the principles of modern transformer models. NanoChat is a variant of the [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama) architecture, with simplified attention mechanism and normalization layers. \n+[NanoChat](https://huggingface.co/karpathy/nanochat-d32) is a compact decoder-only transformer model designed for educational purposes and efficient training. The model features several fundamental architectural innovations which are common in modern transformer models. Therefore, it is a good model to use as a starting point to understand the principles of modern transformer models. NanoChat is a variant of the [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama) architecture, with simplified attention mechanism and normalization layers.\n \n The architecture is based on [nanochat](https://github.com/karpathy/nanochat) by [Andrej Karpathy](https://huggingface.co/karpathy), adapted for the Hugging Face Transformers library by [Ben Burtenshaw](https://huggingface.co/burtenshaw).\n "
        },
        {
            "sha": "9b10674e6d9a3ad20d94f1369c4cd64af73c9ca0",
            "filename": "docs/source/en/model_doc/pe_audio.md",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_audio.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -14,6 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n *This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-16.*\n+\n # PE Audio (Perception Encoder Audio)\n \n ## Overview\n@@ -22,9 +23,11 @@ PE Audio (Perception Encoder Audio) is a state-of-the-art multimodal model that\n The model enables cross-modal retrieval and understanding between audio and text.\n \n **Text input**\n+\n - Produces a single embedding representing the full text.\n \n **Audio input**\n+\n - **PeAudioFrameLevelModel**\n   - Produces a sequence of embeddings, one every 40 ms of audio.\n   - Suitable for audio event localization and fine-grained temporal analysis.\n@@ -33,6 +36,7 @@ The model enables cross-modal retrieval and understanding between audio and text\n   - Suitable for global audio-text retrieval tasks.\n \n **The resulting embeddings can be used for:**\n+\n - Audio event localization\n - Cross-modal (audioâ€“text) retrieval and matching\n \n@@ -45,6 +49,7 @@ TODO\n ```\n \n ## PeAudioFeatureExtractor\n+\n [[autodoc]] PeAudioFeatureExtractor\n     - __call__\n \n@@ -66,9 +71,9 @@ TODO\n [[autodoc]] PeAudioEncoder\n     - forward\n \n-## PeAudioFrameLevelModel \n+## PeAudioFrameLevelModel\n \n-[[autodoc]] PeAudioFrameLevelModel \n+[[autodoc]] PeAudioFrameLevelModel\n     - forward\n \n ## PeAudioModel"
        },
        {
            "sha": "e116724d43f5969fe9bdcd71ea65b63e6402cacd",
            "filename": "docs/source/en/model_doc/pe_audio_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_audio_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_audio_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_audio_video.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -14,6 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n *This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-16.*\n+\n # PE Audio Video (Perception Encoder Audio-Video)\n \n ## Overview"
        },
        {
            "sha": "01cbb9b7805af703b12f470abefa8c25ef4d9040",
            "filename": "docs/source/en/model_doc/pe_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpe_video.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -14,6 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n *This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-16.*\n+\n # PE Video (Perception Encoder Video)\n \n ## Overview"
        },
        {
            "sha": "80d2d9767f1b3a917f0f64da5c3e672cf902ffd1",
            "filename": "docs/source/en/model_doc/pixio.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixio.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -52,7 +52,7 @@ features = outputs.hidden_states[-1] # class tokens + patch tokens before last L\n \n - The example below shows how to split the output tensor into:\n   - a set of global embeddings for the whole image, commonly referred to as `CLS` token,\n-    useful for classification and retrieval. \n+    useful for classification and retrieval.\n     You can either average them (recommended) or concatenate them along the channel dimension.\n   - a set of local embeddings, one for each `16x16` patch of the input image,\n     useful for dense tasks, such as depth estimation and semantic segmentation."
        },
        {
            "sha": "3ec8b7d7b69f0c89dc5198b36023e5e4177ed65f",
            "filename": "docs/source/en/model_doc/sam3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -374,11 +374,13 @@ For faster inference or lower memory usage:\n SAM3 uses the following label conventions:\n \n **For points and boxes:**\n+\n - `1`: Positive prompt (include this region/object)\n - `0`: Negative prompt (exclude this region/object)\n - `-10`: Padding value for batched inputs\n \n **Coordinate formats:**\n+\n - **Input boxes**: `[x1, y1, x2, y2]` (xyxy format) in pixel coordinates\n - **Output boxes** (raw): `[x1, y1, x2, y2]` (xyxy format), normalized to [0, 1]\n - **Output boxes** (post-processed): `[x1, y1, x2, y2]` (xyxy format) in absolute pixel coordinates\n@@ -435,4 +437,3 @@ SAM3 uses the following label conventions:\n \n [[autodoc]] Sam3Model\n     - forward\n-"
        },
        {
            "sha": "8aa2342e4e7aed79d411873acdefada07fa0cdc7",
            "filename": "docs/source/en/model_doc/sam3_tracker.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -12,7 +12,6 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n@@ -40,7 +39,6 @@ The abstract from the paper is the following:\n \n *We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.*\n \n-\n This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan) and [ronghanghu](https://huggingface.co/ronghanghu).\n \n ## Usage example"
        },
        {
            "sha": "97c13d05a8f1b0ea2ba56ca32199d631e4ea243c",
            "filename": "docs/source/en/model_doc/sam3_tracker_video.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_tracker_video.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -12,13 +12,11 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n *This model was released on 2025-11-19 and added to Hugging Face Transformers on 2025-11-19.*\n \n-\n # SAM3 Tracker Video\n \n <div style=\"float: right;\">\n@@ -41,14 +39,12 @@ The abstract from the paper is the following:\n \n *We present Segment Anything Model (SAM) 3, a unified model that detects, segments, and tracks objects in images and videos based on concept prompts, which we define as either short noun phrases (e.g., \"yellow school bus\"), image exemplars, or a combination of both. Promptable Concept Segmentation (PCS) takes such prompts and returns segmentation masks and unique identities for all matching object instances. To advance PCS, we build a scalable data engine that produces a high-quality dataset with 4M unique concept labels, including hard negatives, across images and videos. Our model consists of an image-level detector and a memory-based video tracker that share a single backbone. Recognition and localization are decoupled with a presence head, which boosts detection accuracy. SAM 3 doubles the accuracy of existing systems in both image and video PCS, and improves previous SAM capabilities on visual segmentation tasks. We open source SAM 3 along with our new Segment Anything with Concepts (SA-Co) benchmark for promptable concept segmentation.*\n \n-\n This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan) and [ronghanghu](https://huggingface.co/ronghanghu).\n \n ## Usage example\n \n ### Video Segmentation and Tracking\n \n-\n #### Basic Video Tracking\n \n ```python\n@@ -278,7 +274,6 @@ Tracked 2 objects through 180 frames\n <!-- ## Resources -->\n <!-- A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SAM3 Tracker Video. -->\n \n-\n ## Sam3TrackerVideoConfig\n \n [[autodoc]] Sam3TrackerVideoConfig\n@@ -308,4 +303,3 @@ Tracked 2 objects through 180 frames\n [[autodoc]] Sam3TrackerVideoModel\n     - forward\n     - propagate_in_video_iterator\n-"
        },
        {
            "sha": "6d521be47cf654a2bb6922c9c2bf9e24acdb3bb7",
            "filename": "docs/source/en/model_doc/sam3_video.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3_video.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -228,4 +228,3 @@ For faster inference or lower memory usage:\n [[autodoc]] Sam3VideoModel\n     - forward\n     - propagate_in_video_iterator\n-"
        },
        {
            "sha": "bcadd6022b45d4656e0d0cf96127fd84eab2a67e",
            "filename": "docs/source/en/model_doc/t5gemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma2.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -95,6 +95,7 @@ print(processor.decode(generation[0]))\n [[autodoc]] T5Gemma2EncoderConfig\n \n ## T5Gemma2DecoderConfig\n+\n [[autodoc]] T5Gemma2DecoderConfig\n \n ## T5Gemma2Model"
        },
        {
            "sha": "4edd9f6dbeff7fabf6a0187446f28d552cd71ac1",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -1,6 +1,6 @@\n # Contributing a new model to Transformers\n \n-Modular Transformers lowers the bar for contributing models and significantly reduces the code required to add a model by allowing imports and inheritance. We recommend to go through [general contribution guidelines for new models](./contributing#do-you-want-to-implement-a-new-model) before diving into the details here. \n+Modular Transformers lowers the bar for contributing models and significantly reduces the code required to add a model by allowing imports and inheritance. We recommend to go through [general contribution guidelines for new models](./contributing#do-you-want-to-implement-a-new-model) before diving into the details here.\n \n One of Transformers' core design feature is the [single model, single file](https://huggingface.co/blog/transformers-design-philosophy) policy. Model components - such as attention layers - are repeated across many files and any independent implementations tend to diverge as fixes and changes are applied to specific parts of the code.\n "
        },
        {
            "sha": "67a5237bdd21e72998a44a12e7bf3fc19784244b",
            "filename": "docs/source/en/optimization_overview.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Foptimization_overview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Foptimization_overview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Foptimization_overview.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -102,7 +102,7 @@ model = AutoModelForCausalLM.from_pretrained(\n \n ## Caching\n \n-[Caching](./kv_cache) speeds up generation by reusing past keys and values instead of recomputing them for every token. To offset and reduce the memory cost of storing past keys and values, Transformers \n+[Caching](./kv_cache) speeds up generation by reusing past keys and values instead of recomputing them for every token. To offset and reduce the memory cost of storing past keys and values, Transformers\n supports offloading the cache to the CPU. Only the current layer remains on the GPU.\n \n Use the `cache_implementation` argument in [`~GenerationMixin.generate`] to set a cache strategy.\n@@ -175,4 +175,4 @@ outputs = model.generate_batch(\n for request_id, output in outputs.items():\n     text = tokenizer.decode(output.generated_tokens, skip_special_tokens=True)\n     print(f\"[{request_id}] {text}\")\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "e1fc2a4080b6abd18528fbb4cc83729cd7a088f0",
            "filename": "docs/source/en/peft.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fpeft.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fpeft.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpeft.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -171,7 +171,7 @@ model.load_adapter(file_name_adapter_2, hotswap=True, adapter_name=\"default\")\n # generate outputs with adapter 2\n ```\n \n-For compiled models, it is often necessary to call [`~integrations.peft.PeftAdapterMixin.enable_peft_hotswap`] to avoid recompilation. Call this method _before_ loading the first adapter, while `torch.compile` should be called _after_ loading the first adapter.\n+For compiled models, it is often necessary to call [`~integrations.peft.PeftAdapterMixin.enable_peft_hotswap`] to avoid recompilation. Call this method *before* loading the first adapter, while `torch.compile` should be called *after* loading the first adapter.\n \n ```python\n model = AutoModel.from_pretrained(...)"
        },
        {
            "sha": "3960211ca37ee56b381e9ae044b8fdec39b0f2d2",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -308,4 +308,4 @@ The `placement` attribute tells PyTorch how to place a tensor on devices in `Dev\n \n - Check the [expert parallelism](./expert_parallelism) guide if you're using a mixture-of-experts (MoE) model. These models support tensor parallelism and expert parallelism.\n \n-- Read the [Tensor Parallelism (TP) in Transformers: 5 Minutes to Understand](https://huggingface.co/blog/qgallouedec/tp) blog post for a quick overview of tensor parallelism and learn how column and row parallel setups differ.\n\\ No newline at end of file\n+- Read the [Tensor Parallelism (TP) in Transformers: 5 Minutes to Understand](https://huggingface.co/blog/qgallouedec/tp) blog post for a quick overview of tensor parallelism and learn how column and row parallel setups differ."
        },
        {
            "sha": "f5c6944cb266e711a1cc1469873af78910fead5a",
            "filename": "docs/source/en/quantization/contribute.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -151,4 +151,4 @@ class YourDeserialize(ConversionOps):\n         # Reconstruct the quantized tensor from components\n         reconstructed_tensor = reconstruct_from_components(input_dict)\n         return {full_layer_name: reconstructed_tensor}\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "28ada59006c0f709b533f1c75bd676e8eddc44b7",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -661,7 +661,6 @@ quantized_model.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n tokenizer.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n ```\n \n-\n ```py\n # torchao < 0.15 -> unsafe serialization\n filename = \"llama3-8b-int4wo-128/pytorch_model.bin\""
        },
        {
            "sha": "daf8d78ae8b5a96c2f72471f9aeaecbfc2a8d3ab",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -403,11 +403,11 @@ Make sure to install the required libraries listed in the quantization documenta\n \n #### On the fly quantization\n \n-If you want to quantize a model at runtime, you can specify the --quantization flag in the CLI. Note that not all quantization methods support on-the-fly conversion. The full list of supported methods is available in the quantization [overview](https://huggingface.co/docs/transformers/main/quantization/overview). \n+If you want to quantize a model at runtime, you can specify the --quantization flag in the CLI. Note that not all quantization methods support on-the-fly conversion. The full list of supported methods is available in the quantization [overview](https://huggingface.co/docs/transformers/main/quantization/overview).\n \n Currently, with transformers serve, we only supports some methods: [\"bnb-4bit\", \"bnb-8bit\"]\n \n-For example, to enable 4-bit quantization with bitsandbytes, you need to pass add `--quantization bnb-4bit`: \n+For example, to enable 4-bit quantization with bitsandbytes, you need to pass add `--quantization bnb-4bit`:\n \n ```sh\n transformers serve --quantization bnb-4bit"
        },
        {
            "sha": "bb0eab6c2530180c067230726f3768870aa2a9e6",
            "filename": "docs/source/en/tasks/any_to_any.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fany_to_any.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fany_to_any.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fany_to_any.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -131,4 +131,3 @@ messages = [\n output = pipe(text=messages, fps=1, load_audio_from_video=True, max_new_tokens=20, generation_mode=\"audio\")\n sf.write(\"generated_audio.wav\", out[0][\"generated_audio\"])\n ```\n-"
        },
        {
            "sha": "9fdeff00ab7a83227e33014d0a6c9691e108e597",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -66,7 +66,6 @@ The image inputs look like the following.\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" alt=\"A bee on a pink flower\"/>\n </div>\n \n-\n Structure your conversation as shown below for a single prompt with image and text inputs.\n \n ```python\n@@ -299,7 +298,7 @@ First, install dependencies.\n pip install -U optimum-quanto bitsandbytes\n ```\n \n-To quantize a model during loading, we need to first create [`QuantoConfig`]. Then load the model as usual, but pass `quantization_config`Â during model initialization. \n+To quantize a model during loading, we need to first create [`QuantoConfig`]. Then load the model as usual, but pass `quantization_config`Â during model initialization.\n \n ```python\n from transformers import AutoModelForImageTextToText, QuantoConfig"
        },
        {
            "sha": "cbc7146b6f482aa428816425be8f628fef2e1b2d",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 19,
            "deletions": 12,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -26,7 +26,7 @@ that the prompt is pointing out.\n - Segment Everything mode: In segment everything, given an image, the model generates every mask in the image. To do so, a grid of points is generated and overlaid on the image for inference.\n - Video Inference: The model accepts a video, and a point or box prompt in a video frame, which is tracked throughout the video. You can get more information on how to do video inference by following [SAM 2 docs](../model_doc/sam2).\n \n-Mask generation task is supported by [Segment Anything Model (SAM)](../model_doc/sam) and [Segment Anything Model 2 (SAM2)](../model_doc/sam2), while video inference is supported by [Segment Anything Model 2 (SAM2)](../model_doc/sam2). SAM is a powerful model that consists of a Vision Transformer-based image encoder, a prompt encoder, and a two-way transformer mask decoder. Images and prompts are encoded, and the decoder takes these embeddings and generates valid masks.  Meanwhile, SAM 2 extends SAM by adding a memory module to track the masks. \n+Mask generation task is supported by [Segment Anything Model (SAM)](../model_doc/sam) and [Segment Anything Model 2 (SAM2)](../model_doc/sam2), while video inference is supported by [Segment Anything Model 2 (SAM2)](../model_doc/sam2). SAM is a powerful model that consists of a Vision Transformer-based image encoder, a prompt encoder, and a two-way transformer mask decoder. Images and prompts are encoded, and the decoder takes these embeddings and generates valid masks. Meanwhile, SAM 2 extends SAM by adding a memory module to track the masks.\n \n <div class=\"flex justify-center\">\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam.png\" alt=\"SAM Architecture\"/>\n@@ -228,11 +228,11 @@ plt.show()\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/box_inference.png\" alt=\"Visualized Inference\"/>\n </div>\n \n-## Fine-tuning for Mask Generation \n+## Fine-tuning for Mask Generation\n \n We will fine-tune SAM2.1 on small part of MicroMat dataset for image matting. We need to install the [monai](https://github.com/Project-MONAI/MONAI) library to use DICE loss, and [trackio](https://huggingface.co/docs/trackio/index) for logging the masks during training.\n \n-```bash \n+```bash\n pip install -q datasets monai trackio\n ```Â \n We can now load our dataset and take a look.\n@@ -247,6 +247,7 @@ dataset\n # 'image_path', 'mask_path', 'prompt_path'],  num_rows: 94\n #})\n ```\n+\n We need image, mask and prompt columns. We split for train and test.\n \n ```python\n@@ -256,9 +257,11 @@ val_ds = dataset[\"test\"]\n ```\n \n Let's take a look at a sample.\n+\n ```python\n train_ds[0]\n-``` \n+```\n+\n ```\n  {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=2040x1356>,\n  'mask': <PIL.PngImagePlugin.PngImageFile image mode=L size=2040x1356>,\n@@ -271,13 +274,15 @@ train_ds[0]\n  'mask_path': '/content/MicroMat-mini/mask/0034_34.png',\n  'prompt_path': '/content/MicroMat-mini/prompt/0034_34.json'}\n ```\n+\n Prompts are string of dictionaries, so you can get the bounding boxes as shown below.\n+\n ```python\n import json\n \n json.loads(train_ds[\"prompt\"][0])[\"bbox\"]\n # [0, 701, 251, 1356]\n-``` \n+```\n \n Visualize an example image, prompt and mask.\n \n@@ -335,11 +340,11 @@ class SAMDataset(Dataset):\n \n \n     return inputs\n-``` \n+```\n \n-We can initialize the processor and the dataset with it. \n+We can initialize the processor and the dataset with it.\n \n-```python \n+```python\n from transformers import Sam2Processor\n \n processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-small\")\n@@ -380,7 +385,7 @@ train_dataloader = DataLoader(\n     shuffle=True,\n     collate_fn=collate_fn,\n )\n-``` \n+```\n \n Let's take a look at what the data loader yields.\n \n@@ -395,7 +400,8 @@ for k,v in batch.items():\n # ground_truth_mask torch.Size([4, 1, 256, 256])\n #original_image_size torch.Size([4, 2])\n ```\n-We will now load the model and freeze the vision and the prompt encoder to only train the mask decoder. \n+\n+We will now load the model and freeze the vision and the prompt encoder to only train the mask decoder.\n \n ```python\n from transformers import Sam2Model\n@@ -443,7 +449,7 @@ plt.show()\n \n ![SAM2 result after training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam2_before_training.png)\n \n-We need to log our predictions to trackio so we can monitor the model improvement in the middle of the training. \n+We need to log our predictions to trackio so we can monitor the model improvement in the middle of the training.\n \n ```python\n from PIL import Image\n@@ -478,6 +484,7 @@ def log_eval_masks_trackio(dataset, indices, step, predict_fn,  project=None, sa\n         \n     trackio.log(logs)\n ```\n+\n We can now write our training loop and train!\n \n Notice how we log our loss and evaluation masks with trackio.\n@@ -520,7 +527,6 @@ for epoch in range(num_epochs):\n trackio.finish()\n ```\n \n-\n Let's put the trained model to test.\n \n ```python\n@@ -547,6 +553,7 @@ plt.imshow(overlay)\n plt.axis(\"off\")\n plt.show()\n ```\n+\n Great improvement after only training for 20 epochs on a small dataset!\n \n ![SAM2 result after training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sam2_after_training.png)"
        },
        {
            "sha": "cb40a4a327a95aa794763fb8c8a0f5e1c9db8ba7",
            "filename": "docs/source/en/tasks/video_text_to_text.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e9f0f8e0cb40be6c3addc88f1282723f6932813c/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_text_to_text.md?ref=e9f0f8e0cb40be6c3addc88f1282723f6932813c",
            "patch": "@@ -18,9 +18,9 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Video-text-to-text, also known as video language models are models that can process video and output text. These models can tackle various tasks, from video question answering to video captioning. \n+Video-text-to-text, also known as video language models are models that can process video and output text. These models can tackle various tasks, from video question answering to video captioning.\n \n-These models have nearly the same architecture as [image-text-to-text](../image_text_to_text) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos. \n+These models have nearly the same architecture as [image-text-to-text](../image_text_to_text) models except for some changes to accept video data, since video data is essentially image frames with temporal dependencies. Some image-text-to-text models take in multiple images, but this alone is inadequate for a model to accept videos.\n \n Moreover, video-text-to-text models are often trained with all vision modalities. Each example might have videos, multiple videos, images and multiple images. Some of these models can also take interleaved inputs. For example, you can refer to a specific video inside a string of text by adding a video token in text like \"What is happening in this video? `<video>`\".\n \n@@ -70,16 +70,15 @@ We will infer with two videos, both have cats.\n   </div>\n </div>\n \n-\n-Videos are series of image frames. Depending on the hardware limitations, downsampling is required. If the number of downsampled frames are too little, predictions will be low quality. \n-\n+Videos are series of image frames. Depending on the hardware limitations, downsampling is required. If the number of downsampled frames are too little, predictions will be low quality.\n \n Video-text-to-textÂ models have processors with video processor abstracted in them. You can pass video inference related arguments to [`~ProcessorMixin.apply_chat_template`] function.\n \n > [!WARNING]\n > You can learn more about video processors [here](../main_classes/video_processor).\n \n We can define our chat history, passing in video with a URL like below.\n+\n ```python\n messages = [\n     {\n@@ -92,7 +91,7 @@ messages = [\n ]\n ```\n \n-You can preprocess the videos by passing in messages, setting `do_sample_frames` to True and passing in `num_frames`. Here we sample 10 frames. \n+You can preprocess the videos by passing in messages, setting `do_sample_frames` to True and passing in `num_frames`. Here we sample 10 frames.\n \n ```python\n inputs = processor.apply_chat_template(\n@@ -106,7 +105,8 @@ inputs = processor.apply_chat_template(\n )\n inputs.to(model.device)\n ```\n-The inputs contain `input_ids` for tokenized text, `pixel_values_videos` for 10 frames and `attention_mask` for which tokens . \n+\n+The inputs contain `input_ids` for tokenized text, `pixel_values_videos` for 10 frames and `attention_mask` for which tokens .\n \n We can now infer with our preprocessed inputs and decode them.\n "
        }
    ],
    "stats": {
        "total": 133,
        "additions": 62,
        "deletions": 71
    }
}