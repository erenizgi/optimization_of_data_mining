{
    "author": "Cyrilvallez",
    "message": "Allow passing `tp_plan` in `from_pretrained` directly (#41435)\n\n* start\n\n* allow passing it\n\n* fix plans\n\n* fix\n\n* fix\n\n* style\n\n* style\n\n* fix\n\n* add_test\n\n* oupsi indent\n\n* fix\n\n* fix\n\n* fix for CI without accelerator\n\n* fix import",
    "sha": "3ef6f2c415ccd4f01460fb3733b6ad709e58d098",
    "files": [
        {
            "sha": "f8a96d7a476ef7b39f3d60406d6a984f99237ff8",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef6f2c415ccd4f01460fb3733b6ad709e58d098/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef6f2c415ccd4f01460fb3733b6ad709e58d098/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=3ef6f2c415ccd4f01460fb3733b6ad709e58d098",
            "patch": "@@ -38,16 +38,15 @@\n     from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n \n \n-def initialize_tensor_parallelism(tp_plan, tp_size=None, device_mesh=None, device_map=None):\n+def initialize_tensor_parallelism(\n+    tp_plan: str | dict[str, str] | None, tp_size: int | None = None, device_mesh=None, device_map=None\n+):\n     r\"\"\"\n     Sets up the device mesh and initialized the backend for tensor parallelism.\n     This function is called when the model is loaded and the TP plan is set to 'auto'.\n     \"\"\"\n     if tp_size is not None and tp_plan is None:\n         raise ValueError(\"tp_plan has to be set when tp_size is passed.\")\n-    if tp_plan is not None and tp_plan != \"auto\":\n-        # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n-        raise ValueError(f\"tp_plan supports 'auto' only for now but got {tp_plan}.\")\n     if tp_plan is not None and device_map is not None:\n         raise ValueError(\"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\")\n     if device_mesh is None:\n@@ -80,7 +79,7 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None, device_mesh=None, devic\n             except Exception as e:\n                 raise OSError(\n                     \"We tried to initialize torch.distributed for you, but it failed. Make \"\n-                    \"sure you init torch distributed in your script to use `tp_plan='auto'`.\"\n+                    \"sure you init torch distributed in your script to use `tp_plan`.\"\n                 ) from e\n \n         if device_type != \"cpu\":\n@@ -112,7 +111,7 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None, device_mesh=None, devic\n         tp_size = device_mesh.size()\n         device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n \n-    return tp_device, device_map, device_mesh, tp_size\n+    return device_map, device_mesh, tp_size\n \n \n def _blocks_to_block_sizes(total_size: int, blocks: int | list[int]) -> list[int]:\n@@ -1110,13 +1109,16 @@ def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n         logger.warning(f\"The following layers were not sharded: {', '.join(unsharded_layers)}\")\n \n \n-def distribute_model(model, distributed_config, device_mesh, tp_size):\n+def distribute_model(model, tp_plan, distributed_config, device_mesh, tp_size):\n     model._tp_size = tp_size\n     model._device_mesh = device_mesh\n     if distributed_config is not None:\n         if isinstance(distributed_config, dict):\n             distributed_config = DistributedConfig.from_dict(distributed_config)\n         model.config.distributed_config = distributed_config\n+    # Set the new requested tp_plan on the model\n+    if isinstance(tp_plan, dict):\n+        model.tp_plan = tp_plan\n     model_plan = model.tp_plan\n     if model_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n         for v in model_plan.values():"
        },
        {
            "sha": "bbe80cebb240a0b3457243fe38e8786de35751ee",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 45,
            "deletions": 55,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef6f2c415ccd4f01460fb3733b6ad709e58d098/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef6f2c415ccd4f01460fb3733b6ad709e58d098/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=3ef6f2c415ccd4f01460fb3733b6ad709e58d098",
            "patch": "@@ -68,6 +68,7 @@\n from .integrations.sdpa_attention import sdpa_attention_forward\n from .integrations.sdpa_paged import sdpa_attention_paged_forward\n from .integrations.tensor_parallel import (\n+    ALL_PARALLEL_STYLES,\n     _get_parameter_tp_plan,\n     distribute_model,\n     initialize_tensor_parallelism,\n@@ -1883,10 +1884,12 @@ def post_init(self):\n                             f\" {self.__class__.__name__}\"\n                         )\n \n+        self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n-        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n-        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n-        self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n+        if self.base_model is self:\n+            self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n+            self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n+            self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n         for name, module in self.named_children():\n             if plan := getattr(module, \"_ep_plan\", None):\n                 self._ep_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n@@ -1909,54 +1912,40 @@ def pp_plan(self) -> dict[str, tuple[str, str]]:\n         return self._pp_plan\n \n     @tp_plan.setter\n-    def tp_plan(self, plan: dict[str, str]):\n-        if plan is not None:\n-            # Validate that all parallel styles in the plan are supported\n-            from .integrations.tensor_parallel import ALL_PARALLEL_STYLES\n+    def tp_plan(self, plan: dict[str, str] | None):\n+        if plan is None:\n+            self._tp_plan = {}\n+            return\n+        if not isinstance(plan, dict):\n+            raise ValueError(\"Can only set a dictionary as `tp_plan`\")\n \n-            for layer_pattern, parallel_style in plan.items():\n-                if parallel_style not in ALL_PARALLEL_STYLES:\n-                    raise ValueError(\n-                        f\"Unsupported tensor parallel style '{parallel_style}' for layer '{layer_pattern}'. \"\n-                        f\"Supported styles are {list(ALL_PARALLEL_STYLES.keys())}\"\n-                    )\n+        # Ensure the styles are all valid\n+        for layer_pattern, parallel_style in plan.items():\n+            if parallel_style not in ALL_PARALLEL_STYLES:\n+                raise ValueError(\n+                    f\"Unsupported tensor parallel style '{parallel_style}' for layer '{layer_pattern}'. \"\n+                    f\"Supported styles are {list(ALL_PARALLEL_STYLES.keys())}\"\n+                )\n \n-            # Validate that the layer patterns match existing model structure\n-            # We check this by getting all parameter names and seeing if any match the patterns\n-            if hasattr(self, \"named_parameters\"):\n-                model_param_names = [name for name, _ in self.named_parameters()]\n-                if model_param_names:  # Only validate if model has parameters\n-                    for layer_pattern in plan.keys():\n-                        # Convert pattern to regex (replace * with .*)\n-                        regex_pattern = layer_pattern.replace(\"*\", r\"\\d+\")\n-                        pattern_matched = False\n-                        for param_name in model_param_names:\n-                            if re.match(regex_pattern, param_name):\n-                                pattern_matched = True\n-                                break\n-                        if not pattern_matched:\n-                            # Try more flexible matching - check if pattern components exist\n-                            pattern_parts = layer_pattern.split(\".\")\n-                            flexible_matched = False\n-                            for param_name in model_param_names:\n-                                param_parts = param_name.split(\".\")\n-                                if len(pattern_parts) <= len(param_parts):\n-                                    match_count = 0\n-                                    for i, pattern_part in enumerate(pattern_parts):\n-                                        if pattern_part == \"*\":\n-                                            match_count += 1\n-                                        elif i < len(param_parts) and pattern_part == param_parts[i]:\n-                                            match_count += 1\n-                                    if match_count == len(pattern_parts):\n-                                        flexible_matched = True\n-                                        break\n-                            if not flexible_matched:\n-                                warnings.warn(\n-                                    f\"Layer pattern '{layer_pattern}' does not match any parameters in the model. \"\n-                                    f\"This rule may not be applied during tensor parallelization.\"\n-                                )\n+        # Validate that the layer patterns match existing model structure. We check this by getting all parameter\n+        # names and seeing if any match the patterns\n+        model_param_names = [name for name, _ in self.named_parameters()]\n+        for layer_pattern in plan.keys():\n+            # Convert pattern to regex (replace * with .*)\n+            regex_pattern = layer_pattern.replace(\"*\", r\"\\d+\")\n+            pattern_matched = False\n+            for param_name in model_param_names:\n+                if re.match(regex_pattern, param_name):\n+                    pattern_matched = True\n+                    break\n+            if not pattern_matched:\n+                warnings.warn(\n+                    f\"Layer pattern '{layer_pattern}' does not match any parameters in the model. This rule may not \"\n+                    \"be applied during tensor parallelization, or may lead to dimension mismatches\"\n+                )\n \n-        self._tp_plan = plan if plan is not None else {}\n+        # Set the plan\n+        self._tp_plan = plan\n \n     @pp_plan.setter\n     def pp_plan(self, plan: dict[str, tuple[str, str]]):\n@@ -4233,10 +4222,11 @@ def from_pretrained(\n             max_memory (`Dict`, *optional*):\n                 A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each\n                 GPU and the available CPU RAM if unset.\n-            tp_plan (`str`, *optional*):\n-                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Currently, it only accepts\n-                `tp_plan=\"auto\"` to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with\n-                `torchrun [args] script.py`. This will be much faster than using a `device_map`, but has limitations.\n+            tp_plan (`Optional[Union[dict, str]]`, *optional*):\n+                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Use `tp_plan=\"auto\"` to\n+                use the predefined plan based on the model. If it's a dict, then it should match between module names and desired layout.\n+                Note that if you use it, you should launch your script accordingly with `torchrun [args] script.py`. This will be much\n+                faster than using a `device_map`, but has limitations.\n             tp_size (`str`, *optional*):\n                 A torch tensor parallel degree. If not provided would default to world size.\n             device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n@@ -4333,7 +4323,7 @@ def from_pretrained(\n         ):\n             key_mapping = cls._checkpoint_conversion_mapping\n \n-        if distributed_config is not None:\n+        if distributed_config is not None and tp_plan is None:\n             tp_plan = \"auto\"\n \n         # Not used anymore -- remove them from the kwargs\n@@ -4371,7 +4361,7 @@ def from_pretrained(\n             )\n \n         if tp_plan is not None or tp_size is not None:  # TP warnings, and setup\n-            tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(\n+            device_map, device_mesh, tp_size = initialize_tensor_parallelism(\n                 tp_plan, tp_size=tp_size, device_mesh=device_mesh, device_map=device_map\n             )\n \n@@ -4491,7 +4481,7 @@ def from_pretrained(\n             )\n \n         if _torch_distributed_available and device_mesh is not None:  # add hooks to nn.Modules: no weights\n-            model = distribute_model(model, distributed_config, device_mesh, tp_size)\n+            model = distribute_model(model, tp_plan, distributed_config, device_mesh, tp_size)\n \n         # Prepare the full device map\n         if device_map is not None:"
        },
        {
            "sha": "a230b5e1ad29a397973d4a854eee3411ed8595f1",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 35,
            "deletions": 7,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ef6f2c415ccd4f01460fb3733b6ad709e58d098/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ef6f2c415ccd4f01460fb3733b6ad709e58d098/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=3ef6f2c415ccd4f01460fb3733b6ad709e58d098",
            "patch": "@@ -75,9 +75,6 @@ def test_model_forward(self):\n \n             model_id = \"JackFram/llama-68m\"\n \n-            rank = int(os.environ[\"RANK\"])\n-            world_size = int(os.environ[\"WORLD_SIZE\"])\n-\n             model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", tp_plan=\"auto\")\n             torch.distributed.barrier()\n \n@@ -141,9 +138,6 @@ def test_model_generate(self):\n \n             model_id = \"JackFram/llama-68m\"\n \n-            rank = int(os.environ[\"RANK\"])\n-            world_size = int(os.environ[\"WORLD_SIZE\"])\n-\n             model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", tp_plan=\"auto\")\n             torch.distributed.barrier()\n \n@@ -155,7 +149,7 @@ def test_model_generate(self):\n                     has_dtensor = 1\n                     break\n \n-            assert has_dtensor == 1, \"TP model must has DTensor\"\n+            assert has_dtensor == 1, \"TP model must have DTensor\"\n \n             tokenizer = AutoTokenizer.from_pretrained(model_id)\n             prompt = \"Can I help\"\n@@ -214,6 +208,40 @@ def test_model_save(self):\n                     assert torch.allclose(non_tp_tensor, tp_tensor), f\"Tensor with key: {non_tp_key} does not match\"\n                     del non_tp_tensor, tp_tensor\n \n+    def test_custom_tp_plan(self):\n+        script_to_run = textwrap.dedent(\n+            r\"\"\"\n+            import re\n+            import torch\n+            from torch.distributed.tensor import DTensor\n+            from transformers import AutoModelForCausalLM\n+\n+            model_id = \"JackFram/llama-68m\"\n+            # only shard attentions, but not mlps\n+            tp_plan = {\n+                \"model.layers.*.self_attn.q_proj\": \"colwise\",\n+                \"model.layers.*.self_attn.k_proj\": \"colwise\",\n+                \"model.layers.*.self_attn.v_proj\": \"colwise\",\n+                \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n+            }\n+\n+            # Use custom tp_plan directly in from_pretrained\n+            model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n+\n+            # Check we can generate with the tp_plan\n+            inputs = torch.randint(100, 200, (1, 10), device=model.device)\n+            out = model.generate(inputs, max_new_tokens=10, do_sample=False)\n+\n+            # Check only the attentions are sharded\n+            for name, param in model.named_parameters():\n+                if re.search(r\"\\.self_attn\\.(q|k|v|o)_proj\\.\", name):\n+                    assert isinstance(param, DTensor)\n+                else:\n+                    assert not isinstance(param, DTensor)\n+            \"\"\"\n+        )\n+        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n+\n \n class TestTensorParallelProperties(TestCasePlus):\n     def test_tp_plan_property_setter_getter(self):"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 89,
        "deletions": 69
    }
}