{
    "author": "gante",
    "message": "Test: generate with `torch.compile(model.forward)` as a fast test (#34544)",
    "sha": "ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
    "files": [
        {
            "sha": "9e64cb9e039a7baab20bd2d06cc4516a3bb2f76d",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -349,7 +349,7 @@ In case you are using Sink Cache, you have to crop your inputs to that maximum l\n >>> user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n \n >>> past_key_values = DynamicCache()\n->>> max_cache_length = past_key_values.get_max_length()\n+>>> max_cache_length = past_key_values.get_max_cache_shape()\n \n >>> messages = []\n >>> for prompt in user_prompts:"
        },
        {
            "sha": "427e1d4e3aea86350632d4577b47d9b6f5a05c09",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -29,6 +29,8 @@ class Cache(torch.nn.Module):\n     Base, abstract class for all caches. The actual data structure is specific to each subclass.\n     \"\"\"\n \n+    is_compileable = False\n+\n     def __init__(self):\n         super().__init__()\n \n@@ -1098,6 +1100,8 @@ class StaticCache(Cache):\n         ```\n     \"\"\"\n \n+    is_compileable = True\n+\n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n@@ -1297,6 +1301,7 @@ class SlidingWindowCache(StaticCache):\n     \"\"\"\n \n     is_sliding = True\n+    is_compileable = True\n \n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     def __init__(\n@@ -1421,6 +1426,7 @@ def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n         super().__init__()\n         self.self_attention_cache = self_attention_cache\n         self.cross_attention_cache = cross_attention_cache\n+        self.is_compileable = getattr(self.self_attention_cache, \"is_compileable\", False)\n \n         self.is_updated = {}\n         for layer_idx in range(len(cross_attention_cache.key_cache)):\n@@ -1612,6 +1618,8 @@ class HybridCache(Cache):\n         ```\n     \"\"\"\n \n+    is_compileable = True\n+\n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n@@ -1832,6 +1840,8 @@ class MambaCache:\n         ```\n     \"\"\"\n \n+    is_compileable = True\n+\n     # TODO (joao): remove `=None` in non-optional arguments in v4.46. Remove from `OBJECTS_TO_IGNORE` as well.\n     def __init__(\n         self,\n@@ -1975,6 +1985,8 @@ class OffloadedStaticCache(StaticCache):\n         ```\n     \"\"\"\n \n+    is_compileable = True\n+\n     @deprecate_kwarg(\"layer_device_map\", version=\"4.52.0\")\n     def __init__(\n         self,"
        },
        {
            "sha": "a0e96c31cb590df72ef80cb827a849c91e757a74",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -1579,7 +1579,7 @@ def construct_processor(self, vocab_size: int, device) -> \"WatermarkLogitsProces\n \n \n @dataclass\n-class CompileConfig(object):\n+class CompileConfig:\n     \"\"\"\n     Class that holds arguments relative to `torch.compile` behavior, when using automatic compilation in `generate`.\n     See [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html) for more details on the arguments.\n@@ -1620,7 +1620,9 @@ class CompileConfig(object):\n     backend: Union[str, Callable] = \"inductor\"\n     mode: str = \"reduce-overhead\"\n     options: Optional[dict] = None\n+    # Used to flag our `generate` call to compile on e.g. CPU. Often not optimal, but useful for testing purposes.\n+    _compile_all_devices = None\n \n     def to_dict(self) -> Dict[str, Any]:\n         \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n-        return copy.deepcopy(self.__dict__)\n+        return copy.deepcopy({key: value for key, value in self.__dict__.items() if key != \"_compile_all_devices\"})"
        },
        {
            "sha": "cb6ec15bb901351b7a6275d42f40c7b22e56382e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -3177,9 +3177,11 @@ def _sample(\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n         model_forward = self.__call__\n-        if isinstance(model_kwargs.get(\"past_key_values\"), StaticCache):\n-            if self.device.type == \"cuda\":\n-                logger.warning_once(\"Using `torch.compile`.\")\n+        if isinstance(model_kwargs.get(\"past_key_values\"), Cache):\n+            is_compileable = model_kwargs[\"past_key_values\"].is_compileable\n+            if is_compileable and (\n+                self.device.type == \"cuda\" or generation_config.compile_config._compile_all_devices\n+            ):\n                 os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n                 model_forward = self.get_compiled_call(generation_config.compile_config)\n "
        },
        {
            "sha": "c55d1feb6d9f3d71565662a96b15414205377272",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -708,7 +708,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = False\n \n     def _init_weights(self, module):\n@@ -1561,6 +1561,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             logits_to_keep=logits_to_keep,\n+            cache_position=cache_position,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "8bb79616ea9573208e717bbb1ef099fa4924348f",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -1223,6 +1223,7 @@ def _init_weights(self, module):\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = False\n \n     def _init_weights(self, module):\n@@ -1535,6 +1536,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             logits_to_keep=logits_to_keep,\n+            cache_position=cache_position,\n         )\n \n         logits = outputs[0]"
        },
        {
            "sha": "41458ab6a36183ebef698fa81e16678bcafa59b7",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -833,6 +833,7 @@ class DbrxPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "b31e14910a9b13e81f2414bda08994aca675de21",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -1802,6 +1802,7 @@ def forward(\n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"text_model.lm_head.weight\"]\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "d645a88baf38645a7cdc28834c74c4fa961eb315",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -1113,6 +1113,7 @@ def forward(**super_kwargs):\n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"text_model.lm_head.weight\"]\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "603b9f6922411222bdf98f5250c39cccc68f0ce5",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -52,7 +52,7 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # TODO (fix me): compilation fails due to a stide error?\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "66aef05e67cb6b42108c44b9bacb52f3e649992e",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -843,6 +843,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "c59e05509d0d49465fc6e8f9f1a5e6d2a42b1e7e",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -917,6 +917,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_static_cache = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n \n     def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only"
        },
        {
            "sha": "0d7bdb3394c94db08847bf18bf1a0c9b95085495",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -485,7 +485,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "7890400934c6789565f95f3d77e02e8a26658551",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -45,7 +45,9 @@\n     MistralForSequenceClassification,\n     MistralForTokenClassification,\n     MistralModel,\n+    MistralPreTrainedModel,\n     MistralRMSNorm,\n+    MistralRotaryEmbedding,\n )\n from .configuration_mixtral import MixtralConfig\n \n@@ -313,6 +315,14 @@ def forward(\n         return outputs\n \n \n+class MixtralRotaryEmbedding(MistralRotaryEmbedding):\n+    pass\n+\n+\n+class MixtralPreTrainedModel(MistralPreTrainedModel):\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+\n+\n class MixtralModel(MistralModel):\n     def __init__(self, config: MixtralConfig):\n         super().__init__(config)"
        },
        {
            "sha": "d1a9cdbce9503281f1258fda88f138aa1fcc1fa0",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -767,7 +767,7 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "5e0b95c4612db995e8243c1fa2bf33bf9a9dc239",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -912,7 +912,7 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "78a11176e192a81049d13d0aa0d99ddf32731f20",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -332,7 +332,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "dd0f80cc3e545f98ac0482ac5dea85e33a6d1ed9",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -882,7 +882,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = True\n+    _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "d9b4bbbe8c69aa3d6cba6a3a0f0b5174d41d9e57",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 52,
            "deletions": 22,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -1978,52 +1978,82 @@ def test_generate_with_quant_cache(self):\n                 model.generate(**generation_kwargs, **inputs_dict)\n \n     @pytest.mark.generate\n-    @require_torch_accelerator\n-    @slow\n     def test_generate_compile_model_forward(self):\n         \"\"\"\n-        Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results. Tests\n-        end-to-end compilation and forward pass compilation only.\n+        Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results.\n         ⚠️ Runs two sequential generations to ensure the cache doesn't get stuck after the first compiled run! ⚠️\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n-                self.skipTest(\"This model doesn't support static cache\")\n+                self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n \n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=4)\n \n             model = model_class(config).to(torch_device)\n             model.eval()  # otherwise `self.training` is `True` -- this flag is used at attn mask creation time\n \n-            input_ids = inputs_dict[\"input_ids\"].to(torch_device)\n+            main_input = inputs_dict[model.main_input_name].to(torch_device)\n             # creates two sets of *different* inputs with the same shape\n-            half_batch_size = input_ids.shape[0] // 2\n-            input_ids_sets = [input_ids[:half_batch_size, :], input_ids[half_batch_size : half_batch_size * 2, :]]\n-            self.assertTrue(input_ids_sets[0].shape == input_ids_sets[1].shape)\n+            half_batch_size = main_input.shape[0] // 2\n+            input_1 = {}\n+            input_2 = {}\n+            for key, value in inputs_dict.items():\n+                if isinstance(value, torch.Tensor):\n+                    input_1[key] = value[:half_batch_size, :].to(torch_device)\n+                    input_2[key] = value[half_batch_size : half_batch_size * 2, :].to(torch_device)\n+                else:\n+                    input_1[key] = value\n+                    input_2[key] = value\n+            model_input_sets = [input_1, input_2]\n+            self.assertTrue(\n+                model_input_sets[0][model.main_input_name].shape == model_input_sets[1][model.main_input_name].shape\n+            )\n+\n+            # compilation-specific setup\n+            torch.compiler.reset()  # prevent cached compilation from being used in the test\n+            has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n+            model.generation_config.compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n \n             generation_kwargs = {\n                 \"do_sample\": False,\n-                \"max_new_tokens\": 10,\n+                \"max_new_tokens\": 5,\n                 \"return_dict_in_generate\": True,\n                 \"output_scores\": True,\n-                \"cache_implementation\": \"static\",\n             }\n \n             # get eager + dynamic cache results for future comparison\n             dynamic_outputs = []\n-            for model_inputs in input_ids_sets:\n-                dynamic_outputs.append(model.generate(model_inputs, **generation_kwargs))\n-\n-            # get compiled results\n-            generation_config = copy.deepcopy(model.generation_config)\n-            generation_config.update(**generation_kwargs)\n-            torch.compiler.reset()\n+            for model_inputs in model_input_sets:\n+                gen_out = model.generate(**model_inputs, **generation_kwargs)\n+                dynamic_outputs.append(gen_out)\n+                # sanity checks for the default cache implementation\n+                if not has_defined_cache_implementation:\n+                    decoder_cache = (\n+                        gen_out.past_key_values.self_attention_cache\n+                        if config.is_encoder_decoder\n+                        else gen_out.past_key_values\n+                    )\n+                    self.assertTrue(isinstance(decoder_cache, DynamicCache))\n+                    self.assertFalse(decoder_cache.is_compileable)\n+                    self.assertFalse(hasattr(model, \"_compiled_call\"))  # our auto compile should NOT have been called\n \n-            model.forward = torch.compile(model.forward, fullgraph=True, mode=\"reduce-overhead\")\n+            # get compiled results -- relies on the automatic compilation triggered by specific \"cache_implementation\"\n+            if not has_defined_cache_implementation:\n+                generation_kwargs[\"cache_implementation\"] = \"static\"\n \n             compiled_outputs = []\n-            for model_inputs in input_ids_sets:\n-                compiled_outputs.append(model.generate(model_inputs, generation_config=generation_config))\n+            for model_inputs in model_input_sets:\n+                gen_out = model.generate(**model_inputs, **generation_kwargs)\n+                compiled_outputs.append(gen_out)\n+                # sanity checks\n+                decoder_cache = (\n+                    gen_out.past_key_values.self_attention_cache\n+                    if config.is_encoder_decoder\n+                    else gen_out.past_key_values\n+                )\n+                self.assertFalse(isinstance(decoder_cache, DynamicCache))\n+                self.assertTrue(decoder_cache.is_compileable)\n+                self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n             for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n                 self._check_similar_generate_outputs(dynamic_result, compiled_result)"
        },
        {
            "sha": "56cb9141d6b6e5cf297455daef88e584520b93d5",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -331,11 +331,6 @@ def test_model_rope_scaling(self, scaling_type):\n     def test_batching_equivalence(self):\n         pass\n \n-    # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n-    @unittest.skip(\"Chameleon is not compatible with end-to-end generation compilation\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class ChameleonIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "a3d088e2160b0aba49acc590547f56b7770d0a85",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -368,10 +368,6 @@ def test_disk_offload_safetensors(self):\n     def test_disk_offload_bin(self):\n         pass\n \n-    @unittest.skip(\"Dbrx does not support `torch.compile` with `fullgraph=True`.\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class DbrxModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "01871e81b30eccc0d5b77bb15f5d214b391f98cd",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -780,10 +780,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_custom_4d_attention_mask(self):\n         pass\n \n-    @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n     @unittest.skip(reason=\"We only test the model that takes in multiple images\")\n     def test_model(self):\n         pass"
        },
        {
            "sha": "8864185abf4f92557c80c284237d6fc260d095ff",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -332,10 +332,6 @@ def test_beam_search_low_memory(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"Can't compile fullgraph due to dynamic control flow in `prepare_inputs_for_generate`\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "ce30ea4eaeb0d3a7069871f49137ed2076b035f7",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -1602,6 +1602,11 @@ def test_labels_sequence_max_length_error_after_changing_config(self):\n             with self.assertRaises(ValueError):\n                 model(input_features=input_features, labels=labels)\n \n+    # TODO (joao, eustache): fix me :)\n+    @unittest.skip(reason=\"Whisper's custom generate is not consistent regarding the cache return types\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n \n @require_torch\n @require_torchaudio"
        },
        {
            "sha": "6541dad8d01679bbbbf75023ca0afe2aafbf3c9d",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece8c42488a20ada0dcac87bd5e09f25f503ccf3/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=ece8c42488a20ada0dcac87bd5e09f25f503ccf3",
            "patch": "@@ -364,7 +364,7 @@ def test_sink_cache_iterative_prompts(self):\n             input_ids = gen_out\n \n         # We went well beyond the cache length\n-        self.assertTrue(input_ids.shape[1] > cache.get_max_length() * 1.5)\n+        self.assertTrue(input_ids.shape[1] > cache.get_max_cache_shape() * 1.5)\n \n         # And it still produces a coherent english\n         decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 105,
        "deletions": 53
    }
}