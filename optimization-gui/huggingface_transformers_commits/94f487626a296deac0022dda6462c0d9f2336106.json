{
    "author": "gante",
    "message": "[generate] model defaults being inherited only happens for newer models (#36881)",
    "sha": "94f487626a296deac0022dda6462c0d9f2336106",
    "files": [
        {
            "sha": "9f669e175f19866902e4913ac1fe6ba6690b96c0",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 42,
            "deletions": 18,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f487626a296deac0022dda6462c0d9f2336106/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f487626a296deac0022dda6462c0d9f2336106/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=94f487626a296deac0022dda6462c0d9f2336106",
            "patch": "@@ -23,6 +23,7 @@\n import numpy as np\n import torch\n import torch.distributed as dist\n+from packaging import version\n from torch import nn\n from torch.nn import functional as F\n \n@@ -1552,7 +1553,7 @@ def _prepare_generated_length(\n         return generation_config\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], **kwargs: Dict\n+        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Dict\n     ) -> Tuple[GenerationConfig, Dict]:\n         \"\"\"\n         Prepares the base generation config, then applies any generation configuration options from kwargs. This\n@@ -1591,23 +1592,38 @@ def _prepare_generation_config(\n \n         generation_config = copy.deepcopy(generation_config)\n \n-        # If `generation_config` is provided, let's fallback ALL default values to the model's generation config\n         if not using_model_generation_config:\n-            modified_values = {}\n-            default_generation_config = GenerationConfig()\n-            for key, default_value in default_generation_config.__dict__.items():\n-                if key.startswith(\"_\"):  # metadata\n-                    continue\n-                custom_gen_config_value = getattr(generation_config, key)\n-                model_gen_config_value = getattr(self.generation_config, key)\n-                if custom_gen_config_value == default_value and model_gen_config_value != default_value:\n-                    modified_values[key] = model_gen_config_value\n-                    setattr(generation_config, key, model_gen_config_value)\n-            if len(modified_values) > 0:\n-                logger.warning_once(\n-                    f\"`generation_config` default values have been modified to match model-specific defaults: \"\n-                    f\"{modified_values}. If this is not desired, please set these values explicitly.\"\n-                )\n+            # If `generation_config` is provided:\n+            # - `use_model_defaults`: let's fallback ALL default values to the model's generation config\n+            # - otherwise: legacy behavior, let's just make sure we have the tokens defined\n+            model_base_version = version.parse(version.parse(self.generation_config.transformers_version).base_version)\n+            if use_model_defaults is True or (\n+                use_model_defaults is None and model_base_version >= version.parse(\"4.50.0\")\n+            ):\n+                modified_values = {}\n+                default_generation_config = GenerationConfig()\n+                for key, default_value in default_generation_config.__dict__.items():\n+                    if key.startswith(\"_\") or key == \"transformers_version\":  # metadata\n+                        continue\n+                    custom_gen_config_value = getattr(generation_config, key)\n+                    model_gen_config_value = getattr(self.generation_config, key)\n+                    if custom_gen_config_value == default_value and model_gen_config_value != default_value:\n+                        modified_values[key] = model_gen_config_value\n+                        setattr(generation_config, key, model_gen_config_value)\n+                if len(modified_values) > 0:\n+                    logger.warning_once(\n+                        f\"`generation_config` default values have been modified to match model-specific defaults: \"\n+                        f\"{modified_values}. If this is not desired, please set these values explicitly.\"\n+                    )\n+            else:\n+                if generation_config.bos_token_id is None:\n+                    generation_config.bos_token_id = self.generation_config.bos_token_id\n+                if generation_config.eos_token_id is None:\n+                    generation_config.eos_token_id = self.generation_config.eos_token_id\n+                if generation_config.pad_token_id is None:\n+                    generation_config.pad_token_id = self.generation_config.pad_token_id\n+                if generation_config.decoder_start_token_id is None:\n+                    generation_config.decoder_start_token_id = self.generation_config.decoder_start_token_id\n \n         # Finally, apply any passed kwargs\n         model_kwargs = generation_config.update(**kwargs)\n@@ -1967,6 +1983,7 @@ def generate(\n         streamer: Optional[\"BaseStreamer\"] = None,\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        use_model_defaults: Optional[bool] = None,\n         **kwargs,\n     ) -> Union[GenerateOutput, torch.LongTensor]:\n         r\"\"\"\n@@ -2031,6 +2048,11 @@ def generate(\n                 size. This is an experimental feature, subject to breaking API changes in future versions.\n             negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Attention_mask for `negative_prompt_ids`.\n+            use_model_defaults (`bool`, *optional*):\n+                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n+                generation configuration (`model.generation_config`), as opposed to the global defaults\n+                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n+                `True`.\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n@@ -2058,7 +2080,9 @@ def generate(\n         tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n         assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n \n-        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n+        generation_config, model_kwargs = self._prepare_generation_config(\n+            generation_config, use_model_defaults, **kwargs\n+        )\n         self._validate_model_kwargs(model_kwargs.copy())\n         self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n "
        },
        {
            "sha": "7904b3f8eb9163a3e8df380f481260243aaaa12d",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/94f487626a296deac0022dda6462c0d9f2336106/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/94f487626a296deac0022dda6462c0d9f2336106/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=94f487626a296deac0022dda6462c0d9f2336106",
            "patch": "@@ -575,8 +575,8 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n \n     def test_generation_beyond_sliding_window_with_generation_config(self):\n         \"\"\"\n-        Same as `test_generation_beyond_sliding_window`, but passing a GenerationConfig. Regression test for #36684 --\n-        ensures `cache_implementation='hybrid'` is correctly inherited from the base `model.generation_config`.\n+        Similar to `test_generation_beyond_sliding_window`, but passing a GenerationConfig. Regression test for #36684\n+        -- ensures `cache_implementation='hybrid'` is correctly inherited from the base `model.generation_config`.\n         \"\"\"\n         model_id = \"google/gemma-3-1b-it\"\n         attn_implementation = \"sdpa\"\n@@ -594,12 +594,16 @@ def test_generation_beyond_sliding_window_with_generation_config(self):\n \n         # Make sure prefill is larger than sliding window\n         input_size = inputs.input_ids.shape[-1]\n-        self.assertTrue(input_size > model.config.sliding_window)\n+        self.assertGreater(input_size, model.config.sliding_window)\n \n-        generation_config = GenerationConfig(max_new_tokens=20)\n+        generation_config = GenerationConfig(max_new_tokens=5, min_new_tokens=5)\n+        out = model.generate(**inputs, generation_config=generation_config)\n \n-        out = model.generate(**inputs, generation_config=generation_config)[:, input_size:]\n-        output_text = tokenizer.batch_decode(out)\n+        # Generation works beyond sliding window\n+        self.assertGreater(out.shape[1], model.config.sliding_window)\n+        self.assertEqual(out.shape[1], input_size + 5)\n \n-        EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n+        # Note: Auto-inheritance only works for models saved starting from 4.50.0\n+        model.generation_config.transformers_version = \"4.49.0\"\n+        with self.assertRaises(RuntimeError):  # errors out because it is not using hybrid cache\n+            out = model.generate(**inputs, generation_config=generation_config)"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 54,
        "deletions": 26
    }
}