{
    "author": "omahs",
    "message": "Fix typos (#42354)",
    "sha": "fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
    "files": [
        {
            "sha": "5e3911fcad6016ad09e90b4298a78ba6412032a9",
            "filename": ".github/workflows/get-pr-info.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/.github%2Fworkflows%2Fget-pr-info.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/.github%2Fworkflows%2Fget-pr-info.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fget-pr-info.yml?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -40,7 +40,7 @@ on:\n         description: \"The sha of the merge commit for the pull request (created by GitHub) in the base repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n       PR_MERGE_COMMIT_BASE_SHA:\n-        description: \"The sha of the parent commit of the the merge commit on the target branch in the base repository\"\n+        description: \"The sha of the parent commit of the merge commit on the target branch in the base repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_BASE_SHA }}\n       PR_HEAD_COMMIT_DATE:\n         description: \"The date of the head sha of the pull request branch in the head repository\""
        },
        {
            "sha": "e5dc3f728b2de2e4c020244e53d3bf1d27c661a4",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -414,7 +414,7 @@ def run_benchmarks(\n             self.save_results(model_id, all_results, timestamp=timestamp)\n \n         if len(all_results) < 1:\n-            raise RuntimeError(\"No benchmark was run succesfully\")\n+            raise RuntimeError(\"No benchmark was run successfully\")\n \n         if pretty_print_summary:\n             print()\n@@ -504,4 +504,4 @@ def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestam\n                 repo_type=\"dataset\",\n                 token=PUSH_TO_HUB_TOKEN,\n             )\n-        self.logger.info(f\"Succesfully uploaded results to: {dataset_id}\")\n+        self.logger.info(f\"Successfully uploaded results to: {dataset_id}\")"
        },
        {
            "sha": "3bcc3711bb8676f2be6651cb7bdba09741532d4e",
            "filename": "examples/modular-transformers/configuration_duplicated_method.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_duplicated_method.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class DuplicatedMethodConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "616153811b8d29298bd76d55dfa861999b737423",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -91,15 +91,15 @@ def has_enough_free_blocks(self, n_blocks: int) -> bool:\n         if len(self._uninit_block_ids) >= n_blocks:\n             return True\n         # Exit early if even after uninitializing all initialized blocks, there are not enough free blocks\n-        block_to_unintialize = n_blocks - len(self._uninit_block_ids)\n-        if len(self._init_block_ids) < block_to_unintialize:\n+        block_to_uninitialize = n_blocks - len(self._uninit_block_ids)\n+        if len(self._init_block_ids) < block_to_uninitialize:\n             return False\n         # Uninitialize the required amount of blocks\n-        for _ in range(block_to_unintialize):\n-            id_to_unintialize = self._init_block_ids.popitem()[0]\n-            block = self._id_to_block[id_to_unintialize]\n+        for _ in range(block_to_uninitialize):\n+            id_to_uninitialize = self._init_block_ids.popitem()[0]\n+            block = self._id_to_block[id_to_uninitialize]\n             self._hash_to_id.pop(block.hash)\n-            self._uninit_block_ids.append(id_to_unintialize)\n+            self._uninit_block_ids.append(id_to_uninitialize)\n         return True\n \n     def get_free_blocks(self, n_blocks: int, last_block_id: int | None) -> list[int] | None:"
        },
        {
            "sha": "fe7ad5120cfbfb7dd5ae6211d292034549f942af",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -51,7 +51,7 @@\n \n Smaller slices means more granularity and thus less padding. But since each graph takes up space on the GPU and time to\n create, we don't want to many graphs. And since the size of the KV dimension is the number of queries tokens plus the\n-number of tokens cached, dimension of KV is usually much larger than the the dimension of Q. So we have more granularity\n+number of tokens cached, dimension of KV is usually much larger than the dimension of Q. So we have more granularity\n for the KV dimension than the query dimension.\n \"\"\"\n NUM_Q_CUDA_GRAPHS = 4"
        },
        {
            "sha": "6588095e852182774fecdc59edc67683d9adad80",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -76,7 +76,7 @@ class ApertusConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "a60daa2f8194db5765c248c9947df165c9bf718b",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -94,7 +94,7 @@ class ApertusConfig(LlamaConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "b8a2015bbf7a7a1186629fb1ad5ff48c36b17935",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class ArceeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "006c7b36281c10a8fe351c9f4ea32be686bf0e66",
            "filename": "src/transformers/models/arcee/modular_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodular_arcee.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -85,7 +85,7 @@ class ArceeConfig(LlamaConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "f1d8e4193c2e04056f3680cc0d311a1030b1c7c5",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class AriaTextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "3e500f4bfc245840fd3f8e2b51bc1512af00442b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -149,7 +149,7 @@ class AriaTextConfig(LlamaConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "bc14f0d6cde41574ae704e4e884d7680030ef636",
            "filename": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -170,8 +170,8 @@ def __call__(\n             audio_inputs[\"input_features_mask\"] = padding_mask\n \n             # Compute sequence lengths token counting\n-            audio_lenghts = torch.stack([s.sum() for s in torch.split(padding_mask.sum(-1), per_sample_windows)])\n-            conv_output_lengths = (audio_lenghts - 1) // 2 + 1  # After conv2 downsampling\n+            audio_lengths = torch.stack([s.sum() for s in torch.split(padding_mask.sum(-1), per_sample_windows)])\n+            conv_output_lengths = (audio_lengths - 1) // 2 + 1  # After conv2 downsampling\n             audio_tokens_lengths = (conv_output_lengths - 2) // 2 + 1  # After avg pooling\n \n             # expand audio tokens in text"
        },
        {
            "sha": "c27c6d1a1e6e65358c6c888fbe74cf4ebff52d06",
            "filename": "src/transformers/models/bamba/configuration_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconfiguration_bamba.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -106,7 +106,7 @@ class BambaConfig(PreTrainedConfig):\n         z_loss_coefficient (`float`, *optional*, defaults to 0.0):\n             Coefficient for auxiliary z-loss used to control logit growth during training\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n     \"\"\""
        },
        {
            "sha": "5a88939ff0b24495c72276a0101e91a1e85ff5af",
            "filename": "src/transformers/models/bitnet/configuration_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fconfiguration_bitnet.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class BitNetConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "73603a361d39f1722655801d2439f1c2f9f7ac7a",
            "filename": "src/transformers/models/blt/configuration_blt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fconfiguration_blt.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -211,7 +211,7 @@ class BltPatcherConfig(PreTrainedConfig):\n         intermediate_size (`int`, *optional*, defaults to 2048):\n             Dimension of the MLP representations.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -307,7 +307,7 @@ class BltConfig(PreTrainedConfig):\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "4e0c24c52c5931819310cd03f568c58d64ff885b",
            "filename": "src/transformers/models/chameleon/configuration_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconfiguration_chameleon.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -149,7 +149,7 @@ class ChameleonConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "1ef5b72a8ad4ebfbf3b80eb15e31f4c67f8484f1",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -81,7 +81,7 @@ class CohereConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "835143add51794ce5c9304093caa4a0ed8cef76c",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class Cohere2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "9e8bc6b564e4068b20cfb703c5280f8257a64b88",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -101,7 +101,7 @@ class Cohere2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "1903457f0a7eb4c8ab07d934ffd755912fcbbfa2",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -97,9 +97,9 @@ def __call__(\n         wrapper around the Qwen2VLProcessor's [`~Qwen2VLProcessor.__call__`] method adapted for the ColQwen2 model. It cannot process\n         both text and images at the same time.\n \n-        When preparing the the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n+        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n         [`~Qwen2TokenizerFast.__call__`].\n-        When preparing the the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n+        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n         [`~Qwen2VLImageProcessor.__call__`].\n         Please refer to the doctsring of the above two methods for more information.\n "
        },
        {
            "sha": "076c9f27a549d3ead2304536ac143fc17b83e2df",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -96,9 +96,9 @@ def __call__(\n         wrapper around the Qwen2VLProcessor's [`~Qwen2VLProcessor.__call__`] method adapted for the ColQwen2 model. It cannot process\n         both text and images at the same time.\n \n-        When preparing the the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n+        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n         [`~Qwen2TokenizerFast.__call__`].\n-        When preparing the the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n+        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n         [`~Qwen2VLImageProcessor.__call__`].\n         Please refer to the doctsring of the above two methods for more information.\n "
        },
        {
            "sha": "7b077ae63a52d33142ed1a035a2f653ca3866059",
            "filename": "src/transformers/models/csm/configuration_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconfiguration_csm.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class CsmDepthDecoderConfig(PreTrainedConfig):\n         eos_token_id (`int`, *optional*):\n             End of stream token id.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):\n@@ -230,7 +230,7 @@ class CsmConfig(PreTrainedConfig):\n         audio_eos_token_id (`int`, *optional*, defaults to 128003):\n             End of stream token id for audio in the text input.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "4374e37f462b3c26860643a57ca53cf8c90c3960",
            "filename": "src/transformers/models/cwm/configuration_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fconfiguration_cwm.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -29,7 +29,7 @@ class CwmConfig(PreTrainedConfig):\n     \"\"\"\n     Configuration for Code World Model (CWM).\n     This is an inherited Llama3-compatible configuration with layer-interleaved\n-    sliding-window attention. Configures a `CwmModel`. Designed to yield a configuartion mirroring the model in the\n+    sliding-window attention. Configures a `CwmModel`. Designed to yield a configuration mirroring the model in the\n     [facebook/cwm](https://huggingface.co/facebook/cwm) architecture by default. Other models include:\n     - [facebook/cwm-sft](https://huggingface.co/facebook/cwm-sft)\n     - [facebook/cwm-pretrain](https://huggingface.co/facebook/cwm-pretrain)\n@@ -80,7 +80,7 @@ class CwmConfig(PreTrainedConfig):\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 8192):"
        },
        {
            "sha": "00e63efb0f0edd22695dd23c1eb9f75a1299cf4b",
            "filename": "src/transformers/models/cwm/modular_cwm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcwm%2Fmodular_cwm.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -41,7 +41,7 @@ class CwmConfig(LlamaConfig):\n     \"\"\"\n     Configuration for Code World Model (CWM).\n     This is an inherited Llama3-compatible configuration with layer-interleaved\n-    sliding-window attention. Configures a `CwmModel`. Designed to yield a configuartion mirroring the model in the\n+    sliding-window attention. Configures a `CwmModel`. Designed to yield a configuration mirroring the model in the\n     [facebook/cwm](https://huggingface.co/facebook/cwm) architecture by default. Other models include:\n     - [facebook/cwm-sft](https://huggingface.co/facebook/cwm-sft)\n     - [facebook/cwm-pretrain](https://huggingface.co/facebook/cwm-pretrain)\n@@ -92,7 +92,7 @@ class CwmConfig(LlamaConfig):\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 8192):"
        },
        {
            "sha": "49eae05dfa3802d16b5c8ebb70a46bf700527937",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class DeepseekV2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "aa9a971821031a09f167a8535bc0bd3a2b7cab90",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -86,7 +86,7 @@ class DeepseekV2Config(LlamaConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "df511f576cb1400541b8cbb7df83dba89d4cff92",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -109,7 +109,7 @@ class DeepseekV3Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         rope_interleave (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "dadda0ce372429f8328806a04184f64214adfc09",
            "filename": "src/transformers/models/dia/configuration_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fconfiguration_dia.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -56,7 +56,7 @@ class DiaEncoderConfig(PreTrainedConfig):\n             The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n             `\"relu\"`, `\"swish\"` and `\"gelu_new\"` are supported.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -145,7 +145,7 @@ class DiaDecoderConfig(PreTrainedConfig):\n         num_channels (`int`, *optional*, defaults to 9):\n             Number of channels for the Dia decoder.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):"
        },
        {
            "sha": "20468eaaa8f74a8867cd7c4c31980c08a73293f0",
            "filename": "src/transformers/models/diffllama/configuration_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fconfiguration_diffllama.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -73,7 +73,7 @@ class DiffLlamaConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "27e7b840422564b0f77f0dd2c1fab599c66335d8",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -59,7 +59,7 @@ class DogeConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 8):"
        },
        {
            "sha": "eacea60cf44216f4f44d0b4abd39528e1b31edd8",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -88,7 +88,7 @@ class DogeConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 8):"
        },
        {
            "sha": "779d6fbfe45443fa341cbada68cbed0c77e406e3",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class Dots1Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie the input and output word embeddings.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "0d151b7850ab910c2eb207db90e89fb47afbdd1b",
            "filename": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconfiguration_efficientloftr.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -72,7 +72,7 @@ class EfficientLoFTRConfig(PreTrainedConfig):\n             Dim factor for the RoPE embeddings, in EfficientLoFTR, frequencies should be generated for\n             the whole hidden_size, so this factor is used to compensate.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         fine_matching_slice_dim (`int`, *optional*, defaults to 8):"
        },
        {
            "sha": "c372b3ea156b763a86803c3cef2106740de6bd71",
            "filename": "src/transformers/models/emu3/configuration_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconfiguration_emu3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -159,7 +159,7 @@ class Emu3TextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         mlp_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "6df44230d68e77602c4b9205b9177f4b18f15627",
            "filename": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -69,7 +69,7 @@ class Ernie4_5Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "93fd87f365dc787fdd44e50ace2cd006a5c36cc7",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -72,7 +72,7 @@ class Ernie4_5_MoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "04a632c36f0cf55dbf49da3ea886eef18e46df32",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -73,7 +73,7 @@ class Exaone4Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "d6004db0d28cc50e2068220a1e7383843decef59",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -106,7 +106,7 @@ class Exaone4Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_dropout (`float`, *optional*, defaults to 0.0):"
        },
        {
            "sha": "6515cae36273df22c7a58be6b227bde55d99363f",
            "filename": "src/transformers/models/falcon/configuration_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fconfiguration_falcon.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class FalconConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with, when `alibi` is `False`. Pretrained\n             Falcon models with RoPE support up to 2048 tokens.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         bos_token_id (`int`, *optional*, defaults to 11):"
        },
        {
            "sha": "124cdd2543df477f5c5fe210fb6a51b486b4b586",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -75,7 +75,7 @@ class FlexOlmoConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "8daa8dc1e759d26bbb8569590e95e5cc6f5343ef",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -86,7 +86,7 @@ class FlexOlmoConfig(OlmoeConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "36c3beb1cc988502b093bc3027ba1f27703b277d",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class FuyuConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie input and output embeddings.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         qk_layernorm (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "bc1a2497c7081d58c7951d6d452f8ba150ef5e22",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -76,7 +76,7 @@ class GemmaConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "a6b6b7684f8a1585693c0fa2cede909b20bc5028",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -104,7 +104,7 @@ class GemmaConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "23097820fbec91c921d0228bff429d517795d0ed",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class Gemma2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "f33a0c7ba66689456eb4256508ea7bee82fbd5f6",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -106,7 +106,7 @@ class Gemma2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "eedca6a496246c872175d4721024f6a999506a4b",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -96,7 +96,7 @@ class Gemma3TextConfig(PreTrainedConfig):\n         attn_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the attention scores.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_bidirectional_attention (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "7f9e987035ada77aef573124ca98ea3af537544a",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -127,7 +127,7 @@ class Gemma3TextConfig(Gemma2Config, PreTrainedConfig):\n         attn_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the attention scores.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_bidirectional_attention (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "e518eb5737be0451cc71ffd3e353b3f332711a8a",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -91,7 +91,7 @@ class Gemma3nTextConfig(PreTrainedConfig):\n         bos_token_id (`int`, *optional*, defaults to 2):\n             Beginning of stream token id.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "a226824781cfd04117bd2e08246dfcde9dd5a039",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -117,7 +117,7 @@ class Gemma3nTextConfig(Gemma2Config, PreTrainedConfig):\n         bos_token_id (`int`, *optional*, defaults to 2):\n             Beginning of stream token id.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "93dfad8f125e311e9cda2a99f452142466883506",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -67,7 +67,7 @@ class GlmConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         pad_token_id (`int`, *optional*, defaults to 151329):"
        },
        {
            "sha": "b13bc1350aa994fd1bd5230cd30ba8b3769f4e1b",
            "filename": "src/transformers/models/glm4/configuration_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fconfiguration_glm4.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class Glm4Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         pad_token_id (`int`, *optional*, defaults to 151329):"
        },
        {
            "sha": "7b5b67a273e5e051aeac32d9a1bb2f0b15e9eaa9",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -71,7 +71,7 @@ class Glm4MoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "a4e9475a7f19b20efb0114e714fcad6a726f7e0a",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -85,7 +85,7 @@ class Glm4MoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "66f70bf438af8ceeeaafb086aa415339f59f847f",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -162,7 +162,7 @@ class Glm4vTextConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         image_token_id (`int`, *optional*):"
        },
        {
            "sha": "dce3ef92c9966d1801b9791f6d8ab9c8f0456666",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -199,7 +199,7 @@ class Glm4vTextConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         image_token_id (`int`, *optional*):"
        },
        {
            "sha": "dbb85b40f3672986e04d2c2f6827884d813e74bc",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -161,7 +161,7 @@ class Glm4vMoeTextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `True`, *optional*, defaults to `True`):"
        },
        {
            "sha": "2e8e8ac39f635cf1e6f76597f7a272ab1482d1c6",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -110,7 +110,7 @@ class Glm4vMoeTextConfig(Glm4MoeConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `True`, *optional*, defaults to `True`):"
        },
        {
            "sha": "90a1d92bbb600e9f6cee59b5f6cc4950e9c59f6d",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -75,7 +75,7 @@ class GPTNeoXConfig(PreTrainedConfig):\n             Whether to use a \"parallel\" formulation in each Transformer layer, which can provide a slight training\n             speedup at large scales (e.g. 20B).\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "1f1336099ac6974c2bf3ce98c7c85985fc3c6c9f",
            "filename": "src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fconfiguration_gpt_neox_japanese.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -61,7 +61,7 @@ class GPTNeoXJapaneseConfig(PreTrainedConfig):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_dropout (`float`, *optional*, defaults to 0.1):"
        },
        {
            "sha": "bfed37bd1ce45a66ec92e0d2f79a06471ae19717",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -79,7 +79,7 @@ class GraniteConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "684fc2bb223ff0e4a66b6c06d40533531521fb86",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -79,7 +79,7 @@ class GraniteMoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "d947b2045b35dc0e01b6b487d55de334fc637f02",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -74,7 +74,7 @@ class GraniteMoeHybridConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "00ad7c5ec9a66f28dfdfef72d296fb3135e80913",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -79,7 +79,7 @@ class GraniteMoeSharedConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "40334cfd7967faea72a60316019c7c76cde7410c",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -67,7 +67,7 @@ class HeliumConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         pad_token_id (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "f4828d9112dcbd03df695ede1b8f1645ff655676",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -83,7 +83,7 @@ class HunYuanDenseV1Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "332c86b82a6348fabbe555172e012db637785881",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -85,7 +85,7 @@ class HunYuanMoEV1Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "757174241b93b73d6808a7fe72e49a5cd678d160",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -74,7 +74,7 @@ class JetMoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         rms_norm_eps (`float`, *optional*, defaults to 1e-06):"
        },
        {
            "sha": "efd4074c36fc3554b9a16d100c6d32955635ed8d",
            "filename": "src/transformers/models/kyutai_speech_to_text/configuration_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconfiguration_kyutai_speech_to_text.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -60,7 +60,7 @@ class KyutaiSpeechToTextConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically, set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "3dcff8b260c6570c533634fa00de0a0c58d1fb24",
            "filename": "src/transformers/models/lfm2/configuration_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fconfiguration_lfm2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -67,7 +67,7 @@ class Lfm2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         conv_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "cea82636927b72d4c37c9c6f74ec535132423294",
            "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -49,7 +49,7 @@ class Lfm2MoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         max_position_embeddings (`int`, *optional*, defaults to 128000):"
        },
        {
            "sha": "de6695e1483359e80503caf9d57454fd3259611e",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -82,7 +82,7 @@ class LlamaConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "46e0804cfda781999f30389530fee52f86c8ba25",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -191,7 +191,7 @@ class Llama4TextConfig(PreTrainedConfig):\n         router_aux_loss_coef (`int`, *optional*, defaults to 0.001): TODO\n         router_jitter_noise (`int`, *optional*, defaults to 0.0): TODO\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n             <TODO>"
        },
        {
            "sha": "372b753ef124a7055c118bc2efd6031d4d38efad",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -116,7 +116,7 @@ class MimiConfig(PreTrainedConfig):\n         use_streaming (`bool`, *optional*, defaults to `False`):\n             Whether to use streaming mode. If `True`, the model encode method will return the padding cache that can be used in a subsequent call to the encode method.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 250):"
        },
        {
            "sha": "8661cdd56724a48cf531042bb67918eabf3e15a7",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -96,7 +96,7 @@ class MiniMaxConfig(PreTrainedConfig):\n         router_jitter_noise (`float`, *optional*, defaults to 0.0):\n             Amount of noise to add to the router.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         layer_types (`list`, *optional*):"
        },
        {
            "sha": "51468cfc7148fbe4e0951618f71910d92a9b94f3",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -121,7 +121,7 @@ class MiniMaxConfig(PreTrainedConfig):\n         router_jitter_noise (`float`, *optional*, defaults to 0.0):\n             Amount of noise to add to the router.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         layer_types (`list`, *optional*):"
        },
        {
            "sha": "03bdbdb83b847b58e6e838d40b237b03d08531e9",
            "filename": "src/transformers/models/ministral/configuration_ministral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -65,7 +65,7 @@ class MinistralConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 4096):"
        },
        {
            "sha": "26d659d29b8d06ed0d974c7efcbe1b68b9fa85df",
            "filename": "src/transformers/models/ministral/modular_ministral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -82,7 +82,7 @@ class MinistralConfig(MistralConfig, PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 4096):"
        },
        {
            "sha": "b618ae245c5ea255129207e9f713d48be05e97ba",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -79,7 +79,7 @@ class MistralConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*, defaults to 4096):"
        },
        {
            "sha": "e54db8aa0ecd1c796484e0390782ed0c1745f752",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -95,7 +95,7 @@ class MixtralConfig(PreTrainedConfig):\n         router_jitter_noise (`float`, *optional*, defaults to 0.0):\n             Amount of noise to add to the router.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "a7fb988d6d4768e350c4618ba621fa4bed9ebd18",
            "filename": "src/transformers/models/mllama/configuration_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -167,7 +167,7 @@ class MllamaTextConfig(PreTrainedConfig):\n         intermediate_size (`int`, *optional*, defaults to 14336):\n             Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         rms_norm_eps (`float`, *optional*, defaults to 1e-05):"
        },
        {
            "sha": "774ded93f0e3b994664800b9c6a6e578b63e9cb1",
            "filename": "src/transformers/models/modernbert/configuration_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class ModernBertConfig(PreTrainedConfig):\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         local_attention (`int`, *optional*, defaults to 128):"
        },
        {
            "sha": "8fc3756715833c236d0fccc12ddc2f11e5698d57",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -105,7 +105,7 @@ class ModernBertConfig(PreTrainedConfig):\n         layer_types (`list`, *optional*):\n             Attention pattern for each layer.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         local_attention (`int`, *optional*, defaults to 128):"
        },
        {
            "sha": "90d2305c7441c9e4f25fa975385ad9cc191058b0",
            "filename": "src/transformers/models/modernbert_decoder/configuration_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fconfiguration_modernbert_decoder.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -100,7 +100,7 @@ class ModernBertDecoderConfig(PreTrainedConfig):\n             List of layer types, one for each layer. If not specified, will be automatically generated based on\n             `global_attn_every_n_layers`. Should contain \"full_attention\" or \"sliding_attention\".\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "b39cb440c3e9f4f97c0be5a39b690f28c92d1763",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -121,7 +121,7 @@ class ModernBertDecoderConfig(PreTrainedConfig):\n             List of layer types, one for each layer. If not specified, will be automatically generated based on\n             `global_attn_every_n_layers`. Should contain \"full_attention\" or \"sliding_attention\".\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "7fa72bc2cc038b1f34da56b464f6843a44fb0358",
            "filename": "src/transformers/models/moonshine/configuration_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconfiguration_moonshine.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -84,7 +84,7 @@ class MoonshineConfig(PreTrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.9):"
        },
        {
            "sha": "5afbdb2db3636b562b1c0edf14456ee9efc99ebd",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -107,7 +107,7 @@ class MoonshineConfig(PreTrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.9):"
        },
        {
            "sha": "a054f40e2e77e8cae4b838c7dc5193576aec0fcc",
            "filename": "src/transformers/models/moshi/configuration_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconfiguration_moshi.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -183,7 +183,7 @@ class MoshiConfig(PreTrainedConfig):\n             The maximum sequence length that this model might ever be used with. Typically, set this to something large\n             just in case (e.g., 512 or 1024 or 2048).\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):"
        },
        {
            "sha": "cf49fabaf134e5fdfeff221b24c854420fe1ed86",
            "filename": "src/transformers/models/nemotron/configuration_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconfiguration_nemotron.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class NemotronConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.5): Percentage of the query and keys which will have rotary embedding."
        },
        {
            "sha": "31627115bee24fc59a61d64fd77f98e9c886d39d",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class OlmoConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "6e35ca5729a9491e194b235ff7162f69a050ad3e",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class Olmo2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "152a78c4e46280e13f8161e8e41f3a305ea96fa8",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -94,7 +94,7 @@ class Olmo2Config(OlmoConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "f7e4d1198acd9b2dfe0899db03350ec859904dee",
            "filename": "src/transformers/models/olmo3/configuration_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -73,7 +73,7 @@ class Olmo3Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "265ef3bae96794a77bea066e4dc541ac4bf32ece",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -89,7 +89,7 @@ class Olmo3Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "786146224b05e17a400b24aff590ce1a95c853be",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -67,7 +67,7 @@ class OlmoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "079307547fe523894cd2266fd10f98d03e536a60",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -463,7 +463,7 @@ def _init_weights(self, module):\n         if hasattr(self.config, \"initializer_range\"):\n             std = self.config.initializer_range\n         else:\n-            # 0.02 is the standard default value accross the library\n+            # 0.02 is the standard default value across the library\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, ParakeetEncoderAttention):"
        },
        {
            "sha": "0462c52d039958c4ae2df0f230ccebdb553964ae",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -339,7 +339,7 @@ def _init_weights(self, module):\n         if hasattr(self.config, \"initializer_range\"):\n             std = self.config.initializer_range\n         else:\n-            # 0.02 is the standard default value accross the library\n+            # 0.02 is the standard default value across the library\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, ParakeetEncoderAttention):"
        },
        {
            "sha": "f913b5c137e4dad6bc16f0361eb05f6aaa506fb3",
            "filename": "src/transformers/models/persimmon/configuration_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconfiguration_persimmon.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -61,7 +61,7 @@ class PersimmonConfig(PreTrainedConfig):\n         tie_word_embeddings(`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         qk_layernorm (`bool`, *optional*, default to `True`):"
        },
        {
            "sha": "066e31c6a7d83b3e1a830b192ddd28d01c8362c1",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -76,7 +76,7 @@ class PhiConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.5):"
        },
        {
            "sha": "04dbdb889ce190ea2e1b69d4488c770b12799312",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class Phi3Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to 1.0):"
        },
        {
            "sha": "1a918d9d7f0ed12d098987141ad5cee668b1655f",
            "filename": "src/transformers/models/phi4_multimodal/configuration_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconfiguration_phi4_multimodal.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -293,7 +293,7 @@ class Phi4MultimodalConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to `1.0`):"
        },
        {
            "sha": "c5c673cf68c2ae50e50217c38f86b3bda67a871b",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -330,7 +330,7 @@ class Phi4MultimodalConfig(Phi3Config):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to `1.0`):"
        },
        {
            "sha": "c7b62618628585a44be6f9bc0cb868034df50780",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -73,7 +73,7 @@ class PhimoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*):"
        },
        {
            "sha": "d088d22dee8e8446bcd65ca74ff579a076b9be91",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class Qwen2Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_sliding_window (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "00bcdcab166842b706bde31114d81a1acd8ce116",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -262,7 +262,7 @@ class Qwen2_5OmniTextConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -573,7 +573,7 @@ class Qwen2_5OmniTalkerConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         position_id_per_seconds (`int`, *optional*, defaults to 25):"
        },
        {
            "sha": "685c6b5c86f03bc4f9e6b89114532f38154c972e",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -295,7 +295,7 @@ class Qwen2_5OmniTextConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n@@ -606,7 +606,7 @@ class Qwen2_5OmniTalkerConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         position_id_per_seconds (`int`, *optional*, defaults to 25):"
        },
        {
            "sha": "9060220469aca11f2c7003aed50bf538cabf29ec",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -124,7 +124,7 @@ class Qwen2_5_VLTextConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "271cf3588f1766f55a7758877328e258b16a6d03",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -67,7 +67,7 @@ class Qwen2MoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_sliding_window (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "fe042cf5011edd07ff39dfdcc5a7a83386730722",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -112,7 +112,7 @@ class Qwen2VLTextConfig(PreTrainedConfig):\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n "
        },
        {
            "sha": "3b4ef21bd38669a863670b80bb4286eef040b0ec",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -70,7 +70,7 @@ class Qwen3Config(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "90ae6b1c92f63873eb783ef624ba22a8b04a9892",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class Qwen3MoeConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "4a18f6ed4603a3d5852f35ba878e1b16f5ff0d58",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -68,7 +68,7 @@ class Qwen3NextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         partial_rotary_factor (`float`, *optional*, defaults to 0.25):"
        },
        {
            "sha": "6d49a021e3dec29c0c0abe299d19d37aacbea8f5",
            "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -215,7 +215,7 @@ class Qwen3OmniMoeTextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n@@ -497,7 +497,7 @@ class Qwen3OmniMoeTalkerCodePredictorConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n@@ -659,7 +659,7 @@ class Qwen3OmniMoeTalkerTextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n@@ -949,7 +949,7 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):"
        },
        {
            "sha": "7beea60d6d167f7eb88bcb3680927bb036ed7a19",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -584,7 +584,7 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):"
        },
        {
            "sha": "d81129a171e91f81c3fd7e0fec492ee7aef221e3",
            "filename": "src/transformers/models/qwen3_vl/configuration_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -107,7 +107,7 @@ class Qwen3VLTextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "aab768b1cf1c4b5a4134e4bcbfc9f8cedbc9c4f2",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -946,7 +946,7 @@ def get_rope_index(\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Different from the original implementation, Qwen3VL use timestamps rather than absolute time position ids.\"\"\"\n \n-        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n+        # Since we use timestamps to separate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n         if video_grid_thw is not None:\n             video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)\n             video_grid_thw[:, 0] = 1"
        },
        {
            "sha": "60253ce215514ca00dd7e985e70cce96721186a2",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -148,7 +148,7 @@ class Qwen3VLTextConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n@@ -822,7 +822,7 @@ def get_rope_index(\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Different from the original implementation, Qwen3VL use timestamps rather than absolute time position ids.\"\"\"\n \n-        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n+        # Since we use timestamps to separate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n         if video_grid_thw is not None:\n             video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)\n             video_grid_thw[:, 0] = 1"
        },
        {
            "sha": "08d3300190882b4678e731ff2a6781abf1d734b7",
            "filename": "src/transformers/models/qwen3_vl_moe/configuration_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -84,7 +84,7 @@ class Qwen3VLMoeTextConfig(PreTrainedConfig):\n             The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n             If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         head_dim (`int`, *optional*):"
        },
        {
            "sha": "eab677bce4fee19aea99722d43f61f721aaad347",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -1103,7 +1103,7 @@ def get_rope_index(\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Different from the original implementation, Qwen3VLMoe use timestamps rather than absolute time position ids.\"\"\"\n \n-        # Since we use timestamps to seperate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n+        # Since we use timestamps to separate videos, like <t1> <vision_start> <frame1> <vision_end> <t2> <vision_start> <frame2> <vision_end>, the video_grid_thw should also be split\n         if video_grid_thw is not None:\n             video_grid_thw = torch.repeat_interleave(video_grid_thw, video_grid_thw[:, 0], dim=0)\n             video_grid_thw[:, 0] = 1"
        },
        {
            "sha": "15e99f440baae5a1325ce1c011243567d45a68dd",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -106,7 +106,7 @@ class Qwen3VLMoeTextConfig(PreTrainedConfig):\n             The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n             If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         head_dim (`int`, *optional*):"
        },
        {
            "sha": "e9d94e68f840ac7a41e1ada137c327b57983dd7b",
            "filename": "src/transformers/models/recurrent_gemma/configuration_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconfiguration_recurrent_gemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -78,7 +78,7 @@ class RecurrentGemmaConfig(PreTrainedConfig):\n         partial_rotary_factor (`float`, *optional*, defaults to 0.5):\n             The partial rotary factor used in the initialization of the rotary embeddings.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         block_types (`list[str]`, *optional*, defaults to `('recurrent', 'recurrent', 'attention')`):"
        },
        {
            "sha": "48eceb39225c7cb53efdd2db7b8ab89164233ab4",
            "filename": "src/transformers/models/seed_oss/configuration_seed_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fconfiguration_seed_oss.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -74,7 +74,7 @@ class SeedOssConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "1f8523e5a108dd80bc251db578d3997901e642b1",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -72,7 +72,7 @@ class SmolLM3Config(PreTrainedConfig):\n         eos_token_id (`int`, *optional*, defaults to 128001):\n             The id of the end of sentence token.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_sliding_window (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "995c15f10de0fd939542301a94367423fca4bba9",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -89,7 +89,7 @@ class SmolLM3Config(PreTrainedConfig):\n         eos_token_id (`int`, *optional*, defaults to 128001):\n             The id of the end of sentence token.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_sliding_window (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "c0a5bb2a481ad6065777447a25f7a599b343238a",
            "filename": "src/transformers/models/stablelm/configuration_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fconfiguration_stablelm.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -72,7 +72,7 @@ class StableLmConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         use_qkv_bias (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "89168ea6b3e68633169a5b9fb0e67e411bda6528",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -71,7 +71,7 @@ class Starcoder2Config(PreTrainedConfig):\n         eos_token_id (`int`, *optional*, defaults to 50256):\n             The id of the \"end-of-sequence\" token.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         sliding_window (`int`, *optional*):"
        },
        {
            "sha": "f7191f26f31459be335e960d744bbfbcf2b09aff",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class T5GemmaModuleConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "f0a592293533fd6d834c38706c79513d935c9590",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -114,7 +114,7 @@ class T5GemmaModuleConfig(Gemma2Config):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "7cb56256687778b1f1ed7dfcda2f20e374f7af55",
            "filename": "src/transformers/models/vaultgemma/configuration_vaultgemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -77,7 +77,7 @@ class VaultGemmaConfig(PreTrainedConfig):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "623d4da21d1e250c97c4b28813f2bda7600da90f",
            "filename": "src/transformers/models/vaultgemma/modular_vaultgemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -75,7 +75,7 @@ class VaultGemmaConfig(Gemma2Config):\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether to tie weight embeddings\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):"
        },
        {
            "sha": "0901ccf576076d6e09534d79b67d624825511147",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -95,7 +95,7 @@ class Zamba2Config(PreTrainedConfig):\n         use_mem_rope (`bool`, *optional*, defaults to `False`):\n             If True, includes RoPE in the shared attention layers.\n         rope_parameters (`RopeParameters`, *optional*):\n-            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n         initializer_range (`float`, *optional*, defaults to 0.02):"
        },
        {
            "sha": "13acdc3a0a7ab24ac3a61dbf623120f068dca352",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -176,7 +176,7 @@ def param_element_size(self, model: \"PreTrainedModel\", param_name: str) -> float\n                 CustomDtype.FP8: 1,\n                 CustomDtype.INT2: 0.25,\n             }\n-            # The value passed is actually not used when the method is overriden\n+            # The value passed is actually not used when the method is overridden\n             if (custom_dtype := self.adjust_target_dtype(torch.float16)) in mapping:\n                 return mapping[custom_dtype]\n         return model.get_parameter_or_buffer(param_name).element_size()"
        },
        {
            "sha": "0f7bd7d456ad7ce7d4efda31061d8d6eb81ae6d4",
            "filename": "tests/models/video_llama_3/test_modeling_video_llama_3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -76,7 +76,7 @@ def _test_encoder_eager_matches_sdpa_inference(\n ):\n     \"\"\"\n     This test is written as a regular function to be able to overload it easily with different tolerances.\n-    Otherwise, `paramterezie.expand` prevents it as it removes the original function from the namespace.\n+    Otherwise, `parameterize.expand` prevents it as it removes the original function from the namespace.\n     \"\"\"\n     if not self.has_attentions:\n         self.skipTest(reason=\"Model architecture does not support attentions\")"
        },
        {
            "sha": "66a6dfd1265319332c784fac5c7d97158d1ded2f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd20cdc2e8669b36abfceb8ab59cfc7c957b0469/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=fd20cdc2e8669b36abfceb8ab59cfc7c957b0469",
            "patch": "@@ -159,7 +159,7 @@ def _test_eager_matches_sdpa_inference(\n ):\n     \"\"\"\n     This test is written as a regular function to be able to overload it easily with different tolerances.\n-    Otherwise, `paramterezie.expand` prevents it as it removes the original function from the namespace.\n+    Otherwise, `parameterize.expand` prevents it as it removes the original function from the namespace.\n     \"\"\"\n     if not self.has_attentions:\n         self.skipTest(reason=\"Model architecture does not support attentions\")"
        }
    ],
    "stats": {
        "total": 308,
        "additions": 154,
        "deletions": 154
    }
}