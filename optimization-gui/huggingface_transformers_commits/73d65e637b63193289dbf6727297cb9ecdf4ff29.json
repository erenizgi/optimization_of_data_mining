{
    "author": "zucchini-nlp",
    "message": "T5 compile compatibilty (#34089)\n\n* this worked in normal generation, needs more tests\r\n\r\n* fix almost all tests in t5\r\n\r\n* nit\r\n\r\n* longt5, umt5, mt5\r\n\r\n* style\r\n\r\n* udop, pix2struct\r\n\r\n* more models\r\n\r\n* fix some tests\r\n\r\n* fix onnx tests\r\n\r\n* tracing tests fixed\r\n\r\n* compile enabled and tested for t5 models\r\n\r\n* fix small bug in slow tests\r\n\r\n* [run-slow] t5\r\n\r\n* uncomment\r\n\r\n* style\r\n\r\n* update with new generation refactoring\r\n\r\n* nit\r\n\r\n* fix copies\r\n\r\n* this is the fix, had to change t5 to fix copies\r\n\r\n* update\r\n\r\n* [run-slow] t5\r\n\r\n* [run-slow] t5\r\n\r\n* update\r\n\r\n* add test for encoder only T5\r\n\r\n* clean up after rebase\r\n\r\n* fix pop2piano\r\n\r\n* add comment\r\n\r\n* style\r\n\r\n* fix copies after rebase\r\n\r\n* fix copies  missed this one",
    "sha": "73d65e637b63193289dbf6727297cb9ecdf4ff29",
    "files": [
        {
            "sha": "0f696cc3ac6a4d63ac3cf071ef620a9a083b5a05",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -1475,11 +1475,7 @@ def from_legacy_cache(\n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # check if empty list because in case of static cache it will be a tensors and we can't check `if not torch.Tensor`\n-        if self.self_attention_cache.key_cache == []:\n-            return 0\n-        if len(self.self_attention_cache.key_cache) > 1 and self.self_attention_cache.key_cache[layer_idx] == []:\n-            return 0\n-        return (self.self_attention_cache.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n+        return self.self_attention_cache.get_seq_length(layer_idx)\n \n     def reset(self):\n         if hasattr(self.self_attention_cache, \"reset\"):"
        },
        {
            "sha": "c399a8a2c829c7bb469a6e1049c8db4391f22957",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -1535,8 +1535,12 @@ def _prepare_generation_config(\n     def _get_initial_cache_position(self, input_ids, model_kwargs):\n         \"\"\"Calculates `cache_position` for the pre-fill stage based on `input_ids` and optionally past length\"\"\"\n         # `torch.compile`-friendly `torch.arange` from a shape -- the lines below are equivalent to `torch.arange`\n-        if \"inputs_embeds\" in model_kwargs:\n+        if \"inputs_embeds\" in model_kwargs and not self.config.is_encoder_decoder:\n             cache_position = torch.ones_like(model_kwargs[\"inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n+        elif \"decoder_inputs_embeds\" in model_kwargs and self.config.is_encoder_decoder:\n+            cache_position = (\n+                torch.ones_like(model_kwargs[\"decoder_inputs_embeds\"][0, :, 0], dtype=torch.int64).cumsum(0) - 1\n+            )\n         else:\n             cache_position = torch.ones_like(input_ids[0, :], dtype=torch.int64).cumsum(0) - 1\n \n@@ -1633,7 +1637,7 @@ def get_layer_device_map(execution_device_map: Optional[dict] = None):\n \n             cache_kwargs = {\n                 \"config\": self.config.get_text_config(),\n-                \"max_batch_size\": batch_size,\n+                \"batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n                 \"device\": device,\n                 \"dtype\": cache_dtype,"
        },
        {
            "sha": "b6e7d21b3d677bcb863f411cb7ad6e249a074818",
            "filename": "src/transformers/models/longt5/configuration_longt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -79,7 +79,12 @@ class LongT5Config(PretrainedConfig):\n \n     model_type = \"longt5\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    attribute_map = {\"hidden_size\": \"d_model\", \"num_attention_heads\": \"num_heads\", \"num_hidden_layers\": \"num_layers\"}\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"num_heads\",\n+        \"num_hidden_layers\": \"num_layers\",\n+        \"head_dim\": \"d_kv\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "29536d9ad6f284f274fb4818349c53966791c197",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 299,
            "deletions": 156,
            "changes": 455,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -24,7 +24,9 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -39,6 +41,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -317,7 +320,12 @@ def forward(self, hidden_states):\n \n # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->LongT5\n class LongT5Attention(nn.Module):\n-    def __init__(self, config: LongT5Config, has_relative_attention_bias=False):\n+    def __init__(\n+        self,\n+        config: LongT5Config,\n+        has_relative_attention_bias=False,\n+        layer_idx: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -328,6 +336,13 @@ def __init__(self, config: LongT5Config, has_relative_attention_bias=False):\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -404,11 +419,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -432,94 +450,72 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n-\n-        def shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        def unshape(states):\n-            \"\"\"reshape\"\"\"\n-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n-\n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(key_value_states))\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n             if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n-\n-        # get query states\n-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n-\n-        # get key/value states\n-        key_states = project(\n-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n-\n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -529,22 +525,22 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n-            scores\n-        )  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.dropout(\n-            attn_weights, p=self.dropout, training=self.training\n-        )  # (batch_size, n_heads, seq_length, key_length)\n+\n+        # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -1008,9 +1004,11 @@ def unshape(states):\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->LongT5\n class LongT5LayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.SelfAttention = LongT5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n+        self.SelfAttention = LongT5Attention(\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+        )\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -1023,6 +1021,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n@@ -1033,6 +1032,7 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n@@ -1042,7 +1042,7 @@ def forward(\n class LongT5LayerLocalSelfAttention(nn.Module):\n     \"\"\"Local self attention used in encoder\"\"\"\n \n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.LocalSelfAttention = LongT5LocalAttention(config, has_relative_attention_bias=has_relative_attention_bias)\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n@@ -1073,7 +1073,7 @@ def forward(\n class LongT5LayerTransientGlobalSelfAttention(nn.Module):\n     \"\"\"Transient-Global self attention used in encoder\"\"\"\n \n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.TransientGlobalSelfAttention = LongT5TransientGlobalAttention(\n             config, has_relative_attention_bias=has_relative_attention_bias\n@@ -1105,9 +1105,9 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->LongT5\n class LongT5LayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = LongT5Attention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = LongT5Attention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -1122,6 +1122,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -1134,14 +1135,15 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class LongT5Block(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         if config.is_decoder:\n@@ -1156,9 +1158,11 @@ def __init__(self, config, has_relative_attention_bias=False):\n                 f\"but got {config.encoder_attention_type}.\"\n             )\n         self.layer = nn.ModuleList()\n-        self.layer.append(attention_layer(config, has_relative_attention_bias=has_relative_attention_bias))\n+        self.layer.append(\n+            attention_layer(config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx)\n+        )\n         if self.is_decoder:\n-            self.layer.append(LongT5LayerCrossAttention(config))\n+            self.layer.append(LongT5LayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(LongT5LayerFF(config))\n \n@@ -1176,34 +1180,19 @@ def forward(\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            if not self.is_decoder:\n-                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/\n@@ -1213,35 +1202,25 @@ def forward(\n \n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.layer[1](\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/\n             if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                 clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -1256,7 +1235,7 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs\n+            outputs = outputs + (past_key_value,) + attention_outputs\n         else:\n             outputs = outputs + attention_outputs\n \n@@ -1273,6 +1252,8 @@ class LongT5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LongT5Block\"]\n+    _supports_cache_class = True\n+    _supports_static_cache = False  # TODO: @raushan more involved due to local/global attn\n \n     @property\n     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel.dummy_inputs\n@@ -1376,7 +1357,10 @@ def __init__(self, config, embed_tokens=None):\n         self.block_len = self.local_radius + 1\n \n         self.block = nn.ModuleList(\n-            [LongT5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n+            [\n+                LongT5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i)\n+                for i in range(config.num_layers)\n+            ]\n         )\n         self.final_layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n@@ -1408,6 +1392,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1430,36 +1415,65 @@ def forward(\n             err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n             raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         if inputs_embeds is None:\n             assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n-        if use_cache is True:\n-            assert self.is_decoder, f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n \n-        if attention_mask is None:\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past\n+            mask_seq_length = past_key_values_length + seq_length\n             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        # We use local attention in encoder self-attention, otherwise standard self & cross attentions are used\n         if self.is_decoder:\n-            extended_attention_mask = self.get_extended_attention_mask(\n-                attention_mask, input_shape, inputs_embeds.device\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n             )\n+        # We use local attention in encoder self-attention, otherwise standard self & cross attentions are used\n         elif self.config.encoder_attention_type == \"local\":\n-            extended_attention_mask = _get_local_attention_mask(attention_mask, self.block_len, inputs_embeds.device)\n+            causal_mask = _get_local_attention_mask(attention_mask, self.block_len, inputs_embeds.device)\n         else:  # we need to use both local attention mask and standard extended mask for transient-global attention\n-            extended_attention_mask = attention_mask\n+            causal_mask = attention_mask\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -1472,17 +1486,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n@@ -1491,7 +1497,7 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n \n@@ -1502,7 +1508,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     position_bias,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n@@ -1512,38 +1518,39 @@ def forward(\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    return_dict,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_bias=position_bias,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     encoder_decoder_position_bias=encoder_decoder_position_bias,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    return_dict=return_dict,\n+                    cache_position=cache_position,\n                 )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n             if use_cache is False:\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n \n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n             position_bias = layer_outputs[2]\n             if self.is_decoder and encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[3],)\n@@ -1557,12 +1564,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1571,12 +1584,135 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n LONGT5_START_DOCSTRING = r\"\"\"\n \n@@ -1693,6 +1829,9 @@ def forward(\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n LONGT5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n@@ -1817,6 +1956,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1883,6 +2023,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1975,6 +2116,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -2050,6 +2192,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "267179f81247e8fed943bb5e3da0608ed998b17e",
            "filename": "src/transformers/models/mt5/configuration_mt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -72,7 +72,12 @@ class MT5Config(PretrainedConfig):\n \n     model_type = \"mt5\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    attribute_map = {\"hidden_size\": \"d_model\", \"num_attention_heads\": \"num_heads\", \"num_hidden_layers\": \"num_layers\"}\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"num_heads\",\n+        \"num_hidden_layers\": \"num_layers\",\n+        \"head_dim\": \"d_kv\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "659a84c5fe3784b15900bf379ce6be739d2afa31",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 299,
            "deletions": 150,
            "changes": 449,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -25,7 +25,9 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -43,6 +45,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -214,7 +217,12 @@ def forward(self, hidden_states):\n \n # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->MT5\n class MT5Attention(nn.Module):\n-    def __init__(self, config: MT5Config, has_relative_attention_bias=False):\n+    def __init__(\n+        self,\n+        config: MT5Config,\n+        has_relative_attention_bias=False,\n+        layer_idx: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -225,6 +233,13 @@ def __init__(self, config: MT5Config, has_relative_attention_bias=False):\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -301,11 +316,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -329,94 +347,72 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n-\n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n-\n-        def shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        def unshape(states):\n-            \"\"\"reshape\"\"\"\n-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(key_value_states))\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n             if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n-\n-        # get query states\n-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n-\n-        # get key/value states\n-        key_states = project(\n-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n-\n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -426,22 +422,22 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n-            scores\n-        )  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.dropout(\n-            attn_weights, p=self.dropout, training=self.training\n-        )  # (batch_size, n_heads, seq_length, key_length)\n+\n+        # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -450,9 +446,11 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->MT5\n class MT5LayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.SelfAttention = MT5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n+        self.SelfAttention = MT5Attention(\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+        )\n         self.layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -465,6 +463,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n@@ -475,6 +474,7 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n@@ -483,9 +483,9 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->MT5\n class MT5LayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = MT5Attention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = MT5Attention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -500,6 +500,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -512,6 +513,7 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n@@ -520,13 +522,15 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5Block with T5->MT5\n class MT5Block(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.layer = nn.ModuleList()\n-        self.layer.append(MT5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n+        self.layer.append(\n+            MT5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx)\n+        )\n         if self.is_decoder:\n-            self.layer.append(MT5LayerCrossAttention(config))\n+            self.layer.append(MT5LayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(MT5LayerFF(config))\n \n@@ -544,34 +548,19 @@ def forward(\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            if not self.is_decoder:\n-                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (key / value) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n@@ -585,25 +574,18 @@ def forward(\n \n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.layer[1](\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -614,10 +596,6 @@ def forward(\n                 )\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -636,11 +614,11 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs\n+            outputs = outputs + (past_key_value,) + attention_outputs\n         else:\n             outputs = outputs + attention_outputs\n \n-        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n def load_tf_weights_in_mt5(model, config, tf_checkpoint_path):\n@@ -780,6 +758,9 @@ class MT5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n+    _supports_quantized_cache = False  # enc-dec models don't support yet\n+    _supports_static_cache = True\n+    _supports_cache_class = True\n     _no_split_modules = [\"MT5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]\n \n@@ -892,7 +873,7 @@ def __init__(self, config, embed_tokens=None):\n         self.is_decoder = config.is_decoder\n \n         self.block = nn.ModuleList(\n-            [MT5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n+            [MT5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(config.num_layers)]\n         )\n         self.final_layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n@@ -968,6 +949,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         # Model parallel\n         if self.model_parallel:\n@@ -994,30 +976,71 @@ def forward(\n             err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n             raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         if inputs_embeds is None:\n             if self.embed_tokens is None:\n                 raise ValueError(\"You have to initialize the model with valid token embeddings\")\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n         if use_cache is True:\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n \n-        if attention_mask is None:\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        elif attention_mask is not None:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n+        else:\n+            causal_mask = None\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -1032,17 +1055,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n@@ -1051,15 +1066,15 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)\n                 # Ensure that attention_mask is always on the same device as hidden_states\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask.to(hidden_states.device)\n+                if causal_mask is not None:\n+                    causal_mask = causal_mask.to(hidden_states.device)\n                 if position_bias is not None:\n                     position_bias = position_bias.to(hidden_states.device)\n                 if encoder_hidden_states is not None:\n@@ -1079,7 +1094,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     position_bias,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n@@ -1089,38 +1104,39 @@ def forward(\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    return_dict,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_bias=position_bias,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     encoder_decoder_position_bias=encoder_decoder_position_bias,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    return_dict=return_dict,\n+                    cache_position=cache_position,\n                 )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n             if use_cache is False:\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n \n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n             position_bias = layer_outputs[2]\n             if self.is_decoder and encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[3],)\n@@ -1140,12 +1156,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1154,12 +1176,135 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n MT5_START_DOCSTRING = r\"\"\"\n \n@@ -1454,6 +1599,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1533,6 +1679,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1685,6 +1832,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1779,6 +1927,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "b1ac81bb1f21b61736e59866de838c72f3efc607",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 275,
            "deletions": 132,
            "changes": 407,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -22,7 +22,9 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -38,6 +40,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -184,14 +187,17 @@ def to_projection_shape(states):\n             if self.gradient_checkpointing and self.training:\n                 position_bias.requires_grad = True\n \n-            if attention_mask is None:\n-                attention_mask = torch.ones((batch_size, seq_length), device=scores.device, dtype=scores.dtype)\n-\n             if attention_mask.dim() == 2:\n                 position_bias = position_bias + attention_mask[:, None, None, :].to(position_bias.device)\n-            else:\n+            elif attention_mask is not None:\n                 # (batch_size, n_heads, seq_length, key_length)\n                 position_bias = position_bias + attention_mask.to(position_bias.device)\n+            elif not is_torchdynamo_compiling():\n+                attention_mask = torch.ones(\n+                    (batch_size, seq_length), device=position_bias.device, dtype=position_bias.dtype\n+                )\n+                position_bias = position_bias + attention_mask.to(position_bias.device)\n+\n             position_bias = 1 - position_bias\n \n         position_bias_masked = position_bias.masked_fill(position_bias == 1, torch.finfo(scores.dtype).min)\n@@ -355,6 +361,8 @@ class Pix2StructPreTrainedModel(PreTrainedModel):\n     \"\"\"\n \n     config_class = Pix2StructConfig\n+    _supports_cache_class = True\n+    _supports_static_cache = False\n \n     @property\n     def dummy_inputs(self):\n@@ -673,7 +681,9 @@ def forward(self, hidden_states):\n \n \n class Pix2StructTextAttention(nn.Module):\n-    def __init__(self, config: Pix2StructTextConfig, has_relative_attention_bias=False):\n+    def __init__(\n+        self, config: Pix2StructTextConfig, has_relative_attention_bias=False, layer_idx: Optional[int] = None\n+    ):\n         super().__init__()\n         self.has_relative_attention_bias = has_relative_attention_bias\n         self.relative_attention_num_buckets = config.relative_attention_num_buckets\n@@ -683,6 +693,13 @@ def __init__(self, config: Pix2StructTextConfig, has_relative_attention_bias=Fal\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.query = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n@@ -773,75 +790,56 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal) or (batch_size, 1, query_length, key_length)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n-\n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n-\n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n-\n-        def to_projection_shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.contiguous().view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = to_projection_shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = to_projection_shape(proj_layer(key_value_states))\n-\n-            if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = to_projection_shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n+        query_states = self.query(hidden_states).contiguous()\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        # get query states\n-        # (batch_size, n_heads, seq_length, dim_per_head)\n-        query_states = to_projection_shape(self.query(hidden_states))\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                past_key_value = past_key_value.self_attention_cache\n \n         # get key/value states\n-        key_states = project(\n-            hidden_states, self.key, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.value, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value.key_cache[self.layer_idx]\n+            value_states = past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.key(current_states).contiguous()\n+            value_states = self.value(current_states).contiguous()\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            if past_key_value is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n         # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            real_seq_length = cache_position[-1] + 1 if query_length is None else query_length\n+            key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n                     (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n@@ -851,11 +849,6 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             else:\n                 position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n \n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n-\n             if mask is not None:\n                 position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n \n@@ -883,19 +876,20 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n \n         attn_output = self.output(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if use_cache else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output,) + (past_key_value,) + (position_bias,)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n         return outputs\n \n \n-# Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,self.SelfAttention->self.attention,config.d_model->config.hidden_size\n+# Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,T5LayerSelfAttention->Pix2StructTextLayerSelfAttention,self.SelfAttention->self.attention,config.d_model->config.hidden_size\n class Pix2StructTextLayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.attention = Pix2StructTextAttention(config, has_relative_attention_bias=has_relative_attention_bias)\n+        self.attention = Pix2StructTextAttention(\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+        )\n         self.layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -908,6 +902,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.attention(\n@@ -918,17 +913,18 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n-# Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,self.EncDecAttention->self.attention,config.d_model->config.hidden_size\n+# Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5LayerNorm->Pix2StructLayerNorm,T5Attention->Pix2StructTextAttention,T5LayerCrossAttention->Pix2StructTextLayerCrossAttention,self.EncDecAttention->self.attention,config.d_model->config.hidden_size\n class Pix2StructTextLayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.attention = Pix2StructTextAttention(config, has_relative_attention_bias=False)\n+        self.attention = Pix2StructTextAttention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -943,6 +939,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.attention(\n@@ -955,18 +952,21 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class Pix2StructTextBlock(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n \n         self.self_attention = Pix2StructTextLayerSelfAttention(\n-            config, has_relative_attention_bias=has_relative_attention_bias\n+            config,\n+            has_relative_attention_bias=has_relative_attention_bias,\n+            layer_idx=layer_idx,\n         )\n \n         self.encoder_decoder_attention = Pix2StructTextLayerCrossAttention(config)\n@@ -987,32 +987,19 @@ def forward(\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.self_attention(\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n@@ -1022,35 +1009,25 @@ def forward(\n \n         do_cross_attention = encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.encoder_decoder_attention(\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                 clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -1065,7 +1042,7 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs\n+            outputs = outputs + (past_key_value,) + attention_outputs\n         else:\n             outputs = outputs + attention_outputs\n \n@@ -1187,6 +1164,9 @@ def forward(\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n PIX2STRUCT_INPUTS_DOCSTRING = r\"\"\"\n@@ -1293,7 +1273,10 @@ def __init__(self, config):\n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n \n         self.layer = nn.ModuleList(\n-            [Pix2StructTextBlock(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n+            [\n+                Pix2StructTextBlock(config, has_relative_attention_bias=bool(i == 0), layer_idx=i)\n+                for i in range(config.num_layers)\n+            ]\n         )\n         self.final_layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n@@ -1364,6 +1347,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Union[Tuple[torch.FloatTensor, ...], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n@@ -1405,24 +1389,54 @@ def forward(\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if use_cache or past_key_values is not None:\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        past_key_values_length = 0\n+        if cache_position is not None:\n+            past_key_values_length = cache_position[0]\n+        elif past_key_values is not None:\n+            past_key_values_length = past_key_values.get_seq_length()\n+\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n \n         if attention_mask is None:\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-        if encoder_attention_mask is None and encoder_hidden_states is not None:\n-            encoder_seq_length = encoder_hidden_states.shape[1]\n-            encoder_attention_mask = torch.ones(\n-                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n+            # required mask seq length can be calculated via length of past\n+            mask_seq_length = (\n+                past_key_values.get_seq_length() + seq_length if past_key_values is not None else seq_length\n             )\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.layer)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        else:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -1438,7 +1452,6 @@ def forward(\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions) else None\n@@ -1447,7 +1460,7 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.layer, past_key_values)):\n+        for i, layer_module in enumerate(self.layer):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n             if output_hidden_states:\n@@ -1462,7 +1475,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     position_bias,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n@@ -1472,38 +1485,37 @@ def forward(\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_bias=position_bias,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     encoder_decoder_position_bias=encoder_decoder_position_bias,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    cache_position=cache_position,\n                 )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n             if use_cache is False:\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n \n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n             position_bias = layer_outputs[2]\n             if encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[3],)\n@@ -1527,13 +1539,19 @@ def forward(\n \n             loss = loss_fct(logits.contiguous().view(-1, logits.size(-1)), labels.contiguous().view(-1))\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     loss,\n                     logits,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1543,12 +1561,135 @@ def forward(\n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n             logits=logits,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n @add_start_docstrings(\n     \"A conditional generation model with a language modeling head. Can be used for sequence generation tasks.\",\n@@ -1615,6 +1756,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1723,6 +1865,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             labels=labels,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:"
        },
        {
            "sha": "6a64a27e007b3ec078b999d25a9fb34fb850bb74",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 298,
            "deletions": 153,
            "changes": 451,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -25,7 +25,9 @@\n from transformers.generation import GenerationConfig\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -37,6 +39,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -136,6 +139,9 @@\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n \n@@ -245,7 +251,12 @@ def forward(self, hidden_states):\n \n # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->Pop2Piano,t5->pop2piano\n class Pop2PianoAttention(nn.Module):\n-    def __init__(self, config: Pop2PianoConfig, has_relative_attention_bias=False):\n+    def __init__(\n+        self,\n+        config: Pop2PianoConfig,\n+        has_relative_attention_bias=False,\n+        layer_idx: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -256,6 +267,13 @@ def __init__(self, config: Pop2PianoConfig, has_relative_attention_bias=False):\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -332,11 +350,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -360,94 +381,72 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n-\n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n-\n-        def shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        def unshape(states):\n-            \"\"\"reshape\"\"\"\n-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(key_value_states))\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n             if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n-\n-        # get query states\n-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n-\n-        # get key/value states\n-        key_states = project(\n-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n-\n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -457,22 +456,22 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n-            scores\n-        )  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.dropout(\n-            attn_weights, p=self.dropout, training=self.training\n-        )  # (batch_size, n_heads, seq_length, key_length)\n+\n+        # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -481,9 +480,11 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->Pop2Piano,t5->pop2piano\n class Pop2PianoLayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.SelfAttention = Pop2PianoAttention(config, has_relative_attention_bias=has_relative_attention_bias)\n+        self.SelfAttention = Pop2PianoAttention(\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+        )\n         self.layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -496,6 +497,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n@@ -506,6 +508,7 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n@@ -514,9 +517,9 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->Pop2Piano,t5->pop2piano\n class Pop2PianoLayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = Pop2PianoAttention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = Pop2PianoAttention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -531,6 +534,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -543,6 +547,7 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n@@ -551,13 +556,17 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5Block with T5->Pop2Piano,t5->pop2piano\n class Pop2PianoBlock(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.layer = nn.ModuleList()\n-        self.layer.append(Pop2PianoLayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n+        self.layer.append(\n+            Pop2PianoLayerSelfAttention(\n+                config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+            )\n+        )\n         if self.is_decoder:\n-            self.layer.append(Pop2PianoLayerCrossAttention(config))\n+            self.layer.append(Pop2PianoLayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(Pop2PianoLayerFF(config))\n \n@@ -575,34 +584,19 @@ def forward(\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            if not self.is_decoder:\n-                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (key / value) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n@@ -616,25 +610,18 @@ def forward(\n \n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.layer[1](\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -645,10 +632,6 @@ def forward(\n                 )\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -667,11 +650,11 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs\n+            outputs = outputs + (past_key_value,) + attention_outputs\n         else:\n             outputs = outputs + attention_outputs\n \n-        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n class Pop2PianoPreTrainedModel(PreTrainedModel):\n@@ -684,6 +667,8 @@ class Pop2PianoPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = False\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = False\n     _no_split_modules = [\"Pop2PianoBlock\"]\n     _keep_in_fp32_modules = [\"wo\"]\n \n@@ -769,7 +754,10 @@ def __init__(self, config, embed_tokens=None):\n         self.is_decoder = config.is_decoder\n \n         self.block = nn.ModuleList(\n-            [Pop2PianoBlock(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n+            [\n+                Pop2PianoBlock(config, has_relative_attention_bias=bool(i == 0), layer_idx=i)\n+                for i in range(config.num_layers)\n+            ]\n         )\n         self.final_layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n@@ -803,6 +791,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -825,35 +814,69 @@ def forward(\n             err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n             raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         if inputs_embeds is None:\n             if self.embed_tokens is None:\n                 raise ValueError(\"You have to initialize the model with valid token embeddings\")\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n         if use_cache is True:\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-        if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n-            encoder_seq_length = encoder_hidden_states.shape[1]\n-            encoder_attention_mask = torch.ones(\n-                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        else:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -866,17 +889,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n@@ -885,7 +900,7 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n             if output_hidden_states:\n@@ -895,7 +910,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     position_bias,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n@@ -905,38 +920,37 @@ def forward(\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_bias=position_bias,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     encoder_decoder_position_bias=encoder_decoder_position_bias,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    cache_position=cache_position,\n                 )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n             if use_cache is False:\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n \n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n             position_bias = layer_outputs[2]\n             if self.is_decoder and encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[3],)\n@@ -950,12 +964,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -964,12 +984,135 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class Pop2PianoConcatEmbeddingToMel(nn.Module):\n     \"\"\"Embedding Matrix for `composer` tokens.\"\"\"\n@@ -1122,6 +1265,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1177,6 +1321,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "b150b04eea57b846a2d83c0f80cc45172384b2db",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 301,
            "deletions": 153,
            "changes": 454,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -24,7 +24,9 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     MoEModelOutput,\n     MoEModelOutputWithPastAndCrossAttentions,\n@@ -39,6 +41,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -355,7 +358,12 @@ def forward(self, hidden_states, output_router_logits):\n \n # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->SwitchTransformers\n class SwitchTransformersAttention(nn.Module):\n-    def __init__(self, config: SwitchTransformersConfig, has_relative_attention_bias=False):\n+    def __init__(\n+        self,\n+        config: SwitchTransformersConfig,\n+        has_relative_attention_bias=False,\n+        layer_idx: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -366,6 +374,13 @@ def __init__(self, config: SwitchTransformersConfig, has_relative_attention_bias\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -442,11 +457,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -470,94 +488,72 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n-\n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n-\n-        def shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        def unshape(states):\n-            \"\"\"reshape\"\"\"\n-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(key_value_states))\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n             if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n-\n-        # get query states\n-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n-\n-        # get key/value states\n-        key_states = project(\n-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n-\n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -567,22 +563,22 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n-            scores\n-        )  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.dropout(\n-            attn_weights, p=self.dropout, training=self.training\n-        )  # (batch_size, n_heads, seq_length, key_length)\n+\n+        # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -591,10 +587,10 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->SwitchTransformers\n class SwitchTransformersLayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.SelfAttention = SwitchTransformersAttention(\n-            config, has_relative_attention_bias=has_relative_attention_bias\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n         )\n         self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n@@ -608,6 +604,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n@@ -618,6 +615,7 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n@@ -626,9 +624,11 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->SwitchTransformers\n class SwitchTransformersLayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = SwitchTransformersAttention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = SwitchTransformersAttention(\n+            config, has_relative_attention_bias=False, layer_idx=layer_idx\n+        )\n         self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -643,6 +643,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -655,23 +656,26 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class SwitchTransformersBlock(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False, is_sparse=False):\n+    def __init__(self, config, has_relative_attention_bias=False, is_sparse=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.is_sparse = is_sparse\n         self.layer = nn.ModuleList()\n         self.layer.append(\n-            SwitchTransformersLayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias)\n+            SwitchTransformersLayerSelfAttention(\n+                config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+            )\n         )\n         if self.is_decoder:\n-            self.layer.append(SwitchTransformersLayerCrossAttention(config))\n+            self.layer.append(SwitchTransformersLayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(SwitchTransformersLayerFF(config, is_sparse=self.is_sparse))\n \n@@ -690,34 +694,19 @@ def forward(\n         output_attentions=False,\n         output_router_logits=True,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            if not self.is_decoder:\n-                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n@@ -727,35 +716,25 @@ def forward(\n \n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.layer[1](\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                 clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -775,11 +754,11 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs + (router_tuple,)\n+            outputs = outputs + (past_key_value,) + attention_outputs + (router_tuple,)\n         else:\n             outputs = outputs + attention_outputs + (router_tuple,)\n \n-        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights), (router_tuple)\n+        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights), (router_tuple)\n \n \n class SwitchTransformersPreTrainedModel(PreTrainedModel):\n@@ -791,6 +770,8 @@ class SwitchTransformersPreTrainedModel(PreTrainedModel):\n     config_class = SwitchTransformersConfig\n     base_model_prefix = \"switch_transformers\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = False\n     _no_split_modules = [\"SwitchTransformersBlock\"]\n \n     @property\n@@ -897,7 +878,9 @@ def __init__(self, config, embed_tokens=None):\n             is_sparse = (i % sparse_step == 1 or sparse_step == 1) if sparse_step > 0 else False\n \n             self.block.append(\n-                SwitchTransformersBlock(config, has_relative_attention_bias=bool(i == 0), is_sparse=is_sparse)\n+                SwitchTransformersBlock(\n+                    config, has_relative_attention_bias=bool(i == 0), is_sparse=is_sparse, layer_idx=i\n+                )\n             )\n \n         self.final_layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n@@ -930,6 +913,7 @@ def forward(\n         output_hidden_states=None,\n         output_router_logits=True,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -952,35 +936,69 @@ def forward(\n             err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n             raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         if inputs_embeds is None:\n             if self.embed_tokens is None:\n                 raise ValueError(\"You have to initialize the model with valid token embeddings\")\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n         if use_cache is True:\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-        if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n-            encoder_seq_length = encoder_hidden_states.shape[1]\n-            encoder_attention_mask = torch.ones(\n-                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        else:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -993,17 +1011,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_router_probs = () if output_router_logits else None\n@@ -1013,7 +1023,7 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n \n@@ -1024,7 +1034,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     position_bias,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n@@ -1034,21 +1044,26 @@ def forward(\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    output_router_logits,\n+                    return_dict,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_bias=position_bias,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     encoder_decoder_position_bias=encoder_decoder_position_bias,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n                     output_router_logits=output_router_logits,\n+                    return_dict=return_dict,\n+                    cache_position=cache_position,\n                 )\n \n             router_probs = layer_outputs[-1]\n@@ -1059,17 +1074,14 @@ def forward(\n             if use_cache is False:\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n \n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n             position_bias = layer_outputs[2]\n             if self.is_decoder and encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[3],)\n@@ -1086,12 +1098,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1101,13 +1119,136 @@ def forward(\n             )\n         return MoEModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n             router_probs=all_router_probs,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n SWITCH_TRANSFORMERS_START_DOCSTRING = r\"\"\"\n \n@@ -1228,6 +1369,9 @@ def forward(\n             should not be returned during inference.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n SWITCH_TRANSFORMERS_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n@@ -1355,6 +1499,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqMoEModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1435,6 +1580,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1535,6 +1681,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = True,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqMoEOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1618,6 +1765,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "be6fbe9528d10afbef4b55c0f20677d0a2bd1790",
            "filename": "src/transformers/models/t5/configuration_t5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -73,7 +73,12 @@ class T5Config(PretrainedConfig):\n \n     model_type = \"t5\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    attribute_map = {\"hidden_size\": \"d_model\", \"num_attention_heads\": \"num_heads\", \"num_hidden_layers\": \"num_layers\"}\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"num_heads\",\n+        \"num_hidden_layers\": \"num_layers\",\n+        \"head_dim\": \"d_kv\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "9012c8db9feb0a5286c68de9df81ea18814bb4cd",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 302,
            "deletions": 150,
            "changes": 452,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -25,7 +25,9 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -43,6 +45,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -339,7 +342,12 @@ def forward(self, hidden_states):\n \n \n class T5Attention(nn.Module):\n-    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n+    def __init__(\n+        self,\n+        config: T5Config,\n+        has_relative_attention_bias=False,\n+        layer_idx: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -350,6 +358,13 @@ def __init__(self, config: T5Config, has_relative_attention_bias=False):\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -426,11 +441,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -454,94 +472,72 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n-\n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n-\n-        def shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        def unshape(states):\n-            \"\"\"reshape\"\"\"\n-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(key_value_states))\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n             if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n-\n-        # get query states\n-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n-\n-        # get key/value states\n-        key_states = project(\n-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n-\n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -551,32 +547,34 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n-            scores\n-        )  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.dropout(\n-            attn_weights, p=self.dropout, training=self.training\n-        )  # (batch_size, n_heads, seq_length, key_length)\n+\n+        # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n         return outputs\n \n \n class T5LayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)\n+        self.SelfAttention = T5Attention(\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+        )\n         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -589,6 +587,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n@@ -599,16 +598,17 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class T5LayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -623,6 +623,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -635,20 +636,23 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class T5Block(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.layer = nn.ModuleList()\n-        self.layer.append(T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n+        self.layer.append(\n+            T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx)\n+        )\n         if self.is_decoder:\n-            self.layer.append(T5LayerCrossAttention(config))\n+            self.layer.append(T5LayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(T5LayerFF(config))\n \n@@ -666,34 +670,19 @@ def forward(\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            if not self.is_decoder:\n-                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (key / value) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n@@ -707,25 +696,18 @@ def forward(\n \n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.layer[1](\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -736,10 +718,6 @@ def forward(\n                 )\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -758,11 +736,11 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs\n+            outputs = outputs + (past_key_value,) + attention_outputs\n         else:\n             outputs = outputs + attention_outputs\n \n-        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n class T5ClassificationHead(nn.Module):\n@@ -794,6 +772,9 @@ class T5PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n+    _supports_quantized_cache = False  # enc-dec models don't support yet\n+    _supports_static_cache = True\n+    _supports_cache_class = True\n     _no_split_modules = [\"T5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]\n \n@@ -905,7 +886,7 @@ def __init__(self, config, embed_tokens=None):\n         self.is_decoder = config.is_decoder\n \n         self.block = nn.ModuleList(\n-            [T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)]\n+            [T5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(config.num_layers)]\n         )\n         self.final_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n@@ -981,6 +962,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         # Model parallel\n         if self.model_parallel:\n@@ -1007,30 +989,71 @@ def forward(\n             err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n             raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         if inputs_embeds is None:\n             if self.embed_tokens is None:\n                 raise ValueError(\"You have to initialize the model with valid token embeddings\")\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n         if use_cache is True:\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n+            )\n \n-        if attention_mask is None:\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        elif attention_mask is not None:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n+        else:\n+            causal_mask = None\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -1045,17 +1068,9 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n@@ -1064,15 +1079,15 @@ def forward(\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)\n                 # Ensure that attention_mask is always on the same device as hidden_states\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask.to(hidden_states.device)\n+                if causal_mask is not None:\n+                    causal_mask = causal_mask.to(hidden_states.device)\n                 if position_bias is not None:\n                     position_bias = position_bias.to(hidden_states.device)\n                 if encoder_hidden_states is not None:\n@@ -1092,7 +1107,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     position_bias,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n@@ -1102,38 +1117,39 @@ def forward(\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    return_dict,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_bias=position_bias,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     encoder_decoder_position_bias=encoder_decoder_position_bias,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    return_dict=return_dict,\n+                    cache_position=cache_position,\n                 )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n             if use_cache is False:\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n \n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n             position_bias = layer_outputs[2]\n             if self.is_decoder and encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[3],)\n@@ -1153,12 +1169,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1167,12 +1189,135 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n T5_START_DOCSTRING = r\"\"\"\n \n@@ -1286,6 +1431,9 @@ def forward(\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n T5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n@@ -1446,6 +1594,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1525,6 +1674,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1656,6 +1806,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1750,6 +1901,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "1928ac8a5c20c925de3879f8989feb68172c6ff6",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 289,
            "deletions": 145,
            "changes": 434,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -34,13 +34,16 @@\n )\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     replace_return_docstrings,\n )\n \n@@ -154,6 +157,9 @@\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n \n@@ -411,6 +417,8 @@ class UdopPreTrainedModel(PreTrainedModel):\n     config_class = UdopConfig\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = False\n     _keep_in_fp32_modules = [\"wo\"]\n \n     def _init_weights(self, module):\n@@ -598,7 +606,12 @@ def forward(self, hidden_states):\n \n # Copied from transformers.models.t5.modeling_t5.T5Attention with T5->Udop\n class UdopAttention(nn.Module):\n-    def __init__(self, config: UdopConfig, has_relative_attention_bias=False):\n+    def __init__(\n+        self,\n+        config: UdopConfig,\n+        has_relative_attention_bias=False,\n+        layer_idx: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -609,6 +622,13 @@ def __init__(self, config: UdopConfig, has_relative_attention_bias=False):\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -685,11 +705,14 @@ def _relative_position_bucket(relative_position, bidirectional=True, num_buckets\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None].to(device)\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(\n@@ -713,94 +736,72 @@ def forward(\n         query_length=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         \"\"\"\n         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n-        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n+        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        real_seq_length = seq_length\n-\n-        if past_key_value is not None:\n-            if len(past_key_value) != 2:\n-                raise ValueError(\n-                    f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n-                )\n-            real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length\n-\n-        key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]\n+        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = key_value_states is not None\n \n-        def shape(states):\n-            \"\"\"projection\"\"\"\n-            return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        def unshape(states):\n-            \"\"\"reshape\"\"\"\n-            return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n \n-        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n-            \"\"\"projects hidden states correctly to key/query states\"\"\"\n-            if key_value_states is None:\n-                # self-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(hidden_states))\n-            elif past_key_value is None:\n-                # cross-attn\n-                # (batch_size, n_heads, seq_length, dim_per_head)\n-                hidden_states = shape(proj_layer(key_value_states))\n+        current_states = key_value_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+        else:\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n             if past_key_value is not None:\n-                if key_value_states is None:\n-                    # self-attn\n-                    # (batch_size, n_heads, key_length, dim_per_head)\n-                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n-                elif past_key_value.shape[2] != key_value_states.shape[1]:\n-                    # checking that the `sequence_length` of the `past_key_value` is the same as\n-                    # the provided `key_value_states` to support prefix tuning\n-                    # cross-attn\n-                    # (batch_size, n_heads, seq_length, dim_per_head)\n-                    hidden_states = shape(proj_layer(key_value_states))\n-                else:\n-                    # cross-attn\n-                    hidden_states = past_key_value\n-            return hidden_states\n-\n-        # get query states\n-        query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\n-\n-        # get key/value states\n-        key_states = project(\n-            hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None\n-        )\n-        value_states = project(\n-            hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None\n-        )\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n \n-        # compute scores\n-        scores = torch.matmul(\n-            query_states, key_states.transpose(3, 2)\n-        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n \n         if position_bias is None:\n+            key_length = key_states.shape[-2]\n+            # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+            real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n             if not self.has_relative_attention_bias:\n                 position_bias = torch.zeros(\n-                    (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype\n+                    (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n                 )\n                 if self.gradient_checkpointing and self.training:\n                     position_bias.requires_grad = True\n             else:\n-                position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)\n-\n-            # if key and values are already calculated\n-            # we want only the last query position bias\n-            if past_key_value is not None:\n-                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+                position_bias = self.compute_bias(\n+                    real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+                )\n+                position_bias = position_bias[:, :, -seq_length:, :]\n \n             if mask is not None:\n-                position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)\n+                causal_mask = mask[:, :, :, : key_states.shape[-2]]\n+                position_bias = position_bias + causal_mask\n \n         if self.pruned_heads:\n             mask = torch.ones(position_bias.shape[1])\n@@ -810,22 +811,22 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n             position_bias_masked = position_bias\n \n         scores += position_bias_masked\n-        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n-            scores\n-        )  # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.dropout(\n-            attn_weights, p=self.dropout, training=self.training\n-        )  # (batch_size, n_heads, seq_length, key_length)\n+\n+        # (batch_size, n_heads, seq_length, key_length)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, past_key_value, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -834,9 +835,11 @@ def project(hidden_states, proj_layer, key_value_states, past_key_value):\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerSelfAttention with T5->Udop\n class UdopLayerSelfAttention(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.SelfAttention = UdopAttention(config, has_relative_attention_bias=has_relative_attention_bias)\n+        self.SelfAttention = UdopAttention(\n+            config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+        )\n         self.layer_norm = UdopLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -849,6 +852,7 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n@@ -859,6 +863,7 @@ def forward(\n             past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n@@ -867,9 +872,9 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5LayerCrossAttention with T5->Udop\n class UdopLayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = UdopAttention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = UdopAttention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = UdopLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -884,6 +889,7 @@ def forward(\n         use_cache=False,\n         query_length=None,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -896,6 +902,7 @@ def forward(\n             use_cache=use_cache,\n             query_length=query_length,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n@@ -904,13 +911,17 @@ def forward(\n \n # Copied from transformers.models.t5.modeling_t5.T5Block with T5->Udop\n class UdopBlock(nn.Module):\n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.layer = nn.ModuleList()\n-        self.layer.append(UdopLayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias))\n+        self.layer.append(\n+            UdopLayerSelfAttention(\n+                config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx\n+            )\n+        )\n         if self.is_decoder:\n-            self.layer.append(UdopLayerCrossAttention(config))\n+            self.layer.append(UdopLayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(UdopLayerFF(config))\n \n@@ -928,34 +939,19 @@ def forward(\n         use_cache=False,\n         output_attentions=False,\n         return_dict=True,\n+        cache_position=None,\n     ):\n-        if past_key_value is not None:\n-            if not self.is_decoder:\n-                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n-            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n-\n-            if len(past_key_value) != expected_num_past_key_values:\n-                raise ValueError(\n-                    f\"There should be {expected_num_past_key_values} past states. \"\n-                    f\"{'2 (key / value) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n-                    f\"Got {len(past_key_value)} past key / value states\"\n-                )\n-\n-            self_attn_past_key_value = past_key_value[:2]\n-            cross_attn_past_key_value = past_key_value[2:]\n-        else:\n-            self_attn_past_key_value, cross_attn_past_key_value = None, None\n-\n         self_attention_outputs = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             position_bias=position_bias,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n-        hidden_states, present_key_value_state = self_attention_outputs[:2]\n+        hidden_states, past_key_value = self_attention_outputs[:2]\n         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n@@ -969,25 +965,18 @@ def forward(\n \n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # the actual query length is unknown for cross attention\n-            # if using past key value states. Need to inject it here\n-            if present_key_value_state is not None:\n-                query_length = present_key_value_state[0].shape[2]\n-            else:\n-                query_length = None\n-\n             cross_attention_outputs = self.layer[1](\n                 hidden_states,\n                 key_value_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n-                query_length=query_length,\n+                past_key_value=past_key_value,\n+                query_length=cache_position[-1] + 1,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states = cross_attention_outputs[0]\n+            hidden_states, past_key_value = cross_attention_outputs[:2]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -998,10 +987,6 @@ def forward(\n                 )\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            # Combine self attn and cross attn key value states\n-            if present_key_value_state is not None:\n-                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n-\n             # Keep cross-attention outputs and relative position weights\n             attention_outputs = attention_outputs + cross_attention_outputs[2:]\n \n@@ -1020,11 +1005,11 @@ def forward(\n         outputs = (hidden_states,)\n \n         if use_cache:\n-            outputs = outputs + (present_key_value_state,) + attention_outputs\n+            outputs = outputs + (past_key_value,) + attention_outputs\n         else:\n             outputs = outputs + attention_outputs\n \n-        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n class UdopCellEmbeddings(nn.Module):\n@@ -1286,7 +1271,7 @@ def __init__(self, config, embed_tokens=None, embed_patches=None):\n         self.num_layers = config.num_layers\n \n         self.block = nn.ModuleList(\n-            [UdopBlock(config, has_relative_attention_bias=bool(i == 0)) for i in range(self.num_layers)]\n+            [UdopBlock(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(self.num_layers)]\n         )\n         self.final_layer_norm = UdopLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n \n@@ -1338,6 +1323,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1399,26 +1385,54 @@ def forward(\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n         if use_cache is True:\n             assert self.is_decoder, \"`use_cache` can only be set to `True` if {} is used as a decoder\".format(self)\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(batch_size, mask_seq_length).to(inputs_embeds.device)\n-        if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n-            encoder_seq_length = encoder_hidden_states.shape[1]\n-            encoder_attention_mask = torch.ones(\n-                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        else:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n \n         if self.is_decoder and encoder_attention_mask is not None:\n             encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n@@ -1427,7 +1441,6 @@ def forward(\n \n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n@@ -1436,34 +1449,35 @@ def forward(\n             position_bias = None\n         else:\n             position_bias = self.relative_bias(attention_mask=attention_mask, bbox=bbox)\n-            position_bias = position_bias + extended_attention_mask\n+            position_bias = position_bias + causal_mask\n         encoder_decoder_position_bias = None\n \n         hidden_states = inputs_embeds\n \n         hidden_states = self.dropout(hidden_states)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             layer_outputs = layer_module(\n                 hidden_states,\n-                attention_mask=extended_attention_mask,\n+                attention_mask=causal_mask,\n                 position_bias=position_bias,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_extended_attention_mask,\n                 encoder_decoder_position_bias=encoder_decoder_position_bias,\n                 layer_head_mask=head_mask[i],\n-                past_key_value=past_key_value,\n+                past_key_value=past_key_values,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n+                cache_position=cache_position,\n             )\n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n             if use_cache is False:  # MP fixes\n                 layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-            hidden_states, present_key_value_state = layer_outputs[:2]\n+            hidden_states, next_decoder_cache = layer_outputs[:2]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention weights),\n@@ -1472,9 +1486,6 @@ def forward(\n             position_bias = layer_outputs[2]\n             if self.is_decoder and encoder_hidden_states is not None:\n                 encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n-            # append next layer key value states\n-            if use_cache:\n-                present_key_value_states = present_key_value_states + (present_key_value_state,)\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[2],)  # We keep only self-attention weights for now\n@@ -1488,13 +1499,19 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n                     attention_mask,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1505,12 +1522,135 @@ def forward(\n         return BaseModelOutputWithAttentionMask(\n             last_hidden_state=hidden_states,\n             attention_mask=attention_mask,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n @add_start_docstrings(\n     \"The bare UDOP encoder-decoder Transformer outputting raw hidden-states without any specific head on top.\",\n@@ -1584,6 +1724,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[Tensor, ...]:\n         r\"\"\"\n         Returns:\n@@ -1653,6 +1794,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1759,6 +1901,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[Tensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[Tensor, ...]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1837,6 +1980,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "ba8ea0460ba0717643a53ccc3edf910c98b653de",
            "filename": "src/transformers/models/umt5/configuration_umt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -72,7 +72,12 @@ class UMT5Config(PretrainedConfig):\n \n     model_type = \"umt5\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    attribute_map = {\"hidden_size\": \"d_model\", \"num_attention_heads\": \"num_heads\", \"num_hidden_layers\": \"num_layers\"}\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"num_heads\",\n+        \"num_hidden_layers\": \"num_layers\",\n+        \"head_dim\": \"d_kv\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "985dc5e4426dff0decbad551a8a1eab8798621b1",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 299,
            "deletions": 102,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -23,7 +23,9 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -40,6 +42,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_torch_fx_proxy,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -155,7 +158,7 @@ class UMT5Attention(nn.Module):\n     T5's attention using relative_attention_bias.\n     \"\"\"\n \n-    def __init__(self, config, has_relative_attention_bias=False):\n+    def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.has_relative_attention_bias = has_relative_attention_bias\n@@ -166,6 +169,13 @@ def __init__(self, config, has_relative_attention_bias=False):\n         self.n_heads = config.num_heads\n         self.dropout = config.dropout_rate\n         self.inner_dim = self.n_heads * self.key_value_proj_dim\n+        self.layer_idx = layer_idx\n+        if layer_idx is None and self.is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         # Mesh TensorFlow initialization to avoid scaling before softmax\n         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n@@ -230,11 +240,14 @@ def _relative_position_bucket(self, relative_position):\n         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)\n         return relative_buckets\n \n-    def compute_bias(self, query_length, key_length, device=None):\n+    def compute_bias(self, query_length, key_length, device=None, cache_position=None):\n         \"\"\"Compute binned relative position bias\"\"\"\n         if device is None:\n             device = self.relative_attention_bias.weight.device\n-        context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        if cache_position is None:\n+            context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]\n+        else:\n+            context_position = cache_position[:, None]\n         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]\n         relative_position = memory_position - context_position  # shape (query_length, key_length)\n         relative_position_bucket = self._relative_position_bucket(relative_position)\n@@ -249,78 +262,95 @@ def forward(\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         layer_head_mask: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ):\n-        is_cross_attention = encoder_hidden_states is not None\n         batch_size, seq_length = hidden_states.shape[:2]\n \n-        # use encoder_hidden_states if cross attention\n-        current_states = encoder_hidden_states if encoder_hidden_states is not None else hidden_states\n-        # checking that the `sequence_length` of the `past_key_value` is the same as the he provided\n-        # `encoder_hidden_states` to support prefix tuning\n-        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+        # if encoder_hidden_states are provided this layer is used as a cross-attention layer for the decoder\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        query_states = self.q(hidden_states)\n+        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            is_updated = past_key_value.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                curr_past_key_value = past_key_value.cross_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value.self_attention_cache\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = past_key_value[0]\n-            value_states = past_key_value[1]\n+            key_states = curr_past_key_value.key_cache[self.layer_idx]\n+            value_states = curr_past_key_value.value_cache[self.layer_idx]\n         else:\n-            key_states = self._shape(self.k(current_states))\n-            value_states = self._shape(self.v(current_states))\n-            if past_key_value is not None and not is_cross_attention:\n-                # reuse k, v, self_attention\n-                key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-                value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-\n-        query_states = self._shape(self.q(hidden_states))\n-        attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n+            key_states = self.k(current_states)\n+            value_states = self.v(current_states)\n+            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n+            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        # compute positional bias\n-        if self.has_relative_attention_bias:\n-            query_length = seq_length\n             if past_key_value is not None:\n-                query_length += past_key_value[0].shape[2]\n-            position_bias = self.compute_bias(query_length, key_states.size(2), device=attention_scores.device)\n-        else:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = curr_past_key_value.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                if is_cross_attention:\n+                    past_key_value.is_updated[self.layer_idx] = True\n+\n+        # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n+        scores = torch.matmul(query_states, key_states.transpose(3, 2))\n+\n+        # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n+        real_seq_length = seq_length + past_key_value.get_seq_length() if past_key_value is not None else seq_length\n+        key_length = key_states.shape[-2]\n+        if not self.has_relative_attention_bias:\n             position_bias = torch.zeros(\n-                (1, self.n_heads, seq_length, key_states.size(2)),\n-                device=attention_scores.device,\n-                dtype=attention_scores.dtype,\n-                requires_grad=self.training,\n+                (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n             )\n-        if past_key_value is not None:\n-            position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n+        else:\n+            position_bias = self.compute_bias(\n+                real_seq_length, key_length, device=scores.device, cache_position=cache_position\n+            )\n+            position_bias = position_bias[:, :, -seq_length:, :]\n+\n         if attention_mask is not None:\n-            position_bias = position_bias + attention_mask  # (batch_size, n_heads, seq_length, key_length)\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            position_bias = position_bias + causal_mask\n+\n+        if self.pruned_heads:\n+            mask = torch.ones(position_bias.shape[1])\n+            mask[list(self.pruned_heads)] = 0\n+            position_bias_masked = position_bias[:, mask.bool()]\n+        else:\n+            position_bias_masked = position_bias\n+\n+        scores += position_bias_masked\n \n-        if self.is_decoder:\n-            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n-            # Further calls to cross_attention layer can then reuse all cross-attention\n-            # key/value_states (first \"if\" case)\n-            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n-            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n-            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n-            # if encoder bi-directional self-attention `past_key_value` is always `None`\n-            past_key_value = (key_states, value_states)\n-\n-        attention_scores += position_bias\n         # (batch_size, n_heads, seq_length, key_length)\n-        attn_weights = nn.functional.softmax(attention_scores.float(), dim=-1).type_as(attention_scores)\n+        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n         attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n \n         # Mask heads if we want to\n         if layer_head_mask is not None:\n             attn_weights = attn_weights * layer_head_mask\n \n-        #  attn_output = torch.bmm(attn_probs, value_states) ?\n-        context_states = torch.matmul(attn_weights, value_states)\n-        # attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim) ?\n-        context_states = context_states.permute(0, 2, 1, 3).contiguous().view(batch_size, seq_length, -1)\n-        attn_output = self.o(context_states)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, seq_length, -1)\n+\n+        attn_output = self.o(attn_output)\n         return attn_output, attn_weights, past_key_value\n \n \n class UMT5LayerSelfAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.SelfAttention = UMT5Attention(config, has_relative_attention_bias=True)\n+        self.SelfAttention = UMT5Attention(config, has_relative_attention_bias=True, layer_idx=layer_idx)\n         self.layer_norm = UMT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -330,23 +360,25 @@ def forward(\n         attention_mask=None,\n         layer_head_mask=None,\n         past_key_value=None,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.SelfAttention(\n             normed_hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         hidden_states = hidden_states + self.dropout(attention_output[0])\n         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class UMT5LayerCrossAttention(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.EncDecAttention = UMT5Attention(config, has_relative_attention_bias=False)\n+        self.EncDecAttention = UMT5Attention(config, has_relative_attention_bias=False, layer_idx=layer_idx)\n         self.layer_norm = UMT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -357,6 +389,7 @@ def forward(\n         attention_mask=None,\n         layer_head_mask=None,\n         past_key_value=None,\n+        cache_position=None,\n     ):\n         normed_hidden_states = self.layer_norm(hidden_states)\n         attention_output = self.EncDecAttention(\n@@ -365,20 +398,21 @@ def forward(\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n         layer_output = hidden_states + self.dropout(attention_output[0])\n         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them\n         return outputs\n \n \n class UMT5Block(nn.Module):\n-    def __init__(self, config):\n+    def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n         self.layer = nn.ModuleList()\n-        self.layer.append(UMT5LayerSelfAttention(config))\n+        self.layer.append(UMT5LayerSelfAttention(config, layer_idx=layer_idx))\n         if self.is_decoder:\n-            self.layer.append(UMT5LayerCrossAttention(config))\n+            self.layer.append(UMT5LayerCrossAttention(config, layer_idx=layer_idx))\n \n         self.layer.append(UMT5LayerFF(config))\n \n@@ -393,16 +427,14 @@ def forward(\n         past_key_value=None,\n         use_cache=False,\n         output_attentions=False,\n+        cache_position=None,\n     ):\n-        # Self Attention\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n-\n-        hidden_states, self_attn_weights, present_key_value = self.layer[0](\n+        hidden_states, self_attn_weights, past_key_value = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n-            past_key_value=self_attn_past_key_value,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n         )\n \n         # clamp inf values to enable fp16 training\n@@ -412,27 +444,23 @@ def forward(\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n         # Cross-Attention Block\n-        cross_attn_present_key_value = None\n         cross_attn_weights = None\n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n-            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n-            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.layer[1](\n+            hidden_states, cross_attn_weights, past_key_value = self.layer[1](\n                 hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n                 layer_head_mask=cross_attn_layer_head_mask,\n-                past_key_value=cross_attn_past_key_value,\n+                past_key_value=past_key_value,\n+                cache_position=cache_position,\n             )\n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n                 max_dtype = torch.finfo(hidden_states.dtype).max\n                 clamp_value = torch.where(torch.isinf(hidden_states).any(), max_dtype - 1000, max_dtype)\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-            present_key_value += cross_attn_present_key_value\n-\n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states)\n \n@@ -444,7 +472,7 @@ def forward(\n \n         outputs = (\n             hidden_states,\n-            present_key_value,\n+            past_key_value,\n         )\n \n         if output_attentions:\n@@ -481,6 +509,8 @@ class UMT5PreTrainedModel(PreTrainedModel):\n     config_class = UMT5Config\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n     _no_split_modules = [\"UMT5Block\"]\n     _keep_in_fp32_modules = [\"wo\"]\n \n@@ -594,7 +624,7 @@ def __init__(self, config, embed_tokens=None):\n         super().__init__(config)\n         self.embed_tokens = embed_tokens\n         self.is_decoder = config.is_decoder\n-        self.block = nn.ModuleList([UMT5Block(config) for i in range(config.num_layers)])\n+        self.block = nn.ModuleList([UMT5Block(config, layer_idx=i) for i in range(config.num_layers)])\n         self.final_layer_norm = UMT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n@@ -622,6 +652,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        cache_position=None,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -644,35 +675,71 @@ def forward(\n             err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n             raise ValueError(f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\")\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n         if inputs_embeds is None:\n             if self.embed_tokens is None:\n                 raise ValueError(\"You have to initialize the model with valid token embeddings\")\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         batch_size, seq_length = input_shape\n \n-        # required mask seq length can be calculated via length of past\n-        mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length\n-\n         if use_cache is True:\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-        if self.is_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:\n-            encoder_seq_length = encoder_hidden_states.shape[1]\n-            encoder_attention_mask = torch.ones(\n-                batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long\n+        # initialize past_key_values\n+        return_legacy_cache = False\n+        return_self_attention_cache = False\n+        if self.is_decoder and (use_cache or past_key_values is not None):\n+            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n+                return_self_attention_cache = True\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif not isinstance(past_key_values, EncoderDecoderCache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n+                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        elif not self.is_decoder:\n+            # do not pass cache object down the line for encoder stack\n+            # it messes indexing later in decoder-stack because cache object is modified in-place\n+            past_key_values = None\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + seq_length, device=inputs_embeds.device\n             )\n \n-        # initialize past_key_values with `None` if past does not exist\n-        if past_key_values is None:\n-            past_key_values = [None] * len(self.block)\n+        if attention_mask is None and not is_torchdynamo_compiling():\n+            # required mask seq length can be calculated via length of past cache\n+            mask_seq_length = past_key_values_length + seq_length\n+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.is_decoder:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask,\n+                inputs_embeds,\n+                cache_position,\n+                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                output_attentions,\n+            )\n+        elif attention_mask is not None:\n+            causal_mask = attention_mask[:, None, None, :]\n+            causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n+            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n+        else:\n+            causal_mask = None\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n@@ -685,24 +752,16 @@ def forward(\n         else:\n             encoder_extended_attention_mask = None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # Prepare head mask if needed\n         head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)\n-        present_key_value_states = () if use_cache else None\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.is_decoder else None\n \n         hidden_states = self.dropout(inputs_embeds)\n \n-        for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n+        for i, layer_module in enumerate(self.block):\n             layer_head_mask = head_mask[i]\n             cross_attn_layer_head_mask = cross_attn_head_mask[i]\n \n@@ -713,32 +772,34 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     layer_module.forward,\n                     hidden_states,\n-                    extended_attention_mask,\n+                    causal_mask,\n                     encoder_hidden_states,\n                     encoder_extended_attention_mask,\n                     layer_head_mask,\n                     cross_attn_layer_head_mask,\n                     None,  # past_key_value is always None with gradient checkpointing\n                     use_cache,\n                     output_attentions,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = layer_module(\n                     hidden_states,\n-                    attention_mask=extended_attention_mask,\n+                    attention_mask=causal_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_extended_attention_mask,\n                     layer_head_mask=layer_head_mask,\n                     cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    cache_position=cache_position,\n                 )\n \n                 hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                present_key_value_states += (layer_outputs[1],)\n+                next_decoder_cache = layer_outputs[1]\n \n             if output_attentions:\n                 all_attentions += (layer_outputs[2],)\n@@ -752,12 +813,18 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_self_attention_cache:\n+            next_cache = past_key_values.self_attention_cache\n+        if return_legacy_cache:\n+            next_cache = past_key_values.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    present_key_value_states,\n+                    next_cache,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -766,12 +833,135 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n+            past_key_values=next_cache,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n UMT5_START_DOCSTRING = r\"\"\"\n \n@@ -885,6 +1075,9 @@ def forward(\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. It is used to update the\n+            cache in the correct position and to infer the complete sequence length.\n \"\"\"\n \n UMT5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n@@ -1022,6 +1215,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         Returns:\n@@ -1084,6 +1278,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1197,6 +1392,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1268,6 +1464,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         sequence_output = decoder_outputs[0]"
        },
        {
            "sha": "a9d3e7479e9578d14e8d63aefcfff0e12808beb9",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -31,6 +31,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n@@ -574,6 +575,41 @@ def test_decoder_model_past_with_3d_attn_mask(self):\n             lm_labels,\n         )\n \n+    # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n@@ -602,7 +638,7 @@ def test_export_to_onnx(self):\n                 (config_and_inputs[1], config_and_inputs[3], config_and_inputs[2]),\n                 f\"{tmpdirname}/longt5_test.onnx\",\n                 export_params=True,\n-                opset_version=13,\n+                opset_version=14,\n                 input_names=[\"input_ids\", \"decoder_input_ids\"],\n             )\n "
        },
        {
            "sha": "20412da2e1db06576d9a4a358e28968bc2286c87",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 39,
            "deletions": 7,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -40,6 +40,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         AutoModelForSeq2SeqLM,\n@@ -575,6 +576,9 @@ class MT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     # The small MT5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n \n+    # used in `test_torch_compile`\n+    _torch_compile_test_ckpt = \"google/mt5-small\"\n+\n     def setUp(self):\n         self.model_tester = MT5ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MT5Config, d_model=37)\n@@ -627,12 +631,9 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                     ]\n                     if labels is not None:\n                         input_names.append(\"labels\")\n-\n                     filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n                     input_names = list(filtered_inputs.keys())\n-\n                     model_output = model(**filtered_inputs)\n-\n                     traced_model = symbolic_trace(model, input_names)\n                     traced_output = traced_model(**filtered_inputs)\n                 else:\n@@ -647,7 +648,6 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                         \"visual_feats\",\n                         \"visual_pos\",\n                     ]\n-\n                     labels = inputs.get(\"labels\", None)\n                     start_positions = inputs.get(\"start_positions\", None)\n                     end_positions = inputs.get(\"end_positions\", None)\n@@ -657,15 +657,12 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                         input_names.append(\"start_positions\")\n                     if end_positions is not None:\n                         input_names.append(\"end_positions\")\n-\n                     filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n                     input_names = list(filtered_inputs.keys())\n-\n                     if model.__class__.__name__ in set(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()) and (\n                         not hasattr(model.config, \"problem_type\") or model.config.problem_type is None\n                     ):\n                         model.config.problem_type = \"single_label_classification\"\n-\n                     traced_model = symbolic_trace(model, input_names)\n                     traced_output = traced_model(**filtered_inputs)\n                     model_output = model(**filtered_inputs)\n@@ -718,6 +715,41 @@ def flatten_output(output):\n             # (Even with this call, there are still memory leak by ~0.04MB)\n             self.clear_torch_jit_class_registry()\n \n+    # overwrite because MT5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "39ff67f08ce5a9d858b5ce394523e4c1d38f4bdd",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -620,7 +620,7 @@ def test_export_to_onnx(self):\n                 (config_and_inputs[1], config_and_inputs[3], config_and_inputs[2]),\n                 f\"{tmpdirname}/Pop2Piano_test.onnx\",\n                 export_params=True,\n-                opset_version=9,\n+                opset_version=14,\n                 input_names=[\"input_ids\", \"decoder_input_ids\"],\n             )\n "
        },
        {
            "sha": "7adb1f40c6e696898cf51fe748cfcdf56a3418c6",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -36,6 +36,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         AutoTokenizer,\n@@ -645,6 +646,41 @@ def test_decoder_model_past_with_3d_attn_mask(self):\n             lm_labels,\n         )\n \n+    # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)"
        },
        {
            "sha": "68dd5a52b3d69b88588f7a5c23db88002780bef0",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 105,
            "deletions": 9,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -27,6 +27,7 @@\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n+    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -44,6 +45,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         AutoTokenizer,\n@@ -578,6 +580,9 @@ class T5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     # The small T5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n \n+    # used in `test_torch_compile`\n+    _torch_compile_test_ckpt = \"google-t5/t5-small\"\n+\n     def setUp(self):\n         self.model_tester = T5ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=T5Config, d_model=37)\n@@ -630,12 +635,9 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                     ]\n                     if labels is not None:\n                         input_names.append(\"labels\")\n-\n                     filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n                     input_names = list(filtered_inputs.keys())\n-\n                     model_output = model(**filtered_inputs)\n-\n                     traced_model = symbolic_trace(model, input_names)\n                     traced_output = traced_model(**filtered_inputs)\n                 else:\n@@ -650,7 +652,6 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                         \"visual_feats\",\n                         \"visual_pos\",\n                     ]\n-\n                     labels = inputs.get(\"labels\", None)\n                     start_positions = inputs.get(\"start_positions\", None)\n                     end_positions = inputs.get(\"end_positions\", None)\n@@ -660,15 +661,12 @@ def _create_and_check_torch_fx_tracing(self, config, inputs_dict, output_loss=Fa\n                         input_names.append(\"start_positions\")\n                     if end_positions is not None:\n                         input_names.append(\"end_positions\")\n-\n                     filtered_inputs = {k: v for (k, v) in inputs.items() if k in input_names}\n                     input_names = list(filtered_inputs.keys())\n-\n                     if model.__class__.__name__ in set(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES.values()) and (\n                         not hasattr(model.config, \"problem_type\") or model.config.problem_type is None\n                     ):\n                         model.config.problem_type = \"single_label_classification\"\n-\n                     traced_model = symbolic_trace(model, input_names)\n                     traced_output = traced_model(**filtered_inputs)\n                     model_output = model(**filtered_inputs)\n@@ -721,6 +719,41 @@ def flatten_output(output):\n             # (Even with this call, there are still memory leak by ~0.04MB)\n             self.clear_torch_jit_class_registry()\n \n+    # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n@@ -1482,6 +1515,7 @@ def test_summarization(self):\n             [model.config.prefix + x for x in [FRANCE_ARTICLE, SHORTER_ARTICLE, IRAN_ARTICLE, ARTICLE_SUBWAY]],\n             padding=\"max_length\",\n             truncation=True,\n+            max_length=512,\n             return_tensors=\"pt\",\n         ).to(torch_device)\n         self.assertEqual(512, dct[\"input_ids\"].shape[1])\n@@ -1604,14 +1638,76 @@ def test_contrastive_search_t5(self):\n         outputs = t5_model.generate(input_ids, penalty_alpha=0.5, top_k=5, max_length=64)\n         generated_text = t5_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n \n+        # TODO: @arthur?\n+        # PR #31938 caused regression on this test which was fixed by PR #34089\n         self.assertListEqual(\n             generated_text,\n             [\n-                \"Liana Barrientos has been married 10 times, nine of them in the Bronx. Her husbands filed for \"\n-                \"permanent residence after the marriages, prosecutors say.\"\n+                \"Liana Barrientos has been married 10 times, nine of them in the Bronx . Her husbands filed for \"\n+                \"permanent residence after the marriages, prosecutors say .\"\n             ],\n         )\n \n+    @slow\n+    @require_torch_gpu\n+    def test_compile_static_cache(self):\n+        NUM_TOKENS_TO_GENERATE = 40\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"theory of relativity states that 1) the speed of light is constant in all inertial reference frames. the laws of physics are the same for all inertial reference frames.\",\n+            \"ketchup is my favorite condiment.\",\n+        ]\n+\n+        prompts = [\n+            \"summarize: Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial \"\n+            \"reference frames, and 2) the laws of physics are the same for all inertial reference frames.\\nThe \"\n+            \"theory of relativity is not hard to grasp.\",\n+            \"summarize: My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, \"\n+            \"my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my pizza.\",\n+        ]\n+        model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\").to(torch_device)\n+        tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n+        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+\n+        # Dynamic Cache\n+        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n+        dynamic_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, dynamic_text)\n+\n+        # Static Cache\n+        generated_ids = model.generate(\n+            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n+        )\n+        static_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, static_text)\n+\n+        # Static Cache + compile\n+        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n+        generated_ids = model.generate(\n+            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n+        )\n+        static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_compile_static_cache_encoder(self):\n+        prompts = [\n+            \"summarize: Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial \"\n+            \"reference frames, and 2) the laws of physics are the same for all inertial reference frames.\\nThe \"\n+            \"theory of relativity is not hard to grasp.\",\n+            \"summarize: My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, \"\n+            \"my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my pizza.\",\n+        ]\n+        model = T5EncoderModel.from_pretrained(\"google-t5/t5-small\").to(torch_device)\n+        tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n+        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+\n+        logits = model(**inputs)\n+\n+        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n+        logits_compiled = model(**inputs)\n+        self.assertTrue(torch.allclose(logits[0][:, -3:, -3], logits_compiled[0][:, -3:, -3], atol=1e-5))\n+\n \n @require_torch\n class TestAsymmetricT5(unittest.TestCase):"
        },
        {
            "sha": "9d82173b1aed6c4a11ab8f0601a7bace2e2e8ea6",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -37,6 +37,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import UdopEncoderModel, UdopForConditionalGeneration, UdopModel, UdopProcessor\n \n@@ -348,6 +349,7 @@ def test_forward_signature(self):\n             expected_arg_names = [\n                 \"attention_mask\",\n                 \"bbox\",\n+                \"cache_position\",\n                 \"cross_attn_head_mask\",\n                 \"decoder_attention_mask\",\n                 \"decoder_head_mask\",\n@@ -365,6 +367,43 @@ def test_forward_signature(self):\n                 expected_arg_names = sorted(expected_arg_names)\n             self.assertListEqual(sorted(arg_names[: len(expected_arg_names)]), expected_arg_names)\n \n+    # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+                bbox=input_dict[\"bbox\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                bbox=input_dict[\"bbox\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     @unittest.skip(\n         \"Not currently compatible. Fails with - NotImplementedError: Cannot copy out of meta tensor; no data!\"\n     )\n@@ -534,6 +573,41 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     @unittest.skip(\n         \"Not currently compatible. Fails with - NotImplementedError: Cannot copy out of meta tensor; no data!\"\n     )"
        },
        {
            "sha": "ec4c1d019b6d17899bedc839dcafb21017608d03",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -41,6 +41,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         AutoTokenizer,\n@@ -316,6 +317,9 @@ class UMT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     # The small UMT5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n \n+    # used in `test_torch_compile`\n+    _torch_compile_test_ckpt = \"google/umt5-small\"\n+\n     def setUp(self):\n         self.model_tester = UMT5ModelTester(self)\n \n@@ -486,6 +490,41 @@ def test_inputs_embeds(self):\n             with torch.no_grad():\n                 model(**inputs)[0]\n \n+    # overwrite because T5 doesn't accept position ids as input and expects `decoder_input_ids`\n+    def test_custom_4d_attention_mask(self):\n+        for model_class in self.all_generative_model_classes:\n+            config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32)\n+\n+            (\n+                input_ids,\n+                _,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                _,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(\n+                decoder_input_ids=input_ids,\n+                input_ids=input_dict[\"input_ids\"][:3],\n+            ).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids=input_dict[\"input_ids\"][:1],\n+                decoder_input_ids=input_ids_shared_prefix,\n+                decoder_attention_mask=mask_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n+\n     def test_with_sequence_classification_head(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_with_sequence_classification_head(*config_and_inputs)"
        },
        {
            "sha": "964b7b912b4e0f5e5ac6d0eba3a5ee01b3f440a2",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 19,
            "deletions": 8,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73d65e637b63193289dbf6727297cb9ecdf4ff29/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=73d65e637b63193289dbf6727297cb9ecdf4ff29",
            "patch": "@@ -37,6 +37,7 @@\n from transformers import (\n     AutoModel,\n     AutoModelForCausalLM,\n+    AutoModelForSeq2SeqLM,\n     AutoModelForSequenceClassification,\n     AutoTokenizer,\n     GenerationConfig,\n@@ -5109,10 +5110,15 @@ def test_torch_compile(self):\n         batch_size = 1\n         n_iter = 3\n \n-        tokenizer = AutoTokenizer.from_pretrained(ckpt, revision=revision)\n-        model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n-            torch_device\n-        )\n+        tokenizer = AutoTokenizer.from_pretrained(ckpt)\n+        if self.is_encoder_decoder:\n+            model = AutoModelForSeq2SeqLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n+                torch_device\n+            )\n+        else:\n+            model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n+                torch_device\n+            )\n \n         model.generation_config.max_new_tokens = 4\n \n@@ -5184,10 +5190,15 @@ def test_compile_cuda_graph_time(self):\n \n         os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n \n-        tokenizer = AutoTokenizer.from_pretrained(ckpt, revision=revision)\n-        model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n-            torch_device\n-        )\n+        tokenizer = AutoTokenizer.from_pretrained(ckpt)\n+        if self.is_encoder_decoder:\n+            model = AutoModelForSeq2SeqLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n+                torch_device\n+            )\n+        else:\n+            model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, revision=revision).to(\n+                torch_device\n+            )\n \n         cache_implementation = \"static\"\n         if model.config.model_type == \"gemma2\":"
        }
    ],
    "stats": {
        "total": 3921,
        "additions": 2743,
        "deletions": 1178
    }
}