{
    "author": "weezymatt",
    "message": "[Docs] Translate audio_classification.md from English to Spanish (#39513)\n\n* Docs: translate audio_classification to Spanish\n\n* Update audio_classification.md\n\n* Remove space\r\n* Normalize backticks\n\n* Update audio_classification.md\n\n* Apply corrections recommended by aaronjimv\n\n* Update _toctree.yml\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "7b897fe583708290b2efdf0b017cda9931730551",
    "files": [
        {
            "sha": "85a9aec02e7da00ad2a169a1c96f83742b9d9577",
            "filename": "docs/source/es/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b897fe583708290b2efdf0b017cda9931730551/docs%2Fsource%2Fes%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b897fe583708290b2efdf0b017cda9931730551/docs%2Fsource%2Fes%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2F_toctree.yml?ref=7b897fe583708290b2efdf0b017cda9931730551",
            "patch": "@@ -38,6 +38,8 @@\n     sections:\n     - local: tasks/asr\n       title: Reconocimiento autom√°tico del habla\n+    - local: tasks/audio_classification\n+      title: Clasificaci√≥n de audio\n     title: Audio\n   - isExpanded: false\n     sections:"
        },
        {
            "sha": "69f180ba68baf8bc11c656e176a83c61fce270a9",
            "filename": "docs/source/es/tasks/audio_classification.md",
            "status": "added",
            "additions": 323,
            "deletions": 0,
            "changes": 323,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b897fe583708290b2efdf0b017cda9931730551/docs%2Fsource%2Fes%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b897fe583708290b2efdf0b017cda9931730551/docs%2Fsource%2Fes%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Faudio_classification.md?ref=7b897fe583708290b2efdf0b017cda9931730551",
            "patch": "@@ -0,0 +1,323 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Clasificaci√≥n de audio\n+\n+[[open-in-colab]]\n+\n+<Youtube id=\"KWwzcmG98Ds\"/>\n+\n+Clasificaci√≥n de audio - al igual que con texto ‚Äî asigna una etiqueta de clase como salida desde las entradas de datos. La diferencia √∫nica es en vez de entrada de texto, tiene formas de onda de audio. Algunas aplicaciones pr√°cticas de clasificaci√≥n incluye identificar la intenci√≥n del hablante, identificaci√≥n del idioma, y la clasificaci√≥n de animales por sus sonidos.\n+\n+En esta gu√≠a te mostraremos como: \n+\n+1. Hacer fine-tuning al modelo [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) en el dataset [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) para clasificar la intenci√≥n del hablante.\n+2. Usar tu modelo ajustado para tareas de inferencia.\n+\n+\n+<Tip>\n+\n+Consulta la [p√°gina de la tarea](https://huggingface.co/tasks/audio-classification) de clasificaci√≥n de audio para acceder a m√°s informaci√≥n sobre los modelos, datasets, y m√©tricas asociados.\n+\n+</Tip>\n+\n+Antes de comenzar, aseg√∫rate de haber instalado todas las librer√≠as necesarias:\n+\n+```bash\n+pip install transformers datasets evaluate\n+```\n+\n+Te aconsejamos iniciar sesi√≥n con tu cuenta de Hugging Face para que puedas subir tu modelo y compartirlo con la comunidad. Cuando se te solicite, ingresa tu token para iniciar sesi√≥n:\n+\n+```py\n+>>> from huggingface_hub import notebook_login\n+\n+>>> notebook_login()\n+```\n+\n+## Carga el dataset MInDS-14\n+\n+Comencemos cargando el dataset MInDS-14 con la biblioteca de ü§ó Datasets:\n+\n+```py\n+>>> from datasets import load_dataset, Audio\n+\n+>>> minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n+```\n+\n+Divide el conjunto de `train` (entrenamiento) en un conjunto de entrenamiento y prueba mas peque√±o con el m√©todo [`~datasets.Dataset.train_test_split`]. De esta forma, tendr√°s la oportunidad para experimentar y aseg√∫rate de que todo funci√≥ne antes de invertir m√°s tiempo entrenando con el dataset entero.\n+\n+```py\n+>>> minds = minds.train_test_split(test_size=0.2)\n+```\n+\n+Ahora √©chale un vistazo al dataset:\n+\n+```py\n+>>> minds\n+DatasetDict({\n+    train: Dataset({\n+        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n+        num_rows: 450\n+    })\n+    test: Dataset({\n+        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n+        num_rows: 113\n+    })\n+})\n+```\n+\n+Aunque el dataset contiene mucha informaci√≥n √∫til, como los campos `land_id` (identificador del lenguaje) y `english_transcription` (transcripci√≥n al ingl√©s), en esta gu√≠a nos enfocaremos en los campos `audio` y `intent_class` (clase de intenci√≥n). Puedes quitar las otras columnas con cel m√©todo [`~datasets.Dataset.remove_columns`]:\n+\n+```py\n+>>> minds = minds.remove_columns([\"path\", \"transcription\", \"english_transcription\", \"lang_id\"])\n+```\n+\n+Aqu√≠ est√° un ejemplo:\n+\n+```py\n+>>> minds[\"train\"][0]\n+{'audio': {'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00048828,\n+         -0.00024414, -0.00024414], dtype=float32),\n+  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',\n+  'sampling_rate': 8000},\n+ 'intent_class': 2}\n+```\n+\n+Hay dos campos:\n+\n+- `audio`: un `array` (arreglo) unidimensional de la se√±al de voz que se obtiene al cargar y volver a muestrear el archivo de audio.\n+- `intent_class`: representa el identificador de la clase de la intenci√≥n del hablante.\n+\n+Crea un diccionario que asigne el nombre de la etiqueta a un n√∫mero entero y viceversa para facilitar la obtenci√≥n del nombre de la etiqueta a partir de su identificador.\n+\n+```py\n+>>> labels = minds[\"train\"].features[\"intent_class\"].names\n+>>> label2id, id2label = dict(), dict()\n+>>> for i, label in enumerate(labels):\n+...     label2id[label] = str(i)\n+...     id2label[str(i)] = label\n+```\n+\n+Ahora puedes convertir el identificador de la etiqueta a un nombre de etiqueta:\n+\n+```py\n+>>> id2label[str(2)]\n+'app_error'\n+```\n+\n+## Preprocesamiento\n+\n+Seguidamente carga el feature extractor (funci√≥n de extracci√≥n de caracter√≠sticas) de Wav2Vec para procesar la se√±al de audio:\n+\n+```py\n+>>> from transformers import AutoFeatureExtractor\n+\n+>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n+```\n+\n+El dataset MInDS-14 tiene una tasa de muestreo de 8kHz (puedes encontrar esta informaci√≥n en su [tarjeta de dataset](https://huggingface.co/datasets/PolyAI/minds14)), lo que significa que tendr√°s que volver a muestrear el dataset a 16kHZ para poder usar el modelo Wav2Vec2 preentranado:\n+\n+```py\n+>>> minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n+>>> minds[\"train\"][0]\n+{'audio': {'array': array([ 2.2098757e-05,  4.6582241e-05, -2.2803260e-05, ...,\n+         -2.8419291e-04, -2.3305941e-04, -1.1425107e-04], dtype=float32),\n+  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav',\n+  'sampling_rate': 16000},\n+ 'intent_class': 2}\n+```\n+\n+Ahora vamos a crear una funci√≥n de preprocesamiento:\n+\n+1. Invoque la columna `audio` para cargar, y si es necesario, volver a muestrear al archivo de audio.\n+2. Comprueba si la frecuencia de muestreo del archivo de audio coincide con la frecuencia de muestreo de los datos de audio con los que se entren√≥ previamente el modelo. Puedes encontrar esta informaci√≥n en la [tarjeta de modelo](https://huggingface.co/facebook/wav2vec2-base) de Wav2Vec2.\n+3. Establece una longitud de entrada m√°xima para agrupar entradas m√°s largas sin truncarlas.\n+\n+```py\n+>>> def preprocess_function(examples):\n+...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n+...     inputs = feature_extractor(\n+...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\n+...     )\n+...     return inputs\n+```\n+\n+Para aplicar la funci√≥n de preprocesamiento a todo el dataset, puedes usar la funci√≥n [`~datasets.Dataset.map`] de ü§ó Datasets. Acelera la funci√≥n `map` haciendo `batched=True` para procesar varios elementos del dataset a la vez. Quitas las columnas que no necesites con el m√©todo `[~datasets.Dataset.remove_columns]` y cambia el nombre de `intent_class` a `label`, como requiere el modelo.\n+\n+```py\n+>>> encoded_minds = minds.map(preprocess_function, remove_columns=\"audio\", batched=True)\n+>>> encoded_minds = encoded_minds.rename_column(\"intent_class\", \"label\")\n+```\n+\n+## Evaluaci√≥n\n+A menudo es √∫til incluir una m√©trica durante el entrenamiento para evaluar el rendimiento de tu modelo. Puedes cargar un m√©todo de evaluaci√≥n rapidamente con la biblioteca de ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index). Para esta tarea, puedes usar la m√©trica de [exactitud](https://huggingface.co/spaces/evaluate-metric/accuracy) (accuracy). Puedes ver la [gu√≠a r√°pida](https://huggingface.co/docs/evaluate/a_quick_tour) de ü§ó Evaluate para aprender m√°s de c√≥mo cargar y computar una m√©trica:\n+\n+```py\n+>>> import evaluate\n+\n+>>> accuracy = evaluate.load(\"accuracy\")\n+```\n+\n+Ahora crea una funci√≥n que le pase tus predicciones y etiquetas a [`~evaluate.EvaluationModule.compute`] para calcular la exactitud:\n+\n+```py\n+>>> import numpy as np\n+\n+\n+>>> def compute_metrics(eval_pred):\n+...     predictions = np.argmax(eval_pred.predictions, axis=1)\n+...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n+```\n+\n+Ahora tu funci√≥n `compute_metrics` (computar m√©tricas) est√° lista y podr√°s usarla cuando est√©s preparando tu entrenamiento.\n+\n+## Entrenamiento\n+\n+<frameworkcontent>\n+<pt>\n+<Tip>\n+\n+¬°Si no tienes experiencia haci√©ndo *fine-tuning* a un modelo con el [`Trainer`], √©chale un vistazo al tutorial b√°sico [aqu√≠](../training#train-with-pytorch-trainer)!\n+\n+</Tip>\n+\n+¬°Ya puedes empezar a entrenar tu modelo! Carga Wav2Vec2 con [`AutoModelForAudioClassification`] junto con el especifica el n√∫mero de etiquetas, y pasa al modelo los *mappings* entre el n√∫mero entero de etiqueta y la clase de etiqueta.\n+\n+```py\n+>>> from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n+\n+>>> num_labels = len(id2label)\n+>>> model = AutoModelForAudioClassification.from_pretrained(\n+...     \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n+... )\n+```\n+\n+Al llegar a este punto, solo quedan tres pasos:\n+\n+1. Define tus hiperpar√°metros de entrenamiento en [`TrainingArguments`]. El √∫nico par√°metro obligatorio es `output_dir` (carpeta de salida), el cual especifica d√≥nde guardar tu modelo. Puedes subir este modelo al Hub haciendo `push_to_hub=True` (debes haber iniciado sesi√≥n en Hugging Face para subir tu modelo). Al final de cada √©poca, el [`Trainer`] evaluar√° la exactitud y guardar√° el punto de control del entrenamiento.\n+2. P√°sale los argumentos del entrenamiento al [`Trainer`] junto con el modelo, el dataset, el tokenizer, el data collator y la funci√≥n `compute_metrics`.\n+3. Llama el m√©todo [`~Trainer.train`] para hacerle fine-tuning a tu modelo.\n+\n+```py\n+>>> training_args = TrainingArguments(\n+...     output_dir=\"my_awesome_mind_model\",\n+...     eval_strategy=\"epoch\",\n+...     save_strategy=\"epoch\",\n+...     learning_rate=3e-5,\n+...     per_device_train_batch_size=32,\n+...     gradient_accumulation_steps=4,\n+...     per_device_eval_batch_size=32,\n+...     num_train_epochs=10,\n+...     warmup_ratio=0.1,\n+...     logging_steps=10,\n+...     load_best_model_at_end=True,\n+...     metric_for_best_model=\"accuracy\",\n+...     push_to_hub=True,\n+... )\n+\n+>>> trainer = Trainer(\n+...     model=model,\n+...     args=training_args,\n+...     train_dataset=encoded_minds[\"train\"],\n+...     eval_dataset=encoded_minds[\"test\"],\n+...     processing_class=feature_extractor,\n+...     compute_metrics=compute_metrics,\n+... )\n+\n+>>> trainer.train()\n+```\n+\n+Una vez que el entrenamiento haya sido completado, comparte tu modelo en el Hub con el m√©todo [`~transformers.Trainer.push_to_hub`] para que todo el mundo puede usar tu modelo.\n+\n+```py\n+>>> trainer.push_to_hub()\n+```\n+</pt>\n+</frameworkcontent>\n+\n+<Tip>\n+\n+Para ver un ejemplo m√°s detallado de com√≥ hacerle fine-tuning a un modelo para clasificaci√≥n, √©chale un vistazo al correspondiente [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb).\n+\n+</Tip>\n+\n+## Inference\n+\n+¬°Genial, ahora que le has hecho *fine-tuned* a un modelo, puedes usarlo para hacer inferencia!\n+\n+Carga el archivo de audio para hacer inferencia. Recuerda volver a muestrear la tasa de muestreo del archivo de audio para que sea la misma del modelo si es necesario.\n+\n+```py\n+>>> from datasets import load_dataset, Audio\n+\n+>>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n+>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n+>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n+>>> audio_file = dataset[0][\"audio\"][\"path\"]\n+```\n+\n+La manera m√°s simple de probar tu modelo para hacer inferencia es usarlo en un [`pipeline`]. Puedes instanciar un `pipeline` para clasificaci√≥n de audio con tu modelo y pasarle tu archivo de audio:\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> classifier = pipeline(\"audio-classification\", model=\"stevhliu/my_awesome_minds_model\")\n+>>> classifier(audio_file)\n+[\n+    {'score': 0.09766869246959686, 'label': 'cash_deposit'},\n+    {'score': 0.07998877018690109, 'label': 'app_error'},\n+    {'score': 0.0781070664525032, 'label': 'joint_account'},\n+    {'score': 0.07667109370231628, 'label': 'pay_bill'},\n+    {'score': 0.0755252093076706, 'label': 'balance'}\n+]\n+```\n+\n+Tambi√©n puedes replicar de forma manual los resultados del `pipeline` si lo deseas:\n+\n+<frameworkcontent>\n+<pt>\n+Carga el feature extractor para preprocesar el archivo de audio y devuelve el `input` como un tensor de PyTorch:\n+\n+```py\n+>>> from transformers import AutoFeatureExtractor\n+\n+>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"stevhliu/my_awesome_minds_model\")\n+>>> inputs = feature_extractor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n+```\n+\n+P√°sale tus entradas al modelo y devuelve los logits:\n+\n+```py\n+>>> from transformers import AutoModelForAudioClassification\n+\n+>>> model = AutoModelForAudioClassification.from_pretrained(\"stevhliu/my_awesome_minds_model\")\n+>>> with torch.no_grad():\n+...     logits = model(**inputs).logits\n+```\n+\n+Obt√©n los identificadores de los clases con mayor probabilidad y usa el *mapping* `id2label` del modelo para convertirle a una etiqueta:\n+\n+```py\n+>>> import torch\n+\n+>>> predicted_class_ids = torch.argmax(logits).item()\n+>>> predicted_label = model.config.id2label[predicted_class_ids]\n+>>> predicted_label\n+'cash_deposit'\n+```\n+</pt>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 325,
        "additions": 325,
        "deletions": 0
    }
}