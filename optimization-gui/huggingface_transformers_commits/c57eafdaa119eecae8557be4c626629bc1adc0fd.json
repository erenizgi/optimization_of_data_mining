{
    "author": "farrosalferro",
    "message": "Add Nemotron GGUF Loading Support (#34725)\n\n* Add Nemotron GGUF Loading Support\r\n\r\n* fix the Nemotron architecture assignation\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "c57eafdaa119eecae8557be4c626629bc1adc0fd",
    "files": [
        {
            "sha": "b1ed1f0d492ab9f556107cc8774b4081c4c39cdb",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c57eafdaa119eecae8557be4c626629bc1adc0fd/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c57eafdaa119eecae8557be4c626629bc1adc0fd/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=c57eafdaa119eecae8557be4c626629bc1adc0fd",
            "patch": "@@ -87,6 +87,7 @@ For now the supported model architectures are the architectures that have been v\n - Starcoder2\n - T5\n - Mamba\n+- Nemotron\n \n ## Example usage\n "
        },
        {
            "sha": "57f0af5667e648397f3648a737674c4ad6a8abf5",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/c57eafdaa119eecae8557be4c626629bc1adc0fd/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c57eafdaa119eecae8557be4c626629bc1adc0fd/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=c57eafdaa119eecae8557be4c626629bc1adc0fd",
            "patch": "@@ -248,6 +248,20 @@\n         \"output_norm\": \"backbone.norm_f\",\n         \"output.weight\": \"lm_head.weight\",\n     },\n+    \"nemotron\": {\n+        \"token_embd\": \"model.embed_tokens\",\n+        \"blk\": \"model.layers\",\n+        \"ffn_up\": \"mlp.up_proj\",\n+        \"ffn_down\": \"mlp.down_proj\",\n+        \"ffn_norm\": \"post_attention_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_q\": \"self_attn.q_proj\",\n+        \"attn_v\": \"self_attn.v_proj\",\n+        \"attn_k\": \"self_attn.k_proj\",\n+        \"attn_output\": \"self_attn.o_proj\",\n+        \"output.weight\": \"lm_head.weight\",\n+        \"output_norm\": \"model.norm\",\n+    },\n }\n \n \n@@ -397,6 +411,18 @@\n         \"ssm.time_step_rank\": \"time_step_rank\",\n         \"ssm.inner_size\": \"intermediate_size\",\n     },\n+    \"nemotron\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -793,6 +819,7 @@ def converted(self) -> Tokenizer:\n     \"starcoder2\": GGUFGPTConverter,\n     \"t5\": GGUFT5Converter,\n     \"mamba\": GGUFGPTConverter,\n+    \"nemotron\": GGUFGPTConverter,\n }\n \n "
        },
        {
            "sha": "42b05f18449ded26257ee7e46355101fde7ed95d",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c57eafdaa119eecae8557be4c626629bc1adc0fd/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c57eafdaa119eecae8557be4c626629bc1adc0fd/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=c57eafdaa119eecae8557be4c626629bc1adc0fd",
            "patch": "@@ -61,6 +61,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     starcoder2_original_model_id = \"bigcode/starcoder2-3b\"\n     mamba_original_model_id = \"state-spaces/mamba-2.8b-hf\"\n     mamba_model_id = \"jpodivin/mamba-2.8b-hf-GGUF\"\n+    nemotron_original_model_id = \"nvidia/Nemotron-Mini-4B-Instruct\"\n+    nemotron_model_id = \"bartowski/Nemotron-Mini-4B-Instruct-GGUF\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -106,6 +108,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     fp16_starcoder2_gguf_model_id = \"starcoder2-3b.fp16.gguf\"\n     q6_k_mamba_model_id = \"ggml-model-Q6_K.gguf\"\n     fp16_mamba_model_id = \"ggml-model-f16.gguf\"\n+    q6_k_nemotron_model_id = \"Nemotron-Mini-4B-Instruct-Q6_K.gguf\"\n+    fp16_nemotron_model_id = \"Nemotron-Mini-4B-Instruct-f16.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -792,6 +796,42 @@ def test_mamba_q6_k(self):\n         EXPECTED_TEXT = \"Hello,I answerthe question.\\n\\nA\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_nemotron_weights_conversion_fp16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.nemotron_original_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.nemotron_model_id,\n+            gguf_file=self.fp16_nemotron_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n+    def test_nemotron_q6_k(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.nemotron_model_id,\n+            gguf_file=self.q6_k_nemotron_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.nemotron_model_id, gguf_file=self.q6_k_nemotron_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"'Hello. hotmail.com.'\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 68,
        "deletions": 0
    }
}