{
    "author": "vasqu",
    "message": "[`CI`] Fix copies on main (#41486)\n\nfix copies",
    "sha": "c20849bad1df6a557f9a62fa7fb2abf469201296",
    "files": [
        {
            "sha": "3a61b780c80d7d3082c2c8bfc972df0cdab562aa",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=c20849bad1df6a557f9a62fa7fb2abf469201296",
            "patch": "@@ -177,13 +177,6 @@ def __init__(\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -210,5 +203,13 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n __all__ = [\"ApertusConfig\"]"
        },
        {
            "sha": "57c745629d667e776cab8a445d42542e57b03037",
            "filename": "src/transformers/models/arcee/configuration_arcee.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fconfiguration_arcee.py?ref=c20849bad1df6a557f9a62fa7fb2abf469201296",
            "patch": "@@ -162,13 +162,6 @@ def __init__(\n         head_dim=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -197,5 +190,13 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n __all__ = [\"ArceeConfig\"]"
        },
        {
            "sha": "655f4fe183909771fc90cf64cd594b96b96357fa",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=c20849bad1df6a557f9a62fa7fb2abf469201296",
            "patch": "@@ -157,13 +157,6 @@ def __init__(\n         norm_topk_prob=False,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -195,5 +188,13 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n \n __all__ = [\"FlexOlmoConfig\"]"
        },
        {
            "sha": "64a11b4339bd0ccf7e7ea98db41ff5c7a4e238a3",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=c20849bad1df6a557f9a62fa7fb2abf469201296",
            "patch": "@@ -126,13 +126,6 @@ def __init__(\n         rms_norm_eps=1e-5,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -154,6 +147,14 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n         self.rms_norm_eps = rms_norm_eps\n \n     def _rope_scaling_validation(self):"
        },
        {
            "sha": "e8dbe593dde9aac25d7085b54955d3591c2c3a44",
            "filename": "src/transformers/models/olmo3/configuration_olmo3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c20849bad1df6a557f9a62fa7fb2abf469201296/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py?ref=c20849bad1df6a557f9a62fa7fb2abf469201296",
            "patch": "@@ -177,13 +177,6 @@ def __init__(\n         layer_types=None,\n         **kwargs,\n     ):\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -205,6 +198,14 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n \n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n         self.rms_norm_eps = rms_norm_eps\n \n         self.sliding_window = sliding_window"
        }
    ],
    "stats": {
        "total": 75,
        "additions": 40,
        "deletions": 35
    }
}