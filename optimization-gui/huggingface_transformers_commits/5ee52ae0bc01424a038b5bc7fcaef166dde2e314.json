{
    "author": "zucchini-nlp",
    "message": "Mllama: fix tests (#34000)\n\n* fix tests\r\n\r\n* don't need this\r\n\r\n* style",
    "sha": "5ee52ae0bc01424a038b5bc7fcaef166dde2e314",
    "files": [
        {
            "sha": "4e4a1ee26c12d7e5370abf8049d3d2572dbf2093",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=5ee52ae0bc01424a038b5bc7fcaef166dde2e314",
            "patch": "@@ -813,8 +813,7 @@ def _quantize(self, tensor, axis):\n         if is_optimum_quanto_available():\n             from optimum.quanto import quantize_weight\n \n-            scale, zeropoint = self.optimizer(tensor, self.qtype, axis, self.q_group_size)\n-            qtensor = quantize_weight(tensor, self.qtype, axis, scale, zeropoint, self.q_group_size)\n+            qtensor = quantize_weight(tensor, self.qtype, axis, self.q_group_size)\n             return qtensor\n         elif is_quanto_available():\n             logger.warning_once("
        },
        {
            "sha": "fb7ed2f0b2f553fe345cd36d66044213220c8ca3",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=5ee52ae0bc01424a038b5bc7fcaef166dde2e314",
            "patch": "@@ -425,6 +425,16 @@ def _prepare_attention_mask(model_kwargs: Dict[str, Any], new_length: int, is_en\n         model_kwargs[mask_key] = mask[:, :mask_length_diff]\n     elif mask_length_diff > 0:\n         model_kwargs[mask_key] = torch.cat([mask, mask.new_ones((mask.shape[0], mask_length_diff))], dim=-1)\n+\n+    if \"cross_attention_mask\" in model_kwargs:\n+        # Mllama case is special and has another mask for cross attention model\n+        cross_mask = model_kwargs[\"cross_attention_mask\"]\n+        if mask_length_diff < 0:\n+            model_kwargs[\"cross_attention_mask\"] = cross_mask[:, :mask_length_diff]\n+        elif mask_length_diff > 0:\n+            new_mask = cross_mask[:, -1:, :, :].repeat(1, mask_length_diff, 1, 1)\n+            model_kwargs[\"cross_attention_mask\"] = torch.cat([cross_mask, new_mask], dim=1)\n+\n     return model_kwargs\n \n "
        },
        {
            "sha": "0bc77eaeec332466610d8292b849d1eddd118b0e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=5ee52ae0bc01424a038b5bc7fcaef166dde2e314",
            "patch": "@@ -1042,7 +1042,7 @@ class MllamaPreTrainedModel(PreTrainedModel):\n         \"MllamaSelfAttentionDecoderLayer\",\n     ]\n     _supports_cache_class = True\n-    _supports_static_cache = False\n+    _supports_static_cache = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n \n@@ -1980,6 +1980,8 @@ def forward(\n     MLLAMA_START_DOCSTRING,\n )\n class MllamaForConditionalGeneration(MllamaPreTrainedModel, GenerationMixin):\n+    _supports_quantized_cache = False  # quant cache not supported in encoder-decoder setting\n+\n     def __init__(self, config: MllamaConfig):\n         super().__init__(config)\n         self.vocab_size = config.text_config.vocab_size"
        },
        {
            "sha": "5c5ca3985ee08f168f2d1d98f0019f5aaa184c12",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 39,
            "deletions": 89,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ee52ae0bc01424a038b5bc7fcaef166dde2e314/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=5ee52ae0bc01424a038b5bc7fcaef166dde2e314",
            "patch": "@@ -138,14 +138,6 @@ def setUp(self):\n     def test_eager_matches_sdpa_generate(self):\n         super().test_eager_matches_sdpa_generate()\n \n-    @unittest.skip(reason=\"The outputs don't match, no idea why\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Quanto test is borken\")\n-    def test_generate_with_quant_cache(self):\n-        pass\n-\n \n class MllamaVisionText2TextModelTester:\n     def __init__(\n@@ -208,6 +200,7 @@ def __init__(\n         self.image_size = 224\n         self.max_num_images = 1\n         self.max_image_tiles = 4\n+        self.image_length = 904\n \n     def get_config(self):\n         return MllamaConfig(\n@@ -329,6 +322,43 @@ def test_inputs_embeds_matches_input_ids(self):\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n             self.assertTrue(torch.allclose(out_embeds, out_ids))\n \n+    def _check_attentions_for_generate(\n+        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+    ):\n+        # Mllama has cross attention layers and those have a different shape than normal attention layers\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n+        )\n+        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+\n+        cross_attention_layers = self.model_tester.text_config[\"cross_attention_layers\"]\n+\n+        for idx, iter_attentions in enumerate(attentions):\n+            tgt_len = min_length + idx if not use_cache else 1\n+            src_len = min_length + idx\n+\n+            expected_shape = (\n+                batch_size * num_beam_groups,\n+                config.num_attention_heads,\n+                tgt_len,\n+                src_len,\n+            )\n+\n+            expected_shape_cross = (\n+                batch_size * num_beam_groups,\n+                config.num_attention_heads,\n+                tgt_len,\n+                self.model_tester.image_length,\n+            )\n+\n+            expected_shapes = [\n+                expected_shape if layer_idx not in cross_attention_layers else expected_shape_cross\n+                for layer_idx in range(len(iter_attentions))\n+            ]\n+\n+            self.assertListEqual([layer_attention.shape for layer_attention in iter_attentions], expected_shapes)\n+\n     @require_torch_sdpa\n     @slow\n     @is_flaky()\n@@ -342,94 +372,14 @@ def test_eager_matches_sdpa_inference_1_bfloat16(self):\n         # A workaround to override parametrized test with flaky decorator\n         super().test_eager_matches_sdpa_inference_1_bfloat16()\n \n-    @unittest.skip(reason=\"Static cache not supported\")\n-    def test_static_cache_matches_dynamic(self):\n-        # TypeError: list indices must be integers or slices, not tuple\n-        # TODO: @raushan, please look into this for new cache format\n-        pass\n-\n-    @unittest.skip(reason=\"Mllama has dynamic control flow which is not yet supported by compile\")\n-    def test_generate_compile_fullgraph(self):\n-        pass\n-\n-    @unittest.skip(reason=\"The outputs don't match, no idea why\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Mllama is not yet supported by compile\")\n+    @unittest.skip(\"For some unknown reasons the tests fails in CrossAttention layer when doing torch.sdpa(). \")\n     def test_sdpa_can_compile_dynamic(self):\n-        # TODO: look into this, AttributeError(\"'tensor' object has no attribute '__pow__'\")\n-        # relevant issue: https://github.com/pytorch/pytorch/issues/133166\n-        pass\n-\n-    @unittest.skip(reason=\"The test itself is broken\")  # TODO @zucchini-nlp\n-    def test_generate_with_quant_cache(self):\n         pass\n \n     @unittest.skip(reason=\"AssertionError: Items in the second set but not the first: might be a setting issue\")\n     def test_model_parallelism(self):\n         pass\n \n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_compile_cuda_graph_time(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_torch_compile_fullgraph(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Device side assert triggered\")\n-    def test_assisted_decoding_with_num_logits_to_keep(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_beam_sample_generate_dict_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_beam_search_generate_dict_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_constrained_beam_search_generate_dict_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_dola_decoding_sample(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_generate_methods_with_num_logits_to_keep(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_greedy_generate_dict_outputs(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_group_beam_search_generate_dict_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_model_parallel_beam_search(self):\n-        pass\n-\n-    @is_flaky()  # TODO (joao, raushan) - investigate why this test is flaky (probably depends on the model initialization)\n-    def test_new_cache_format_0(self):\n-        super().test_new_cache_format_0()\n-\n-    @is_flaky()  # TODO (joao, raushan) - investigate why this test is flaky (probably depends on the model initialization)\n-    def test_new_cache_format_1(self):\n-        super().test_new_cache_format_1()\n-\n-    @is_flaky()  # TODO (joao, raushan) - investigate why this test is flaky (probably depends on the model initialization)\n-    def test_new_cache_format_2(self):\n-        super().test_new_cache_format_2()\n-\n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_sample_generate_dict_output(self):\n-        pass\n-\n     def test_generate_text_only_with_cache(self):\n         \"\"\"\n         Tests that our cached generation with text-only inputs works. When mllama was introduced, this feature"
        }
    ],
    "stats": {
        "total": 145,
        "additions": 53,
        "deletions": 92
    }
}