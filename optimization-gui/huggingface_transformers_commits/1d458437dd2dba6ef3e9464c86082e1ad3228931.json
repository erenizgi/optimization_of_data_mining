{
    "author": "fabxoe",
    "message": "ğŸŒ [i18n-KO] Translated `model_doc/mamba.md` to Korean (#33626)\n\n* docs: ko: model_doc/mamba.md\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\n---------\r\n\r\nCo-authored-by: Ahnjj_DEV <ahnjj.dev@gmail.com>",
    "sha": "1d458437dd2dba6ef3e9464c86082e1ad3228931",
    "files": [
        {
            "sha": "a3c8c5e992ea694d4ae4194c017b46b0235fb98f",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d458437dd2dba6ef3e9464c86082e1ad3228931/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d458437dd2dba6ef3e9464c86082e1ad3228931/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=1d458437dd2dba6ef3e9464c86082e1ad3228931",
            "patch": "@@ -432,6 +432,8 @@\n         title: (ë²ˆì—­ì¤‘) LUKE\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) M2M100\n+      - local: model_doc/mamba\n+        title: Mamba\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) MarianMT\n       - local: in_translation"
        },
        {
            "sha": "404b9b06b41353949a8db18867d4fce4a8fa452e",
            "filename": "docs/source/ko/model_doc/mamba.md",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d458437dd2dba6ef3e9464c86082e1ad3228931/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d458437dd2dba6ef3e9464c86082e1ad3228931/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmamba.md?ref=1d458437dd2dba6ef3e9464c86082e1ad3228931",
            "patch": "@@ -0,0 +1,105 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ë§˜ë°”[[mamba]]\n+\n+## ê°œìš”[[overview]]\n+\n+ë§˜ë°”(Mamba) ëª¨ë¸ì€ Albert Gu, Tri Daoê°€ ì œì•ˆí•œ [ë§˜ë°”: ì„ íƒì  ìƒíƒœ ê³µê°„ì„ ì´ìš©í•œ ì„ í˜• ì‹œê°„ ì‹œí€€ìŠ¤ ëª¨ë¸ë§](https://arxiv.org/abs/2312.00752)ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì†Œê°œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ì´ ëª¨ë¸ì€ `state-space-models`ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ì§ê´€ì ì¸ ì´í•´ë¥¼ ì–»ê³  ì‹¶ë‹¤ë©´ [ì´ê³³](https://srush.github.io/annotated-s4/)ì„ ì°¸ê³  í•˜ì„¸ìš”.\n+\n+í•´ë‹¹ ë…¼ë¬¸ì˜ ì´ˆë¡ì…ë‹ˆë‹¤:\n+\n+*í˜„ì¬ ë”¥ëŸ¬ë‹ì—ì„œ í¥ë¯¸ë¡œìš´ ì‘ìš© í”„ë¡œê·¸ë¨ì„ êµ¬ë™í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ê¸°ì´ˆ ëª¨ë¸ë“¤ì€ ê±°ì˜ ë³´í¸ì ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì™€ ê·¸ í•µì‹¬ ì–´í…ì…˜ ëª¨ë“ˆì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. ì„ í˜• ì–´í…ì…˜, ê²Œì´íŠ¸ëœ ì»¨ë³¼ë£¨ì…˜ê³¼ ìˆœí™˜ ëª¨ë¸, êµ¬ì¡°í™”ëœ ìƒíƒœ ê³µê°„ ëª¨ë¸(SSM) ë“± ë§ì€ ì¤€ì´ì°¨ì‹œê°„(subquadratic-time) ì•„í‚¤í…ì²˜ê°€ ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê³„ì‚° ë¹„íš¨ìœ¨ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œë˜ì—ˆì§€ë§Œ, ì–¸ì–´ì™€ ê°™ì€ ì¤‘ìš”í•œ ì–‘ì‹ì—ì„œëŠ” ì–´í…ì…˜ë§Œí¼ ì„±ëŠ¥ì„ ë‚´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì˜ ì£¼ìš” ì•½ì ì´ ë‚´ìš© ê¸°ë°˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì§€ ëª»í•œë‹¤ëŠ” ì ì„ì„ ì•Œê³  ëª‡ ê°€ì§€ë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ì²«ì§¸, SSM ë§¤ê°œë³€ìˆ˜ë¥¼ ì…ë ¥ì˜ í•¨ìˆ˜ë¡œ ë§Œë“œëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì´ì‚° ëª¨ë‹¬ë¦¬í‹°(discrete modalities)ì˜ ì•½ì ì„ í•´ê²°í•  ìˆ˜ ìˆì–´, í˜„ì¬ í† í°ì— ë”°ë¼ ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¨ì›ì„ ë”°ë¼ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ì „íŒŒí•˜ê±°ë‚˜ ìŠì„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ì´ëŸ¬í•œ ë³€ê²½ìœ¼ë¡œ íš¨ìœ¨ì ì¸ ì»¨ë³¼ë£¨ì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê²Œ ë˜ì—ˆì§€ë§Œ, ìš°ë¦¬ëŠ” ìˆœí™˜ ëª¨ë“œì—ì„œ í•˜ë“œì›¨ì–´ë¥¼ ì¸ì‹í•˜ëŠ” ë³‘ë ¬ ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì„ íƒì  SSMì„ ì–´í…ì…˜ì´ë‚˜ MLP ë¸”ë¡ë„ ì—†ëŠ” ë‹¨ìˆœí™”ëœ ì¢…ë‹¨ê°„ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì¸ ë§˜ë°”ì— í†µí•©ì‹œì¼°ìŠµë‹ˆë‹¤. ë§˜ë°”ëŠ” ë¹ ë¥¸ ì¶”ë¡ (íŠ¸ëœìŠ¤í¬ë¨¸ë³´ë‹¤ 5ë°° ë†’ì€ ì²˜ë¦¬ëŸ‰)ê³¼ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ëŒ€í•œ ì„ í˜• í™•ì¥ì„±ì„ ëˆ„ë¦¬ë©°, ë°±ë§Œ ê¸¸ì´ ì‹œí€€ìŠ¤ê¹Œì§€ ì‹¤ì œ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì‹œí€€ìŠ¤ ëª¨ë¸ ë°±ë³¸ìœ¼ë¡œì„œ ë§˜ë°”ëŠ” ì–¸ì–´, ì˜¤ë””ì˜¤, ìœ ì „ì²´í•™ê³¼ ê°™ì€ ì—¬ëŸ¬ ì–‘ì‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì–¸ì–´ ëª¨ë¸ë§ì—ì„œ ìš°ë¦¬ì˜ ë§˜ë°”-3B ëª¨ë¸ì€ ê°™ì€ í¬ê¸°ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ ëŠ¥ê°€í•˜ê³  ë‘ ë°° í¬ê¸°ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ë§ë¨¹ëŠ” ì„±ëŠ¥ì„ ë³´ì´ë©°, ì‚¬ì „ í›ˆë ¨ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.*\n+\n+íŒ:\n+\n+- ë§˜ë°”ëŠ” ê³ ì „ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ê²¬ì¤„ ë§Œí•œ ìƒˆë¡œìš´ `ìƒíƒœ ê³µê°„ ëª¨ë¸` ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. ì´ëŠ” êµ¬ì¡°í™”ëœ ìƒíƒœ ê³µê°„ ëª¨ë¸ì˜ ë°œì „ ì„ ìƒì— ìˆìœ¼ë©°, [í”Œë˜ì‹œì–´í…ì…˜](https://github.com/Dao-AILab/flash-attention)ì˜ ì •ì‹ ì„ ë”°ë¥´ëŠ” íš¨ìœ¨ì ì¸ í•˜ë“œì›¨ì–´ ì¸ì‹ ì„¤ê³„ì™€ êµ¬í˜„ì„ íŠ¹ì§•ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n+- ë§˜ë°”ëŠ” `ì–´í…ì…˜` ë ˆì´ì–´ì™€ ë™ë“±í•œ `ë¯¹ì„œ(mixer)` ë ˆì´ì–´ë¥¼ ìŒ“ìŠµë‹ˆë‹¤. `ë§˜ë°”`ì˜ í•µì‹¬ ë¡œì§ì€ `MambaMixer` í´ë˜ìŠ¤ì— ìˆìŠµë‹ˆë‹¤.\n+- ë‘ ê°€ì§€ êµ¬í˜„ì´ ê³µì¡´í•©ë‹ˆë‹¤: í•˜ë‚˜ëŠ” ìµœì í™”ë˜ì–´ ë¹ ë¥¸ cudaì»¤ë„ì„ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹¨ìˆœí•˜ì§€ë§Œ ëª¨ë“  ì¥ì¹˜ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n+- í˜„ì¬ êµ¬í˜„ì€ ì›ë³¸ cudaì»¤ë„ì„ í™œìš©í•©ë‹ˆë‹¤: ë§˜ë°”ë¥¼ ìœ„í•œ í”Œë˜ì‹œ ì–´í…ì…˜ì˜ ì—­í• ì„ í•˜ëŠ” ê²ƒì€ [`mamba-ssm`](https://github.com/state-spaces/mamba)ì™€ [`causal_conv1d`](https://github.com/Dao-AILab/causal-conv1d) ì €ì¥ì†Œì— í˜¸ìŠ¤íŒ…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•˜ë“œì›¨ì–´ê°€ ì§€ì›í•œë‹¤ë©´ ë°˜ë“œì‹œ ì„¤ì¹˜í•˜ì„¸ìš”!\n+- cuda ì»¤ë„ì„ ìµœì í™”í•˜ëŠ” ë°©í–¥ ë³´ë‹¤ëŠ”, ë‹¨ìˆœí•˜ì§€ë§Œ ëª¨ë“  ì¥ì¹˜ì—ì„œ ì‹¤í–‰ê°€ëŠ¥í•˜ë„ë¡í•˜ëŠ” ë°©í–¥ì¸ 'ë‹¨ìˆœêµ¬í˜„'ì˜ ì„±ëŠ¥ì„ ë¹ ë¥´ê²Œ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ì—¬ë¥¼ ë” í™˜ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ¤—\n+\n+ì´ ëª¨ë¸ì€ [ArthurZ](https://huggingface.co/ArthurZ)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n+ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/state-spaces/mamba)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+# ì‚¬ìš©\n+\n+### ê°„ë‹¨í•œ ìƒì„± ì˜ˆì œ\n+ \n+```python \n+from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n+import torch\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n+model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n+input_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n+\n+out = model.generate(input_ids, max_new_tokens=10)\n+print(tokenizer.batch_decode(out))\n+```\n+\n+### Peft íŒŒì¸íŠœë‹\n+ëŠë¦° ë²„ì „ì€ í•™ìŠµì—ì„œ ì•„ì£¼ ì•ˆì •ì ì´ì§„ ì•ŠìŠµë‹ˆë‹¤. ë¹ ë¥¸ ë²„ì „ì€ `float32`ê°€ í•„ìš”í•©ë‹ˆë‹¤!\n+\n+```python \n+from datasets import load_dataset\n+from trl import SFTTrainer\n+from peft import LoraConfig\n+from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n+model_id = \"state-spaces/mamba-130m-hf\"\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id)\n+dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\")\n+training_args = TrainingArguments(\n+    output_dir=\"./results\",\n+    num_train_epochs=3,\n+    per_device_train_batch_size=4,\n+    logging_dir='./logs',\n+    logging_steps=10,\n+    learning_rate=2e-3\n+)\n+lora_config =  LoraConfig(\n+        r=8,\n+        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n+        task_type=\"CAUSAL_LM\",\n+        bias=\"none\"\n+)\n+trainer = SFTTrainer(\n+    model=model,\n+    tokenizer=tokenizer,\n+    args=training_args,\n+    peft_config=lora_config,\n+    train_dataset=dataset,\n+    dataset_text_field=\"quote\",\n+)\n+trainer.train()\n+```\n+\n+## MambaConfig\n+\n+[[autodoc]] MambaConfig\n+\n+## MambaModel\n+\n+[[autodoc]] MambaModel\n+    - forward\n+\n+## MambaLMHeadModel\n+\n+[[autodoc]] MambaForCausalLM\n+    - forward"
        }
    ],
    "stats": {
        "total": 107,
        "additions": 107,
        "deletions": 0
    }
}