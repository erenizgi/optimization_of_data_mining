{
    "author": "SamuelBarryCS",
    "message": "Add Fast PromptDepthAnything Processor (#40602)\n\n* Test & import setup\n\n* First version passing tests\n\n* Ruff\n\n* Dummy post processing\n\n* Add numerical test\n\n* Adjust\n\n* Doc\n\n* Ruff\n\n* remove unused arg\n\n* Refine interpolation method and push test script\n\n* update bench\n\n* Comments\n\n* Update src/transformers/models/auto/image_processing_auto.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Remove benchmrk script\n\n* Update docstrings\n\n* Update src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Update src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* doc\n\n* further process kwargs\n\n* remove it\n\n* remove\n\n* Remove to dict\n\n* remove crop middle\n\n* Remove param specific handling\n\n* Update testing logic\n\n* remove ensure multiple of as kwargs\n\n* fix formatting\n\n* Remove none default and get image size\n\n* Move stuff to _preprocess_image_like_inputs and refacto\n\n* Clean\n\n* ruff\n\n* End of file & comments\n\n* ruff again\n\n* Padding fixed\n\n* Remove comments to pass tests\n\n* Remove prompt depth from kwargs\n\n* Adjust output_size logic\n\n* Docstring for preprocess\n\n* auto_docstring for preprocess\n\n* pass as an arg\n\n* update test batched\n\n* stack images\n\n* remove prompt scale to meter\n\n* return tensors back in preprocess\n\n* remove copying of images\n\n* Update behavior to match old processoer\n\n* Fix batch size of tests\n\n* fix test and fast\n\n* Fix slow processor\n\n* Put tests back to pytorch\n\n* remove check and modify batched tests\n\n* test do_pad + slow processor fix\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "ff26fe83023ed88b6473208c715cce5e079ac8e5",
    "files": [
        {
            "sha": "5af13c5d630e5dc15ca67913d089cae4a4e28b50",
            "filename": "docs/source/en/model_doc/prompt_depth_anything.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff26fe83023ed88b6473208c715cce5e079ac8e5/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff26fe83023ed88b6473208c715cce5e079ac8e5/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md?ref=ff26fe83023ed88b6473208c715cce5e079ac8e5",
            "patch": "@@ -93,5 +93,11 @@ If you are interested in submitting a resource to be included here, please feel\n ## PromptDepthAnythingImageProcessor\n \n [[autodoc]] PromptDepthAnythingImageProcessor\n+    - preprocess\n+    - post_process_depth_estimation\n+\n+## PromptDepthAnythingImageProcessorFast\n+\n+[[autodoc]] PromptDepthAnythingImageProcessorFast\n     - preprocess\n     - post_process_depth_estimation\n\\ No newline at end of file"
        },
        {
            "sha": "ebaa4a30849d84c7c0499878db92be20adf0c766",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=ff26fe83023ed88b6473208c715cce5e079ac8e5",
            "patch": "@@ -151,7 +151,7 @@\n             (\"pix2struct\", (\"Pix2StructImageProcessor\", None)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"poolformer\", (\"PoolFormerImageProcessor\", \"PoolFormerImageProcessorFast\")),\n-            (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\", None)),\n+            (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\", \"PromptDepthAnythingImageProcessorFast\")),\n             (\"pvt\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"pvt_v2\", (\"PvtImageProcessor\", \"PvtImageProcessorFast\")),\n             (\"qwen2_5_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),"
        },
        {
            "sha": "0f2206150f0ac1d5aba781c2aee6654dd03023fe",
            "filename": "src/transformers/models/prompt_depth_anything/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2F__init__.py?ref=ff26fe83023ed88b6473208c715cce5e079ac8e5",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_prompt_depth_anything import PromptDepthAnythingConfig\n     from .image_processing_prompt_depth_anything import PromptDepthAnythingImageProcessor\n+    from .image_processing_prompt_depth_anything_fast import PromptDepthAnythingImageProcessorFast\n     from .modeling_prompt_depth_anything import (\n         PromptDepthAnythingForDepthEstimation,\n         PromptDepthAnythingPreTrainedModel,"
        },
        {
            "sha": "a5fad19b1a1b4f8c4b1e6c33bca19cbd319fa3f6",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=ff26fe83023ed88b6473208c715cce5e079ac8e5",
            "patch": "@@ -13,7 +13,6 @@\n \"\"\"Image processor class for PromptDepthAnything.\"\"\"\n \n import math\n-from collections.abc import Iterable\n from typing import TYPE_CHECKING, Optional, Union\n \n \n@@ -68,13 +67,11 @@ def _constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n \n def _get_resize_output_image_size(\n     input_image: np.ndarray,\n-    output_size: Union[int, Iterable[int]],\n+    output_size: tuple[int, int],\n     keep_aspect_ratio: bool,\n     multiple: int,\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n ) -> tuple[int, int]:\n-    output_size = (output_size, output_size) if isinstance(output_size, int) else output_size\n-\n     input_height, input_width = get_image_size(input_image, input_data_format)\n     output_height, output_width = output_size\n \n@@ -266,11 +263,11 @@ def _get_pad(size, size_divisor):\n \n         height, width = get_image_size(image, input_data_format)\n \n-        pad_size_left, pad_size_right = _get_pad(height, size_divisor)\n-        pad_size_top, pad_size_bottom = _get_pad(width, size_divisor)\n+        pad_size_top, pad_size_bottom = _get_pad(height, size_divisor)\n+        pad_size_left, pad_size_right = _get_pad(width, size_divisor)\n \n         padded_image = pad(\n-            image, ((pad_size_left, pad_size_right), (pad_size_top, pad_size_bottom)), data_format=data_format\n+            image, ((pad_size_top, pad_size_bottom), (pad_size_left, pad_size_right)), data_format=data_format\n         )\n         return padded_image\n \n@@ -452,7 +449,8 @@ def preprocess(\n                     # We can simply select one pixel and set it to a small value.\n                     depth[0, 0] = depth[0, 0] + 1e-6\n                 depth = depth[..., None].astype(np.float32)\n-                depth = to_channel_dimension_format(depth, data_format, input_channel_dim=input_data_format)\n+                # Always use LAST as input format since we add channel dim with [..., None]\n+                depth = to_channel_dimension_format(depth, data_format, input_channel_dim=ChannelDimension.LAST)\n \n                 processed_prompt_depths.append(depth)\n             prompt_depths = processed_prompt_depths"
        },
        {
            "sha": "4cb6c6732e90e46abb3ce4a7c75f545c0cac6ed4",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py",
            "status": "added",
            "additions": 379,
            "deletions": 0,
            "changes": 379,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff26fe83023ed88b6473208c715cce5e079ac8e5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py?ref=ff26fe83023ed88b6473208c715cce5e079ac8e5",
            "patch": "@@ -0,0 +1,379 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for PromptDepthAnything.\"\"\"\n+\n+import math\n+from typing import TYPE_CHECKING, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...processing_utils import Unpack\n+\n+\n+if TYPE_CHECKING:\n+    from ...modeling_outputs import DepthEstimatorOutput\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    requires_backends,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+def _constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n+    \"\"\"Constrain a value to be a multiple of another value.\"\"\"\n+    x = round(val / multiple) * multiple\n+\n+    if max_val is not None and x > max_val:\n+        x = math.floor(val / multiple) * multiple\n+\n+    if x < min_val:\n+        x = math.ceil(val / multiple) * multiple\n+\n+    return x\n+\n+\n+def _get_resize_output_image_size(\n+    input_image: \"torch.Tensor\",\n+    output_size: tuple[int, int],\n+    keep_aspect_ratio: bool,\n+    multiple: int,\n+) -> tuple[int, int]:\n+    \"\"\"Get the output size for resizing an image.\"\"\"\n+    input_height, input_width = input_image.shape[-2:]\n+    output_height, output_width = output_size\n+\n+    # determine new height and width\n+    scale_height = output_height / input_height\n+    scale_width = output_width / input_width\n+\n+    if keep_aspect_ratio:\n+        # scale as little as possible\n+        if abs(1 - scale_width) < abs(1 - scale_height):\n+            # fit width\n+            scale_height = scale_width\n+        else:\n+            # fit height\n+            scale_width = scale_height\n+\n+    new_height = _constrain_to_multiple_of(scale_height * input_height, multiple=multiple)\n+    new_width = _constrain_to_multiple_of(scale_width * input_width, multiple=multiple)\n+\n+    return (new_height, new_width)\n+\n+\n+class PromptDepthAnythingFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    keep_aspect_ratio (`bool`, *optional*):\n+        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.\n+    ensure_multiple_of (`int`, *optional*):\n+        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value.\n+    do_pad (`bool`, *optional*):\n+        Whether to apply center padding.\n+    size_divisor (`int`, *optional*):\n+        If `do_pad` is `True`, pads the image dimensions to be divisible by this value.\n+    prompt_scale_to_meter (`float`, *optional*):\n+        Scale factor to convert the prompt depth to meters.\n+    \"\"\"\n+\n+    keep_aspect_ratio: Optional[bool]\n+    ensure_multiple_of: Optional[int]\n+    do_pad: Optional[bool]\n+    size_divisor: Optional[int]\n+    prompt_scale_to_meter: Optional[float]\n+\n+\n+@auto_docstring\n+class PromptDepthAnythingImageProcessorFast(BaseImageProcessorFast):\n+    model_input_names = [\"pixel_values\", \"prompt_depth\"]\n+\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    keep_aspect_ratio = False\n+    ensure_multiple_of = 1\n+    do_pad = False\n+    size_divisor = None\n+    prompt_scale_to_meter = 0.001\n+    valid_kwargs = PromptDepthAnythingFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[PromptDepthAnythingFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        prompt_depth: Optional[ImageInput] = None,\n+        **kwargs: Unpack[PromptDepthAnythingFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        prompt_depth (`ImageInput`, *optional*):\n+            Prompt depth to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, prompt_depth, **kwargs)\n+\n+    def resize_with_aspect_ratio(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        keep_aspect_ratio: bool = False,\n+        ensure_multiple_of: int = 1,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to target size while optionally maintaining aspect ratio and ensuring dimensions are multiples.\n+        \"\"\"\n+        # Set default interpolation to BICUBIC to match the slow processor (causes slight numerical differences otherwise)\n+        if interpolation is None:\n+            interpolation = F.InterpolationMode.BICUBIC\n+\n+        # Custom resize with aspect ratio preservation and ensure_multiple_of constraint\n+        output_size = _get_resize_output_image_size(\n+            image,\n+            output_size=(size[\"height\"], size[\"width\"]),\n+            keep_aspect_ratio=keep_aspect_ratio,\n+            multiple=ensure_multiple_of,\n+        )\n+\n+        # Standard resize method with calculated output size\n+        return self.resize(\n+            image=image,\n+            size=SizeDict(height=output_size[0], width=output_size[1]),\n+            interpolation=interpolation,\n+        )\n+\n+    def pad_image(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Center pad an image to be a multiple of size_divisor.\n+        \"\"\"\n+\n+        def _get_pad(size, size_divisor):\n+            new_size = math.ceil(size / size_divisor) * size_divisor\n+            pad_size = new_size - size\n+            pad_size_left = pad_size // 2\n+            pad_size_right = pad_size - pad_size_left\n+            return pad_size_left, pad_size_right\n+\n+        height, width = image.shape[-2:]\n+\n+        # Match slow processor and PyTorch convention: width->left/right, height->top/bottom\n+        pad_size_left, pad_size_right = _get_pad(width, size_divisor)\n+        pad_size_top, pad_size_bottom = _get_pad(height, size_divisor)\n+\n+        # Use torchvision padding for fast processing\n+        # /!\\ NB: torchvision F.pad expects (left, top, right, bottom) for the last two dims (W then H)\n+        # Source: https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Pad.html\n+        # So: (left=width_pad, top=height_pad, right=width_pad, bottom=height_pad)\n+        padding = [pad_size_left, pad_size_top, pad_size_right, pad_size_bottom]\n+        padded_image = F.pad(image, padding=padding)\n+\n+        return padded_image\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        prompt_depth: Optional[ImageInput],\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        prompt_scale_to_meter: Optional[float] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs: Unpack[PromptDepthAnythingFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs, including the main images and optional prompt depth.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=False, input_data_format=input_data_format, device=device\n+        )  # always use do_convert_rgb=False rather than defining it as a param to match slow processor\n+\n+        # Process images with the standard pipeline\n+        pixel_values = self._preprocess(images, return_tensors=return_tensors, **kwargs)\n+\n+        data = {\"pixel_values\": pixel_values}\n+\n+        # Process prompt depth if provided\n+        if prompt_depth is not None:\n+            processed_prompt_depths = self._prepare_image_like_inputs(\n+                images=prompt_depth,\n+                do_convert_rgb=False,  # Depth maps should not be converted\n+                input_data_format=input_data_format,\n+                device=images[0].device if images else device,\n+                expected_ndims=2,\n+            )\n+\n+            # Validate prompt_depths has same length as images as in slow processor\n+            if len(processed_prompt_depths) != len(images):\n+                raise ValueError(\n+                    f\"Number of prompt depth images ({len(processed_prompt_depths)}) does not match number of input images ({len(images)})\"\n+                )\n+\n+            final_prompt_depths = []\n+            for depth in processed_prompt_depths:\n+                depth = depth * prompt_scale_to_meter\n+\n+                # Handle case where depth is constant (min == max)\n+                if depth.min() == depth.max():\n+                    depth[0, 0] = depth[0, 0] + 1e-6  # Add small variation to avoid numerical issues\n+\n+                if depth.ndim == 2:  # Add channel dimension if needed\n+                    depth = depth.unsqueeze(0)  # [H, W] -> [1, H, W] (channels first)\n+\n+                depth = depth.float()  # Convert to float32 to match slow processor\n+                final_prompt_depths.append(depth)\n+\n+            if return_tensors:\n+                # Stack while preserving the [H, W, C] format that the slow processor uses\n+                final_prompt_depths = torch.stack(final_prompt_depths, dim=0)\n+\n+            data[\"prompt_depth\"] = final_prompt_depths\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        keep_aspect_ratio: Optional[bool],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        ensure_multiple_of: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        size_divisor: Optional[int] = None,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Override the base _preprocess method to handle custom PromptDepthAnything parameters.\n+        \"\"\"\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize_with_aspect_ratio(\n+                    image=stacked_images,\n+                    size=size,\n+                    keep_aspect_ratio=keep_aspect_ratio,\n+                    ensure_multiple_of=ensure_multiple_of,\n+                    interpolation=interpolation,\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+\n+            if do_pad and size_divisor is not None:\n+                stacked_images = self.pad_image(stacked_images, size_divisor)\n+\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Only stack tensors if they all have the same shape and return_tensors is specified\n+        if return_tensors == \"pt\":\n+            processed_images = torch.stack(processed_images, dim=0)\n+\n+        return processed_images\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"DepthEstimatorOutput\",\n+        target_sizes: Optional[Union[TensorType, list[tuple[int, int]], None]] = None,\n+    ) -> list[dict[str, TensorType]]:\n+        \"\"\"\n+        Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images.\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`TensorType` or `list[tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+\n+        Returns:\n+            `list[dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+\n+        if (target_sizes is not None) and (len(predicted_depth) != len(target_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\"\n+            )\n+\n+        results = []\n+        target_sizes = [None] * len(predicted_depth) if target_sizes is None else target_sizes\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                depth = torch.nn.functional.interpolate(\n+                    depth.unsqueeze(0).unsqueeze(1), size=target_size, mode=\"bicubic\", align_corners=False\n+                ).squeeze()\n+\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"PromptDepthAnythingImageProcessorFast\"]"
        },
        {
            "sha": "99bbcdb54d87509a9da82fe4100752e2cd003cdb",
            "filename": "tests/models/prompt_depth_anything/test_image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 94,
            "deletions": 26,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff26fe83023ed88b6473208c715cce5e079ac8e5/tests%2Fmodels%2Fprompt_depth_anything%2Ftest_image_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff26fe83023ed88b6473208c715cce5e079ac8e5/tests%2Fmodels%2Fprompt_depth_anything%2Ftest_image_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprompt_depth_anything%2Ftest_image_processing_prompt_depth_anything.py?ref=ff26fe83023ed88b6473208c715cce5e079ac8e5",
            "patch": "@@ -17,15 +17,18 @@\n \n import numpy as np\n \n-from transformers.file_utils import is_vision_available\n from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import PromptDepthAnythingImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import PromptDepthAnythingImageProcessorFast\n+\n \n class PromptDepthAnythingImageProcessingTester(unittest.TestCase):\n     def __init__(\n@@ -84,6 +87,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class PromptDepthAnythingImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = PromptDepthAnythingImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = PromptDepthAnythingImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -94,45 +98,109 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n-        self.assertTrue(hasattr(image_processing, \"prompt_scale_to_meter\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+            self.assertTrue(hasattr(image_processing, \"prompt_scale_to_meter\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_keep_aspect_ratio(self):\n         size = {\"height\": 512, \"width\": 512}\n-        image_processor = PromptDepthAnythingImageProcessor(size=size, keep_aspect_ratio=True, ensure_multiple_of=32)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(size=size, keep_aspect_ratio=True, ensure_multiple_of=32)\n \n-        image = np.zeros((489, 640, 3))\n+            image = np.zeros((489, 640, 3))\n \n-        pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+            pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n-        self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n+            self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n \n     def test_prompt_depth_processing(self):\n         size = {\"height\": 756, \"width\": 756}\n-        image_processor = PromptDepthAnythingImageProcessor(size=size, keep_aspect_ratio=True, ensure_multiple_of=32)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(size=size, keep_aspect_ratio=True, ensure_multiple_of=32)\n+\n+            image = np.zeros((756, 1008, 3))\n+            prompt_depth = np.random.random((192, 256))\n+\n+            outputs = image_processor(image, prompt_depth=prompt_depth, return_tensors=\"pt\")\n+            pixel_values = outputs.pixel_values\n+            prompt_depth_values = outputs.prompt_depth\n+\n+            self.assertEqual(list(pixel_values.shape), [1, 3, 768, 1024])\n+            self.assertEqual(list(prompt_depth_values.shape), [1, 1, 192, 256])\n+\n+    @require_torch\n+    @require_vision\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n         image = np.zeros((756, 1008, 3))\n         prompt_depth = np.random.random((192, 256))\n \n-        outputs = image_processor(image, prompt_depth=prompt_depth, return_tensors=\"pt\")\n-        pixel_values = outputs.pixel_values\n-        prompt_depth_values = outputs.prompt_depth\n+        size = {\"height\": 756, \"width\": 756}\n+        image_processor_slow = self.image_processing_class(\n+            size=size, keep_aspect_ratio=True, ensure_multiple_of=32, do_pad=True, size_divisor=51\n+        )\n+        image_processor_fast = self.fast_image_processing_class(\n+            size=size, keep_aspect_ratio=True, ensure_multiple_of=32, do_pad=True, size_divisor=51\n+        )\n+\n+        encoding_slow = image_processor_slow(image, prompt_depth=prompt_depth, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(image, prompt_depth=prompt_depth, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self.assertEqual(encoding_slow.prompt_depth.dtype, encoding_fast.prompt_depth.dtype)\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.prompt_depth, encoding_fast.prompt_depth)\n+\n+    @require_torch\n+    @require_vision\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        batch_size = self.image_processor_tester.batch_size\n+        images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        prompt_depths = [np.random.random((192, 256)) for _ in range(batch_size)]\n+\n+        size = {\"height\": 756, \"width\": 756}\n+        image_processor_slow = self.image_processing_class(size=size, keep_aspect_ratio=False, ensure_multiple_of=32)\n+        image_processor_fast = self.fast_image_processing_class(\n+            size=size, keep_aspect_ratio=False, ensure_multiple_of=32\n+        )\n+\n+        encoding_slow = image_processor_slow(images, prompt_depth=prompt_depths, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(images, prompt_depth=prompt_depths, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self.assertEqual(encoding_slow.prompt_depth.dtype, encoding_fast.prompt_depth.dtype)\n \n-        self.assertEqual(list(pixel_values.shape), [1, 3, 768, 1024])\n-        self.assertEqual(list(prompt_depth_values.shape), [1, 1, 192, 256])\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.prompt_depth, encoding_fast.prompt_depth)"
        }
    ],
    "stats": {
        "total": 522,
        "additions": 487,
        "deletions": 35
    }
}