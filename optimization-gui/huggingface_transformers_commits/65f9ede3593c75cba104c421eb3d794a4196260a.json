{
    "author": "ydshieh",
    "message": "Set seed for `Glm4vIntegrationTest` (#40905)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "65f9ede3593c75cba104c421eb3d794a4196260a",
    "files": [
        {
            "sha": "6c3845b10e8856f3340cf536d36d62163517ea19",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 40,
            "deletions": 14,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/65f9ede3593c75cba104c421eb3d794a4196260a/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65f9ede3593c75cba104c421eb3d794a4196260a/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=65f9ede3593c75cba104c421eb3d794a4196260a",
            "patch": "@@ -282,6 +282,8 @@ def test_inputs_embeds_matches_input_ids(self):\n @require_torch\n class Glm4vIntegrationTest(unittest.TestCase):\n     def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n         self.processor = AutoProcessor.from_pretrained(\"THUDM/GLM-4.1V-9B-Thinking\")\n         self.message = [\n             {\n@@ -340,8 +342,11 @@ def test_small_model_integration_test(self):\n         # verify generation\n         inputs = inputs.to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         output = model.generate(**inputs, max_new_tokens=30)\n-        EXPECTED_DECODED_TEXT = \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\"\n+        EXPECTED_DECODED_TEXT = \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\"\n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -357,12 +362,15 @@ def test_small_model_integration_test_batch(self):\n             batch_messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n         ).to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\"\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture has a stocky body, thick fur, and a face that's\"\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -395,10 +403,13 @@ def test_small_model_integration_test_with_video(self):\n         inputs = processor.apply_chat_template(\n             messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\", padding=True\n         ).to(torch_device)\n+\n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         output = model.generate(**inputs, max_new_tokens=30)\n-        EXPECTED_DECODED_TEXT = [\n-            \"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is an indoor tennis court. There are two players: one in the foreground wearing\"\n-        ]  # fmt: skip\n+        EXPECTED_DECODED_TEXT = [\"\\n012345Describe this video.\\n<think>Got it, let's analyze the video. First, the scene is an indoor tennis court. There are two players: one in a white shirt\"]  # fmt: skip\n+\n         self.assertEqual(\n             processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -413,6 +424,9 @@ def test_small_model_integration_test_expand(self):\n             self.message, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n         ).to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n \n         EXPECTED_DECODED_TEXT = [\n@@ -442,12 +456,15 @@ def test_small_model_integration_test_batch_wo_image(self):\n             padding=True,\n         ).to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n-            '\\nWho are you?\\n<think>Got it, the user is asking \"Who are you?\" I need to respond appropriately. First, I should clarify that I\\'m an AI assistant'\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+            \"\\nWho are you?\\n<think>Got it, let's look at the user's question: \\\"Who are you?\\\" This is a common question when someone is just starting a conversation\"\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -469,12 +486,15 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n             padding=True,\n         ).to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\"\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\",\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -501,12 +521,15 @@ def test_small_model_integration_test_batch_flashatt2(self):\n             padding=True,\n         ).to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog. Wait, it's a cat,\",\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. Wait, the animals here are cats, not dogs. The question is about a dog, but\"\n         ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n@@ -536,12 +559,15 @@ def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n             padding=True,\n         ).to(torch_device)\n \n+        # This model on the hub has `do_sample=True`.\n+        torch.manual_seed(42)\n+\n         # it should not matter whether two images are the same size or not\n         output = model.generate(**inputs, max_new_tokens=30)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n-            '\\nWho are you?\\n<think>Got it, let\\'s look at the question. The user is asking \"Who are you?\" which is a common question when someone meets an AI'\n+            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+            \"\\nWho are you?\\n<think>Got it, let's look at the user's question: \\\"Who are you?\\\" This is a common question when someone is just starting a conversation\"\n         ]  # fmt: skip\n \n         self.assertEqual("
        }
    ],
    "stats": {
        "total": 54,
        "additions": 40,
        "deletions": 14
    }
}