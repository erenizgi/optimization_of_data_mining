{
    "author": "3outeille",
    "message": "V4.57.1 training ci: Refactor `test_tensor_parallel.py` (#41918)\n\n* refactor test to not depends on subprocess (this way we can easily debug test with breakpoint)\n\n* make test more robust by testing on more process (2 4 8)\n\n* remove 8 gpus tests because llama is too tiny to apply TP then => RuntimeError. This will imply bigger llama for test but since TP=2/4 works already, no need\n\n* linting",
    "sha": "4418728dfa815fe3163d27c43f1b16135bd75528",
    "files": [
        {
            "sha": "35fb538fff7acae28e2185438288265a1f839ca5",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 215,
            "deletions": 200,
            "changes": 415,
            "blob_url": "https://github.com/huggingface/transformers/blob/4418728dfa815fe3163d27c43f1b16135bd75528/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4418728dfa815fe3163d27c43f1b16135bd75528/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=4418728dfa815fe3163d27c43f1b16135bd75528",
            "patch": "@@ -12,26 +12,68 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-# Run the test: CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/tensor_parallel/test_tensor_parallel.py\n+# Run all tests: RUN_SLOW=1 pytest -v tests/tensor_parallel/test_tensor_parallel.py\n+# Run specific config: RUN_SLOW=1 pytest -v tests/tensor_parallel/test_tensor_parallel.py -k \"2Proc\"\n+# Run multiple configs: RUN_SLOW=1 pytest -v tests/tensor_parallel/test_tensor_parallel.py -k \"2Proc or 4Proc\"\n+# Run spefic test: RUN_SLOW=1 pytest -v tests/tensor_parallel/test_tensor_parallel.py::TestTensorParallel2Proc::test_model_forward\n \n import os\n import tempfile\n-import textwrap\n+import warnings\n \n-from transformers import is_torch_available\n+from safetensors import safe_open\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, is_torch_available\n from transformers.integrations.tensor_parallel import get_packed_weights, repack_weights\n from transformers.testing_utils import (\n     TestCasePlus,\n     backend_device_count,\n+    get_torch_dist_unique_port,\n     require_huggingface_hub_greater_or_equal,\n     require_torch_multi_accelerator,\n     torch_device,\n-    torchrun,\n )\n \n \n if is_torch_available():\n     import torch\n+    import torch.multiprocessing as mp\n+\n+\n+def global_wrapper(rank, func, tp, port, func_args, func_kwargs):\n+    def setup_dist_env(rank, world_size, port):\n+        os.environ[\"WORLD_SIZE\"] = str(world_size)\n+        os.environ[\"RANK\"] = str(rank)\n+        os.environ[\"LOCAL_RANK\"] = str(rank)\n+        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n+        os.environ[\"MASTER_PORT\"] = str(port)\n+\n+    world_size = tp\n+    setup_dist_env(rank, world_size, port)\n+\n+    if torch.cuda.is_available():\n+        torch.cuda.set_device(rank)\n+        torch.distributed.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n+    else:\n+        torch.distributed.init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n+\n+    func(rank, *func_args, **func_kwargs)\n+\n+    torch.distributed.barrier()\n+    torch.distributed.destroy_process_group()\n+\n+\n+def init_distributed(tp: int):\n+    def _init_distributed(func):\n+        def wrapper(*args, **kwargs):\n+            world_size = tp\n+            port = get_torch_dist_unique_port()\n+            spawn_args = (func, tp, port, args, kwargs)\n+            mp.spawn(global_wrapper, args=spawn_args, nprocs=world_size)\n+\n+        return wrapper\n+\n+    return _init_distributed\n \n \n class TestTensorParallelUtils(TestCasePlus):\n@@ -63,191 +105,9 @@ def size(self):\n         assert torch.allclose(unpacked_weights, original_packed_weights)\n \n \n-class TestTensorParallel(TestCasePlus):\n-    nproc_per_node = 2\n-\n-    def test_model_forward(self):\n-        script_to_run = textwrap.dedent(\n-            \"\"\"\n-            import torch\n-            import os\n-            from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-            model_id = \"JackFram/llama-68m\"\n-\n-            model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", tp_plan=\"auto\")\n-            torch.distributed.barrier()\n-\n-            has_dtensor = 0\n-            for name, parameter in model.named_parameters():\n-                if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n-                    has_dtensor = 1\n-                    break\n-\n-            assert has_dtensor == 1, \"TP model must has DTensor\"\n-\n-            tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n-            prompt = \"Can I help\"\n-\n-            inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n-            outputs = model(inputs)\n-\n-            next_token_logits = outputs[0][:, -1, :]\n-            next_token = torch.argmax(next_token_logits, dim=-1)\n-            response = tokenizer.decode(next_token)\n-            assert response == \"with\"\n-\n-            torch.distributed.barrier()\n-            torch.distributed.destroy_process_group()\n-            \"\"\"\n-        )\n-        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n-\n-    def test_model_backward_pass(self):\n-        script_to_run = textwrap.dedent(\n-            \"\"\"\n-            import torch\n-            import os\n-            from transformers import AutoModelForCausalLM\n-            from torch import nn\n-\n-            model_id = \"JackFram/llama-68m\"\n-\n-            model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float32, tp_plan=\"auto\")\n-            torch.distributed.barrier()\n-\n-            # Dummy forward and backward pass\n-            # Note that loss.backward() will fail if there is a bug in the TP implementation\n-            inputs = torch.randint(0, model.config.vocab_size, (2, 10), device=model.device)\n-            labels = torch.randint(0, model.config.vocab_size, (2, 10), device=model.device)\n-            loss = model(inputs, labels=labels).loss\n-            loss.backward()\n-\n-            torch.distributed.barrier()\n-            torch.distributed.destroy_process_group()\n-            \"\"\"\n-        )\n-        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n-\n-    def test_model_generate(self):\n-        script_to_run = textwrap.dedent(\n-            \"\"\"\n-            import torch\n-            import os\n-            from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-            model_id = \"JackFram/llama-68m\"\n-\n-            model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", tp_plan=\"auto\")\n-            torch.distributed.barrier()\n-\n-            model.forward = torch.compile(model.forward)\n-\n-            has_dtensor = 0\n-            for name, parameter in model.named_parameters():\n-                if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n-                    has_dtensor = 1\n-                    break\n-\n-            assert has_dtensor == 1, \"TP model must have DTensor\"\n-\n-            tokenizer = AutoTokenizer.from_pretrained(model_id)\n-            prompt = \"Can I help\"\n-\n-            inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n-            outputs = model.generate(inputs, max_new_tokens=10, cache_implementation=\"static\")\n-\n-            output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-            assert output_text[0].startswith(prompt), f\"Expected output to start with '{prompt}', got '{output_text[0]}'\"\n-\n-            torch.distributed.barrier()\n-            torch.distributed.destroy_process_group()\n-            \"\"\"\n-        )\n-        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n-\n-    @require_huggingface_hub_greater_or_equal(\"0.31.4\")\n-    def test_model_save(self):\n-        from safetensors import safe_open\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            for is_torchrun in [True, False]:\n-                script_to_run = textwrap.dedent(\n-                    f\"\"\"\n-                    import torch\n-                    import os\n-                    from transformers import AutoModelForCausalLM\n-\n-                    model_id = \"JackFram/llama-68m\"\n-                    kwargs = dict()\n-\n-                    if os.environ.get(\"RANK\", None) is not None:\n-                        kwargs[\"tp_plan\"] = \"auto\"\n-                        result_dir = \"{tmp_dir}/tp\"\n-                    else:\n-                        result_dir = \"{tmp_dir}/nontp\"\n-\n-                    model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n-                    model.save_pretrained(result_dir)\n-                    \"\"\"\n-                )\n-                torchrun(script_to_run, self.nproc_per_node, is_torchrun=is_torchrun, env=self.get_env())\n-\n-            non_tp_model_path = os.path.join(tmp_dir, \"nontp\")\n-            tp_model_path = os.path.join(tmp_dir, \"tp\")\n-\n-            for filename in os.listdir(non_tp_model_path):\n-                if not filename.endswith(\".safetensors\"):\n-                    continue\n-\n-                non_tp_model = safe_open(os.path.join(non_tp_model_path, filename), device=\"cpu\", framework=\"pt\")\n-                tp_model = safe_open(os.path.join(tp_model_path, filename), device=\"cpu\", framework=\"pt\")\n-                for non_tp_key in non_tp_model.keys():\n-                    non_tp_tensor = non_tp_model.get_tensor(non_tp_key)\n-                    tp_tensor = tp_model.get_tensor(non_tp_key)\n-                    assert torch.allclose(non_tp_tensor, tp_tensor), f\"Tensor with key: {non_tp_key} does not match\"\n-                    del non_tp_tensor, tp_tensor\n-\n-    def test_custom_tp_plan(self):\n-        script_to_run = textwrap.dedent(\n-            r\"\"\"\n-            import re\n-            import torch\n-            from torch.distributed.tensor import DTensor\n-            from transformers import AutoModelForCausalLM\n-\n-            model_id = \"JackFram/llama-68m\"\n-            # only shard attentions, but not mlps\n-            tp_plan = {\n-                \"model.layers.*.self_attn.q_proj\": \"colwise\",\n-                \"model.layers.*.self_attn.k_proj\": \"colwise\",\n-                \"model.layers.*.self_attn.v_proj\": \"colwise\",\n-                \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n-            }\n-\n-            # Use custom tp_plan directly in from_pretrained\n-            model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=tp_plan)\n-\n-            # Check we can generate with the tp_plan\n-            inputs = torch.randint(100, 200, (1, 10), device=model.device)\n-            out = model.generate(inputs, max_new_tokens=10, do_sample=False)\n-\n-            # Check only the attentions are sharded\n-            for name, param in model.named_parameters():\n-                if re.search(r\"\\.self_attn\\.(q|k|v|o)_proj\\.\", name):\n-                    assert isinstance(param, DTensor)\n-                else:\n-                    assert not isinstance(param, DTensor)\n-            \"\"\"\n-        )\n-        torchrun(script_to_run, self.nproc_per_node, env=self.get_env())\n-\n-\n class TestTensorParallelProperties(TestCasePlus):\n     def test_tp_plan_property_setter_getter(self):\n         \"\"\"Test that tp_plan property can be set and retrieved correctly.\"\"\"\n-        from transformers import AutoModelForCausalLM\n-\n         model_id = \"JackFram/llama-68m\"\n         model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\")\n \n@@ -275,8 +135,6 @@ def test_tp_plan_property_setter_getter(self):\n \n     def test_tp_plan_validation_invalid_style(self):\n         \"\"\"Test that invalid parallel styles are rejected.\"\"\"\n-        from transformers import AutoModelForCausalLM\n-\n         model_id = \"JackFram/llama-68m\"\n         model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\")\n \n@@ -289,9 +147,6 @@ def test_tp_plan_validation_invalid_style(self):\n \n     def test_tp_plan_validation_nonexistent_layer_warning(self):\n         \"\"\"Test that warnings are issued for non-existent layer patterns.\"\"\"\n-        import warnings\n-\n-        from transformers import AutoModelForCausalLM\n \n         model_id = \"JackFram/llama-68m\"\n         model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\")\n@@ -308,10 +163,6 @@ def test_tp_plan_validation_nonexistent_layer_warning(self):\n \n     def test_tp_plan_valid_layer_patterns(self):\n         \"\"\"Test that valid layer patterns are accepted without warnings.\"\"\"\n-        import warnings\n-\n-        from transformers import AutoModelForCausalLM\n-\n         model_id = \"JackFram/llama-68m\"\n         model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\")\n \n@@ -347,8 +198,6 @@ def test_tp_plan_valid_layer_patterns(self):\n \n     def test_tp_plan_none_handling(self):\n         \"\"\"Test that None values are handled correctly.\"\"\"\n-        from transformers import AutoModelForCausalLM\n-\n         model_id = \"JackFram/llama-68m\"\n         model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\")\n \n@@ -361,6 +210,172 @@ def test_tp_plan_none_handling(self):\n         self.assertEqual(model.tp_plan, {\"model.layers.*.self_attn.q_proj\": \"colwise\"})\n \n \n-@require_torch_multi_accelerator\n-class TestTensorParallelAccelerator(TestTensorParallel):\n-    nproc_per_node = backend_device_count(torch_device)\n+# ====== TEST FUNCTIONS ======\n+def _test_model_forward_impl(rank):\n+    \"\"\"Implementation of test_model_forward for distributed execution.\"\"\"\n+    model_id = \"JackFram/llama-68m\"\n+\n+    int(os.environ[\"RANK\"])\n+    int(os.environ[\"WORLD_SIZE\"])\n+    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", tp_plan=\"auto\")\n+    torch.distributed.barrier()\n+\n+    has_dtensor = 0\n+    for name, parameter in model.named_parameters():\n+        if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n+            has_dtensor = 1\n+            break\n+\n+    assert has_dtensor == 1, \"TP model must has DTensor\"\n+\n+    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n+    prompt = \"Can I help\"\n+\n+    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n+    outputs = model(inputs)\n+\n+    next_token_logits = outputs[0][:, -1, :]\n+    next_token = torch.argmax(next_token_logits, dim=-1)\n+    response = tokenizer.decode(next_token)\n+    assert response == \"with\"\n+    print(\"response:\", response)\n+    torch.distributed.barrier()\n+\n+\n+def _test_model_backward_pass_impl(rank):\n+    \"\"\"Implementation of test_model_backward_pass for distributed execution.\"\"\"\n+    model_id = \"JackFram/llama-68m\"\n+\n+    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float32, tp_plan=\"auto\")\n+    torch.distributed.barrier()\n+\n+    # Dummy forward and backward pass\n+    # Note that loss.backward() will fail if there is a bug in the TP implementation\n+    inputs = torch.randint(0, model.config.vocab_size, (2, 10), device=model.device)\n+    labels = torch.randint(0, model.config.vocab_size, (2, 10), device=model.device)\n+    loss = model(inputs, labels=labels).loss\n+    loss.backward()\n+\n+    torch.distributed.barrier()\n+\n+\n+def _test_model_generate_impl(rank):\n+    \"\"\"Implementation of test_model_generate for distributed execution.\"\"\"\n+    model_id = \"JackFram/llama-68m\"\n+\n+    int(os.environ[\"RANK\"])\n+    int(os.environ[\"WORLD_SIZE\"])\n+\n+    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", tp_plan=\"auto\")\n+    torch.distributed.barrier()\n+\n+    model.forward = torch.compile(model.forward)\n+\n+    has_dtensor = 0\n+    for name, parameter in model.named_parameters():\n+        if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n+            has_dtensor = 1\n+            break\n+\n+    assert has_dtensor == 1, \"TP model must has DTensor\"\n+\n+    tokenizer = AutoTokenizer.from_pretrained(model_id)\n+    prompt = \"Can I help\"\n+\n+    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n+    outputs = model.generate(inputs, max_new_tokens=10, cache_implementation=\"static\")\n+\n+    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+    assert output_text[0].startswith(prompt), f\"Expected output to start with '{prompt}', got '{output_text[0]}'\"\n+\n+    torch.distributed.barrier()\n+\n+\n+def _test_model_save_impl(rank, tmp_dir, is_torchrun):\n+    \"\"\"Implementation of test_model_save for distributed execution.\"\"\"\n+    model_id = \"JackFram/llama-68m\"\n+    kwargs = {}\n+\n+    if os.environ.get(\"RANK\", None) is not None:\n+        kwargs[\"tp_plan\"] = \"auto\"\n+        result_dir = f\"{tmp_dir}/tp\"\n+    else:\n+        result_dir = f\"{tmp_dir}/nontp\"\n+\n+    model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n+    model.save_pretrained(result_dir)\n+\n+\n+class TestTensorParallelBase(TestCasePlus):\n+    \"\"\"Base class for tensor parallel tests. Subclasses must set nproc_per_node.\"\"\"\n+\n+    nproc_per_node = None\n+\n+    @require_torch_multi_accelerator\n+    def test_model_forward(self):\n+        if self.nproc_per_node is None:\n+            self.skipTest(\"nproc_per_node not set\")\n+        if backend_device_count(torch_device) < self.nproc_per_node:\n+            self.skipTest(f\"Need at least {self.nproc_per_node} devices, have {backend_device_count(torch_device)}\")\n+\n+        init_distributed(tp=self.nproc_per_node)(_test_model_forward_impl)()\n+\n+    @require_torch_multi_accelerator\n+    def test_model_backward_pass(self):\n+        if self.nproc_per_node is None:\n+            self.skipTest(\"nproc_per_node not set\")\n+        if backend_device_count(torch_device) < self.nproc_per_node:\n+            self.skipTest(f\"Need at least {self.nproc_per_node} devices, have {backend_device_count(torch_device)}\")\n+\n+        init_distributed(tp=self.nproc_per_node)(_test_model_backward_pass_impl)()\n+\n+    @require_torch_multi_accelerator\n+    def test_model_generate(self):\n+        if self.nproc_per_node is None:\n+            self.skipTest(\"nproc_per_node not set\")\n+        if backend_device_count(torch_device) < self.nproc_per_node:\n+            self.skipTest(f\"Need at least {self.nproc_per_node} devices, have {backend_device_count(torch_device)}\")\n+\n+        init_distributed(tp=self.nproc_per_node)(_test_model_generate_impl)()\n+\n+    @require_huggingface_hub_greater_or_equal(\"0.31.4\")\n+    @require_torch_multi_accelerator\n+    def test_model_save(self):\n+        if self.nproc_per_node is None:\n+            self.skipTest(\"nproc_per_node not set\")\n+        if backend_device_count(torch_device) < self.nproc_per_node:\n+            self.skipTest(f\"Need at least {self.nproc_per_node} devices, have {backend_device_count(torch_device)}\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            # First run with TP (distributed)\n+            init_distributed(tp=self.nproc_per_node)(_test_model_save_impl)(tmp_dir, True)\n+\n+            # Then run without TP (non-distributed)\n+            _test_model_save_impl(0, tmp_dir, False)\n+\n+            non_tp_model_path = os.path.join(tmp_dir, \"nontp\")\n+            tp_model_path = os.path.join(tmp_dir, \"tp\")\n+\n+            for filename in os.listdir(non_tp_model_path):\n+                if not filename.endswith(\".safetensors\"):\n+                    continue\n+\n+                non_tp_model = safe_open(os.path.join(non_tp_model_path, filename), device=\"cpu\", framework=\"pt\")\n+                tp_model = safe_open(os.path.join(tp_model_path, filename), device=\"cpu\", framework=\"pt\")\n+                for non_tp_key in non_tp_model.keys():\n+                    non_tp_tensor = non_tp_model.get_tensor(non_tp_key)\n+                    tp_tensor = tp_model.get_tensor(non_tp_key)\n+                    assert torch.allclose(non_tp_tensor, tp_tensor), f\"Tensor with key: {non_tp_key} does not match\"\n+                    del non_tp_tensor, tp_tensor\n+\n+\n+class TestTensorParallel2Proc(TestTensorParallelBase):\n+    \"\"\"Test tensor parallel with 2 processes.\"\"\"\n+\n+    nproc_per_node = 2\n+\n+\n+class TestTensorParallel4Proc(TestTensorParallelBase):\n+    \"\"\"Test tensor parallel with 4 processes.\"\"\"\n+\n+    nproc_per_node = 4"
        }
    ],
    "stats": {
        "total": 415,
        "additions": 215,
        "deletions": 200
    }
}