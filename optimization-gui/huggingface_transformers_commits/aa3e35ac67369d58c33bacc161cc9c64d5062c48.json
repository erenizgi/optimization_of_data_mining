{
    "author": "amosyou",
    "message": "Fix warning message for fp32_cpu_offloading in bitsandbytes configs (#34079)\n\n* change cpu offload warning for fp8 quantization\r\n\r\n* change cpu offload warning for fp4 quantization\r\n\r\n* change cpu offload variable name for fp8 and fp4 quantization",
    "sha": "aa3e35ac67369d58c33bacc161cc9c64d5062c48",
    "files": [
        {
            "sha": "98d57e2252490235e7ac2de3df8b13f9d2c178a9",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa3e35ac67369d58c33bacc161cc9c64d5062c48/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa3e35ac67369d58c33bacc161cc9c64d5062c48/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=aa3e35ac67369d58c33bacc161cc9c64d5062c48",
            "patch": "@@ -102,7 +102,7 @@ def validate_environment(self, *args, **kwargs):\n                 raise ValueError(\n                     \"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\n                     \"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\n-                    \"in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \"\n+                    \"in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \"\n                     \"`from_pretrained`. Check \"\n                     \"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \"\n                     \"for more details. \"\n@@ -285,7 +285,7 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import get_keys_to_not_convert, replace_with_bnb_linear\n \n-        load_in_8bit_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n+        llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n \n         # We keep some modules such as the lm_head in their original dtype for numerical stability reasons\n         if self.quantization_config.llm_int8_skip_modules is None:\n@@ -302,7 +302,7 @@ def _process_model_before_weight_loading(\n         if isinstance(device_map, dict) and len(device_map.keys()) > 1:\n             keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n \n-            if len(keys_on_cpu) > 0 and not load_in_8bit_fp32_cpu_offload:\n+            if len(keys_on_cpu) > 0 and not llm_int8_enable_fp32_cpu_offload:\n                 raise ValueError(\n                     \"If you want to offload some keys to `cpu` or `disk`, you need to set \"\n                     \"`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \""
        },
        {
            "sha": "093d612b914cefbb9e472bfa4f68f42d914ff480",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa3e35ac67369d58c33bacc161cc9c64d5062c48/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa3e35ac67369d58c33bacc161cc9c64d5062c48/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=aa3e35ac67369d58c33bacc161cc9c64d5062c48",
            "patch": "@@ -101,7 +101,7 @@ def validate_environment(self, *args, **kwargs):\n                 raise ValueError(\n                     \"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\n                     \"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\n-                    \"in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \"\n+                    \"in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \"\n                     \"`from_pretrained`. Check \"\n                     \"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \"\n                     \"for more details. \"\n@@ -250,7 +250,7 @@ def _process_model_before_weight_loading(\n     ):\n         from ..integrations import get_keys_to_not_convert, replace_with_bnb_linear\n \n-        load_in_8bit_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n+        llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n \n         # We keep some modules such as the lm_head in their original dtype for numerical stability reasons\n         if self.quantization_config.llm_int8_skip_modules is None:\n@@ -267,7 +267,7 @@ def _process_model_before_weight_loading(\n         if isinstance(device_map, dict) and len(device_map.keys()) > 1:\n             keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n \n-            if len(keys_on_cpu) > 0 and not load_in_8bit_fp32_cpu_offload:\n+            if len(keys_on_cpu) > 0 and not llm_int8_enable_fp32_cpu_offload:\n                 raise ValueError(\n                     \"If you want to offload some keys to `cpu` or `disk`, you need to set \"\n                     \"`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \""
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}