{
    "author": "jiqing-feng",
    "message": "fix mpt test of different outputs from cuda (#37691)\n\n* fix mpt test\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix mpt tests with Expectations\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix typo\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix output\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "555693fbfaedae26231f282a796e77629ed1ca76",
    "files": [
        {
            "sha": "4c315fb4330c43548b218494712dab7589056e36",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/555693fbfaedae26231f282a796e77629ed1ca76/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/555693fbfaedae26231f282a796e77629ed1ca76/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=555693fbfaedae26231f282a796e77629ed1ca76",
            "patch": "@@ -20,6 +20,7 @@\n from transformers.testing_utils import (\n     Expectations,\n     require_bitsandbytes,\n+    require_deterministic_for_xpu,\n     require_torch,\n     require_torch_accelerator,\n     slow,\n@@ -483,6 +484,7 @@ def test_generation(self):\n         decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n         self.assertEqual(decoded_output, expected_output)\n \n+    @require_deterministic_for_xpu\n     def test_generation_batched(self):\n         model_id = \"mosaicml/mpt-7b\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -498,10 +500,19 @@ def test_generation_batched(self):\n \n         inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True).to(torch_device)\n \n-        expected_output = [\n-            \"Hello my name is Tiffany and I am a mother of two beautiful children. I have been a nanny for the\",\n-            \"Today I am going at the gym and then I am going to go to the grocery store. I am going to buy some food and some\",\n-        ]\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): [\n+                    \"Hello my name is Tiffany. I am a mother of two beautiful children. I have been a nanny for over\",\n+                    \"Today I am going at the gym and then I am going to go to the mall with my mom. I am going to go to the\",\n+                ],\n+                (\"cuda\", 7): [\n+                    \"Hello my name is Tiffany and I am a mother of two beautiful children. I have been a nanny for the\",\n+                    \"Today I am going at the gym and then I am going to go to the grocery store. I am going to buy some food and some\",\n+                ],\n+            }\n+        )\n+        expected_output = expected_outputs.get_expectation()\n         outputs = model.generate(**inputs, max_new_tokens=20)\n \n         decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 15,
        "deletions": 4
    }
}