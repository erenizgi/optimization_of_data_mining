{
    "author": "cyyever",
    "message": "Fix more inefficient PT operations (#37060)\n\n* Fix inefficient operations\n\n* Remove cpu() call\n\n* Reorder detach()\n\n* Reorder detach()\n\n* tolist without detach\n\n* item without detach\n\n* Update src/transformers/models/rag/modeling_rag.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update tests/models/encodec/test_modeling_encodec.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Use detach().cpu().numpy\n\n* Revert some numpy operations\n\n* More fixes\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
    "files": [
        {
            "sha": "bc91b29b581dda2b115f9e6483eb542d476ba6bc",
            "filename": "benchmark/llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/benchmark%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/benchmark%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fllama.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -204,7 +204,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             time_to_first_token = end - start\n             logger.info(f\"completed first compile generation in: {time_to_first_token}s\")\n             cache_position += 1\n-            all_generated_tokens += next_token.clone().detach().cpu().tolist()\n+            all_generated_tokens += next_token.tolist()\n \n             cache_position = torch.tensor([seq_length], device=device)\n             ### First compile, decoding\n@@ -217,7 +217,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             time_to_second_token = end - start\n             logger.info(f\"completed second compile generation in: {time_to_second_token}s\")\n             cache_position += 1\n-            all_generated_tokens += next_token.clone().detach().cpu().tolist()\n+            all_generated_tokens += next_token.tolist()\n \n             ### Second compile, decoding\n             start = perf_counter()\n@@ -229,13 +229,13 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             time_to_third_token = end - start\n             logger.info(f\"completed third compile forward in: {time_to_third_token}s\")\n             cache_position += 1\n-            all_generated_tokens += next_token.clone().detach().cpu().tolist()\n+            all_generated_tokens += next_token.tolist()\n \n             ### Using cuda graphs decoding\n \n             start = perf_counter()\n             for _ in range(1, num_tokens_to_generate):\n-                all_generated_tokens += next_token.clone().detach().cpu().tolist()\n+                all_generated_tokens += next_token.tolist()\n                 next_token = decode_one_token(\n                     model, next_token.clone(), cache_position=cache_position, past_key_values=past_key_values\n                 )"
        },
        {
            "sha": "757024a0c389d707634268385eb2e4cc804c3168",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -68,7 +68,7 @@ def set_seed(args):\n \n \n def to_list(tensor):\n-    return tensor.detach().cpu().tolist()\n+    return tensor.tolist()\n \n \n def train(args, train_dataset, model, tokenizer):"
        },
        {
            "sha": "2da60745fae091efaad4cbbf322040a6ecd54af2",
            "filename": "src/transformers/agents/agent_types.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fagents%2Fagent_types.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fagents%2Fagent_types.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagent_types.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -129,7 +129,7 @@ def to_raw(self):\n             return self._raw\n \n         if self._tensor is not None:\n-            array = self._tensor.cpu().detach().numpy()\n+            array = self._tensor.detach().cpu().numpy()\n             return Image.fromarray((255 - array * 255).astype(np.uint8))\n \n     def to_string(self):\n@@ -147,7 +147,7 @@ def to_string(self):\n             return self._path\n \n         if self._tensor is not None:\n-            array = self._tensor.cpu().detach().numpy()\n+            array = self._tensor.detach().cpu().numpy()\n \n             # There is likely simpler than load into image into save\n             img = Image.fromarray((255 - array * 255).astype(np.uint8))"
        },
        {
            "sha": "65f47ad6ddb50e29b220529ca45428ebf994c94b",
            "filename": "src/transformers/agents/text_to_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fagents%2Ftext_to_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fagents%2Ftext_to_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftext_to_speech.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -64,4 +64,4 @@ def forward(self, inputs):\n \n     def decode(self, outputs):\n         with torch.no_grad():\n-            return self.post_processor(outputs).cpu().detach()\n+            return self.post_processor(outputs).detach().cpu()"
        },
        {
            "sha": "c57999ba21694bd7738245e645f60db08b84415e",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -612,7 +612,7 @@ def process(\n                     if is_beam_token_worse_than_top_num_beams:\n                         continue\n \n-                    completes_constraint = self.check_completes_constraints(input_ids[batch_beam_idx].cpu().tolist())\n+                    completes_constraint = self.check_completes_constraints(input_ids[batch_beam_idx].tolist())\n                     if completes_constraint:\n                         if beam_indices is not None:\n                             beam_index = beam_indices[batch_beam_idx]\n@@ -718,19 +718,19 @@ def step_sentence_constraint(\n             # hypotheses.\n \n             topk_state = topk_contraint_states[seq_idx]\n-            topk_state.reset(full_hypotheses[seq_idx].cpu().tolist())\n+            topk_state.reset(full_hypotheses[seq_idx].tolist())\n \n             advance_state = advance_constraint_states[seq_idx]\n-            advance_state.reset(pre_seq.cpu().tolist())\n+            advance_state.reset(pre_seq.tolist())\n \n             if not advance_state.completed:\n                 advance_tokens = torch.LongTensor(advance_state.advance()).to(device)\n                 for advance_token in advance_tokens:\n                     # since adding each `advance_token` leads to a different hypothesis, create new state instance.\n                     new_state = advance_state.copy(stateful=True)\n-                    new_state.add(advance_token.cpu().tolist())\n+                    new_state.add(advance_token.tolist())\n \n-                    advance_seq = torch.cat((pre_seq, advance_token.unsqueeze(0)), -1).cpu().tolist()\n+                    advance_seq = torch.cat((pre_seq, advance_token.unsqueeze(0)), -1).tolist()\n                     if advance_seq not in track_new[\"new_seqs\"]:\n                         # prevent duplicates, which are basically bound to happen in this process.\n                         track_new[\"new_seqs\"].append(advance_seq)\n@@ -763,7 +763,7 @@ def step_sentence_constraint(\n \n                 advance_state = advance_constraint_states[seq_idx]\n \n-                advance_seq = advance_seq.cpu().tolist()\n+                advance_seq = advance_seq.tolist()\n \n                 advance_state.reset(advance_seq)\n                 if advance_seq not in track_new[\"new_seqs\"]:\n@@ -843,7 +843,7 @@ def finalize(\n                 final_score = final_beam_scores[batch_beam_idx].item()\n                 final_tokens = input_ids[batch_beam_idx]\n \n-                completes_constraint = self.check_completes_constraints(final_tokens.cpu().tolist())\n+                completes_constraint = self.check_completes_constraints(final_tokens.tolist())\n                 if completes_constraint:\n                     beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n                     generated_len = final_tokens.shape[-1] - decoder_prompt_len"
        },
        {
            "sha": "604c35dfefc09d11e08725207457f6404b537fa1",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -3768,7 +3768,7 @@ def _beam_search(\n             device=input_ids.device,\n         )\n         running_sequences[:, :, :cur_len] = self._unflatten_beam_dim(input_ids, batch_size, num_beams)\n-        sequences = running_sequences.clone().detach()\n+        sequences = running_sequences.detach().clone()\n \n         # per batch, beam-item score, logprobs\n         # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n@@ -3789,7 +3789,7 @@ def _beam_search(\n         running_beam_indices = torch.full(\n             (batch_size, num_beams, max_length - cur_len), fill_value=-1, dtype=torch.int32, device=input_ids.device\n         )\n-        beam_indices = running_beam_indices.clone().detach()\n+        beam_indices = running_beam_indices.detach().clone()\n \n         # 4. run the generation loop\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n@@ -5100,7 +5100,7 @@ def _dola_select_contrast(\n \n     # 6. Reduce the batchmean\n     js_divs = js_divs.mean(-1)  # shape: (num_premature_layers,)\n-    premature_layer = candidate_premature_layers[int(js_divs.argmax().cpu().item())]\n+    premature_layer = candidate_premature_layers[int(js_divs.argmax().item())]\n \n     base_logits = candidate_premature_logits[premature_layer]\n     final_logits, base_logits = _relative_top_filter(final_logits, base_logits)"
        },
        {
            "sha": "88a4ac7cf4fa8ac13e5f96e75e757c3ec4f666c1",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -175,8 +175,8 @@ def loss_labels_vfl(self, outputs, targets, indices, num_boxes, log=True):\n \n         src_boxes = outputs[\"pred_boxes\"][idx]\n         target_boxes = torch.cat([_target[\"boxes\"][i] for _target, (_, i) in zip(targets, indices)], dim=0)\n-        ious, _ = box_iou(center_to_corners_format(src_boxes), center_to_corners_format(target_boxes))\n-        ious = torch.diag(ious).detach()\n+        ious, _ = box_iou(center_to_corners_format(src_boxes.detach()), center_to_corners_format(target_boxes))\n+        ious = torch.diag(ious)\n \n         src_logits = outputs[\"logits\"]\n         target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n@@ -190,7 +190,7 @@ def loss_labels_vfl(self, outputs, targets, indices, num_boxes, log=True):\n         target_score_original[idx] = ious.to(target_score_original.dtype)\n         target_score = target_score_original.unsqueeze(-1) * target\n \n-        pred_score = F.sigmoid(src_logits).detach()\n+        pred_score = F.sigmoid(src_logits.detach())\n         weight = self.alpha * pred_score.pow(self.gamma) * (1 - target) + target_score\n \n         loss = F.binary_cross_entropy_with_logits(src_logits, target_score, weight=weight, reduction=\"none\")"
        },
        {
            "sha": "f3ee5e11fc675b5156bd77fc465c6bb547785faf",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -123,7 +123,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "213711ae121b42f2f6fc4666c005ec3d48c94f16",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -148,7 +148,7 @@ def get_alignment(music_tokens, labels, prior, config):\n             del w_hop\n         weights = torch.cat(w_hops, dim=0)\n         del w_hops\n-        alignment_hop = weights.float().cpu().numpy()\n+        alignment_hop = weights.to(device=\"cpu\", dtype=torch.float).numpy()\n         del weights\n \n         # alignment_hop has shape (bs, n_ctx, nb_relevant_lyric_tokens)"
        },
        {
            "sha": "7eb51a01de6b838a97bef8bea973f42f83ce16dc",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -1568,7 +1568,7 @@ def post_process_segmentation(self, outputs, target_sizes, threshold=0.9, mask_t\n         def to_tuple(tup):\n             if isinstance(tup, tuple):\n                 return tup\n-            return tuple(tup.cpu().tolist())\n+            return tuple(tup.tolist())\n \n         for cur_logits, cur_masks, size in zip(out_logits, raw_masks, target_sizes):\n             # we filter empty queries and detection below threshold\n@@ -1677,7 +1677,7 @@ def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_\n         def to_tuple(tup):\n             if isinstance(tup, tuple):\n                 return tup\n-            return tuple(tup.cpu().tolist())\n+            return tuple(tup.tolist())\n \n         for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n             out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes"
        },
        {
            "sha": "28a89291405044705b7ad49cdb155be3909cc42c",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -831,7 +831,7 @@ def post_process_segmentation(self, outputs, target_sizes, threshold=0.9, mask_t\n         def to_tuple(tup):\n             if isinstance(tup, tuple):\n                 return tup\n-            return tuple(tup.cpu().tolist())\n+            return tuple(tup.tolist())\n \n         for cur_logits, cur_masks, size in zip(out_logits, raw_masks, target_sizes):\n             # we filter empty queries and detection below threshold\n@@ -940,7 +940,7 @@ def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_\n         def to_tuple(tup):\n             if isinstance(tup, tuple):\n                 return tup\n-            return tuple(tup.cpu().tolist())\n+            return tuple(tup.tolist())\n \n         for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n             out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes"
        },
        {
            "sha": "7445802e624d7dfa357dc1d08adab848f07c2396",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -120,7 +120,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "2e1f70a5496516c16f6544db9e17d57786a7748c",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -951,7 +951,7 @@ def forward(\n         >>> height = image_processor.size[\"height\"]\n         >>> width = image_processor.size[\"width\"]\n \n-        >>> samples = output[:, 1:].cpu().detach().numpy()\n+        >>> samples = output[:, 1:].detach().cpu().numpy()\n         >>> samples_img = [\n         ...     np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [height, width, 3]).astype(np.uint8) for s in samples\n         ... ]  # convert color cluster tokens back to pixels"
        },
        {
            "sha": "5af06e2f139bc5a4c46e2a424f23a666cbc0ad6f",
            "filename": "src/transformers/models/mgp_str/processing_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -182,7 +182,7 @@ def _decode_helper(self, pred_logits, format):\n         for index in range(batch_size):\n             pred_eos = preds_str[index].find(eos_str)\n             pred = preds_str[index][:pred_eos]\n-            pred_index = preds_index[index].cpu().tolist()\n+            pred_index = preds_index[index].tolist()\n             pred_eos_index = pred_index.index(eos_token) if eos_token in pred_index else -1\n             pred_max_prob = preds_max_prob[index][: pred_eos_index + 1]\n             confidence_score = pred_max_prob.cumprod(dim=0)[-1] if pred_max_prob.nelement() != 0 else 0.0"
        },
        {
            "sha": "80742db8129d067115a3ba9b335c2a1e1e230df8",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -1167,7 +1167,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "c44ba0f4ff02c3ddede22d5b20f237409d4e9b3a",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -583,7 +583,7 @@ def forward(\n \n                 retriever_outputs = self.retriever(\n                     input_ids,\n-                    question_encoder_last_hidden_state.cpu().detach().to(torch.float32).numpy(),\n+                    question_encoder_last_hidden_state.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                     prefix=self.generator.config.prefix,\n                     n_docs=n_docs,\n                     return_tensors=\"pt\",\n@@ -974,7 +974,7 @@ def generate(\n             question_hidden_states = self.question_encoder(input_ids, attention_mask=attention_mask)[0]\n             context_input_ids = self.retriever(\n                 input_ids,\n-                question_hidden_states.cpu().detach().to(torch.float32).numpy(),\n+                question_hidden_states.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                 prefix=self.generator.config.prefix,\n                 n_docs=n_docs,\n                 return_tensors=\"pt\",\n@@ -1462,7 +1462,7 @@ def generate(\n             question_hidden_states = self.question_encoder(input_ids, attention_mask=attention_mask)[0]\n             out = self.retriever(\n                 input_ids,\n-                question_hidden_states.cpu().detach().to(torch.float32).numpy(),\n+                question_hidden_states.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                 prefix=self.generator.config.prefix,\n                 n_docs=n_docs,\n                 return_tensors=\"pt\","
        },
        {
            "sha": "d5f773299c9da6311b743c8a582b33226329a239",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -123,7 +123,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "aab4850a1269a09a68aaa847b9596e0f5e2b1e6c",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -112,7 +112,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "617fb9b5927aa9eedf0533e6907f3c2232341dfc",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -142,7 +142,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "ef81069eaeeb5c2f28cbfb3104f3231b4c96055a",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -155,7 +155,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "fb8edc3938efc6d05cb2f5d9f3e777dae41cb11e",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -172,7 +172,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "5f43532d9df8089332460aa817b7458a0f1e3403",
            "filename": "src/transformers/models/univnet/feature_extraction_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Funivnet%2Ffeature_extraction_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Funivnet%2Ffeature_extraction_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Ffeature_extraction_univnet.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -276,7 +276,7 @@ def batch_decode(self, waveforms, waveform_lengths=None) -> List[np.ndarray]:\n             `List[np.ndarray]`: A ragged list of 1D waveform arrays with padding removed.\n         \"\"\"\n         # Collapse the batched waveform tensor to a list of 1D audio waveforms\n-        waveforms = [waveform.detach().clone().cpu().numpy() for waveform in waveforms]\n+        waveforms = [waveform.detach().to(device=\"cpu\", copy=True).numpy() for waveform in waveforms]\n \n         if waveform_lengths is not None:\n             waveforms = [waveform[: waveform_lengths[i]] for i, waveform in enumerate(waveforms)]"
        },
        {
            "sha": "d192e4f8ebb2a947481e7a487fb3f4584d45fe65",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -131,7 +131,9 @@ def forward(self, pixel_values, bool_masked_pos):\n         embeddings = self.patch_embeddings(pixel_values)\n \n         # add position embeddings\n-        embeddings = embeddings + self.position_embeddings.type_as(embeddings).to(embeddings.device).detach().clone()\n+        embeddings = embeddings + self.position_embeddings.detach().type_as(embeddings).to(\n+            device=embeddings.device, copy=True\n+        )\n         # only keep visible patches\n         # ~bool_masked_pos means visible\n         if bool_masked_pos is not None:\n@@ -856,7 +858,7 @@ def forward(\n         if bool_masked_pos is None:\n             raise ValueError(\"One must provided a boolean mask \")\n         expanded_position_embeddings = self.position_embeddings.expand(batch_size, -1, -1).type_as(pixel_values)\n-        expanded_position_embeddings = expanded_position_embeddings.to(pixel_values.device).detach().clone()\n+        expanded_position_embeddings = expanded_position_embeddings.detach().to(device=pixel_values.device, copy=True)\n         pos_emb_visible = expanded_position_embeddings[~bool_masked_pos].reshape(batch_size, -1, num_channels)\n         pos_emb_mask = expanded_position_embeddings[bool_masked_pos].reshape(batch_size, -1, num_channels)\n "
        },
        {
            "sha": "4537621f2d6e922a7f5d3d35e835eb10789828e7",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -190,7 +190,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "0fb591542f476280e1a3c8e55e264a5deea00d90",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -147,7 +147,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "4dde34c73ce5e9117b20a90bc970854c96913785",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -165,7 +165,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "270d48f837862461373c204cff09ecf84b996052",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -128,7 +128,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "e265e15c38b9e96159d298aeeddaac39919e612f",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -150,7 +150,7 @@ def compute_num_masked_span(input_length):\n \n     # compute number of masked spans in batch\n     input_lengths = (\n-        attention_mask.sum(-1).detach().tolist()\n+        attention_mask.detach().sum(-1).tolist()\n         if attention_mask is not None\n         else [sequence_length for _ in range(batch_size)]\n     )"
        },
        {
            "sha": "d0fbe5f115d88b40ca1cf3edf5859eec0c7b9324",
            "filename": "src/transformers/pipelines/text_to_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_to_audio.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -18,6 +18,8 @@\n \n \n if is_torch_available():\n+    import torch\n+\n     from ..models.auto.modeling_auto import MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING\n     from ..models.speecht5.modeling_speecht5 import SpeechT5HifiGan\n \n@@ -213,7 +215,7 @@ def postprocess(self, waveform):\n             waveform = waveform[\"waveform\"]\n         elif isinstance(waveform, tuple):\n             waveform = waveform[0]\n-        output_dict[\"audio\"] = waveform.cpu().float().numpy()\n+        output_dict[\"audio\"] = waveform.to(device=\"cpu\", dtype=torch.float).numpy()\n         output_dict[\"sampling_rate\"] = self.sampling_rate\n \n         return output_dict"
        },
        {
            "sha": "26d38204c75760ba9a83a25454cf397d0aaa05e5",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -2524,9 +2524,7 @@ def _inner_training_loop(\n                         else:\n                             input_tokens = inputs[main_input_name].numel()\n                             input_tokens = torch.tensor(input_tokens, device=self.args.device, dtype=torch.int64)\n-                            self.state.num_input_tokens_seen += (\n-                                self.accelerator.gather(input_tokens).sum().cpu().item()\n-                            )\n+                            self.state.num_input_tokens_seen += self.accelerator.gather(input_tokens).sum().item()\n                     if rng_to_sync:\n                         self._load_rng_state(resume_from_checkpoint)\n                         rng_to_sync = False\n@@ -3076,7 +3074,7 @@ def _maybe_log_save_evaluate(\n \n             logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n             if grad_norm is not None:\n-                logs[\"grad_norm\"] = grad_norm.detach().item() if isinstance(grad_norm, torch.Tensor) else grad_norm\n+                logs[\"grad_norm\"] = grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm\n             if learning_rate is not None:\n                 logs[\"learning_rate\"] = learning_rate\n             else:\n@@ -4559,7 +4557,7 @@ def prediction_step(\n                 if has_labels or loss_without_labels:\n                     with self.compute_loss_context_manager():\n                         loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n-                    loss = loss.mean().detach()\n+                    loss = loss.detach().mean()\n \n                     if isinstance(outputs, dict):\n                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])"
        },
        {
            "sha": "6cd9815afba616f308d58b24bef5d0acec791eac",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -1204,7 +1204,7 @@ def smp_nested_concat(tensor):\n             return type(tensor)({k: smp_nested_concat(v) for k, v in tensor.items()})\n         # It doesn't seem possible to check here if `tensor` is a StepOutput because StepOutput lives in `smp.step`\n         # which is also the name of the decorator so Python is confused.\n-        return tensor.concat().detach().cpu()\n+        return tensor.detach().concat().cpu()\n \n \n @dataclass"
        },
        {
            "sha": "e9fa797f0628e37fc2478e793129c8355cb3ebf4",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -351,9 +351,9 @@ def prediction_step(\n                 with self.compute_loss_context_manager():\n                     outputs = model(**inputs)\n                 if self.label_smoother is not None:\n-                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n+                    loss = self.label_smoother(outputs, inputs[\"labels\"]).detach().mean()\n                 else:\n-                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n+                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).detach().mean()\n             else:\n                 loss = None\n "
        },
        {
            "sha": "6be318ac87fa855ff26c395de073353d4a38e21b",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -276,7 +276,7 @@ def to_py_obj(obj):\n         return [to_py_obj(o) for o in obj]\n \n     framework_to_py_obj = {\n-        \"pt\": lambda obj: obj.detach().cpu().tolist(),\n+        \"pt\": lambda obj: obj.tolist(),\n         \"tf\": lambda obj: obj.numpy().tolist(),\n         \"jax\": lambda obj: np.asarray(obj).tolist(),\n         \"np\": lambda obj: obj.tolist(),"
        },
        {
            "sha": "f4f3fa4ae4642f44269c6c3a63ce60594a954496",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -564,7 +564,7 @@ def is_torch_fp16_available_on_device(device):\n     import torch\n \n     try:\n-        x = torch.zeros(2, 2, dtype=torch.float16).to(device)\n+        x = torch.zeros(2, 2, dtype=torch.float16, device=device)\n         _ = x @ x\n \n         # At this moment, let's be strict of the check: check if `LayerNorm` is also supported on device, because many"
        },
        {
            "sha": "8de00019504a25793d5edf4f09d8f3db8ff36ce2",
            "filename": "tests/generation/test_beam_search.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fgeneration%2Ftest_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fgeneration%2Ftest_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_beam_search.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -522,9 +522,9 @@ def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n         # set to same device. we don't care what device.\n \n         if not isinstance(tensor_1, list):\n-            tensor_1 = tensor_1.cpu().tolist()\n+            tensor_1 = tensor_1.tolist()\n         if not isinstance(tensor_2, list):\n-            tensor_2 = tensor_2.cpu().tolist()\n+            tensor_2 = tensor_2.tolist()\n \n         in_order = len(tensor_1) <= len(tensor_2)\n         longer = tensor_2 if in_order else tensor_1"
        },
        {
            "sha": "aa56b87dee6f93df57f3d42c612e58d45569b35e",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -2595,9 +2595,9 @@ def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n         # set to same device. we don't care what device.\n \n         if not isinstance(tensor_1, list):\n-            tensor_1 = tensor_1.cpu().tolist()\n+            tensor_1 = tensor_1.tolist()\n         if not isinstance(tensor_2, list):\n-            tensor_2 = tensor_2.cpu().tolist()\n+            tensor_2 = tensor_2.tolist()\n \n         in_order = len(tensor_1) <= len(tensor_2)\n         longer = tensor_2 if in_order else tensor_1"
        },
        {
            "sha": "603c6c2ab687f65fa61ea13e46017f54100cefc0",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -363,7 +363,7 @@ def test_batch_generation(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n "
        },
        {
            "sha": "2cc2f74dd75c6bf937e9676e89b655cf322986a3",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -406,7 +406,7 @@ def test_batch_generation(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n "
        },
        {
            "sha": "ee0511a055d9c50a89beaa7523f8b53c847db5a3",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -352,7 +352,7 @@ def test_inference_image_classification_head_imagenet_1k(self):\n         torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         expected_top2 = [model.config.label2id[i] for i in [\"remote control, remote\", \"tabby, tabby cat\"]]\n-        self.assertEqual(logits[0].topk(2).indices.cpu().tolist(), expected_top2)\n+        self.assertEqual(logits[0].topk(2).indices.tolist(), expected_top2)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "f7e9d639157b87b0300de1b321ba88426dd1ef7e",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -117,7 +117,7 @@ def prepare_config_and_inputs_for_normalization(self):\n         config.normalize = True\n \n         processor = EncodecFeatureExtractor(feature_size=config.audio_channels, sampling_rate=config.sampling_rate)\n-        input_values = list(input_values.cpu().numpy())\n+        input_values = input_values.tolist()\n         inputs_dict = processor(\n             input_values, sampling_rate=config.sampling_rate, padding=True, return_tensors=\"pt\"\n         ).to(torch_device)\n@@ -495,7 +495,7 @@ def test_integration_24kHz(self):\n                 # use max bandwidth for best possible reconstruction\n                 encoder_outputs = model.encode(inputs[\"input_values\"], bandwidth=float(bandwidth))\n \n-                audio_code_sums = [a[0].sum().cpu().item() for a in encoder_outputs[0]]\n+                audio_code_sums = [a[0].sum().item() for a in encoder_outputs[0]]\n \n                 # make sure audio encoded codes are correct\n                 self.assertListEqual(audio_code_sums, expected_codesums[bandwidth])\n@@ -552,7 +552,7 @@ def test_integration_48kHz(self):\n                 encoder_outputs = model.encode(\n                     inputs[\"input_values\"], inputs[\"padding_mask\"], bandwidth=float(bandwidth), return_dict=False\n                 )\n-                audio_code_sums = [a[0].sum().cpu().item() for a in encoder_outputs[0]]\n+                audio_code_sums = [a[0].sum().item() for a in encoder_outputs[0]]\n \n                 # make sure audio encoded codes are correct\n                 self.assertListEqual(audio_code_sums, expected_codesums[bandwidth])\n@@ -610,8 +610,8 @@ def test_batch_48kHz(self):\n             with torch.no_grad():\n                 # use max bandwidth for best possible reconstruction\n                 encoder_outputs = model.encode(input_values, bandwidth=float(bandwidth), return_dict=False)\n-                audio_code_sums_0 = [a[0][0].sum().cpu().item() for a in encoder_outputs[0]]\n-                audio_code_sums_1 = [a[0][1].sum().cpu().item() for a in encoder_outputs[0]]\n+                audio_code_sums_0 = [a[0][0].sum().item() for a in encoder_outputs[0]]\n+                audio_code_sums_1 = [a[0][1].sum().item() for a in encoder_outputs[0]]\n \n                 # make sure audio encoded codes are correct\n                 self.assertListEqual(audio_code_sums_0, expected_codesums[bandwidth][0])"
        },
        {
            "sha": "603d8d88a7442134f0c1b334fb413c61c0eac604",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -662,7 +662,7 @@ def test_batch_generation(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n \n@@ -724,7 +724,7 @@ def test_batch_generation_2heads(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n "
        },
        {
            "sha": "9a447e2cadeca460322da79957f1ba904c9a750c",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -552,7 +552,7 @@ def test_batch_generation(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n "
        },
        {
            "sha": "ddc1521821f3e58833b8eab1f6dd368f023e8900",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -466,7 +466,7 @@ def test_batch_generation(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n "
        },
        {
            "sha": "1cf819d4017aae11f64caf06bd7333588f71ffa6",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -540,7 +540,7 @@ def test_integration(self):\n                     # use max bandwidth for best possible reconstruction\n                     encoder_outputs = model.encode(inputs[\"input_values\"], num_quantizers=int(num_codebooks))\n \n-                    audio_code_sums = encoder_outputs[0].sum().cpu().item()\n+                    audio_code_sums = encoder_outputs[0].sum().item()\n \n                     # make sure audio encoded codes are correct\n                     # assert relative difference less than a threshold, because `audio_code_sums` varies a bit"
        },
        {
            "sha": "2f92e74d5a77f806d3c1f21ab3f90cd2aa836b89",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -951,8 +951,8 @@ def test_moshika_conditional_greedy(self):\n         expected_text_token = 452\n         expected_audio_tokens = [916, 1396, 1238, 579, 1105, 914, 1257, 810]  # fmt: skip\n \n-        self.assertTrue(expected_text_token == model_outputs.sequences[0, -2].cpu().item())\n-        self.assertTrue(expected_audio_tokens == model_outputs.audio_codes[0, :, -1].cpu().tolist())\n+        self.assertTrue(expected_text_token == model_outputs.sequences[0, -2].item())\n+        self.assertTrue(expected_audio_tokens == model_outputs.audio_codes[0, :, -1].tolist())\n \n     @slow\n     def test_moshiko_greedy_unconditional_fp16_eager(self):\n@@ -966,7 +966,7 @@ def test_moshiko_greedy_unconditional_fp16_eager(self):\n         )\n \n         # eager equivalence is not as strict as sdpa.\n-        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].cpu().tolist())\n+        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].tolist())\n \n     @slow\n     def test_moshiko_greedy_unconditional_fp32(self):\n@@ -986,8 +986,8 @@ def test_moshiko_greedy_unconditional_fp32(self):\n         audio_code_sums = model_outputs.audio_codes.sum().item()\n         self.assertTrue(np.abs(audio_code_sums - expected_audio_codesum) <= (3e-3 * audio_code_sums))\n \n-        self.assertTrue(expected_text_tokens == model_outputs.sequences[0, 1:].cpu().tolist())\n-        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].cpu().tolist())\n+        self.assertTrue(expected_text_tokens == model_outputs.sequences[0, 1:].tolist())\n+        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].tolist())\n \n     @slow\n     @require_torch_fp16\n@@ -1008,8 +1008,8 @@ def test_moshiko_greedy_unconditional_fp16(self):\n         audio_code_sums = model_outputs.audio_codes.sum().item()\n         self.assertTrue(np.abs(audio_code_sums - expected_audio_codesum) <= (3e-3 * audio_code_sums))\n \n-        self.assertTrue(expected_text_tokens == model_outputs.sequences[0, 1:].cpu().tolist())\n-        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].cpu().tolist())\n+        self.assertTrue(expected_text_tokens == model_outputs.sequences[0, 1:].tolist())\n+        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].tolist())\n \n     @slow\n     @require_torch_fp16\n@@ -1030,5 +1030,5 @@ def test_moshika_greedy_unconditional_fp16(self):\n         audio_code_sums = model_outputs.audio_codes.sum().item()\n         self.assertTrue(np.abs(audio_code_sums - expected_audio_codesum) <= 2048)\n \n-        self.assertTrue(expected_text_tokens == model_outputs.sequences[0, 1:].cpu().tolist())\n-        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].cpu().tolist())\n+        self.assertTrue(expected_text_tokens == model_outputs.sequences[0, 1:].tolist())\n+        self.assertTrue(some_expected_audio_tokens == model_outputs.audio_codes[0, :, :2].tolist())"
        },
        {
            "sha": "6fc00d6daa4c8aba09ff093905e62d23b8ef3443",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -486,7 +486,7 @@ def test_batch_generation(self):\n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_non_padded = model.generate(input_ids=inputs_non_padded)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n         output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n "
        },
        {
            "sha": "9b38fa1a9f9a9f49ec784c71ebc421400b6f3acb",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -989,7 +989,7 @@ def test_inference_object_detection(self):\n             outputs, text_labels=text_labels\n         )\n \n-        objects_labels = post_processed_output_with_text_labels[0][\"labels\"].cpu().tolist()\n+        objects_labels = post_processed_output_with_text_labels[0][\"labels\"].tolist()\n         self.assertListEqual(objects_labels, [0, 0])\n \n         objects_text_labels = post_processed_output_with_text_labels[0][\"text_labels\"]"
        },
        {
            "sha": "ecda5b178ee9396b34afba1cc302aa11604e9744",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -975,7 +975,7 @@ def test_inference_object_detection(self):\n             outputs, text_labels=text_labels\n         )\n \n-        objects_labels = post_processed_output_with_text_labels[0][\"labels\"].cpu().tolist()\n+        objects_labels = post_processed_output_with_text_labels[0][\"labels\"].tolist()\n         self.assertListEqual(objects_labels, [0, 0])\n \n         objects_text_labels = post_processed_output_with_text_labels[0][\"text_labels\"]"
        },
        {
            "sha": "35c14b64c6c9180b2f446d6ff83fa44b25d057dc",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -311,7 +311,7 @@ def check_model_generate_from_context_input_ids(\n \n             out = retriever(\n                 input_ids,\n-                question_hidden_states.cpu().detach().to(torch.float32).numpy(),\n+                question_hidden_states.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                 prefix=config.generator.prefix,\n                 return_tensors=\"pt\",\n             )\n@@ -379,7 +379,7 @@ def check_model_without_retriever(\n \n             out = retriever(\n                 input_ids,\n-                question_hidden_states.cpu().detach().to(torch.float32).numpy(),\n+                question_hidden_states.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                 prefix=config.generator.prefix,\n                 return_tensors=\"pt\",\n             )\n@@ -438,7 +438,7 @@ def check_model_custom_n_docs(\n \n             out = retriever(\n                 input_ids,\n-                question_hidden_states.cpu().detach().to(torch.float32).numpy(),\n+                question_hidden_states.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                 prefix=config.generator.prefix,\n                 return_tensors=\"pt\",\n                 n_docs=n_docs,\n@@ -507,7 +507,7 @@ def check_model_with_mismatch_n_docs_value(\n \n             out = retriever(\n                 input_ids,\n-                question_hidden_states.cpu().detach().to(torch.float32).numpy(),\n+                question_hidden_states.detach().to(device=\"cpu\", dtype=torch.float32).numpy(),\n                 prefix=config.generator.prefix,\n                 return_tensors=\"pt\",\n                 n_docs=retriever_n_docs,\n@@ -964,7 +964,7 @@ def test_rag_sequence_generate_batch_from_context_input_ids(self):\n \n         question_hidden_states = rag_sequence.question_encoder(input_ids, attention_mask=attention_mask)[0]\n         docs_dict = retriever(\n-            input_ids.cpu().detach().numpy(), question_hidden_states.cpu().detach().numpy(), return_tensors=\"pt\"\n+            input_ids.detach().cpu().numpy(), question_hidden_states.detach().cpu().numpy(), return_tensors=\"pt\"\n         )\n         doc_scores = torch.bmm(\n             question_hidden_states.unsqueeze(1),"
        },
        {
            "sha": "073d21de65e861b0a06d0ec1214cd47a673ac10c",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -1044,7 +1044,7 @@ def test_model_for_pretraining(self):\n         ).loss\n \n         # loss_more_masked has to be bigger or equal loss since more masked inputs have to be predicted\n-        self.assertTrue(loss.detach().item() <= loss_more_masked.detach().item())\n+        self.assertTrue(loss.item() <= loss_more_masked.item())\n \n     def test_mask_feature_prob_ctc(self):\n         model = Wav2Vec2ForCTC.from_pretrained("
        },
        {
            "sha": "57b2dba41456299b3dd8c8ed378f7563a4855fe6",
            "filename": "tests/models/wav2vec2/test_tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -670,7 +670,7 @@ def test_offsets_integration(self):\n         #\n         #        input_values = feature_extractor(sample[\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n         #        logits = model(input_values).logits\n-        #        pred_ids = torch.argmax(logits, axis=-1).cpu().tolist()\n+        #        pred_ids = torch.argmax(logits, axis=-1).tolist()\n         # ```\n         # fmt: off\n         pred_ids = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 11, 0, 0, 0, 22, 0, 0, 4, 4, 4, 14, 0, 0, 0, 0, 0, 8, 8, 0, 5, 5, 0, 12, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 10, 0, 0, 0, 15, 0, 0, 10, 0, 0, 0, 12, 0, 0, 0, 0, 0, 7, 0, 9, 0, 0, 14, 0, 0, 0, 13, 0, 7, 0, 0, 4, 4, 0, 15, 8, 8, 0, 0, 8, 0, 26, 0, 0, 4, 4, 0, 0, 15, 0, 0, 0, 0, 0, 0, 10, 0, 26, 5, 5, 0, 4, 4, 0, 0, 12, 11, 0, 0, 5, 4, 4, 4, 0, 18, 0, 0, 0, 7, 9, 9, 0, 6, 0, 12, 12, 4, 4, 0, 6, 0, 0, 8, 0, 4, 4, 4, 0, 19, 0, 0, 8, 9, 9, 0, 0, 0, 0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 16, 16, 0, 0, 17, 5, 5, 5, 0, 4, 4, 4, 0, 0, 29, 29, 0, 0, 0, 0, 8, 11, 0, 9, 9, 0, 0, 0, 4, 4, 0, 12, 12, 0, 0, 0, 9, 0, 0, 0, 0, 0, 8, 18, 0, 0, 0, 4, 4, 0, 0, 8, 9, 0, 4, 4, 0, 6, 11, 5, 0, 4, 4, 0, 13, 13, 0, 0, 0, 10, 0, 0, 25, 0, 0, 6, 0, 4, 4, 0, 0, 0, 0, 7, 0, 0, 23, 0, 0, 4, 4, 0, 0, 0, 6, 11, 0, 5, 4, 4, 18, 0, 0, 0, 0, 0, 0, 7, 15, 0, 0, 0, 15, 15, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
        },
        {
            "sha": "17ba172972b31ae380fb35ebb95480443bb04bbe",
            "filename": "tests/models/xlm/test_modeling_xlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -527,4 +527,4 @@ def test_lm_generate_xlm_mlm_en_2048(self):\n         ]  # the president the president the president the president the president the president the president the president the president the president\n         # TODO(PVP): this and other input_ids I tried for generation give pretty bad results. Not sure why. Model might just not be made for auto-regressive inference\n         output_ids = model.generate(input_ids, do_sample=False)\n-        self.assertListEqual(output_ids[0].cpu().numpy().tolist(), expected_output_ids)\n+        self.assertListEqual(output_ids[0].tolist(), expected_output_ids)"
        },
        {
            "sha": "a8e6f0a4347ca02449b2635785e5f8290a4eb0f8",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -4231,7 +4231,7 @@ def test_torch_compile_for_training(self):\n         loss = model(**inputs).loss\n         loss.backward()\n \n-        params = {name: param.grad.clone().detach().cpu() for name, param in model.named_parameters()}\n+        params = {name: param.grad.detach().to(device=\"cpu\", copy=True) for name, param in model.named_parameters()}\n         model.zero_grad()\n         del loss\n "
        },
        {
            "sha": "872369c1752b0c1c2fad4bcdaccfb81a596d4756",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/786d9c5ed920a099573ea7b6dbf265f1aeb32fc0/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=786d9c5ed920a099573ea7b6dbf265f1aeb32fc0",
            "patch": "@@ -2378,14 +2378,14 @@ def check_to_4d(self, mask_converter, q_len, kv_len, additional_mask=None, bsz=3\n             num_tokens_masked = bsz * (q_len * (q_len - 1) // 2)\n \n             if 0 not in mask_2d:\n-                assert (mask_4d != 0).sum().cpu().item() == num_tokens_masked\n+                assert (mask_4d != 0).sum().item() == num_tokens_masked\n             if 0 in mask_2d:\n                 # at least causal mask + maybe more\n-                assert (mask_4d != 0).sum().cpu().item() >= num_tokens_masked\n+                assert (mask_4d != 0).sum().item() >= num_tokens_masked\n                 self.check_non_causal(bsz, q_len, kv_len, mask_2d, mask_4d)\n         elif not mask_converter.is_causal and context is None:\n             if 0 not in mask_2d:\n-                assert (mask_4d != 0).sum().cpu().item() == 0\n+                assert (mask_4d != 0).sum().item() == 0\n             if 0 in mask_2d:\n                 self.check_non_causal(bsz, q_len, kv_len, mask_2d, mask_4d)\n         elif mask_converter.is_causal and context is not None:\n@@ -2394,10 +2394,10 @@ def check_to_4d(self, mask_converter, q_len, kv_len, additional_mask=None, bsz=3\n             num_tokens_masked = bsz * num_tokens_masked\n \n             if 0 not in mask_2d:\n-                assert (mask_4d != 0).sum().cpu().item() == num_tokens_masked\n+                assert (mask_4d != 0).sum().item() == num_tokens_masked\n             if 0 in mask_2d:\n                 # at least causal mask + maybe more\n-                assert (mask_4d != 0).sum().cpu().item() >= num_tokens_masked\n+                assert (mask_4d != 0).sum().item() >= num_tokens_masked\n                 self.check_non_causal(bsz, q_len, kv_len, mask_2d, mask_4d)\n \n     def check_to_causal(self, mask_converter, q_len, kv_len, bsz=3):\n@@ -2415,15 +2415,15 @@ def check_to_causal(self, mask_converter, q_len, kv_len, bsz=3):\n             # k * (k+1) / 2 tokens are masked in triangualar masks\n             num_tokens_masked = bsz * (q_len * (q_len - 1) // 2)\n \n-            assert (mask_4d != 0).sum().cpu().item() == num_tokens_masked\n+            assert (mask_4d != 0).sum().item() == num_tokens_masked\n         elif not mask_converter.is_causal and context is None:\n-            assert (mask_4d != 0).sum().cpu().item() == 0\n+            assert (mask_4d != 0).sum().item() == 0\n         elif mask_converter.is_causal and context is not None:\n             # k * (k+1) / 2 tokens are masked in triangualar masks\n             num_tokens_masked = (q_len * (q_len - 1) // 2) + self.compute_num_context_mask(kv_len, context, q_len)\n             num_tokens_masked = bsz * num_tokens_masked\n \n-            assert (mask_4d != 0).sum().cpu().item() == num_tokens_masked\n+            assert (mask_4d != 0).sum().item() == num_tokens_masked\n \n     def compute_num_context_mask(self, kv_len, context, q_len):\n         # This function computes the # of attention tokens that are added for"
        }
    ],
    "stats": {
        "total": 210,
        "additions": 106,
        "deletions": 104
    }
}