{
    "author": "MichaelCurrin",
    "message": "Docs - update formatting of llama3 model card (#33438)\n\nupdate formatting of llama3 content",
    "sha": "e0ff4321d1f3be70567f6ad8ae7c5b1d263d0374",
    "files": [
        {
            "sha": "9c77db44fcf308190e3d08d816c995ccf0634f57",
            "filename": "docs/source/en/model_doc/llama3.md",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0ff4321d1f3be70567f6ad8ae7c5b1d263d0374/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0ff4321d1f3be70567f6ad8ae7c5b1d263d0374/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama3.md?ref=e0ff4321d1f3be70567f6ad8ae7c5b1d263d0374",
            "patch": "@@ -57,25 +57,26 @@ Tips:\n - The tokenizer is a BPE model based on [tiktoken](https://github.com/openai/tiktoken) (vs the one based on sentencepiece implementation for Llama2). The main difference that it ignores BPE merge rules when an input token is part of the vocab. This means that if no merge exist to produce `\"hugging\"`, instead of having the smallest units, like `[\"hug\",\"ging\"] form 2 tokens, if `\"hugging\"` is part of the vocab, it will be automatically returned as a token.\n - The original model uses `pad_id = -1` which means that there is no padding token. We can't have the same logic, make sure to add a padding token using `tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})` and resize the token embedding accordingly. You should also set the `model.config.pad_token_id`. The `embed_tokens` layer of the model is initialized with `self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)`, which makes sure that encoding the padding token will output zeros, so passing it when initializing is recommended.\n - The original checkpoint can be converted using the [conversion script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). The script can be called with the following (example) command:\n-\n-```bash\n-python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n-    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path --llama_version 3\n-```\n+    \n+    ```bash\n+    python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n+        --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path --llama_version 3\n+    ```\n \n - After conversion, the model and tokenizer can be loaded via:\n \n-```python\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+    ```python\n+    from transformers import AutoModelForCausalLM, AutoTokenizer\n+    \n+    tokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\n+    model = AutoModelForCausalLM.from_pretrained(\"/output/path\")\n+    ```\n \n-tokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\n-model = AutoModelForCausalLM.from_pretrained(\"/output/path\")\n-```\n-\n-Note that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\n-come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 75B model, it's thus 145GB of RAM needed.\n+    Note that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions\n+    come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 75B model, it's thus 145GB of RAM needed.\n \n - When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `torch_dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n \n ## Resources\n+\n A ton of cool resources are already available on the documentation page of [Llama2](./llama2), inviting contributors to add new resources curated for Llama3 here! ðŸ¤—"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 14,
        "deletions": 13
    }
}