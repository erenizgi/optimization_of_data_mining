{
    "author": "Cyrilvallez",
    "message": "Much more efficient and clear weight initialization and tie weights (#42191)\n\n* everything untilo informer\n\n* everything until perceiver\n\n* all of them finally\n\n* style\n\n* replace by transformers init everywhere\n\n* use relative import instead\n\n* deprecated models\n\n* style\n\n* start contexts\n\n* small fixes\n\n* fix modular\n\n* remove class switch\n\n* do not initialize tied weights\n\n* typo\n\n* fix\n\n* improve\n\n* improve comments\n\n* improve\n\n* improve\n\n* fix zamba\n\n* fix import\n\n* add the post_init\n\n* more post_init\n\n* fix\n\n* protect\n\n* more post_init\n\n* fix\n\n* fixes\n\n* fix\n\n* fix\n\n* switch flag name\n\n* more fixes\n\n* fixes\n\n* fixes\n\n* copies\n\n* fix\n\n* finally find the culprit\n\n* style\n\n* last small\n\n* big bird\n\n* better\n\n* update init check\n\n* final touch\n\n* do it everywhere",
    "sha": "8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
    "files": [
        {
            "sha": "2147a45d7503e31a2925bae75982467dcc4d97cd",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,7 +26,6 @@\n from contextlib import contextmanager\n from dataclasses import dataclass, field\n from functools import partial\n-from types import MethodType\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n import torch\n@@ -313,120 +312,6 @@ class ConversionEntry:\n GLOBAL_WORKERS = min(16, (os.cpu_count() or 8) * 2)  # NVMe: 8-16; HDD/NFS: 2-4\n \n \n-# Factory function to create LoadedParameter subclasses dynamically\n-def get_loaded_parameter_class(base_cls):\n-    \"\"\"\n-    base_cls: an nn.Parameter subclass (or nn.Parameter) or a Tensor\n-    Returns a new class that combines the base_cls with LoadedParameterMixin\n-\n-    \"\"\"\n-\n-    class LoadedParam(base_cls):\n-        _inplace_methods = [\n-            \"add_\",\n-            \"mul_\",\n-            \"clamp_\",\n-            \"zero_\",\n-            \"fill_\",\n-            \"normal_\",\n-            \"uniform_\",\n-            \"copy_\",\n-            \"erfinv_\",\n-            \"log_\",\n-            \"__getitem__\",\n-            \"neg_\",\n-            \"exp_\",\n-            \"sub_\",\n-        ]\n-\n-        def __new__(cls, from_existing, **kwargs):\n-            if isinstance(from_existing, torch.nn.Parameter):\n-                inst = super().__new__(cls, from_existing.data, from_existing.requires_grad, **from_existing.__dict__)\n-            else:\n-                inst = super().__new__(cls, from_existing)\n-            # we store the original object to get it back later on\n-            inst._original = from_existing\n-            # Explicitly override all in-place methods per instance\n-            for method_name in inst._inplace_methods:\n-                setattr(inst, method_name, MethodType(inst._skip, inst))\n-\n-            return inst\n-\n-        def _skip(self, *args, **kwargs):\n-            \"\"\"Helper to skip in-place operations.\"\"\"\n-            return self\n-\n-        def __repr__(self):\n-            return f\"LoadedParameter(data={self.data})\"\n-\n-        @property\n-        def data(self):\n-            return super().data\n-\n-        @data.setter\n-        def data(self, new):\n-            pass\n-\n-    def __lt__(self, other):\n-        return torch.Tensor.__lt__(self, other)\n-\n-    def __le__(self, other):\n-        return torch.Tensor.__le__(self, other)\n-\n-    def __gt__(self, other):\n-        return torch.Tensor.__gt__(self, other)\n-\n-    def __ge__(self, other):\n-        return torch.Tensor.__ge__(self, other)\n-\n-    def __eq__(self, other):\n-        return torch.Tensor.__eq__(self, other)\n-\n-    def __ne__(self, other):\n-        return torch.Tensor.__ne__(self, other)\n-\n-    def __iadd__(self, *args, **kwargs):\n-        return self\n-\n-    def __isub__(self, *args, **kwargs):\n-        return self\n-\n-    def __imul__(self, *args, **kwargs):\n-        return self\n-\n-    def __imatmul__(self, *args, **kwargs):\n-        return self\n-\n-    def __itruediv__(self, *args, **kwargs):\n-        return self\n-\n-    def __ifloordiv__(self, *args, **kwargs):\n-        return self\n-\n-    def __imod__(self, *args, **kwargs):\n-        return self\n-\n-    def __ipow__(self, *args, **kwargs):\n-        return self\n-\n-    def __iand__(self, *args, **kwargs):\n-        return self\n-\n-    def __ior__(self, *args, **kwargs):\n-        return self\n-\n-    def __ixor__(self, *args, **kwargs):\n-        return self\n-\n-    def __ilshift__(self, *args, **kwargs):\n-        return self\n-\n-    def __irshift__(self, *args, **kwargs):\n-        return self\n-\n-    return LoadedParam\n-\n-\n def _materialize_copy(tensor, dtype=None):\n     tensor = tensor[...]\n     if dtype is not None:\n@@ -527,7 +412,6 @@ def set_param_for_module(\n                     param_value = param_value.to_local()\n             if param_name not in module_obj._buffers:\n                 param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n-        param_value = get_loaded_parameter_class(param_value.__class__)(from_existing=param_value)\n \n         # Remove from missing keys (it's either mismatched, or all good)\n         missing_keys.discard(layer_name)"
        },
        {
            "sha": "348dae74929be79b4ff066dae18cd320f438485e",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCELoss\n \n+from .. import initialization as init\n from ..modeling_utils import PreTrainedModel\n from ..utils import ModelOutput, logging\n from .configuration_utils import PreTrainedConfig, WatermarkingConfig\n@@ -387,7 +388,7 @@ def __init__(self, config):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Parameter):\n-            module.weight.normal_(mean=0.0, std=0.02)\n+            init.normal_(module.weight, mean=0.0, std=0.02)\n \n     def _compute_posterior(\n         self,"
        },
        {
            "sha": "27780e9ac719597161543dc91bdaedf2b4a8a7bf",
            "filename": "src/transformers/initialization.py",
            "status": "added",
            "additions": 191,
            "deletions": 0,
            "changes": 191,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Finitialization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Finitialization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Finitialization.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -0,0 +1,191 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import sys\n+from collections import defaultdict\n+from contextlib import contextmanager\n+\n+import torch\n+\n+\n+# Record all the torch primitives in advance, so that we can use them without them being modified when we patch torch\n+# in context managers\n+TORCH_INIT_FUNCTIONS = {\n+    \"uniform_\": torch.nn.init.uniform_,\n+    \"normal_\": torch.nn.init.normal_,\n+    \"constant_\": torch.nn.init.constant_,\n+    \"ones_\": torch.nn.init.ones_,\n+    \"zeros_\": torch.nn.init.zeros_,\n+    \"eye_\": torch.nn.init.eye_,\n+    \"dirac_\": torch.nn.init.dirac_,\n+    \"xavier_uniform_\": torch.nn.init.xavier_uniform_,\n+    \"xavier_normal_\": torch.nn.init.xavier_normal_,\n+    \"kaiming_uniform_\": torch.nn.init.kaiming_uniform_,\n+    \"kaiming_normal_\": torch.nn.init.kaiming_normal_,\n+    \"trunc_normal_\": torch.nn.init.trunc_normal_,\n+    \"orthogonal_\": torch.nn.init.orthogonal_,\n+    \"sparse_\": torch.nn.init.sparse_,\n+}\n+\n+\n+def uniform_(\n+    tensor: torch.Tensor, a: float = 0.0, b: float = 1.0, generator: torch.Generator | None = None\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"uniform_\"](tensor, a=a, b=b, generator=generator)\n+    return tensor\n+\n+\n+def normal_(\n+    tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, generator: torch.Generator | None = None\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"normal_\"](tensor, mean=mean, std=std, generator=generator)\n+    return tensor\n+\n+\n+def constant_(tensor: torch.Tensor, val: float) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"constant_\"](tensor, val=val)\n+    return tensor\n+\n+\n+def ones_(tensor: torch.Tensor) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"ones_\"](tensor)\n+    return tensor\n+\n+\n+def zeros_(tensor: torch.Tensor) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"zeros_\"](tensor)\n+    return tensor\n+\n+\n+def eye_(tensor: torch.Tensor) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"eye_\"](tensor)\n+    return tensor\n+\n+\n+def dirac_(tensor: torch.Tensor, groups: int = 1) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"dirac_\"](tensor, groups=groups)\n+    return tensor\n+\n+\n+def xavier_uniform_(tensor: torch.Tensor, gain: float = 1.0, generator: torch.Generator | None = None) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"xavier_uniform_\"](tensor, gain=gain, generator=generator)\n+    return tensor\n+\n+\n+def xavier_normal_(tensor: torch.Tensor, gain: float = 1.0, generator: torch.Generator | None = None) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"xavier_normal_\"](tensor, gain=gain, generator=generator)\n+    return tensor\n+\n+\n+def kaiming_uniform_(\n+    tensor: torch.Tensor,\n+    a: float = 0,\n+    mode: str = \"fan_in\",\n+    nonlinearity: str = \"leaky_relu\",\n+    generator: torch.Generator | None = None,\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"kaiming_uniform_\"](\n+            tensor, a=a, mode=mode, nonlinearity=nonlinearity, generator=generator\n+        )\n+    return tensor\n+\n+\n+def kaiming_normal_(\n+    tensor: torch.Tensor,\n+    a: float = 0,\n+    mode: str = \"fan_in\",\n+    nonlinearity: str = \"leaky_relu\",\n+    generator: torch.Generator | None = None,\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"kaiming_normal_\"](\n+            tensor, a=a, mode=mode, nonlinearity=nonlinearity, generator=generator\n+        )\n+    return tensor\n+\n+\n+def trunc_normal_(\n+    tensor: torch.Tensor,\n+    mean: float = 0.0,\n+    std: float = 1.0,\n+    a: float = -2.0,\n+    b: float = 2.0,\n+    generator: torch.Generator | None = None,\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"trunc_normal_\"](tensor, mean=mean, std=std, a=a, b=b, generator=generator)\n+    return tensor\n+\n+\n+def orthogonal_(\n+    tensor: torch.Tensor,\n+    gain: float = 1,\n+    generator: torch.Generator | None = None,\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"orthogonal_\"](tensor, gain=gain, generator=generator)\n+    return tensor\n+\n+\n+def sparse_(\n+    tensor: torch.Tensor, sparsity: float, std: float = 0.01, generator: torch.Generator | None = None\n+) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        return TORCH_INIT_FUNCTIONS[\"sparse_\"](tensor, sparsity=sparsity, std=std, generator=generator)\n+    return tensor\n+\n+\n+def copy_(tensor: torch.Tensor, other: torch.Tensor) -> torch.Tensor:\n+    if not getattr(tensor, \"_is_hf_initialized\", False):\n+        with torch.no_grad():\n+            return tensor.copy_(other)\n+    return tensor\n+\n+\n+@contextmanager\n+def guard_torch_init_functions():\n+    \"\"\"\n+    Guard the `torch.nn.init` primitive functions to behave exactly like the functions in this file, i.e. be\n+    protected against the `_is_hf_initialized` flag to avoid re-init if the param was already loaded.\n+\n+    Usually, all models are using the init from `transformers` which are already guarded, but just to make extra sure\n+    and for remote code, we also use this context manager.\n+    \"\"\"\n+    originals = defaultdict(dict)\n+    try:\n+        # Replace all torch funcs by the ones in this file\n+        for name in TORCH_INIT_FUNCTIONS.keys():\n+            # Here, we need to check all modules imported, and hot patch all of them, as usually torch does\n+            # something like `from torch.nn.init import xavier_uniform_` in their internals (e.g in torch.nn.modules,\n+            # where MultiHeadAttention lives), so the function name is binded at import time and just doing\n+            # `setattr(torch.nn.init, name, gloabls()[name])` is thus not enough\n+            for module in sys.modules.values():\n+                if module and hasattr(module, name):\n+                    originals[module][name] = getattr(module, name)\n+                    setattr(module, name, globals()[name])\n+        yield\n+    finally:\n+        # Set back the original functions on all modules\n+        for module, functions in originals.items():\n+            for name, func in functions.items():\n+                setattr(module, name, func)"
        },
        {
            "sha": "965084d0c24a9bcd49d1cc2dba990de4db34d872",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 71,
            "deletions": 106,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -44,6 +44,7 @@\n from torch.distributions import constraints\n from torch.utils.checkpoint import checkpoint\n \n+from . import initialization as init\n from .configuration_utils import PreTrainedConfig\n from .conversion_mapping import get_checkpoint_conversion_mapping\n from .core_model_loading import WeightConverter, convert_and_load_state_dict_in_model, revert_weight_conversion\n@@ -471,9 +472,7 @@ def _get_tied_weight_keys(module: nn.Module) -> list[str]:\n     tied_weight_keys: list[str] = []\n     for name, submodule in module.named_modules():\n         tied = getattr(submodule, \"_tied_weights_keys\", {}) or {}\n-        tied_weights_dict = list(tied.keys())\n-        # tied_weights_dict.extend(tied.values())\n-        tied_weight_keys.extend([f\"{name}.{k}\" if name else k for k in tied_weights_dict])\n+        tied_weight_keys.extend([f\"{name}.{k}\" if name else k for k in tied.keys()])\n     return tied_weight_keys\n \n \n@@ -901,35 +900,6 @@ def _get_dtype(\n     return config, dtype, dtype_orig\n \n \n-@contextmanager\n-def guard_nn_init_functions(flag_name: str = \"_is_hf_initialized\"):\n-    import torch.nn.init as init\n-\n-    originals = {}\n-\n-    def make_wrapper(fn):\n-        @wraps(fn)\n-        def wrapped(*args, **kwargs):\n-            # Tensor can come positionally or as a kwarg (e.g. via DeviceContext)\n-            t = args[0] if args else kwargs.get(\"tensor\", kwargs.get(\"input\"))\n-            if t is not None and getattr(t, flag_name, False):\n-                # mimic init.* return convention (returns the tensor)\n-                return t\n-            return fn(*args, **kwargs)  # TODO we could set is init here.\n-\n-        return wrapped\n-\n-    try:\n-        for name in TORCH_INIT_FUNCTIONS:\n-            if hasattr(init, name):\n-                originals[name] = getattr(init, name)\n-                setattr(init, name, make_wrapper(originals[name]))\n-        yield\n-    finally:\n-        for name, fn in originals.items():\n-            setattr(init, name, fn)\n-\n-\n class PipelineParallel(Enum):\n     inputs = 0\n     outputs = 1\n@@ -1521,26 +1491,35 @@ def post_init(self):\n         \"\"\"\n         A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n         modules properly initialized (such as weight initialization).\n-\n-        This is also used when the user is running distributed code. We add hooks to the modules here, according to\n-        the model's tp_plan!\n         \"\"\"\n-        self.init_weights()\n-        self._backward_compatibility_gradient_checkpointing()\n-\n+        # Attach the different parallel plans and tied weight keys to the top-most model, so that everything is\n+        # easily available\n         self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n+        # Current submodel should register its tied weights keys only if the config is asking for it\n+        if not self.config.tie_word_embeddings and not self.config.tie_encoder_decoder:\n+            self.all_tied_weights_keys = {}\n+        else:\n+            self.all_tied_weights_keys = self._tied_weights_keys.copy() if self._tied_weights_keys is not None else {}\n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         if self.base_model is self:\n             self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n             self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n             self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n         for name, module in self.named_children():\n+            # Parallel plans\n             if plan := getattr(module, \"_ep_plan\", None):\n                 self._ep_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n             if plan := getattr(module, \"_tp_plan\", None):\n                 self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n             if plan := getattr(module, \"_pp_plan\", None):\n                 self._pp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n+            # Always attach the keys of the children (if the children's config says to NOT tie, then it's empty)\n+            if tied_keys := getattr(module, \"all_tied_weights_keys\", None):\n+                self.all_tied_weights_keys.update({f\"{name}.{k}\": f\"{name}.{v}\" for k, v in tied_keys.copy().items()})\n+\n+        # Maybe initialize the weights and tie the keys\n+        self.init_weights()\n+        self._backward_compatibility_gradient_checkpointing()\n \n     @property\n     def tp_plan(self) -> dict[str, str]:\n@@ -2275,22 +2254,25 @@ def _init_weights(self, module):\n         \"\"\"\n         if hasattr(self.config, \"initializer_range\"):\n             std = self.config.initializer_range or 0.02\n+        elif hasattr(self.config, \"init_std\"):\n+            std = self.config.init_std\n+        elif hasattr(self.config, \"initializer_factor\"):\n+            std = self.config.initializer_factor\n         else:\n             # 0.02 is the standard default value across the library\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n             if getattr(module, \"weight\", None) is not None:\n-                module.weight.normal_(mean=0.0, std=std)\n+                init.normal_(module.weight, mean=0.0, std=std)\n             if getattr(module, \"bias\", None) is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n             if getattr(module, \"weight\", None) is not None:\n-                module.weight.normal_(mean=0.0, std=std)\n-            if getattr(\n-                self.config, \"pad_token_id\", None\n-            ) is not None and self.config.pad_token_id < module.weight.size(0):\n-                module.weight[self.config.pad_token_id].zero_()\n+                init.normal_(module.weight, mean=0.0, std=std)\n+                # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+                if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                    init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.MultiheadAttention):\n             # This uses torch's original init\n             module._reset_parameters()\n@@ -2303,15 +2285,15 @@ def _init_weights(self, module):\n         ):\n             # Norms can exist without weights (in which case they are None from torch primitives)\n             if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.fill_(1.0)\n+                init.ones_(module.weight)\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         if isinstance(getattr(module, \"gate_up_proj\", None), nn.Parameter):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n         if isinstance(getattr(module, \"down_proj\", None), nn.Parameter):\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         if isinstance(getattr(module, \"gate\", None), nn.Parameter):\n-            module.gate.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate, mean=0.0, std=std)\n \n     def _initialize_weights(self, module):\n         \"\"\"\n@@ -2324,7 +2306,7 @@ def _initialize_weights(self, module):\n         module._is_hf_initialized = True\n \n     @torch.no_grad()\n-    @guard_nn_init_functions()\n+    @init.guard_torch_init_functions()\n     def initialize_weights(self):\n         \"\"\"\n         This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composite models.\n@@ -2355,12 +2337,7 @@ def smart_apply(self, fn):\n         # Let the magic happen with this simple call\n         self.smart_apply(self._initialize_weights)\n \n-    def tie_weight_source_and_target(\n-        self,\n-        top_level: \"PreTrainedModel\",\n-        missing_keys: Optional[set[str]] = None,\n-        module_prefix: str = \"\",\n-    ):\n+    def tie_weights(self, missing_keys: Optional[set[str]] = None):\n         \"\"\"\n         If set in the config, tie the weights between the input embeddings and the output embeddings,\n         and the encoder and decoder. This relies on the `_tied_weights_keys` dict.\n@@ -2409,22 +2386,18 @@ def __init__(self):\n         If you call this function, it will always tie. There is only 1 tricky case, if all weights are missing, you still want to mention that\n         the ones you tied were missing.\n         \"\"\"\n-        mapping = getattr(self, \"_tied_weights_keys\", None)\n+        # TODO Cyril: using this fixed set of keys (set in post_init()) does not allow to switch the config flag and re-tie\n+        mapping = getattr(self, \"all_tied_weights_keys\", None)\n         if not isinstance(mapping, dict):\n             return\n-        if (  # we only tie for ourselves, so we look at our config\n-            not self.config.tie_word_embeddings\n-            and not self.config.tie_encoder_decoder  # if missing keys is None we init?\n-        ):\n-            return\n \n         # TODO let's pray this is not too slow :)\n-        top_level_params = dict(top_level.named_parameters(remove_duplicate=False)) | dict(\n-            top_level.named_buffers(remove_duplicate=False)\n+        top_level_params = dict(self.named_parameters(remove_duplicate=False)) | dict(\n+            self.named_buffers(remove_duplicate=False)\n         )\n         for target_name, source_name in mapping.items():\n-            source_name = f\"^{module_prefix}.{source_name}\" if module_prefix else \"^\" + source_name\n-            target_name = f\"^{module_prefix}.{target_name}\" if module_prefix else \"^\" + target_name\n+            source_name = \"^\" + source_name\n+            target_name = \"^\" + target_name\n \n             source_is_there = bool(missing_keys) and not re.search(\n                 source_name, \"\\n\".join(missing_keys), flags=re.MULTILINE\n@@ -2440,10 +2413,10 @@ def __init__(self):\n                 for target_n, source_n in zip(target_params, cycle(source_params)):\n                     if \".\" in target_n:\n                         parent_path, last = target_n.rsplit(\".\", 1)\n-                        parent = top_level.get_submodule(parent_path)\n+                        parent = self.get_submodule(parent_path)\n                     else:\n                         parent_path, last = \"\", target_n\n-                        parent = top_level  # top-level\n+                        parent = self  # top-level\n                     setattr(parent, last, top_level_params[source_n])\n                     self._adjust_bias(parent, top_level_params[source_n])\n                     if missing_keys and source_is_there:  # test_model_weights_reload_no_missing_tied_weights\n@@ -2470,19 +2443,6 @@ def _adjust_bias(self, output_embeddings, input_embeddings):\n         if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n             output_embeddings.out_features = input_embeddings.num_embeddings\n \n-    def tie_weights(self, missing_keys: Optional[set[str]] = None):\n-        \"\"\"\n-        Recursively (for all submodels) tie all the weights of the model.\n-        \"\"\"\n-        # Note that `self` is included in `self.modules` so we also apply to current PreTrainedModel with this call\n-        if missing_keys is None:\n-            # called from `post_init`\n-            self.tie_weight_source_and_target(self, missing_keys, \"\")\n-        else:  # this is from_pretrained, so its not called on every sub module\n-            for module_prefix, module in self.named_modules():\n-                if isinstance(module, PreTrainedModel):\n-                    module.tie_weight_source_and_target(self, missing_keys, module_prefix)\n-\n     def _get_no_split_modules(self, device_map: str):\n         \"\"\"\n         Get the modules of the model that should not be spit when using device_map. We iterate through the modules to\n@@ -4301,19 +4261,23 @@ def _load_pretrained_model(\n             for k in all_pointer:\n                 k.__exit__(None, None, None)\n \n+        # Marks tied weights as `_is_hf_initialized` to avoid initializing them (it's very important for efficiency)\n+        model.mark_tied_weights_as_initialized()\n+\n         # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n         # loading the weights as they are not in the loaded state dict)\n-        # Remove tied weights keys and etc\n         miss_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n         model._move_missing_keys_from_meta_to_cpu(miss_and_mismatched, dtype, hf_quantizer)\n \n-        # correctly initialize the missing (and potentially mismatched) keys\n+        # Correctly initialize the missing (and potentially mismatched) keys (all parameters without the `_is_hf_initialzed` flag)\n         model._initialize_missing_keys(is_quantized)\n-        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys, False)\n \n-        # We make sure we tie after _init_. We need the missing keys to remove the ones we do tie, and not random remove\n+        # Tie the weights\n         model.tie_weights(missing_keys)\n \n+        # Adjust missing and unexpected keys\n+        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys)\n+\n         # Post-processing for tensor parallelism\n         if device_mesh is not None:\n             # When using TP, the device map is a single device for all parameters\n@@ -4556,7 +4520,10 @@ def _move_missing_keys_from_meta_to_cpu(\n             return\n \n         model_state_dict = self.state_dict()\n-        for key in missing_keys:\n+        # The tied weight keys are in the \"missing\" usually, but they should not be moved (they will be tied anyway)\n+        # This is especially important because if they are moved, they will lose the `_is_hf_initialized` flag, and they\n+        # will be re-initialized for nothing (which can be quite long)\n+        for key in missing_keys - self.all_tied_weights_keys.keys():\n             param = model_state_dict[key]\n             # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n             if param.device == torch.device(\"meta\"):\n@@ -4585,23 +4552,13 @@ def _initialize_missing_keys(self, is_quantized: bool) -> None:\n         else:\n             self.initialize_weights()\n \n-        # Replace the loaded parameters class back to nn.Parameter (they were changed to easily skip initialization\n-        # when performed in-place on the tensors)\n-        for name, p in list(self.named_parameters()) + list(self.named_buffers()):\n-            # We get back the original parameter that we stored in _original. This attribute was created when we initialized LoadedParam when loading the checkpoints.\n-            if hasattr(p, \"_original\"):\n-                if \".\" in name:\n-                    module, name = name.rsplit(\".\", 1)\n-                    module = self.get_submodule(module)\n-                else:\n-                    module = self\n-                setattr(module, name, p._original)\n-\n     def _adjust_missing_and_unexpected_keys(\n-        self, missing_keys: set[str], unexpected_keys: set[str], loading_task_model_from_base_state_dict: bool\n+        self, missing_keys: set[str], unexpected_keys: set[str]\n     ) -> tuple[set[str], set[str]]:\n         \"\"\"Adjust the `missing_keys` and `unexpected_keys` based on current model's exception rules, to avoid\n         raising unneeded warnings/errors.\n+        Also, set the `_is_hf_initialized` on tied weight keys, to avoid initializing them as they are going to\n+        be tied anyway.\n         \"\"\"\n         # Old checkpoints may have keys for rotary_emb.inv_freq forach layer, however we moved this buffer to the main model\n         # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n@@ -4625,14 +4582,22 @@ def _adjust_missing_and_unexpected_keys(\n         if ignore_unexpected_regex is not None:\n             unexpected_keys = {key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None}\n \n-        # Note: only the unexpected keys should remove the added prefix here, to correctly display the original name\n-        # in the warnings. For missing keys, we should show the prefix in the warning as it's part of the final model\n-        if loading_task_model_from_base_state_dict:\n-            _prefix = f\"{self.base_model_prefix}.\"\n-            unexpected_keys = {k.removeprefix(_prefix) for k in unexpected_keys}\n-\n         return missing_keys, unexpected_keys\n \n+    def mark_tied_weights_as_initialized(self):\n+        \"\"\"Adds the `_is_hf_initialized` flag on parameters that will be tied, in order to avoid initializing them\n+        later as they will be tied (overwritten) anyway.\n+        This is very important as most embeddings are tied, and they are huge params (vocabularies are often 256k), so\n+        running inits on them is very costly.\"\"\"\n+        for tied_param in self.all_tied_weights_keys.keys():\n+            # It's always a proper weight except for 2 or 3 old models where it's a regex or module set to None\n+            # -> just skip it in those cases (they will just re-init before tying, so they loose the added optimization)\n+            try:\n+                param = self.get_parameter(tied_param)\n+                param._is_hf_initialized = True\n+            except AttributeError:\n+                pass\n+\n     def get_parameter_or_buffer(self, target: str):\n         \"\"\"\n         Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combines"
        },
        {
            "sha": "4c7e81ed59ccde0d6f6b82617c872ceb23c11507",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n@@ -410,9 +411,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if hasattr(module, \"logit_scale\"):\n             if isinstance(module.logit_scale, nn.Parameter):\n-                module.logit_scale.fill_(math.log(1 / 0.07))\n+                init.constant_(module.logit_scale, math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n-            module.cls_token.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring("
        },
        {
            "sha": "4262efd5b45b340d4d91c5494c28e108f493f35e",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n@@ -453,9 +454,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if hasattr(module, \"logit_scale\"):\n             if isinstance(module.logit_scale, nn.Parameter):\n-                module.logit_scale.fill_(math.log(1 / 0.07))\n+                init.constant_(module.logit_scale, math.log(1 / 0.07))\n         elif isinstance(module, Aimv2AttentionPoolingHead):\n-            module.cls_token.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring("
        },
        {
            "sha": "bb3679064b0cdbb1c40ecd02a8f73053f318a3b0",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...masking_utils import create_bidirectional_mask\n from ...modeling_outputs import (\n@@ -306,18 +307,19 @@ class AlbertPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, AlbertMLMHead):\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n @dataclass"
        },
        {
            "sha": "84ac48e7675cf179534f43aed6c754d90f3247a1",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -828,20 +829,21 @@ def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, AlignModel):\n-            nn.init.xavier_uniform_(module.text_projection.weight)\n-            module.text_projection.bias.zero_()\n-            module.temperature.fill_(self.config.temperature_init_value)\n+            init.xavier_uniform_(module.text_projection.weight)\n+            init.zeros_(module.text_projection.bias)\n+            init.constant_(module.temperature, self.config.temperature_init_value)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         if isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n @auto_docstring("
        },
        {
            "sha": "8cb6bbb3ba2fbbaea5c5e08288dd1e7087aea69b",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 20,
            "deletions": 18,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -776,43 +777,44 @@ def _init_weights(self, module):\n         factor = self.config.initializer_factor\n         if isinstance(module, AltCLIPVisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, AltCLIPAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, AltCLIPMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, AltCLIPModel):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_factor)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_factor)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_factor)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_factor)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n class AltCLIPVisionTransformer(nn.Module):"
        },
        {
            "sha": "76f3da90b1da1581ef83b1b5c37ba8388666c927",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -589,7 +590,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, AriaGroupedExpertsGemm):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring\n@@ -613,7 +614,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, AriaProjector):\n-            nn.init.trunc_normal_(module.query, std=self.config.initializer_range)\n+            init.trunc_normal_(module.query, std=self.config.initializer_range)\n \n \n class AriaTextRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "28b62c390f7d0cde924245421028ce727d7e051e",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n@@ -1200,7 +1201,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, AriaGroupedExpertsGemm):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n@@ -1213,7 +1214,7 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, AriaProjector):\n-            nn.init.trunc_normal_(module.query, std=self.config.initializer_range)\n+            init.trunc_normal_(module.query, std=self.config.initializer_range)\n \n \n class AriaTextModel(LlamaModel):"
        },
        {
            "sha": "0f76c15e1f08e13f9640014a3d0190c53c47118d",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 13,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, SequenceClassifierOutput\n@@ -304,22 +305,16 @@ class ASTPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, ASTEmbeddings):\n-            module.cls_token.zero_()\n-            module.position_embeddings.zero_()\n-            module.distillation_token.zero_()\n+            init.zeros_(module.cls_token)\n+            init.zeros_(module.position_embeddings)\n+            init.zeros_(module.distillation_token)\n \n \n @auto_docstring"
        },
        {
            "sha": "1e6691d1bc6a0ac098a3e2eca3ad6674ebacee80",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -264,28 +264,6 @@ class AudioFlamingo3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        # important: this ported version of AudioFlamingo3 isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.audio_config.initializer_range\n-        )\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "581985fa0c4c443f0651d1d57052510fca8fc677",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n@@ -318,9 +319,9 @@ class AutoformerSinusoidalPositionalEmbedding(nn.Embedding):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n-        super().__init__(num_positions, embedding_dim)\n+        super().__init__(num_positions, embedding_dim, _freeze=True)\n \n-    def _init_weight(self):\n+    def create_weight(self):\n         \"\"\"\n         Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n         the 2nd half of the vector. [dim // 2:]\n@@ -333,7 +334,7 @@ def _init_weight(self):\n         sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n         out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n         out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n-        self.weight = nn.Parameter(out, requires_grad=False)\n+        return out\n \n     @torch.no_grad()\n     def forward(\n@@ -830,18 +831,19 @@ class AutoformerPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module):\n         std = self.config.init_std\n         if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, AutoformerSinusoidalPositionalEmbedding):\n-            module._init_weight()\n+            init.copy_(module.weight, module.create_weight())\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n     # copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask("
        },
        {
            "sha": "2428222e0dbe88733e8cee9a5fb9c85a46f5e0e0",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -32,6 +32,7 @@\n \n from transformers.activations import ACT2FN\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n@@ -1130,9 +1131,9 @@ class BambaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BambaMixer):\n-            module.dt_bias.fill_(1.0)\n-            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n-            module.D.fill_(1.0)\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n \n \n @auto_docstring"
        },
        {
            "sha": "d29273940b8aaf50fac1a589ad939256b7f03ebc",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -42,6 +42,7 @@\n     segment_sum,\n )\n \n+from ... import initialization as init\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -804,9 +805,9 @@ class BambaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, BambaMixer):\n-            module.dt_bias.fill_(1.0)\n-            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n-            module.D.fill_(1.0)\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n \n \n @auto_docstring"
        },
        {
            "sha": "791678604f12d535e3c1d658c1b4d788b2dd48c9",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 19,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -329,25 +329,6 @@ class BarkPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _supports_flash_attn = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear,)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            if getattr(module, \"bias\", None) is not None:\n-                module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @property\n     def device(self) -> torch.device:\n         \"\"\"\n@@ -1318,6 +1299,8 @@ def __init__(self, config):\n \n         self.config = config\n \n+        self.post_init()\n+\n     @classmethod\n     def can_generate(cls) -> bool:\n         # Bark has a unique model structure, where the external class (`BarkModel`) doesn't need to inherit from"
        },
        {
            "sha": "6e25fa4d30fcccd842ba121ce71b6f684fc23fae",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -476,21 +476,6 @@ class BartPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "76e20239648bb558c2a16c18f91de47ce42b0c5c",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import Tensor, nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -677,29 +678,19 @@ class BeitPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, BeitEmbeddings):\n-            module.cls_token.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, BeitEmbeddings):\n+            init.zeros_(module.cls_token)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n             if module.position_embeddings is not None:\n-                module.position_embeddings.zero_()\n+                init.zeros_(module.position_embeddings)\n         elif isinstance(module, BeitRelativePositionBias):\n-            module.relative_position_bias_table.zero_()\n+            init.zeros_(module.relative_position_bias_table)\n         elif isinstance(module, BeitLayer):\n             if module.lambda_1 is not None:\n-                module.lambda_1.fill_(self.config.layer_scale_init_value)\n-                module.lambda_2.fill_(self.config.layer_scale_init_value)\n+                init.constant_(module.lambda_1, self.config.layer_scale_init_value)\n+                init.constant_(module.lambda_2, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "b89382ca00a2dfc18cd90dd6b3372b75a0ab869a",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -565,19 +566,9 @@ class BertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, BertLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, BertLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @dataclass"
        },
        {
            "sha": "176ce4d003ae061036332d710c48c60d6a6ce7cf",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -459,19 +460,9 @@ class BertGenerationPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, BertGenerationOnlyLMHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, BertGenerationOnlyLMHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring("
        },
        {
            "sha": "1a90dd028c4923f6a03fd61be5c9ad77563682d4",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -1517,19 +1518,9 @@ class BigBirdPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, BigBirdLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, BigBirdLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @dataclass"
        },
        {
            "sha": "9fb140bd1e2b0c78b7b9e45c09be175413d7dec3",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -1539,21 +1539,6 @@ class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "61879e54b5a5ecd2ab5d906de5ba21677862e415",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import (\n     BackboneOutput,\n@@ -631,17 +632,17 @@ class BitPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Conv2d):\n-            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n         # copied from the `reset_parameters` method of `class Linear(Module)` in `torch`.\n         elif isinstance(module, nn.Linear):\n-            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n-            nn.init.constant_(module.weight, 1)\n-            nn.init.constant_(module.bias, 0)\n+            init.constant_(module.weight, 1)\n+            init.constant_(module.bias, 0)\n \n \n @auto_docstring"
        },
        {
            "sha": "63021a27ca48034ff8943b66a8f34eaaeb33f4b3",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -435,24 +435,8 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "85e342619ff9c8dc79db2c3cca6eb75fe10a89bf",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -428,24 +428,8 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "a1f06ae3a37ee687f74a51a8dc740d65cf7516ac",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn.functional import normalize\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -422,32 +423,13 @@ class BlipPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_range\n-        if isinstance(module, (nn.Conv2d, nn.Embedding, nn.Linear)):\n-            module.weight.normal_(mean=0.0, std=factor)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.zero_()\n-\n+        super()._init_weights(module)\n+        std = self.config.initializer_range\n         if isinstance(module, BlipVisionEmbeddings):\n             if hasattr(self.config, \"vision_config\"):\n-                factor = self.config.vision_config.initializer_range\n-            nn.init.trunc_normal_(\n-                module.position_embedding,\n-                mean=0.0,\n-                std=factor,\n-            )\n-\n-            nn.init.trunc_normal_(\n-                module.class_embedding,\n-                mean=0.0,\n-                std=factor,\n-            )\n-\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+                std = self.config.vision_config.initializer_range\n+            init.trunc_normal_(module.position_embedding, mean=0.0, std=std)\n+            init.trunc_normal_(module.class_embedding, mean=0.0, std=std)\n \n \n class BlipEncoder(nn.Module):"
        },
        {
            "sha": "eb67aaa45c3cc867d8649876e346c39e5e104063",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -504,17 +504,6 @@ class BlipTextPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     _no_split_modules = []\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Embedding)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n-\n \n # Adapted from https://github.com/salesforce/BLIP/blob/3a29b7410476bf5f2ba0955827390eb6ea1f4f9d/models/med.py#L571\n class BlipTextModel(BlipTextPreTrainedModel):"
        },
        {
            "sha": "542eaca5ca2963796abf6ad8f16954f2f78cc90c",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 15,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -411,20 +412,11 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=factor)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=factor)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, Blip2VisionEmbeddings):\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        super()._init_weights(module)\n+        std = self.config.initializer_range\n+        if isinstance(module, Blip2VisionEmbeddings):\n+            init.trunc_normal_(module.position_embedding, mean=0.0, std=std)\n+            init.trunc_normal_(module.class_embedding, mean=0.0, std=std)\n         elif isinstance(\n             module,\n             (\n@@ -435,7 +427,7 @@ def _init_weights(self, module):\n                 Blip2ForImageTextRetrieval,\n             ),\n         ):\n-            module.query_tokens.zero_()\n+            init.zeros_(module.query_tokens)\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2"
        },
        {
            "sha": "8b8d4f715274a67c668dca40560aaa4a83c2df60",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -418,27 +418,8 @@ class BloomPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BloomBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-\n     _can_compile_fullgraph = True\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module: nn.Module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class BloomModel(BloomPreTrainedModel):"
        },
        {
            "sha": "66de121e78c9431df73728a792031eed2de3f9d3",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, QuickGELUActivation\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n@@ -927,24 +928,24 @@ def _init_weights(self, module: nn.Module):\n             attn_std = self.config.hidden_size**-0.5\n             fc_std = (2 * self.config.hidden_size) ** -0.5\n             for block in module.transformer.resblocks:\n-                nn.init.normal_(block.attn.in_proj_weight, std=attn_std * std)\n-                block.attn.in_proj_bias.zero_()\n-                nn.init.normal_(block.attn.out_proj.weight, std=proj_std * std)\n-                nn.init.normal_(block.mlp.c_fc.weight, std=fc_std * std)\n-                nn.init.normal_(block.mlp.c_proj.weight, std=proj_std * std)\n-\n-            nn.init.normal_(module.embeddings.class_embedding, std=attn_std * std)\n-            nn.init.normal_(module.embeddings.position_embedding.weight, std=attn_std * std)\n+                init.normal_(block.attn.in_proj_weight, std=attn_std * std)\n+                init.zeros_(block.attn.in_proj_bias)\n+                init.normal_(block.attn.out_proj.weight, std=proj_std * std)\n+                init.normal_(block.mlp.c_fc.weight, std=fc_std * std)\n+                init.normal_(block.mlp.c_proj.weight, std=proj_std * std)\n+\n+            init.normal_(module.embeddings.class_embedding, std=attn_std * std)\n+            init.normal_(module.embeddings.position_embedding.weight, std=attn_std * std)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.Embedding)):\n-            module.weight.normal_(mean=0.0, std=0.05 * std)\n+            init.normal_(module.weight, mean=0.0, std=0.05 * std)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, BridgeTowerForContrastiveLearning):\n-            module.logit_scale.fill_(self.config.logit_scale_init_value)\n+            init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n \n         if isinstance(module, (nn.Linear, BridgeTowerMLMHead)) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class BridgeTowerVisionModel(BridgeTowerPreTrainedModel):"
        },
        {
            "sha": "1705d6a47b956043c4838f5b896438ff31329c99",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 8,
            "deletions": 17,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -517,20 +518,10 @@ class BrosPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, BrosRelationExtractor):\n-            nn.init.normal_(module.dummy_node, std=std)\n+        if isinstance(module, BrosRelationExtractor):\n+            init.normal_(module.dummy_node, std=std)\n \n \n @auto_docstring\n@@ -549,7 +540,7 @@ def __init__(self, config, add_pooling_layer=True):\n \n         self.pooler = BrosPooler(config) if add_pooling_layer else None\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.embeddings.word_embeddings\n@@ -693,7 +684,7 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n-        self.init_weights()\n+        self.post_init()\n \n     @can_return_tuple\n     @auto_docstring\n@@ -812,7 +803,7 @@ def __init__(self, config):\n         # Subsequent token classification for Entity Extraction (NER)\n         self.subsequent_token_classifier = BrosRelationExtractor(config)\n \n-        self.init_weights()\n+        self.post_init()\n \n     @can_return_tuple\n     @auto_docstring\n@@ -949,7 +940,7 @@ def __init__(self, config):\n \n         self.entity_linker = BrosRelationExtractor(config)\n \n-        self.init_weights()\n+        self.post_init()\n \n     @can_return_tuple\n     @auto_docstring"
        },
        {
            "sha": "797ecaea2bd881e7a8b25368ba8343ed7c511015",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -413,19 +414,9 @@ class CamembertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, CamembertLMHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, CamembertLMHead):\n+            init.zeros_(module.bias)\n \n \n class CamembertEmbeddings(nn.Module):"
        },
        {
            "sha": "1a9fd30d419e073764d9994aa5cea2a74ee145d5",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -719,21 +719,6 @@ class CaninePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"canine\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class CanineModel(CaninePreTrainedModel):"
        },
        {
            "sha": "1bf5e50beed651da364fb8f94c095b6e002ed42a",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -568,47 +569,47 @@ def _init_weights(self, module):\n         factor = self.config.initializer_factor\n         if isinstance(module, ChineseCLIPVisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, ChineseCLIPTextEmbeddings):\n-            nn.init.normal_(module.word_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n-            nn.init.normal_(module.position_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n-            nn.init.normal_(module.token_type_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.word_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.position_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.token_type_embeddings.weight, mean=0.0, std=self.config.initializer_range)\n             for embedding in [module.word_embeddings, module.position_embeddings, module.token_type_embeddings]:\n                 if embedding.padding_idx is not None:\n-                    embedding.weight[embedding.padding_idx].zero_()\n+                    init.zeros_(embedding.weight[embedding.padding_idx])\n         elif isinstance(module, ChineseCLIPVisionAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, ChineseCLIPVisionMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, ChineseCLIPModel):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n # Copied from transformers.models.align.modeling_align.AlignTextEncoder with Align->ChineseCLIP"
        },
        {
            "sha": "583ac01290b8c4469191e76ba1cc228fdd914875",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -1314,23 +1315,23 @@ def _init_weights(self, module: nn.Module):\n         factor = self.config.initializer_factor\n \n         if isinstance(module, ClapTextEmbeddings):\n-            module.position_embeddings.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.token_type_embeddings.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embeddings.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_type_embeddings.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, ClapModel):\n-            module.logit_scale_a.fill_(math.log(self.config.logit_scale_init_value))\n-            module.logit_scale_t.fill_(math.log(self.config.logit_scale_init_value))\n+            init.constant_(module.logit_scale_a, math.log(self.config.logit_scale_init_value))\n+            init.constant_(module.logit_scale_t, math.log(self.config.logit_scale_init_value))\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, (nn.Conv2d, nn.Linear)):\n             in_proj_std = (self.config.hidden_size**-0.5) * ((2 * self.config.num_hidden_layers) ** -0.5) * factor\n-            nn.init.normal_(module.weight, std=in_proj_std)\n+            init.normal_(module.weight, std=in_proj_std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, ClapAudioSelfAttention):\n-            module.relative_position_bias_table.zero_()\n+            init.zeros_(module.relative_position_bias_table)\n \n \n class ClapAudioModel(ClapPreTrainedModel):"
        },
        {
            "sha": "49bab19b971fbc71b6180073414dafce57029e34",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -413,57 +414,57 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, CLIPTextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, CLIPVisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, CLIPAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, CLIPMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, CLIPModel):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, CLIPVisionModelWithProjection):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, CLIPTextModelWithProjection):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, CLIPForImageClassification):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.classifier.weight,\n                 std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class CLIPEncoder(nn.Module):"
        },
        {
            "sha": "bba971644a233afeea0ba9c50ac23ee024b70010",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -432,42 +433,42 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, CLIPSegTextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, CLIPSegVisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, CLIPSegAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, CLIPSegMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, CLIPSegModel):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->CLIPSeg"
        },
        {
            "sha": "6c8171547aa3af54ffa9640235ad4eb8e0da5e06",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, get_activation\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationConfig, GenerationMixin\n@@ -786,37 +787,37 @@ def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, (nn.Linear, Conv1D, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.weight, mean=0.0, std=factor * 0.02)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, ClvpRMSNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, ClvpEncoderMLP):\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.proj.weight if getattr(module.fc1, \"proj\") else module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.proj.weight if getattr(module.fc1, \"proj\") else module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, ClvpEncoder):\n             config = self.config.get_text_config()\n             factor = config.initializer_factor\n-            module.projection.weight.normal_(mean=0.0, std=factor * (config.hidden_size**-0.5))\n+            init.normal_(module.projection.weight, mean=0.0, std=factor * (config.hidden_size**-0.5))\n         elif isinstance(module, ClvpConditioningEncoder):\n-            module.mel_conv.weight.normal_(mean=0.0, std=factor)\n-            module.mel_conv.bias.zero_()\n+            init.normal_(module.mel_conv.weight, mean=0.0, std=factor)\n+            init.zeros_(module.mel_conv.bias)\n         elif isinstance(module, ClvpForCausalLM):\n             for name, p in module.named_parameters():\n                 if name == \"c_proj.weight\":\n-                    p.normal_(\n-                        mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.num_hidden_layers))\n+                    init.normal_(\n+                        p, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.num_hidden_layers)\n                     )\n         elif isinstance(module, ClvpModelForConditionalGeneration):\n-            module.logit_scale.fill_(self.config.logit_scale_init_value)\n+            init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n \n         if isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n class ClvpEncoder(ClvpPreTrainedModel):"
        },
        {
            "sha": "307a1c9b7a4185d15e273f9b48accfd83161dd19",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -277,27 +277,8 @@ class CodeGenPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CodeGenBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-\n     _can_compile_fullgraph = True\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear,)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class CodeGenModel(CodeGenPreTrainedModel):"
        },
        {
            "sha": "75e35c4126c0fb07e4bbd6f708a1fbcd6030b2cc",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n \n from transformers import AutoModelForImageTextToText\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple\n@@ -47,13 +48,14 @@ def _init_weights(self, module):\n         )\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n @dataclass"
        },
        {
            "sha": "922d9bf3bd86f8ad5ca0212bef295230236b8a1c",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n \n from transformers import AutoModelForImageTextToText\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torch_available\n@@ -55,13 +56,14 @@ def _init_weights(self, module):\n         )\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n @dataclass"
        },
        {
            "sha": "9843dda12a60ae8ece640ff3472e46173e207183",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -976,21 +977,22 @@ def _init_weights(self, module):\n         xavier_std = self.config.init_xavier_std\n \n         if isinstance(module, ConditionalDetrMHAttentionMap):\n-            nn.init.zeros_(module.k_linear.bias)\n-            nn.init.zeros_(module.q_linear.bias)\n-            nn.init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n-            nn.init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n+            init.zeros_(module.k_linear.bias)\n+            init.zeros_(module.q_linear.bias)\n+            init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n+            init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n         elif isinstance(module, ConditionalDetrLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n # Copied from transformers.models.detr.modeling_detr.DetrEncoder with Detr->ConditionalDetr,DETR->ConditionalDETR\n@@ -1910,8 +1912,8 @@ def __init__(self, dim, fpn_dims, context_dim):\n \n         for m in self.modules():\n             if isinstance(m, nn.Conv2d):\n-                nn.init.kaiming_uniform_(m.weight, a=1)\n-                nn.init.constant_(m.bias, 0)\n+                init.kaiming_uniform_(m.weight, a=1)\n+                init.constant_(m.bias, 0)\n \n     def forward(self, x: Tensor, bbox_mask: Tensor, fpns: list[Tensor]):\n         # here we concatenate x, the projected feature map, of shape (batch_size, d_model, height/32, width/32) with"
        },
        {
            "sha": "ac33f13ad2b23e562e2eefa69395413a7671bc27",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, get_activation\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -111,22 +112,12 @@ class ConvBertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, SeparableConv1D):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, SeparableConv1D):\n+            init.zeros_(module.bias)\n         elif isinstance(module, GroupedLinearLayer):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            module.bias.zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.bias)\n \n \n class SeparableConv1D(nn.Module):"
        },
        {
            "sha": "851bee0060e16166410077bf5a9ea85a2027b6bb",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import (\n     BackboneOutput,\n@@ -243,16 +244,10 @@ class ConvNextPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, (nn.LayerNorm, ConvNextLayerNorm)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, ConvNextLayer):\n+        super()._init_weights(module)\n+        if isinstance(module, ConvNextLayer):\n             if module.layer_scale_parameter is not None:\n-                module.layer_scale_parameter.fill_(self.config.layer_scale_init_value)\n+                init.constant_(module.layer_scale_parameter, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "02e780aa70aaf382f8b5b99906cbdded3cdaaa81",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import (\n     BackboneOutput,\n@@ -263,16 +264,10 @@ class ConvNextV2PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, (nn.LayerNorm, ConvNextV2LayerNorm)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, ConvNextV2GRN):\n-            module.weight.zero_()\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, ConvNextV2GRN):\n+            init.zeros_(module.weight)\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "c1e778bef1a6fbbae7e08e6c88db18c047c4e00b",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -528,21 +529,11 @@ class CpmAntPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, CpmAntLayerNorm):\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n+        if isinstance(module, CpmAntLayerNorm):\n+            init.ones_(module.weight)\n         elif isinstance(module, CpmAntSegmentPositionEmbedding):\n-            module.relative_attention_bias.normal_(mean=0.0, std=self.config.init_std)\n+            init.normal_(module.relative_attention_bias, mean=0.0, std=self.config.init_std)\n \n \n @auto_docstring"
        },
        {
            "sha": "e92630e9591cc787b66b7e44bdde475aaa3f253a",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n \n from transformers.utils.generic import check_model_inputs\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -415,7 +416,7 @@ def _init_weights(self, module):\n         if isinstance(module, CsmCodebooksHead):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n-                module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+                init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "96ccda50cecba867da3c6762f61be5b591454c44",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers.utils.generic import check_model_inputs\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n@@ -146,7 +147,7 @@ def _init_weights(self, module):\n         if isinstance(module, CsmCodebooksHead):\n             num_codebooks = module.num_codebooks\n             for i in range(num_codebooks - 1):\n-                module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+                init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "a494b17c1cc949ff1900d1df5e7c49b23368005b",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,7 +26,6 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import Conv1D\n from ...utils import (\n     auto_docstring,\n     logging,\n@@ -188,21 +187,6 @@ class CTRLPreTrainedModel(PreTrainedModel):\n     config: CTRLConfig\n     base_model_prefix = \"transformer\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class CTRLModel(CTRLPreTrainedModel):"
        },
        {
            "sha": "698e1f8d33a187fd7e1173894d908bea2287ad14",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...modeling_outputs import ImageClassifierOutputWithNoAttention, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -493,17 +494,15 @@ class CvtPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.copy_(nn.init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range))\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, CvtStage):\n             if self.config.cls_token[module.stage]:\n-                module.cls_token.copy_(\n-                    nn.init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n-                )\n+                init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "d61b96ce566b3640d77cfa48a5a6dd8a433b912c",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,9 +24,9 @@\n \n import torch\n import torch.nn.functional as F\n-import torch.nn.init as init\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2CLS, ACT2FN\n from ...image_transforms import center_to_corners_format, corners_to_center_format\n from ...modeling_outputs import BaseModelOutput\n@@ -453,22 +453,22 @@ def _init_weights(self, module):\n                 for layer in module.class_embed:\n                     prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n                     bias = float(-math.log((1 - prior_prob) / prior_prob))\n-                    nn.init.xavier_uniform_(layer.weight)\n-                    nn.init.constant_(layer.bias, bias)\n+                    init.xavier_uniform_(layer.weight)\n+                    init.constant_(layer.bias, bias)\n \n             if module.bbox_embed is not None:\n                 for layer in module.bbox_embed:\n-                    nn.init.constant_(layer.layers[-1].weight, 0)\n-                    nn.init.constant_(layer.layers[-1].bias, 0)\n+                    init.constant_(layer.layers[-1].weight, 0)\n+                    init.constant_(layer.layers[-1].bias, 0)\n \n             if hasattr(module, \"reg_scale\"):\n-                module.reg_scale.fill_(self.config.reg_scale)\n+                init.constant_(module.reg_scale, self.config.reg_scale)\n \n             if hasattr(module, \"up\"):\n-                module.up.fill_(self.config.up)\n+                init.constant_(module.up, self.config.up)\n \n         if isinstance(module, DFineMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -478,22 +478,21 @@ def _init_weights(self, module):\n             grid_init = grid_init.reshape(module.n_heads, 1, 2).tile([1, sum(module.num_points_list), 1])\n             scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n             grid_init *= scaling\n-            with torch.no_grad():\n-                module.sampling_offsets.bias[...] = grid_init.flatten()\n+            init.copy_(module.sampling_offsets.bias, grid_init.flatten())\n \n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n \n         if isinstance(module, DFineModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))\n-            nn.init.xavier_uniform_(module.enc_score_head.weight)\n-            nn.init.constant_(module.enc_score_head.bias, bias)\n+            init.xavier_uniform_(module.enc_score_head.weight)\n+            init.constant_(module.enc_score_head.bias, bias)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n         if isinstance(module, DFineGate):\n             bias = float(-math.log((1 - 0.5) / 0.5))\n@@ -505,13 +504,13 @@ def _init_weights(self, module):\n             init.constant_(module.reg_conf.layers[-1].weight, 0)\n \n         if isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n-            nn.init.xavier_uniform_(module.weight_embedding.weight)\n+            init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n-            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n+            init.xavier_uniform_(module.denoising_class_embed.weight)\n \n \n class DFineIntegral(nn.Module):"
        },
        {
            "sha": "e0468b32e048ce2cbd9a4daca9a0664f03adb25c",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -17,9 +17,9 @@\n \n import torch\n import torch.nn.functional as F\n-import torch.nn.init as init\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2CLS\n from ...configuration_utils import PreTrainedConfig\n from ...image_transforms import corners_to_center_format\n@@ -591,28 +591,29 @@ def forward(\n class DFinePreTrainedModel(RTDetrPreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n         # initialize linear layer bias value according to a given probability value.\n         if isinstance(module, (DFineForObjectDetection, DFineDecoder)):\n             if module.class_embed is not None:\n                 for layer in module.class_embed:\n                     prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n                     bias = float(-math.log((1 - prior_prob) / prior_prob))\n-                    nn.init.xavier_uniform_(layer.weight)\n-                    nn.init.constant_(layer.bias, bias)\n+                    init.xavier_uniform_(layer.weight)\n+                    init.constant_(layer.bias, bias)\n \n             if module.bbox_embed is not None:\n                 for layer in module.bbox_embed:\n-                    nn.init.constant_(layer.layers[-1].weight, 0)\n-                    nn.init.constant_(layer.layers[-1].bias, 0)\n+                    init.constant_(layer.layers[-1].weight, 0)\n+                    init.constant_(layer.layers[-1].bias, 0)\n \n             if hasattr(module, \"reg_scale\"):\n-                module.reg_scale.fill_(self.config.reg_scale)\n+                init.constant_(module.reg_scale, self.config.reg_scale)\n \n             if hasattr(module, \"up\"):\n-                module.up.fill_(self.config.up)\n+                init.constant_(module.up, self.config.up)\n \n         if isinstance(module, DFineMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -622,22 +623,21 @@ def _init_weights(self, module):\n             grid_init = grid_init.reshape(module.n_heads, 1, 2).tile([1, sum(module.num_points_list), 1])\n             scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n             grid_init *= scaling\n-            with torch.no_grad():\n-                module.sampling_offsets.bias[...] = grid_init.flatten()\n+            init.copy_(module.sampling_offsets.bias, grid_init.flatten())\n \n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n \n         if isinstance(module, DFineModel):\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias = float(-math.log((1 - prior_prob) / prior_prob))\n-            nn.init.xavier_uniform_(module.enc_score_head.weight)\n-            nn.init.constant_(module.enc_score_head.bias, bias)\n+            init.xavier_uniform_(module.enc_score_head.weight)\n+            init.constant_(module.enc_score_head.bias, bias)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n         if isinstance(module, DFineGate):\n             bias = float(-math.log((1 - 0.5) / 0.5))\n@@ -649,13 +649,13 @@ def _init_weights(self, module):\n             init.constant_(module.reg_conf.layers[-1].weight, 0)\n \n         if isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n         if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n-            nn.init.xavier_uniform_(module.weight_embedding.weight)\n+            init.xavier_uniform_(module.weight_embedding.weight)\n         if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n-            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n+            init.xavier_uniform_(module.denoising_class_embed.weight)\n \n \n class DFineIntegral(nn.Module):"
        },
        {
            "sha": "e1d25bc121d002adaf8e8b8a8e4cc60688294a2c",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -821,29 +822,30 @@ def _init_weights(self, module):\n         xavier_std = self.config.init_xavier_std\n \n         if isinstance(module, DabDetrMHAttentionMap):\n-            nn.init.zeros_(module.k_linear.bias)\n-            nn.init.zeros_(module.q_linear.bias)\n-            nn.init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n-            nn.init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n+            init.zeros_(module.k_linear.bias)\n+            init.zeros_(module.q_linear.bias)\n+            init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n+            init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, DabDetrForObjectDetection):\n-            nn.init.constant_(module.bbox_predictor.layers[-1].weight, 0)\n-            nn.init.constant_(module.bbox_predictor.layers[-1].bias, 0)\n+            init.constant_(module.bbox_predictor.layers[-1].weight, 0)\n+            init.constant_(module.bbox_predictor.layers[-1].bias, 0)\n \n             # init prior_prob setting for focal loss\n             prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n             bias_value = -math.log((1 - prior_prob) / prior_prob)\n-            module.class_embed.bias.fill_(bias_value)\n+            init.constant_(module.class_embed.bias, bias_value)\n         elif isinstance(module, nn.PReLU):\n             module.reset_parameters()\n "
        },
        {
            "sha": "d74a369d3d339dab0b28f01adc5b0dc8cccb9e39",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...modeling_utils import PreTrainedAudioTokenizerBase\n from ...utils import ModelOutput, auto_docstring\n from .configuration_dac import DacConfig\n@@ -480,14 +481,14 @@ class DacPreTrainedModel(PreTrainedAudioTokenizerBase):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Conv1d):\n-            nn.init.trunc_normal_(module.weight, std=0.02)\n-            nn.init.constant_(module.bias, 0)\n+            init.trunc_normal_(module.weight, std=0.02)\n+            init.constant_(module.bias, 0)\n         elif isinstance(module, Snake1d):\n-            module.alpha.fill_(1.0)\n+            init.ones_(module.alpha)\n         elif isinstance(module, nn.ConvTranspose1d):\n             module.reset_parameters()\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=0.02)\n+            init.normal_(module.weight, mean=0.0, std=0.02)\n \n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm"
        },
        {
            "sha": "5288c36de86e3112dc25ce64da969b2377fef46e",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -485,26 +486,26 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Data2VecAudioFeatureProjection):\n             k = math.sqrt(1 / module.projection.in_features)\n-            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n-            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+            init.uniform_(module.projection.weight, a=-k, b=k)\n+            init.uniform_(module.projection.bias, a=-k, b=k)\n         elif isinstance(module, Data2VecAudioPositionalConvLayer):\n-            nn.init.constant_(module.conv.bias, 0)\n+            init.constant_(module.conv.bias, 0)\n         elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n             if module.weight is not None:\n-                module.weight.fill_(1.0)\n+                init.ones_(module.weight)\n         elif isinstance(module, nn.Conv1d):\n-            nn.init.kaiming_normal_(module.weight)\n+            init.kaiming_normal_(module.weight)\n \n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-k, b=k)\n+                init.uniform_(module.bias, a=-k, b=k)\n \n     def _get_feat_extract_output_lengths(\n         self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None\n@@ -1043,7 +1044,7 @@ def __init__(self, config):\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n         self.num_labels = config.num_labels\n \n-        self.init_weights()\n+        self.post_init()\n \n     def freeze_feature_encoder(self):\n         \"\"\"\n@@ -1199,7 +1200,7 @@ def __init__(self, config):\n \n         self.objective = AMSoftmaxLoss(config.xvector_output_dim, config.num_labels)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def freeze_feature_encoder(self):\n         \"\"\""
        },
        {
            "sha": "190ebea39aa645ddc85b60b31543452373af3b16",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -494,25 +494,6 @@ class Data2VecTextPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": Data2VecTextCrossAttention,\n     }\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.zero_()\n-            if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.fill_(1.0)\n-\n \n class Data2VecTextEncoder(nn.Module):\n     def __init__(self, config):"
        },
        {
            "sha": "d21dfd325e4b4cae5d5b3f9fa222ab4d553f4b0a",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -691,29 +692,19 @@ class Data2VecVisionPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, Data2VecVisionEmbeddings):\n-            module.cls_token.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, Data2VecVisionEmbeddings):\n+            init.zeros_(module.cls_token)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n             if module.position_embeddings is not None:\n-                module.position_embeddings.zero_()\n+                init.zeros_(module.position_embeddings)\n         elif isinstance(module, Data2VecVisionRelativePositionBias):\n-            module.relative_position_bias_table.zero_()\n+            init.zeros_(module.relative_position_bias_table)\n         elif isinstance(module, Data2VecVisionLayer):\n             if module.lambda_1 is not None:\n-                module.lambda_1.fill_(self.config.layer_scale_init_value)\n-                module.lambda_2.fill_(self.config.layer_scale_init_value)\n+                init.constant_(module.lambda_1, self.config.layer_scale_init_value)\n+                init.constant_(module.lambda_2, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "78e739911cb5d2c72ee62864d6db8e4c2aa053aa",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import Wav2Vec2BaseModelOutput\n@@ -149,26 +150,26 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Data2VecAudioFeatureProjection):\n             k = math.sqrt(1 / module.projection.in_features)\n-            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n-            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+            init.uniform_(module.projection.weight, a=-k, b=k)\n+            init.uniform_(module.projection.bias, a=-k, b=k)\n         elif isinstance(module, Data2VecAudioPositionalConvLayer):\n-            nn.init.constant_(module.conv.bias, 0)\n+            init.constant_(module.conv.bias, 0)\n         elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n             if module.weight is not None:\n-                module.weight.fill_(1.0)\n+                init.ones_(module.weight)\n         elif isinstance(module, nn.Conv1d):\n-            nn.init.kaiming_normal_(module.weight)\n+            init.kaiming_normal_(module.weight)\n \n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-k, b=k)\n+                init.uniform_(module.bias, a=-k, b=k)\n \n     def _get_adapters(self):\n         raise AttributeError(\"Not needed for Data2VecAudio\")"
        },
        {
            "sha": "7cce78815954004b8cc26acf793ef75d28fa56ec",
            "filename": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -81,25 +81,6 @@ class Data2VecTextPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": Data2VecTextCrossAttention,\n     }\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.zero_()\n-            if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class Data2VecTextModel(RobertaModel):"
        },
        {
            "sha": "ddf5fce4dfcec566df484444c598c783c4d9434c",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 6,
            "deletions": 16,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -468,23 +469,12 @@ class DbrxPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n+        super()._init_weights(module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, DbrxExpertGLU):\n-            module.w1.normal_(mean=0.0, std=std)\n-            module.v1.normal_(mean=0.0, std=std)\n-            module.w2.normal_(mean=0.0, std=std)\n+        if isinstance(module, DbrxExpertGLU):\n+            init.normal_(module.w1, mean=0.0, std=std)\n+            init.normal_(module.v1, mean=0.0, std=std)\n+            init.normal_(module.w2, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "42a9079cb01206b24cea813dafd547d98bf9211c",
            "filename": "src/transformers/models/dbrx/modular_dbrx.py",
            "status": "modified",
            "additions": 6,
            "deletions": 16,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -338,23 +339,12 @@ class DbrxPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n+        super()._init_weights(module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, DbrxExpertGLU):\n-            module.w1.normal_(mean=0.0, std=std)\n-            module.v1.normal_(mean=0.0, std=std)\n-            module.w2.normal_(mean=0.0, std=std)\n+        if isinstance(module, DbrxExpertGLU):\n+            init.normal_(module.w1, mean=0.0, std=std)\n+            init.normal_(module.v1, mean=0.0, std=std)\n+            init.normal_(module.w2, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "b97938feb4fc4c7f6f51bc7ca69931835e570966",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -617,22 +618,12 @@ class DebertaPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, DebertaLayerNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, DisentangledSelfAttention):\n-            module.q_bias.zero_()\n-            module.v_bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, DisentangledSelfAttention):\n+            init.zeros_(module.q_bias)\n+            init.zeros_(module.v_bias)\n         elif isinstance(module, (LegacyDebertaLMPredictionHead, DebertaLMPredictionHead)):\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "52299f766fbb75890142ee3c1a5ca7e781425919",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -696,19 +697,9 @@ class DebertaV2PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, (LegacyDebertaV2LMPredictionHead, DebertaV2LMPredictionHead)):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, (LegacyDebertaV2LMPredictionHead, DebertaV2LMPredictionHead)):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring\n@@ -1282,7 +1273,7 @@ def __init__(self, config):\n         drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n         self.dropout = nn.Dropout(drop_out)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.deberta.get_input_embeddings()"
        },
        {
            "sha": "efbb6cbc36d72fbf83706914dcb24c95cd2274e4",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 27,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -374,17 +375,7 @@ def __init__(self, *inputs, **kwargs):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n@@ -396,7 +387,7 @@ def _init_weights(self, module):\n             for name, p in module.named_parameters():\n                 if \"c_proj\" in name and \"weight\" in name:\n                     # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                    p.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n+                    init.normal_(p, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n \n \n class DecisionTransformerGPT2Model(DecisionTransformerGPT2PreTrainedModel):\n@@ -614,21 +605,6 @@ class DecisionTransformerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"states\"\n     supports_gradient_checkpointing = False\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "1d0ccb2e8f2a32d0277cef4606ba9da669368799",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -465,7 +466,7 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DeepseekV2Moe):\n-            module.gate.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.gate.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "3deed89d041fce0c91c20e841dd795158dc45677",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...modeling_rope_utils import RopeParameters, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -436,7 +437,7 @@ class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV2Moe):\n-            module.gate.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.gate.weight, mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV2Model(LlamaModel):"
        },
        {
            "sha": "a18d0b5083fdfca86e56f265d842edb313c81751",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -12,6 +12,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -552,7 +553,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DeepseekV3TopkRouter):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "23d0f1e568d43c0eb38cca1e6f4e680f110a9815",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -6,6 +6,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GenericForTokenClassification\n@@ -308,7 +309,7 @@ class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV3TopkRouter):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV3Model(LlamaModel):"
        },
        {
            "sha": "e8dffe07e84f9783806ab51ce36e9d6048b5a374",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -132,15 +132,6 @@ class DeepseekVLPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        # Required only for Linear layer in DeepseekVLAligner\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-\n \n @auto_docstring\n class DeepseekVLModel(DeepseekVLPreTrainedModel):"
        },
        {
            "sha": "18f2b65a669e1c1faa0d12330ec1c74ea419bb58",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -134,15 +134,6 @@ def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n class DeepseekVLPreTrainedModel(JanusPreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        # Required only for Linear layer in DeepseekVLAligner\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-\n \n @auto_docstring\n class DeepseekVLModel(JanusModel):"
        },
        {
            "sha": "99e83417a90399a14a006e1f7208325c45e1aac8",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n@@ -218,18 +219,18 @@ class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.text_config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Conv2d):\n-            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, DeepseekVLHybridLayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, DeepseekVLHybridModel):\n-            module.high_res_vision_alpha.zero_()\n+            init.zeros_(module.high_res_vision_alpha)\n \n \n DEEPSEEK_VL_COMMON_CUSTOM_ARGS = r\"\"\""
        },
        {
            "sha": "4501ed7810d2e8b08f202b721dad8494a1e4a9e6",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,6 +18,7 @@\n import torch.nn as nn\n from torchvision.transforms.v2 import functional as F\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -220,18 +221,18 @@ class DeepseekVLHybridPreTrainedModel(DeepseekVLPreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.text_config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.text_config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Conv2d):\n-            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, DeepseekVLHybridLayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, DeepseekVLHybridModel):\n-            module.high_res_vision_alpha.zero_()\n+            init.zeros_(module.high_res_vision_alpha)\n \n \n class DeepseekVLHybridModel(DeepseekVLModel):"
        },
        {
            "sha": "e92ce998e8b87eb225c2bd353c13133182712540",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n@@ -931,10 +932,10 @@ def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, DeformableDetrLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, DeformableDetrMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -947,27 +948,28 @@ def _init_weights(self, module):\n             )\n             for i in range(module.n_points):\n                 grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight)\n-            nn.init.constant_(module.value_proj.bias, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight)\n-            nn.init.constant_(module.output_proj.bias, 0.0)\n+\n+            init.copy_(module.sampling_offsets.bias, grid_init.view(-1))\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n+            init.xavier_uniform_(module.value_proj.weight)\n+            init.constant_(module.value_proj.bias, 0.0)\n+            init.xavier_uniform_(module.output_proj.weight)\n+            init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n-            nn.init.normal_(module.level_embed)\n+            init.normal_(module.level_embed)\n \n \n class DeformableDetrEncoder(DeformableDetrPreTrainedModel):"
        },
        {
            "sha": "62a787e11507320c47a60af2fe3c2c7ce0b58887",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -370,24 +371,18 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, DeiTEmbeddings):\n-            module.cls_token.zero_()\n-            module.position_embeddings.zero_()\n-            module.distillation_token.zero_()\n+            init.zeros_(module.cls_token)\n+            init.zeros_(module.position_embeddings)\n+            init.zeros_(module.distillation_token)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n \n \n @auto_docstring"
        },
        {
            "sha": "aa416298f5489347251ca22ab273c567782dfe73",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 24,
            "deletions": 22,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....file_utils import (\n     ModelOutput,\n@@ -573,7 +574,7 @@ def __init__(self, config: DetaConfig, num_heads: int, n_points: int):\n         self._reset_parameters()\n \n     def _reset_parameters(self):\n-        nn.init.constant_(self.sampling_offsets.weight.data, 0.0)\n+        init.constant_(self.sampling_offsets.weight.data, 0.0)\n         default_dtype = torch.get_default_dtype()\n         thetas = torch.arange(self.n_heads, dtype=torch.int64).to(default_dtype) * (2.0 * math.pi / self.n_heads)\n         grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n@@ -586,12 +587,12 @@ def _reset_parameters(self):\n             grid_init[:, :, i, :] *= i + 1\n         with torch.no_grad():\n             self.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-        nn.init.constant_(self.attention_weights.weight.data, 0.0)\n-        nn.init.constant_(self.attention_weights.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.value_proj.weight.data)\n-        nn.init.constant_(self.value_proj.bias.data, 0.0)\n-        nn.init.xavier_uniform_(self.output_proj.weight.data)\n-        nn.init.constant_(self.output_proj.bias.data, 0.0)\n+        init.constant_(self.attention_weights.weight.data, 0.0)\n+        init.constant_(self.attention_weights.bias.data, 0.0)\n+        init.xavier_uniform_(self.value_proj.weight.data)\n+        init.constant_(self.value_proj.bias.data, 0.0)\n+        init.xavier_uniform_(self.output_proj.weight.data)\n+        init.constant_(self.output_proj.bias.data, 0.0)\n \n     def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n         return tensor if position_embeddings is None else tensor + position_embeddings\n@@ -993,23 +994,24 @@ def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, DetaLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, DetaMultiscaleDeformableAttention):\n             module._reset_parameters()\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n-            nn.init.normal_(module.level_embed)\n+            init.normal_(module.level_embed)\n \n \n DETA_START_DOCSTRING = r\"\"\"\n@@ -1812,15 +1814,15 @@ def __init__(self, config: DetaConfig):\n         prior_prob = 0.01\n         bias_value = -math.log((1 - prior_prob) / prior_prob)\n         self.class_embed.bias.data.fill_(bias_value)\n-        nn.init.constant_(self.bbox_embed.layers[-1].weight.data, 0)\n-        nn.init.constant_(self.bbox_embed.layers[-1].bias.data, 0)\n+        init.constant_(self.bbox_embed.layers[-1].weight.data, 0)\n+        init.constant_(self.bbox_embed.layers[-1].bias.data, 0)\n \n         # if two-stage, the last class_embed and bbox_embed is for region proposal generation\n         num_pred = (config.decoder_layers + 1) if config.two_stage else config.decoder_layers\n         if config.with_box_refine:\n             self.class_embed = _get_clones(self.class_embed, num_pred)\n             self.bbox_embed = _get_clones(self.bbox_embed, num_pred)\n-            nn.init.constant_(self.bbox_embed[0].layers[-1].bias.data[2:], -2.0)\n+            init.constant_(self.bbox_embed[0].layers[-1].bias.data[2:], -2.0)\n             # hack implementation for iterative bounding box refinement\n             self.model.decoder.bbox_embed = self.bbox_embed\n             self._tied_weights_keys.update(\n@@ -1829,7 +1831,7 @@ def __init__(self, config: DetaConfig):\n                 }\n             )\n         else:\n-            nn.init.constant_(self.bbox_embed.layers[-1].bias.data[2:], -2.0)\n+            init.constant_(self.bbox_embed.layers[-1].bias.data[2:], -2.0)\n             self.class_embed = nn.ModuleList([self.class_embed for _ in range(num_pred)])\n             self.bbox_embed = nn.ModuleList([self.bbox_embed for _ in range(num_pred)])\n             self.model.decoder.bbox_embed = None\n@@ -1842,7 +1844,7 @@ def __init__(self, config: DetaConfig):\n                 }\n             )\n             for box_embed in self.bbox_embed:\n-                nn.init.constant_(box_embed.layers[-1].bias.data[2:], 0.0)\n+                init.constant_(box_embed.layers[-1].bias.data[2:], 0.0)\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "8f1f657c5f4258c6b87ce437792f411add09bfaa",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -498,17 +498,6 @@ class EfficientFormerPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n \n-    @torch.no_grad()\n-    def _init_weights(self, module: nn.Module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n EFFICIENTFORMER_START_DOCSTRING = r\"\"\"\n     This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module) subclass. Use it as a"
        },
        {
            "sha": "a22e21bd7a49117359365498388b642d43ab5cc2",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -368,21 +368,6 @@ class ErnieMPreTrainedModel(PreTrainedModel):\n     config: ErnieMConfig\n     base_model_prefix = \"ernie_m\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n ERNIE_M_START_DOCSTRING = r\"\"\"\n "
        },
        {
            "sha": "291677c6caa4eab21c44de0604e3a5bd5e057093",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 22,
            "deletions": 21,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,6 +18,7 @@\n import torch\n import torch.nn as nn\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n@@ -533,56 +534,56 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(factor * 1.0)\n-            module.bias.zero_()\n+            init.constant_(module.weight, factor * 1.0)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.weight, mean=0.0, std=factor * 1.0)\n         elif isinstance(module, GPTSanJapaneseModel):\n             # Mesh TensorFlow embeddings initialization\n             # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n-            module.embed_tokens.weight.normal_(mean=0.0, std=factor * 1.0)\n-            module.position_embeddings.weight.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.embed_tokens.weight, mean=0.0, std=factor * 1.0)\n+            init.normal_(module.position_embeddings.weight, mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"extra_position_embeddings\") and module.extra_position_embeddings is not None:\n-                module.extra_position_embeddings.weight.normal_(mean=0.0, std=factor * 1.0)\n+                init.normal_(module.extra_position_embeddings.weight, mean=0.0, std=factor * 1.0)\n         elif isinstance(module, (GPTSanJapaneseModel, GPTSanJapaneseForConditionalGeneration)):\n             # Mesh TensorFlow embeddings initialization\n             # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n-            module.final_logits_bias.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.final_logits_bias, mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n-                module.lm_head.weight.normal_(mean=0.0, std=factor * 1.0)\n+                init.normal_(module.lm_head.weight, mean=0.0, std=factor * 1.0)\n         elif isinstance(module, GPTSanJapaneseDenseActDense):\n             # Mesh TensorFlow FF initialization\n             # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n             # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n-            module.wi.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                module.wi.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, GPTSanJapaneseAttention):\n             # Multi-headed attention\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_model\n             n_heads = self.config.num_heads\n-            module.k_proj.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.v_proj.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.q_proj.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.out_proj.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.k_proj.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.v_proj.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.q_proj.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.out_proj.weight, mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n         elif isinstance(module, GPTSanJapaneseSparseMLP):\n             # Mesh TensorFlow attention initialization to avoid scaling before softmax\n             # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_model\n             n_heads = self.config.num_heads\n-            module.router.classifier.weight.normal_(mean=0.0, std=factor * 1)\n+            init.normal_(module.router.classifier.weight, mean=0.0, std=factor * 1)\n             for idx in range(self.config.num_experts):\n-                module.experts[f\"expert_{idx}\"].wi.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-                module.experts[f\"expert_{idx}\"].wo.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n+                init.normal_(module.experts[f\"expert_{idx}\"].wi.weight, mean=0.0, std=factor * (d_model**-0.5))\n+                init.normal_(module.experts[f\"expert_{idx}\"].wo.weight, mean=0.0, std=factor * (d_model**-0.5))\n \n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id"
        },
        {
            "sha": "0069abd2ce6fe1e5fdba82e07bb41f79f470bfb8",
            "filename": "src/transformers/models/deprecated/graphormer/modeling_graphormer.py",
            "status": "modified",
            "additions": 23,
            "deletions": 41,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n@@ -346,17 +347,17 @@ def reset_parameters(self):\n         if self.qkv_same_dim:\n             # Empirically observed the convergence to be much better with\n             # the scaled initialization\n-            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n-            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n-            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n+            init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))\n+            init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))\n+            init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))\n         else:\n-            nn.init.xavier_uniform_(self.k_proj.weight)\n-            nn.init.xavier_uniform_(self.v_proj.weight)\n-            nn.init.xavier_uniform_(self.q_proj.weight)\n+            init.xavier_uniform_(self.k_proj.weight)\n+            init.xavier_uniform_(self.v_proj.weight)\n+            init.xavier_uniform_(self.q_proj.weight)\n \n-        nn.init.xavier_uniform_(self.out_proj.weight)\n+        init.xavier_uniform_(self.out_proj.weight)\n         if self.out_proj.bias is not None:\n-            nn.init.constant_(self.out_proj.bias, 0.0)\n+            init.constant_(self.out_proj.bias, 0.0)\n \n     def forward(\n         self,\n@@ -709,27 +710,23 @@ class GraphormerPreTrainedModel(PreTrainedModel):\n     main_input_name_nodes = \"input_nodes\"\n     main_input_name_edges = \"input_edges\"\n \n-    def normal_(self, data: torch.Tensor):\n-        # with FSDP, module params will be on CUDA, so we cast them back to CPU\n-        # so that the RNG is consistent with and without FSDP\n-        data.copy_(data.cpu().normal_(mean=0.0, std=0.02).to(data.device))\n-\n     def init_graphormer_params(self, module: Union[nn.Linear, nn.Embedding, GraphormerMultiheadAttention]):\n         \"\"\"\n         Initialize the weights specific to the Graphormer Model.\n         \"\"\"\n         if isinstance(module, nn.Linear):\n-            self.normal_(module.weight.data)\n+            init.normal_(module.weight.data, mean=0.0, std=0.02)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         if isinstance(module, nn.Embedding):\n-            self.normal_(module.weight.data)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+            init.normal_(module.weight.data, mean=0.0, std=0.02)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         if isinstance(module, GraphormerMultiheadAttention):\n-            self.normal_(module.q_proj.weight.data)\n-            self.normal_(module.k_proj.weight.data)\n-            self.normal_(module.v_proj.weight.data)\n+            init.normal_(module.q_proj.weight.data, mean=0.0, std=0.02)\n+            init.normal_(module.k_proj.weight.data, mean=0.0, std=0.02)\n+            init.normal_(module.v_proj.weight.data, mean=0.0, std=0.02)\n \n     @torch.no_grad()\n     def _init_weights(\n@@ -741,31 +738,16 @@ def _init_weights(\n         \"\"\"\n         Initialize the weights\n         \"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # We might be missing part of the Linear init, dependent on the layer num\n-            module.weight.normal_(mean=0.0, std=0.02)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=0.02)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, GraphormerMultiheadAttention):\n-            module.q_proj.weight.normal_(mean=0.0, std=0.02)\n-            module.k_proj.weight.normal_(mean=0.0, std=0.02)\n-            module.v_proj.weight.normal_(mean=0.0, std=0.02)\n+        super()._init_weights(module)\n+        if isinstance(module, GraphormerMultiheadAttention):\n+            init.normal_(module.q_proj.weight, mean=0.0, std=0.02)\n+            init.normal_(module.k_proj.weight, mean=0.0, std=0.02)\n+            init.normal_(module.v_proj.weight, mean=0.0, std=0.02)\n             module.reset_parameters()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n         elif isinstance(module, GraphormerGraphEncoder):\n             if module.apply_graphormer_init:\n                 module.apply(self.init_graphormer_params)\n \n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n class GraphormerModel(GraphormerPreTrainedModel):\n     \"\"\"The Graphormer model is a graph-encoder model."
        },
        {
            "sha": "e71d82b745e3942e626576eeee44cec2fd731330",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 21,
            "deletions": 20,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import LayerNorm as FusedLayerNorm\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, logging\n@@ -604,20 +605,20 @@ class JukeboxVQVAE(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, nn.Embedding):  # embed_tokens\n-            module.weight.normal_(mean=0.0, std=0.02 * self.config.init_scale)\n+            init.normal_(module.weight, mean=0.0, std=0.02 * self.config.init_scale)\n         elif isinstance(module, JukeboxConv1D):\n             if self.config.zero_out:\n-                module.weight.zero_()\n+                init.zeros_(module.weight)\n             else:\n-                module.weight.normal_(mean=0.0, std=0.02 * self.config.init_scale)\n+                init.normal_(module.weight, mean=0.0, std=0.02 * self.config.init_scale)\n         elif isinstance(module, JukeboxResConv1DBlock) and self.config.zero_out:\n-            module.conv1d_2.weight.zero_()\n-            module.conv1d_2.bias.zero_()\n+            init.zeros_(module.conv1d_2.weight)\n+            init.zeros_(module.conv1d_2.bias)\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n     def __init__(self, config: JukeboxVQVAEConfig):\n         super().__init__(config)\n@@ -1796,28 +1797,28 @@ def _init_weights(self, module):\n         init_scale = self.config.init_scale\n \n         if isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=0.02 * init_scale)\n+            init.normal_(module.weight, mean=0.0, std=0.02 * init_scale)\n         elif isinstance(module, JukeboxConv1D):\n             if self.config.zero_out:\n-                module.weight.zero_()\n+                init.zeros_(module.weight)\n             else:\n-                module.weight.normal_(mean=0.0, std=0.02 * init_scale)\n+                init.normal_(module.weight, mean=0.0, std=0.02 * init_scale)\n         elif isinstance(module, JukeboxPositionalEmbedding):\n-            module.pos_emb.normal_(mean=0.0, std=0.01 * init_scale)\n+            init.normal_(module.pos_emb, mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxRangeEmbedding):\n-            module.emb.weight.normal_(mean=0.0, std=0.01 * init_scale)\n+            init.normal_(module.emb.weight, mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxConditionalAutoregressive) and hasattr(module, \"lm_head\"):\n-            module.lm_head.weight.normal_(mean=0.0, std=0.02 * init_scale)\n+            init.normal_(module.lm_head.weight, mean=0.0, std=0.02 * init_scale)\n         elif isinstance(module, JukeboxConditionalAutoregressive) and hasattr(module, \"start_token\"):\n-            module.start_token.normal_(mean=0.0, std=0.01 * init_scale)\n+            init.normal_(module.start_token, mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxResConv1DBlock) and self.config.zero_out:\n-            module.conv1d_2.weight.zero_()\n-            module.conv1d_2.bias.zero_()\n+            init.zeros_(module.conv1d_2.weight)\n+            init.zeros_(module.conv1d_2.bias)\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n     def __init__(self, config: JukeboxPriorConfig, level=None, nb_priors=3, vqvae_encoder=None, vqvae_decoder=None):\n         super().__init__(config)"
        },
        {
            "sha": "8f0530c407569f0ee92a5316d030ce2c1e3767a9",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 5,
            "deletions": 19,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n from ....integrations.deepspeed import is_deepspeed_zero3_enabled\n@@ -395,25 +396,10 @@ class MCTCTPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, MCTCTLayerNorm):\n-            module.singleton_weight.fill_(1.0)\n-            module.singleton_bias.zero_()\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, MCTCTLayerNorm):\n+            init.ones_(module.singleton_weight)\n+            init.zeros_(module.singleton_bias)\n \n     def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n         \"\"\""
        },
        {
            "sha": "eb0e93d636db336fd06f15a8bf0b788705e561e3",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 25,
            "deletions": 19,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....cache_utils import Cache\n from ....modeling_outputs import (\n@@ -1338,44 +1339,49 @@ def _init_weights(self, module):\n         if isinstance(module, MegaMultiDimensionDampedEma):\n             with torch.no_grad():\n                 # delta & alpha\n-                nn.init.normal_(module.damping_factor, mean=0.0, std=self.config.ema_delta_alpha_range)\n-                nn.init.normal_(module.decay_factor, mean=0.0, std=self.config.ema_delta_alpha_range)\n+                init.normal_(module.damping_factor, mean=0.0, std=self.config.ema_delta_alpha_range)\n+                init.normal_(module.decay_factor, mean=0.0, std=self.config.ema_delta_alpha_range)\n                 # beta [1, -1, 1, -1, ...] seems more stable.\n                 val = torch.ones(self.config.ema_projection_size, 1)\n                 if self.config.ema_projection_size > 1:\n                     idx = torch.tensor(list(range(1, self.config.ema_projection_size, 2)))\n                     val.index_fill_(0, idx, -1.0)\n-                module.ema_expansion_matrix.normal_(mean=0.0, std=self.config.ema_beta_range).add_(val)\n+                init.copy_(\n+                    module.ema_expansion_matrix,\n+                    torch.normal(mean=0.0, std=self.config.ema_beta_range, size=module.ema_expansion_matrix.shape)\n+                    + val,\n+                )\n                 # gamma & omega\n-                nn.init.normal_(module.kernel_projection_matrix, mean=0.0, std=self.config.ema_gamma_omega_range)\n-                nn.init.normal_(module.residual_weight, mean=0.0, std=self.config.ema_gamma_omega_range)\n+                init.normal_(module.kernel_projection_matrix, mean=0.0, std=self.config.ema_gamma_omega_range)\n+                init.normal_(module.residual_weight, mean=0.0, std=self.config.ema_gamma_omega_range)\n         elif isinstance(module, MegaSimpleRelativePositionalBias):\n-            nn.init.normal_(module.rel_pos_bias, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.rel_pos_bias, mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, MegaRotaryRelativePositionalBias):\n-            nn.init.normal_(module.alpha, mean=0.0, std=self.config.initializer_range)\n-            nn.init.normal_(module.b_param, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.alpha, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.b_param, mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, MegaScaleNorm):\n             if self.config.norm_affine:\n-                nn.init.constant_(module.scalar, 1.0)\n+                init.constant_(module.scalar, 1.0)\n         elif isinstance(module, MegaRMSNorm):\n             if self.config.norm_affine:\n-                nn.init.constant_(module.weight, 1.0)\n+                init.constant_(module.weight, 1.0)\n         elif isinstance(module, MegaMovingAverageGatedAttention):\n             # linear layers covered separately by the generic nn.Linear init below\n-            nn.init.normal_(module.qk_weight, mean=0.0, std=self.config.initializer_range)\n-            nn.init.constant_(module.qk_bias, 0.0)\n+            init.normal_(module.qk_weight, mean=0.0, std=self.config.initializer_range)\n+            init.constant_(module.qk_bias, 0.0)\n         elif isinstance(module, nn.Linear):\n             # initializes all linear layers in the entire network\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n MEGA_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "20e2d62ee57f18636d8547d4f28b342e4e314ae4",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -592,17 +592,6 @@ class NatPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"nat\"\n     main_input_name = \"pixel_values\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n NAT_START_DOCSTRING = r\"\"\"\n     This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use"
        },
        {
            "sha": "3bb957339e42e46ba0d01ff056abb594794d8985",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -587,21 +587,6 @@ class NezhaPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"nezha\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @dataclass\n class NezhaForPreTrainingOutput(ModelOutput):"
        },
        {
            "sha": "261fa5c9770821fdf5c4fb7e659836d04a22617d",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....cache_utils import Cache\n from ....modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n@@ -443,16 +444,17 @@ class OpenLlamaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         std = self.config.initializer_range\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n             if self.config.use_stable_embedding:\n-                torch.nn.init.xavier_normal_(module.weight)\n+                init.xavier_normal_(module.weight)\n             else:\n-                module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+                init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n OPEN_LLAMA_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "ca7f491508f24b64250d82cf89ef841ecbed4784",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -597,21 +597,6 @@ class QDQBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n QDQBERT_START_DOCSTRING = r\"\"\"\n "
        },
        {
            "sha": "d4ea5198965a915ce71eae17dbc238282dc4f666",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -787,21 +787,6 @@ class RealmPreTrainedModel(PreTrainedModel):\n     config: RealmConfig\n     base_model_prefix = \"realm\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n     def _flatten_inputs(self, *inputs):\n         \"\"\"Flatten inputs' shape to (-1, input_shape[-1])\"\"\"\n         flattened_inputs = []"
        },
        {
            "sha": "9158e8cb83e377d28ed9121c3dc86eaeec964d27",
            "filename": "src/transformers/models/deprecated/retribert/modeling_retribert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Fmodeling_retribert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -42,21 +42,6 @@ class RetriBertPreTrainedModel(PreTrainedModel):\n     config: RetriBertConfig\n     base_model_prefix = \"retribert\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n RETRIBERT_START_DOCSTRING = r\"\"\"\n "
        },
        {
            "sha": "f0ca446cc8381d58d4e32651495a7ee10db17f44",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....cache_utils import Cache\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n@@ -373,20 +374,10 @@ class Speech2Text2PreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, Speech2Text2SinusoidalPositionalEmbedding):\n+        super()._init_weights(module)\n+        if isinstance(module, Speech2Text2SinusoidalPositionalEmbedding):\n             weight = module.get_embedding(*module.weight.shape, module.padding_idx)\n-            weight = nn.Parameter(weight, requires_grad=False)\n-            weight.detach_()\n-            module.weight = weight\n+            init.copy_(module.weight, weight)\n \n \n SPEECH_TO_TEXT_2_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "3b8cb4e2edf0e3965e16d623d844bea57deadf03",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from .... import initialization as init\n from ....cache_utils import Cache\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_utils import PreTrainedModel\n@@ -87,19 +88,19 @@ class TrajectoryTransformerPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Linear, nn.Embedding)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if isinstance(module, nn.Linear) and module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, EinLinear):\n             for i in range(module.n_models):\n-                nn.init.kaiming_uniform_(module.weight[i], a=math.sqrt(5) / self.config.kaiming_initializer_range)\n+                init.kaiming_uniform_(module.weight[i], a=math.sqrt(5) / self.config.kaiming_initializer_range)\n                 if module.bias is not None:\n-                    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight[i])\n+                    fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight[i])\n                     bound = (1 / math.sqrt(fan_in)) * self.config.initializer_range\n-                    nn.init.uniform_(module.bias[i], -bound, bound)\n+                    init.uniform_(module.bias[i], -bound, bound)\n \n \n TRAJECTORY_TRANSFORMER_START_DOCSTRING = r\"\"\"\n@@ -158,11 +159,11 @@ def __init__(self, n_models, in_features, out_features, bias):\n \n     def reset_parameters(self):\n         for i in range(self.n_models):\n-            nn.init.kaiming_uniform_(self.weight[i], a=math.sqrt(5))\n+            init.kaiming_uniform_(self.weight[i], a=math.sqrt(5))\n             if self.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight[i])\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight[i])\n                 bound = 1 / math.sqrt(fan_in)\n-                nn.init.uniform_(self.bias[i], -bound, bound)\n+                init.uniform_(self.bias[i], -bound, bound)\n \n     def forward(self, input):\n         \"\"\""
        },
        {
            "sha": "d890edab89d9bebf9122d8747dc94f98ad0798f6",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from .... import initialization as init\n from ....modeling_utils import PreTrainedModel\n from ....utils import (\n     ModelOutput,\n@@ -331,12 +332,12 @@ class TransfoXLPreTrainedModel(PreTrainedModel):\n \n     def _init_weight(self, weight):\n         if self.config.init == \"uniform\":\n-            nn.init.uniform_(weight, -self.config.init_range, self.config.init_range)\n+            init.uniform_(weight, -self.config.init_range, self.config.init_range)\n         elif self.config.init == \"normal\":\n-            nn.init.normal_(weight, 0.0, self.config.init_std)\n+            init.normal_(weight, 0.0, self.config.init_std)\n \n     def _init_bias(self, bias):\n-        nn.init.constant_(bias, 0.0)\n+        init.constant_(bias, 0.0)\n \n     def _init_weights(self, m):\n         \"\"\"Initialize the weights.\"\"\"\n@@ -350,7 +351,7 @@ def _init_weights(self, m):\n             if hasattr(m, \"emb_projs\"):\n                 for i in range(len(m.emb_projs)):\n                     if m.emb_projs[i] is not None:\n-                        nn.init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)\n+                        init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)\n         elif classname.find(\"Embedding\") != -1:\n             if hasattr(m, \"weight\"):\n                 self._init_weight(m.weight)\n@@ -362,10 +363,10 @@ def _init_weights(self, m):\n             if hasattr(m, \"out_projs\"):\n                 for i in range(len(m.out_projs)):\n                     if m.out_projs[i] is not None:\n-                        nn.init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)\n+                        init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)\n         elif classname.find(\"LayerNorm\") != -1:\n             if hasattr(m, \"weight\"):\n-                nn.init.normal_(m.weight, 1.0, self.config.init_std)\n+                init.normal_(m.weight, 1.0, self.config.init_std)\n             if hasattr(m, \"bias\") and m.bias is not None:\n                 self._init_bias(m.bias)\n         else:"
        },
        {
            "sha": "99872fe5ea52f3d7d4019a5f121659e41d4ee862",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -548,17 +548,6 @@ class TvltPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n TVLT_START_DOCSTRING = r\"\"\"\n     This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it"
        },
        {
            "sha": "3b2b484487a13fd48000884ecf8c220fdf8c03cf",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n@@ -363,18 +364,18 @@ class VanPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            nn.init.trunc_normal_(module.weight, std=self.config.initializer_range)\n+            init.trunc_normal_(module.weight, std=self.config.initializer_range)\n             if isinstance(module, nn.Linear) and module.bias is not None:\n-                nn.init.constant_(module.bias, 0)\n+                init.constant_(module.bias, 0)\n         elif isinstance(module, nn.LayerNorm):\n-            nn.init.constant_(module.bias, 0)\n-            nn.init.constant_(module.weight, 1.0)\n+            init.constant_(module.bias, 0)\n+            init.constant_(module.weight, 1.0)\n         elif isinstance(module, nn.Conv2d):\n             fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n             fan_out //= module.groups\n-            module.weight.normal_(0, math.sqrt(2.0 / fan_out))\n+            init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n VAN_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "d414277e3e70f36587e8063e570687531294b59e",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 25,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from .... import initialization as init\n from ....activations import ACT2FN\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n@@ -461,34 +462,16 @@ class ViTHybridPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, ViTHybridEmbeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.position_embeddings.dtype)\n-            )\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(\n-                    module.cls_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.cls_token.dtype)\n-            )\n-            module.mask_token.zero_()\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.mask_token)\n \n \n VIT_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "05fa850a176bccf23ebda07780502ffd857475ad",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -520,17 +520,6 @@ class XLMProphetNetPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"prophetnet\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id\n         pad_token_id = self.config.pad_token_id"
        },
        {
            "sha": "158179e0418492169a65cb8e018baa70192cbf48",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -216,17 +216,6 @@ class DepthAnythingPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n class DepthAnythingNeck(nn.Module):\n     \"\"\""
        },
        {
            "sha": "23e96f838c4d0f9e701f52bddc8f6bcf2e7040d9",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from ..auto import AutoModel\n@@ -612,16 +613,16 @@ class DepthProPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n-            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "968859bf3d2139ced1d0ee5777cc6521fe1b6947",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -733,21 +734,22 @@ def _init_weights(self, module):\n         xavier_std = self.config.init_xavier_std\n \n         if isinstance(module, DetrMHAttentionMap):\n-            nn.init.zeros_(module.k_linear.bias)\n-            nn.init.zeros_(module.q_linear.bias)\n-            nn.init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n-            nn.init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n+            init.zeros_(module.k_linear.bias)\n+            init.zeros_(module.q_linear.bias)\n+            init.xavier_uniform_(module.k_linear.weight, gain=xavier_std)\n+            init.xavier_uniform_(module.q_linear.weight, gain=xavier_std)\n         elif isinstance(module, DetrLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n class DetrEncoder(DetrPreTrainedModel):\n@@ -1613,8 +1615,8 @@ def __init__(self, dim, fpn_dims, context_dim):\n \n         for m in self.modules():\n             if isinstance(m, nn.Conv2d):\n-                nn.init.kaiming_uniform_(m.weight, a=1)\n-                nn.init.constant_(m.bias, 0)\n+                init.kaiming_uniform_(m.weight, a=1)\n+                init.constant_(m.bias, 0)\n \n     def forward(self, x: Tensor, bbox_mask: Tensor, fpns: list[Tensor]):\n         # here we concatenate x, the projected feature map, of shape (batch_size, d_model, height/32, width/32) with"
        },
        {
            "sha": "d19e9299f8f5ed03f263e121bf9b00e3b91c228e",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n@@ -600,10 +601,10 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DiffLlamaAttention):\n-            module.lambda_q1.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k1.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_q2.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k2.normal_(0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_q1, 0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_k1, 0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_q2, 0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_k2, 0, self.config.lambda_std_dev)\n \n \n @auto_docstring"
        },
        {
            "sha": "c99d7b2feec23a7e916d22390742dad2c59eaaf0",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, StaticCache\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_utils import PreTrainedModel\n@@ -403,10 +404,10 @@ class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DiffLlamaAttention):\n-            module.lambda_q1.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k1.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_q2.normal_(0, self.config.lambda_std_dev)\n-            module.lambda_k2.normal_(0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_q1, 0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_k1, 0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_q2, 0, self.config.lambda_std_dev)\n+            init.normal_(module.lambda_k2, 0, self.config.lambda_std_dev)\n \n \n class DiffLlamaModel(LlamaModel):"
        },
        {
            "sha": "d2b1112266083f4dedc09a10dd0718cf6526bb5e",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -561,17 +561,6 @@ class DinatPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     input_modalities = \"image\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class DinatModel(DinatPreTrainedModel):"
        },
        {
            "sha": "1ce9ff9a63cfc66fdc98e2f6ac3a209946aad412",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 28,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n@@ -418,39 +419,19 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, Dinov2Embeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.position_embeddings.dtype)\n-            )\n-\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(\n-                    module.cls_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.cls_token.dtype)\n-            )\n-\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n             if self.config.use_mask_token:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n         elif isinstance(module, Dinov2LayerScale):\n-            module.lambda1.fill_(self.config.layerscale_value)\n+            init.constant_(module.lambda1, self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "5feb0e7b5b44261549bb577009f6da16d0d706a5",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 10,
            "deletions": 29,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n@@ -435,39 +436,19 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, Dinov2WithRegistersEmbeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.position_embeddings.dtype)\n-            )\n-\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(\n-                    module.cls_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.cls_token.dtype)\n-            )\n-\n-            module.mask_token.zero_()\n-            module.register_tokens.zero_()\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.mask_token)\n+            init.zeros_(module.register_tokens)\n         elif isinstance(module, Dinov2WithRegistersLayerScale):  # noqa: F821\n-            module.lambda1.fill_(self.config.layerscale_value)\n+            init.constant_(module.lambda1, self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "2ae67b405f6c095f4141bf7fd00c2102f18c0cf1",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 10,
            "deletions": 29,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n     Dinov2PatchEmbeddings,\n     Dinov2PreTrainedModel,\n )\n+from ... import initialization as init\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...processing_utils import Unpack\n@@ -281,39 +282,19 @@ class Dinov2WithRegistersPreTrainedModel(Dinov2PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, Dinov2WithRegistersEmbeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.position_embeddings.dtype)\n-            )\n-\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(\n-                    module.cls_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.cls_token.dtype)\n-            )\n-\n-            module.mask_token.zero_()\n-            module.register_tokens.zero_()\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.mask_token)\n+            init.zeros_(module.register_tokens)\n         elif isinstance(module, Dinov2WithRegistersLayerScale):  # noqa: F821\n-            module.lambda1.fill_(self.config.layerscale_value)\n+            init.constant_(module.lambda1, self.config.layerscale_value)\n \n \n class Dinov2WithRegistersModel(Dinov2Model):"
        },
        {
            "sha": "3ed4f38217ddbff414590a0516d1e9eb1208d905",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput, BaseModelOutputWithPoolingAndNoAttention\n from ...modeling_utils import PreTrainedModel\n@@ -194,16 +195,10 @@ class DINOv3ConvNextPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, (nn.LayerNorm, DINOv3ConvNextLayerNorm)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, DINOv3ConvNextLayer):\n+        super()._init_weights(module)\n+        if isinstance(module, DINOv3ConvNextLayer):\n             if module.gamma is not None:\n-                module.gamma.fill_(self.config.layer_scale_init_value)\n+                init.constant_(module.gamma, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "09edeed17543c343e7b99d86a48b03fa4b47c9be",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 28,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutputWithPooling\n@@ -452,39 +453,19 @@ class DINOv3ViTPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(\n-                    module.weight.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.weight.dtype)\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, DINOv3ViTEmbeddings):\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(\n-                    module.cls_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.cls_token.dtype)\n-            )\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n             if module.config.num_register_tokens > 0:\n-                module.register_tokens.copy_(\n-                    nn.init.trunc_normal_(\n-                        module.register_tokens.to(torch.float32),\n-                        mean=0.0,\n-                        std=self.config.initializer_range,\n-                    ).to(module.register_tokens.dtype)\n-                )\n-            module.mask_token.zero_()\n+                init.trunc_normal_(module.register_tokens, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.mask_token)\n         elif isinstance(module, DINOv3ViTLayerScale):\n-            module.lambda1.fill_(self.config.layerscale_value)\n+            init.constant_(module.lambda1, self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "7ae7d16320536022fb51c4cd32249bd301a7f6c9",
            "filename": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 29,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodular_dinov3_vit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -32,6 +32,7 @@\n from transformers.models.llama.modeling_llama import LlamaMLP\n from transformers.models.pixtral.modeling_pixtral import PixtralAttention, rotate_half\n \n+from ... import initialization as init\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -344,42 +345,22 @@ class DINOv3ViTPreTrainedModel(Dinov2PreTrainedModel):\n     }\n \n     @torch.no_grad()\n-    def _init_weights(self, module):\n+    def _init_weights(self, module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(\n-                    module.weight.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.weight.dtype)\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, DINOv3ViTEmbeddings):\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(\n-                    module.cls_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.cls_token.dtype)\n-            )\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=self.config.initializer_range)\n             if module.config.num_register_tokens > 0:\n-                module.register_tokens.copy_(\n-                    nn.init.trunc_normal_(\n-                        module.register_tokens.to(torch.float32),\n-                        mean=0.0,\n-                        std=self.config.initializer_range,\n-                    ).to(module.register_tokens.dtype)\n-                )\n-            module.mask_token.zero_()\n+                init.trunc_normal_(module.register_tokens, mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.mask_token)\n         elif isinstance(module, DINOv3ViTLayerScale):\n-            module.lambda1.fill_(self.config.layerscale_value)\n+            init.constant_(module.lambda1, self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "4de5389e890840542c60ef6db0ea4dafa322ca42",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 16,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import get_activation\n from ...configuration_utils import PreTrainedConfig\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n@@ -65,9 +66,9 @@ def create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor):\n \n         with deepspeed.zero.GatheredParameters(out, modifier_rank=0):\n             if torch.distributed.get_rank() == 0:\n-                _create_sinusoidal_embeddings(n_pos=n_pos, dim=dim, out=out)\n+                return _create_sinusoidal_embeddings(n_pos=n_pos, dim=dim, out=out)\n     else:\n-        _create_sinusoidal_embeddings(n_pos=n_pos, dim=dim, out=out)\n+        return _create_sinusoidal_embeddings(n_pos=n_pos, dim=dim, out=out)\n \n \n def _create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor):\n@@ -76,6 +77,7 @@ def _create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor):\n     out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n     out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n     out.detach_()\n+    return out\n \n \n class Embeddings(nn.Module):\n@@ -302,20 +304,15 @@ class DistilBertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n-            create_sinusoidal_embeddings(\n-                self.config.max_position_embeddings, self.config.dim, module.position_embeddings.weight\n+        super()._init_weights(module)\n+        if isinstance(module, Embeddings) and self.config.sinusoidal_pos_embds:\n+            init.copy_(\n+                module.position_embeddings.weight,\n+                create_sinusoidal_embeddings(\n+                    self.config.max_position_embeddings,\n+                    self.config.dim,\n+                    torch.empty_like(module.position_embeddings.weight),\n+                ),\n             )\n \n "
        },
        {
            "sha": "b9ebf9856264ee662846561344e43d74d7e7e28b",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -530,12 +531,12 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n-                module.A.zero_()\n+                init.zeros_(module.A)\n         elif isinstance(module, DogeDecoderLayer):\n             if hasattr(module, \"input_residual\"):\n-                module.input_residual.fill_(1.0)\n+                init.ones_(module.input_residual)\n             if hasattr(module, \"post_attention_residual\"):\n-                module.post_attention_residual.fill_(1.0)\n+                init.ones_(module.post_attention_residual)\n \n \n @auto_docstring"
        },
        {
            "sha": "008466fbf4acf44cd73ae92a89f8ccc8634edf5e",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n@@ -546,12 +547,12 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n-                module.A.zero_()\n+                init.zeros_(module.A)\n         elif isinstance(module, DogeDecoderLayer):\n             if hasattr(module, \"input_residual\"):\n-                module.input_residual.fill_(1.0)\n+                init.ones_(module.input_residual)\n             if hasattr(module, \"post_attention_residual\"):\n-                module.post_attention_residual.fill_(1.0)\n+                init.ones_(module.post_attention_residual)\n \n \n class DogeModel(MixtralModel):"
        },
        {
            "sha": "7e8a6ae5d90ff616b8818a0d18588cccf11aa2b1",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n@@ -792,20 +793,14 @@ class DonutSwinPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, DonutSwinEmbeddings):\n+        super()._init_weights(module)\n+        if isinstance(module, DonutSwinEmbeddings):\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n             if module.position_embeddings is not None:\n-                module.position_embeddings.zero_()\n+                init.zeros_(module.position_embeddings)\n         elif isinstance(module, DonutSwinSelfAttention):\n-            module.relative_position_bias_table.zero_()\n+            init.zeros_(module.relative_position_bias_table)\n \n \n @auto_docstring"
        },
        {
            "sha": "ac5e156f9049fa326fc51ba6d53bbf75725fad4c",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -470,7 +471,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Dots1TopkRouter):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "e2886062951f25da33bbf5c4c5d158cc6cdb7451",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -105,21 +105,6 @@ class DPRReaderOutput(ModelOutput):\n class DPRPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n class DPREncoder(DPRPreTrainedModel):\n     base_model_prefix = \"bert_model\""
        },
        {
            "sha": "e5a3d7367c584bb5ca3945926714fef2568c9927",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n@@ -735,16 +736,10 @@ class DPTPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n         if isinstance(module, (DPTViTEmbeddings, DPTViTHybridEmbeddings)):\n-            module.cls_token.zero_()\n-            module.position_embeddings.zero_()\n+            init.zeros_(module.cls_token)\n+            init.zeros_(module.position_embeddings)\n \n \n @auto_docstring"
        },
        {
            "sha": "d0034a642b2fb19e85c2c2802ba98d09c8f3eadd",
            "filename": "src/transformers/models/edgetam/configuration_edgetam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,7 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PreTrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n "
        },
        {
            "sha": "b23370e47f14f81a41f963d597157209d0973006",
            "filename": "src/transformers/models/edgetam/modeling_edgetam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -32,6 +32,7 @@\n \n from transformers.utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -310,21 +311,10 @@ class EdgeTamPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, EdgeTamLayerNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+        super()._init_weights(module)\n         if isinstance(module, EdgeTamModel):\n             if module.no_memory_embedding is not None:\n-                module.no_memory_embedding.zero_()\n+                init.zeros_(module.no_memory_embedding)\n \n \n # copied and adapted from original implementation, also practically equal to DetrSinePositionEmbedding"
        },
        {
            "sha": "0037d49af0ae5dcc1b90425e41d51f8d6c29d470",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -17,7 +17,6 @@\n from typing import Optional, Union\n \n import torch\n-import torch.nn as nn\n import torch.utils.checkpoint\n \n from transformers.models.sam2.configuration_sam2 import Sam2Config, Sam2MaskDecoderConfig, Sam2PromptEncoderConfig\n@@ -33,7 +32,9 @@\n )\n from transformers.utils.generic import TransformersKwargs, check_model_inputs\n \n+from ... import initialization as init\n from ...configuration_utils import PreTrainedConfig\n+from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     auto_docstring,\n@@ -176,21 +177,10 @@ class EdgeTamFeedForward(Sam2FeedForward):\n class EdgeTamPreTrainedModel(Sam2PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, EdgeTamLayerNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, EdgeTamModel):\n             if module.no_memory_embedding is not None:\n-                module.no_memory_embedding.zero_()\n+                init.zeros_(module.no_memory_embedding)\n \n \n @auto_docstring("
        },
        {
            "sha": "f6f0b02874fe9f4fc848bf3674561ae20a669a07",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 18,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -34,6 +34,7 @@\n \n from transformers.utils.generic import OutputRecorder\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -780,30 +781,19 @@ class EdgeTamVideoPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, EdgeTamVideoLayerNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, EdgeTamVideoModel):\n+        super()._init_weights(module)\n+        if isinstance(module, EdgeTamVideoModel):\n             if module.no_memory_positional_encoding is not None:\n-                module.no_memory_positional_encoding.zero_()\n+                init.zeros_(module.no_memory_positional_encoding)\n             if module.memory_temporal_positional_encoding is not None:\n-                module.memory_temporal_positional_encoding.zero_()\n+                init.zeros_(module.memory_temporal_positional_encoding)\n             if module.no_object_pointer is not None:\n-                module.no_object_pointer.zero_()\n+                init.zeros_(module.no_object_pointer)\n             if module.occlusion_spatial_embedding_parameter is not None:\n-                module.occlusion_spatial_embedding_parameter.zero_()\n+                init.zeros_(module.occlusion_spatial_embedding_parameter)\n         if isinstance(module, EdgeTamVideoMemoryFuserCXBlock):\n             if module.scale is not None:\n-                module.scale.zero_()\n+                init.zeros_(module.scale)\n \n \n class EdgeTamVideoInferenceCache:"
        },
        {
            "sha": "bdf6dd67ae48c17142ff70c5eb9d2f9b2de9e8ce",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,6 +18,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2CLS, ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n@@ -679,12 +680,12 @@ class EfficientLoFTRPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n     # Copied from transformers.models.superpoint.modeling_superpoint.SuperPointPreTrainedModel.extract_one_channel_pixel_values with SuperPoint->EfficientLoFTR\n     def extract_one_channel_pixel_values(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:"
        },
        {
            "sha": "b4635b3fcadb44edc32ec73fd4d57dcb65516c08",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n@@ -440,9 +441,9 @@ class EfficientNetPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "a7698b31fad0b914d4cebd695c476f521b7dc48f",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -532,21 +532,6 @@ class ElectraPreTrainedModel(PreTrainedModel):\n         \"cross_attentions\": ElectraCrossAttention,\n     }\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @dataclass\n @auto_docstring(\n@@ -1317,7 +1302,7 @@ def __init__(self, config):\n         self.generator_predictions = ElectraGeneratorPredictions(config)\n         self.generator_lm_head = nn.Linear(config.embedding_size, config.vocab_size)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_output_embeddings(self):\n         return self.generator_lm_head"
        },
        {
            "sha": "bf0bb66c8880cb0a8fd5cc7bd7e0a50cd31aa3a3",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -941,24 +942,25 @@ class Emu3VQVAE(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n-            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in)\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.Linear):\n-            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n-            nn.init.constant_(module.weight, 1.0)\n-            nn.init.constant_(module.bias, 0.0)\n+            init.constant_(module.weight, 1.0)\n+            init.constant_(module.bias, 0.0)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_()\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n     def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a70fd220b58251370e89e1dee67484697d558d2c",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutputWithPast\n@@ -691,24 +692,25 @@ class Emu3VQVAE(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         if isinstance(module, (nn.Conv2d, nn.Conv3d)):\n-            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in)\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.Linear):\n-            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm3d, nn.GroupNorm)):\n-            nn.init.constant_(module.weight, 1.0)\n-            nn.init.constant_(module.bias, 0.0)\n+            init.constant_(module.weight, 1.0)\n+            init.constant_(module.bias, 0.0)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_()\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n     def __init__(self, config: Emu3VQVAEConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a02ee98e9a30ef88660f46fe4a33c67d98a978bf",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...modeling_utils import PreTrainedAudioTokenizerBase\n from ...utils import (\n     ModelOutput,\n@@ -458,21 +459,21 @@ class EncodecPreTrainedModel(PreTrainedAudioTokenizerBase):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.GroupNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Conv1d):\n-            nn.init.kaiming_normal_(module.weight)\n+            init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-k, b=k)\n+                init.uniform_(module.bias, a=-k, b=k)\n         elif isinstance(module, nn.ConvTranspose1d):\n             module.reset_parameters()\n         elif isinstance(module, nn.LSTM):\n             for name, param in module.named_parameters():\n                 if \"weight\" in name:\n-                    nn.init.xavier_uniform_(param)\n+                    init.xavier_uniform_(param)\n                 elif \"bias\" in name:\n-                    nn.init.constant_(param, 0.0)\n+                    init.constant_(param, 0.0)\n \n \n @auto_docstring("
        },
        {
            "sha": "fe3bec2f57cce4b32c1cc03ab163c64f9c03c14e",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -164,7 +164,7 @@ def __init__(\n             )\n \n         # tie encoder, decoder weights if config set accordingly\n-        self.tie_weights()\n+        self.post_init()\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "344342b470badce7e86934bb984cca60a282b4c3",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -30,6 +30,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...file_utils import ModelOutput, is_scipy_available, requires_backends\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1000,26 +1001,25 @@ class EomtPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=1)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=1)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, EomtLayerScale):\n             if hasattr(module, \"lambda1\"):\n-                module.lambda1.fill_(self.config.layerscale_value)\n+                init.constant_(module.lambda1, self.config.layerscale_value)\n         elif isinstance(module, EomtEmbeddings):\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(module.cls_token.to(torch.float32), mean=0.0, std=std).to(module.cls_token.dtype)\n-            )\n-            module.register_tokens.zero_()\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n+            init.zeros_(module.register_tokens)\n \n \n @auto_docstring("
        },
        {
            "sha": "c22571acbb97e9ffa80e89757ec2598e861a2dbb",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...file_utils import (\n     ModelOutput,\n@@ -405,26 +406,25 @@ class EomtPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n-            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n             if module.bias is not None:\n-                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(module.weight)\n                 bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n-                nn.init.uniform_(module.bias, -bound, bound)\n+                init.uniform_(module.bias, -bound, bound)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=1)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=1)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, EomtLayerScale):\n             if hasattr(module, \"lambda1\"):\n-                module.lambda1.fill_(self.config.layerscale_value)\n+                init.constant_(module.lambda1, self.config.layerscale_value)\n         elif isinstance(module, EomtEmbeddings):\n-            module.cls_token.copy_(\n-                nn.init.trunc_normal_(module.cls_token.to(torch.float32), mean=0.0, std=std).to(module.cls_token.dtype)\n-            )\n-            module.register_tokens.zero_()\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n+            init.zeros_(module.register_tokens)\n \n \n @auto_docstring("
        },
        {
            "sha": "5f01a8afec6d01741df0abb26bc123c7ce2adf3d",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -549,21 +550,9 @@ class ErniePreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, ErnieLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, ErnieLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring("
        },
        {
            "sha": "f633a4c25091099c4c7f682fcff94545c7bef752",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_outputs import (\n@@ -165,21 +166,9 @@ class ErniePreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            # Slightly different from the TF version which uses truncated_normal for initialization\n-            # cf https://github.com/pytorch/pytorch/pull/5617\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, ErnieLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, ErnieLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n class ErnieModel(BertModel):"
        },
        {
            "sha": "633f97fd2090d05fad96ef6b960e3fd5229af1b9",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -499,7 +500,7 @@ class Ernie4_5_MoePreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n-            module.e_score_correction_bias.zero_()\n+            init.zeros_(module.e_score_correction_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "1cd1f216f674d1440165db1c379c296660163def",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n@@ -240,7 +241,7 @@ class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n-            module.e_score_correction_bias.zero_()\n+            init.zeros_(module.e_score_correction_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "c4b3e1ea11ec319b3608039dbc71014cfadb4088",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -554,19 +555,9 @@ class EsmPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, EsmLMHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, EsmLMHead):\n+            init.zeros_(module.bias)\n \n     def get_output_embeddings(self):\n         # NOTE: get_output_embeddings() must return None to prevent accidental weight tying."
        },
        {
            "sha": "0e195b57e28926a6309c5527140f522cd8afbff3",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 41,
            "deletions": 59,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,12 +24,12 @@\n import torch.nn as nn\n from torch.nn import LayerNorm\n \n+from ... import initialization as init\n from ...integrations.deepspeed import is_deepspeed_available\n from ...modeling_outputs import ModelOutput\n from ...utils import (\n     ContextManagers,\n     auto_docstring,\n-    is_scipy_available,\n     logging,\n )\n from .modeling_esm import EsmModel, EsmPreTrainedModel\n@@ -207,33 +207,6 @@ def dict_multimap(fn, dicts):\n     return new_dict\n \n \n-def trunc_normal_init_(weights, scale=1.0, fan=\"fan_in\"):\n-    shape = weights.shape\n-    scale = scale / max(1, shape[1])\n-\n-    if not is_scipy_available():\n-        logger.warning(\n-            \"This init requires scipy, but scipy was not found, default to an approximation that might not be\"\n-            \" equivalent.\"\n-        )\n-        std = math.sqrt(scale)\n-        torch.nn.init.normal_(weights, std=std).clamp(min=0.0, max=2.0 * std)\n-\n-    else:\n-        from scipy.stats import truncnorm\n-\n-        std = math.sqrt(scale) / truncnorm.std(a=-2, b=2, loc=0, scale=1)\n-        samples = truncnorm.rvs(a=-2, b=2, loc=0, scale=std, size=weights.numel())\n-        samples = np.reshape(samples, shape)\n-        weights.copy_(torch.tensor(samples, device=weights.device))\n-\n-\n-def ipa_point_weights_init_(weights):\n-    with torch.no_grad():\n-        softplus_inverse_1 = 0.541324854612918\n-        weights.fill_(softplus_inverse_1)\n-\n-\n class EsmFoldLinear(nn.Linear):\n     \"\"\"\n     A Linear layer with built-in nonstandard initializations. Called just like torch.nn.Linear.\n@@ -923,40 +896,47 @@ def _init_weights(self, module):\n                 if module.init_fn is not None:\n                     module.init_fn(module.weight, module.bias)\n                 elif module.init == \"default\":\n-                    trunc_normal_init_(module.weight, scale=1.0)\n+                    shape = module.weight.shape\n+                    scale = 1.0 / max(1, shape[1])\n+                    std = math.sqrt(scale)\n+                    init.normal_(module.weight, std=std)\n                 elif module.init == \"relu\":\n-                    trunc_normal_init_(module.weight, scale=2.0)\n+                    shape = module.weight.shape\n+                    scale = 2.0 / max(1, shape[1])\n+                    std = math.sqrt(scale)\n+                    init.normal_(module.weight, std=std)\n                 elif module.init == \"glorot\":\n-                    nn.init.xavier_uniform_(module.weight, gain=1)\n+                    init.xavier_uniform_(module.weight, gain=1)\n                 elif module.init == \"gating\":\n-                    module.weight.fill_(0.0)\n+                    init.zeros_(module.weight)\n                     if module.bias:\n-                        module.bias.fill_(1.0)\n+                        init.ones(module.bias)\n                 elif module.init == \"normal\":\n-                    torch.nn.init.kaiming_normal_(module.weight, nonlinearity=\"linear\")\n+                    init.kaiming_normal_(module.weight, nonlinearity=\"linear\")\n                 elif module.init == \"final\":\n-                    module.weight.fill_(0.0)\n+                    init.zeros_(module.weight)\n         elif isinstance(module, EsmFoldInvariantPointAttention):\n-            ipa_point_weights_init_(module.head_weights)\n+            softplus_inverse_1 = 0.541324854612918\n+            init.constant_(module.head_weights, softplus_inverse_1)\n         elif isinstance(module, EsmFoldTriangularSelfAttentionBlock):\n-            torch.nn.init.zeros_(module.tri_mul_in.linear_z.weight)\n-            torch.nn.init.zeros_(module.tri_mul_in.linear_z.bias)\n-            torch.nn.init.zeros_(module.tri_mul_out.linear_z.weight)\n-            torch.nn.init.zeros_(module.tri_mul_out.linear_z.bias)\n-            torch.nn.init.zeros_(module.tri_att_start.mha.linear_o.weight)\n-            torch.nn.init.zeros_(module.tri_att_start.mha.linear_o.bias)\n-            torch.nn.init.zeros_(module.tri_att_end.mha.linear_o.weight)\n-            torch.nn.init.zeros_(module.tri_att_end.mha.linear_o.bias)\n-\n-            torch.nn.init.zeros_(module.sequence_to_pair.o_proj.weight)\n-            torch.nn.init.zeros_(module.sequence_to_pair.o_proj.bias)\n-            torch.nn.init.zeros_(module.pair_to_sequence.linear.weight)\n-            torch.nn.init.zeros_(module.seq_attention.o_proj.weight)\n-            torch.nn.init.zeros_(module.seq_attention.o_proj.bias)\n-            torch.nn.init.zeros_(module.mlp_seq.mlp[-2].weight)\n-            torch.nn.init.zeros_(module.mlp_seq.mlp[-2].bias)\n-            torch.nn.init.zeros_(module.mlp_pair.mlp[-2].weight)\n-            torch.nn.init.zeros_(module.mlp_pair.mlp[-2].bias)\n+            init.zeros_(module.tri_mul_in.linear_z.weight)\n+            init.zeros_(module.tri_mul_in.linear_z.bias)\n+            init.zeros_(module.tri_mul_out.linear_z.weight)\n+            init.zeros_(module.tri_mul_out.linear_z.bias)\n+            init.zeros_(module.tri_att_start.mha.linear_o.weight)\n+            init.zeros_(module.tri_att_start.mha.linear_o.bias)\n+            init.zeros_(module.tri_att_end.mha.linear_o.weight)\n+            init.zeros_(module.tri_att_end.mha.linear_o.bias)\n+\n+            init.zeros_(module.sequence_to_pair.o_proj.weight)\n+            init.zeros_(module.sequence_to_pair.o_proj.bias)\n+            init.zeros_(module.pair_to_sequence.linear.weight)\n+            init.zeros_(module.seq_attention.o_proj.weight)\n+            init.zeros_(module.seq_attention.o_proj.bias)\n+            init.zeros_(module.mlp_seq.mlp[-2].weight)\n+            init.zeros_(module.mlp_seq.mlp[-2].bias)\n+            init.zeros_(module.mlp_pair.mlp[-2].weight)\n+            init.zeros_(module.mlp_pair.mlp[-2].bias)\n         else:\n             super()._init_weights(module)\n \n@@ -975,12 +955,12 @@ def __init__(self, embed_dim, num_heads, head_width, gated=False):\n         self.gated = gated\n         if gated:\n             self.g_proj = nn.Linear(embed_dim, embed_dim)\n-            torch.nn.init.zeros_(self.g_proj.weight)\n-            torch.nn.init.ones_(self.g_proj.bias)\n+            init.zeros_(self.g_proj.weight)\n+            init.ones_(self.g_proj.bias)\n \n         self.rescale_factor = self.head_width**-0.5\n \n-        torch.nn.init.zeros_(self.o_proj.bias)\n+        init.zeros_(self.o_proj.bias)\n \n     def forward(self, x, mask=None, bias=None, indices=None):\n         \"\"\"\n@@ -1053,8 +1033,8 @@ def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim):\n         self.proj = nn.Linear(sequence_state_dim, inner_dim * 2, bias=True)\n         self.o_proj = nn.Linear(2 * inner_dim, pairwise_state_dim, bias=True)\n \n-        torch.nn.init.zeros_(self.proj.bias)\n-        torch.nn.init.zeros_(self.o_proj.bias)\n+        init.zeros_(self.proj.bias)\n+        init.zeros_(self.o_proj.bias)\n \n     def forward(self, sequence_state):\n         \"\"\"\n@@ -2052,6 +2032,8 @@ def __init__(self, config):\n             nn.Linear(self.config.esmfold_config.lddt_head_hid_dim, 37 * self.lddt_bins),\n         )\n \n+        self.post_init()\n+\n     @staticmethod\n     def _af2_to_esm_from_vocab_list(vocab_list: list[str]) -> torch.Tensor:\n         # Remember that t is shifted from residue_constants by 1 (0 is padding)."
        },
        {
            "sha": "f4d0ce11255fc761b7478ad16d039fc90fd1d959",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 20,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -517,22 +518,6 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n     def __init__(self, config: SaProtConfig):\n@@ -1274,11 +1259,11 @@ def _init_weights(self, module):\n         std = self.config.initializer_range\n         super()._init_weights(module)\n         if isinstance(module, EvollaSequenceAlignerCrossAttention):\n-            module.gate_attention.zero_()\n-            module.gate_ffw.zero_()\n-            module.attention_norm.weight.fill_(1.0)\n+            init.zeros_(module.gate_attention)\n+            init.zeros_(module.gate_ffw)\n+            init.ones_(module.attention_norm.weight)\n         elif isinstance(module, EvollaSequenceCompressorResampler):\n-            module.latents.normal_(mean=0.0, std=std)\n+            init.normal_(module.latents, mean=0.0, std=std)\n \n \n class EvollaModel(EvollaPreTrainedModel):"
        },
        {
            "sha": "ed6ff25b1cdb9a03fcd19d244a1023e9ecd4fc2b",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 20,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n@@ -202,22 +203,6 @@ class EvollaSaProtPreTrainedModel(PreTrainedModel):\n         ],\n     }\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n class EvollaSaProtProteinEncoder(EvollaSaProtPreTrainedModel):\n     def __init__(self, config: SaProtConfig):\n@@ -738,11 +723,11 @@ def _init_weights(self, module):\n         std = self.config.initializer_range\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, EvollaSequenceAlignerCrossAttention):\n-            module.gate_attention.zero_()\n-            module.gate_ffw.zero_()\n-            module.attention_norm.weight.fill_(1.0)\n+            init.zeros_(module.gate_attention)\n+            init.zeros_(module.gate_ffw)\n+            init.ones_(module.attention_norm.weight)\n         elif isinstance(module, EvollaSequenceCompressorResampler):\n-            module.latents.normal_(mean=0.0, std=std)\n+            init.normal_(module.latents, mean=0.0, std=std)\n \n \n class EvollaModel(EvollaPreTrainedModel):"
        },
        {
            "sha": "7df7c3a0b14845ffb35b5850e0fb16917ee95883",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import get_activation\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n@@ -672,26 +673,16 @@ class FalconPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"FalconDecoderLayer\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-\n     _can_compile_fullgraph = True\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear, FalconLinear)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+        super()._init_weights(module)\n+        if isinstance(module, FalconLinear):\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+                init.zeros_(module.bias)\n \n     # Adapted from transformers.modeling_utils.PreTrainedModel._check_and_enable_sdpa\n     @classmethod"
        },
        {
            "sha": "a6fd7a5aba992923747e6841747a279addcbb354",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -33,6 +33,7 @@\n \n from transformers.activations import ACT2FN\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n@@ -1196,21 +1197,11 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Module):\n-            for name, param in module.named_parameters(recurse=True):\n-                if not param.requires_grad:\n-                    continue\n-                if \"layernorm\" in name.lower() and \"weight\" in name:\n-                    # LayerNorm weights usually initialized to 1\n-                    param.fill_(1.0)\n-                elif \"bias\" in name:\n-                    param.zero_()\n-                else:\n-                    try:\n-                        param.normal_(mean=0.0, std=std)\n-                    except Exception as e:\n-                        print(f\"Skipping init for {name} due to error: {e}\")\n+        super()._init_weights(module)\n+        if isinstance(module, FalconH1Mixer):\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n \n \n def compute_mup_vector(config):"
        },
        {
            "sha": "386266b89bf3cc4a23c76dbb968b5364d5e4ae97",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -45,6 +45,7 @@\n     segment_sum,\n )\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -922,21 +923,11 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Module):\n-            for name, param in module.named_parameters(recurse=True):\n-                if not param.requires_grad:\n-                    continue\n-                if \"layernorm\" in name.lower() and \"weight\" in name:\n-                    # LayerNorm weights usually initialized to 1\n-                    param.fill_(1.0)\n-                elif \"bias\" in name:\n-                    param.zero_()\n-                else:\n-                    try:\n-                        param.normal_(mean=0.0, std=std)\n-                    except Exception as e:\n-                        print(f\"Skipping init for {name} due to error: {e}\")\n+        super()._init_weights(module)\n+        if isinstance(module, FalconH1Mixer):\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n \n \n def compute_mup_vector(config):"
        },
        {
            "sha": "1819b9d0af0b8f556153aaa3aa9bcceb6e46f34a",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 16,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n@@ -577,14 +578,14 @@ def _init_weights(self, module):\n             # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n             A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n-            module.A_log.copy_(torch.log(A))\n-            module.D.fill_(1.0)\n+            init.copy_(module.A_log, torch.log(A))\n+            init.ones_(module.D)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale\n             if self.config.time_step_init_scheme == \"constant\":\n-                nn.init.constant_(module.dt_proj.weight, dt_init_std)\n+                init.constant_(module.dt_proj.weight, dt_init_std)\n             elif self.config.time_step_init_scheme == \"random\":\n-                nn.init.uniform_(module.dt_proj.weight, -dt_init_std, dt_init_std)\n+                init.uniform_(module.dt_proj.weight, -dt_init_std, dt_init_std)\n \n             dt = torch.exp(\n                 torch.rand(self.config.intermediate_size)\n@@ -593,14 +594,12 @@ def _init_weights(self, module):\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n-            module.dt_proj.bias.copy_(inv_dt)\n-            module.dt_proj.bias._no_reinit = True\n+            init.copy_(module.dt_proj.bias, inv_dt)\n \n-            nn.init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n             if module.conv1d.bias is not None:\n-                if not getattr(module.conv1d.bias, \"_no_reinit\", False):\n-                    nn.init.zeros_(module.conv1d.bias)\n-            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n+                init.zeros_(module.conv1d.bias)\n+            init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n \n             if self.config.rescale_prenorm_residual:\n                 # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n@@ -617,15 +616,13 @@ def _init_weights(self, module):\n                 p /= math.sqrt(self.config.num_hidden_layers)\n \n         if isinstance(module, nn.Linear):\n-            if not getattr(module.weight, \"_no_reinit\", False):\n-                nn.init.normal_(module.weight, std=std)\n+            init.normal_(module.weight, std=std)\n             if module.bias is not None:\n-                if not getattr(module.bias, \"_no_reinit\", False):\n-                    nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n         elif isinstance(module, FalconMambaRMSNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, std=std)\n+            init.normal_(module.weight, std=std)\n \n \n @dataclass"
        },
        {
            "sha": "5db835f1fd9420a349608359cb025b3a823c0472",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 15,
            "deletions": 19,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -995,24 +996,25 @@ class FastSpeech2ConformerPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            nn.init.normal_(module.weight, std=1.0 / math.sqrt(module.weight.size(1)))\n+            init.normal_(module.weight, std=1.0 / math.sqrt(module.weight.size(1)))\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Conv1d):\n-            nn.init.kaiming_normal_(module.weight)\n+            init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 key = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-key, b=key)\n+                init.uniform_(module.bias, a=-key, b=key)\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_()\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, FastSpeech2ConformerAttention):\n-            nn.init.xavier_uniform_(module.pos_bias_u)\n-            nn.init.xavier_uniform_(module.pos_bias_v)\n+            init.xavier_uniform_(module.pos_bias_u)\n+            init.xavier_uniform_(module.pos_bias_v)\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, FastSpeech2ConformerEncoder):\n@@ -1404,14 +1406,6 @@ def __init__(self, config: FastSpeech2ConformerHifiGanConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @torch.no_grad()\n-    def _init_weights(self, module: nn.Module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-\n     def apply_weight_norm(self):\n         weight_norm = nn.utils.weight_norm\n         if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n@@ -1498,6 +1492,8 @@ def __init__(self, config: FastSpeech2ConformerWithHifiGanConfig):\n \n         self.config = config\n \n+        self.post_init()\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "93ca89acd1e224b1a95e022b6bb0d03c930c2803",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 17,
            "deletions": 9,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import gelu, get_activation\n from ...cache_utils import DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -51,6 +52,7 @@ def create_sinusoidal_embeddings(n_pos, dim, out):\n     out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n     out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n     out.detach_()\n+    return out\n \n \n # Copied from transformers.models.xlm.modeling_xlm.get_masks\n@@ -676,20 +678,26 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, nn.Embedding):\n             if self.config is not None and self.config.embed_init_std is not None:\n-                nn.init.normal_(module.weight, mean=0, std=self.config.embed_init_std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+                init.normal_(module.weight, mean=0, std=self.config.embed_init_std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         if isinstance(module, nn.Linear):\n             if self.config is not None and self.config.init_std is not None:\n-                nn.init.normal_(module.weight, mean=0, std=self.config.init_std)\n+                init.normal_(module.weight, mean=0, std=self.config.init_std)\n                 if module.bias is not None:\n-                    nn.init.constant_(module.bias, 0.0)\n+                    init.constant_(module.bias, 0.0)\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, FlaubertModel) and self.config.sinusoidal_embeddings:\n-            create_sinusoidal_embeddings(\n-                self.config.max_position_embeddings, self.config.emb_dim, out=module.position_embeddings.weight\n+            init.copy_(\n+                module.position_embeddings.weight,\n+                create_sinusoidal_embeddings(\n+                    self.config.max_position_embeddings,\n+                    self.config.emb_dim,\n+                    out=torch.empty_like(module.position_embeddings.weight),\n+                ),\n             )\n \n "
        },
        {
            "sha": "effb5111cf968c53579617c32d78a47c0c471d41",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n@@ -668,29 +669,19 @@ class FlavaPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, FlavaMaskedPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, FlavaMaskedPredictionHead):\n+            init.zeros_(module.bias)\n         elif isinstance(module, FlavaImageEmbeddings):\n-            module.cls_token.zero_()\n-            module.position_embeddings.zero_()\n+            init.zeros_(module.cls_token)\n+            init.zeros_(module.position_embeddings)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n         elif isinstance(module, FlavaMultimodalModel):\n             if module.use_cls_token:\n-                module.cls_token.zero_()\n+                init.zeros_(module.cls_token)\n         elif isinstance(module, FlavaModel):\n-            module.logit_scale.fill_(self.config.logit_scale_init_value)\n+            init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "b948b420ad63f94c00bc52e3b81fb6e203d75122",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -434,10 +435,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, FlexOlmoExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, FlexOlmoTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "a273167710cc834b0a712fb7185af5e9374c29ac",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -374,22 +374,6 @@ class FNetPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"fnet\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            # NOTE: Original code uses same initialization as weights for biases as well.\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "e55cde0bb98d299e1412f63afbb7956429c28115",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n@@ -584,20 +585,14 @@ class FocalNetPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, FocalNetEmbeddings):\n+        super()._init_weights(module)\n+        if isinstance(module, FocalNetEmbeddings):\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n         elif isinstance(module, FocalNetLayer):\n             if self.config.use_layerscale:\n-                module.gamma_1.fill_(self.config.layerscale_value)\n-                module.gamma_2.fill_(self.config.layerscale_value)\n+                init.constant_(module.gamma_1, self.config.layerscale_value)\n+                init.constant_(module.gamma_2, self.config.layerscale_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "cc4607023fbd6ae90fc3ebf6500a0dfa3982b18a",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -34,6 +34,7 @@\n from torch import Tensor, nn\n from torch.nn import CrossEntropyLoss, LayerNorm\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -223,18 +224,17 @@ class PretrainedFSMTModel(PreTrainedModel):\n     def _init_weights(self, module):\n         std = self.config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, SinusoidalPositionalEmbedding):\n             weight = module.get_embedding(*module.weight.shape, module.padding_idx)\n-            weight = nn.Parameter(weight, requires_grad=False)\n-            weight.detach_()\n-            module.weight = weight\n+            init.copy_(module.weight, weight)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n     @property\n     def dummy_inputs(self):"
        },
        {
            "sha": "f23e962347f49f6c67ee8a4bc0a3aa091c9a4a21",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -682,20 +683,20 @@ def _init_weights(self, module):\n                     std = np.sqrt(1.0 / float(fan_in + fan_out))\n                 else:\n                     std = self.config.initializer_std\n-                nn.init.normal_(module.weight, std=std)\n+                init.normal_(module.weight, std=std)\n             if getattr(module, \"bias\", None) is not None:\n-                nn.init.constant_(module.bias, 0.0)\n+                init.constant_(module.bias, 0.0)\n         elif classname == \"FunnelRelMultiheadAttention\":\n-            nn.init.uniform_(module.r_w_bias, b=self.config.initializer_range)\n-            nn.init.uniform_(module.r_r_bias, b=self.config.initializer_range)\n-            nn.init.uniform_(module.r_kernel, b=self.config.initializer_range)\n-            nn.init.uniform_(module.r_s_bias, b=self.config.initializer_range)\n-            nn.init.uniform_(module.seg_embed, b=self.config.initializer_range)\n+            init.uniform_(module.r_w_bias, b=self.config.initializer_range)\n+            init.uniform_(module.r_r_bias, b=self.config.initializer_range)\n+            init.uniform_(module.r_kernel, b=self.config.initializer_range)\n+            init.uniform_(module.r_s_bias, b=self.config.initializer_range)\n+            init.uniform_(module.seg_embed, b=self.config.initializer_range)\n         elif classname == \"FunnelEmbeddings\":\n             std = 1.0 if self.config.initializer_std is None else self.config.initializer_std\n-            nn.init.normal_(module.word_embeddings.weight, std=std)\n+            init.normal_(module.word_embeddings.weight, std=std)\n             if module.word_embeddings.padding_idx is not None:\n-                module.word_embeddings.weight[module.word_embeddings.padding_idx].zero_()\n+                init.zeros_(module.word_embeddings.weight[module.word_embeddings.padding_idx])\n \n \n class FunnelClassificationHead(nn.Module):"
        },
        {
            "sha": "e0a969fea3a8f561292355a6f050267d67b4053a",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -44,18 +44,6 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _skip_keys_device_placement = \"past_key_values\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "8834ba8c356492463a7bec45d676e1801c4a4181",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -352,10 +353,9 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n-\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "1445baef96bf1e0de37fb1f8bb77f7ba30b24085",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n from ...masking_utils import create_causal_mask\n@@ -397,10 +398,9 @@ class GemmaPreTrainedModel(LlamaPreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n-\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n \n class GemmaModel(LlamaModel):"
        },
        {
            "sha": "69e4860321074eb45881a97cbb2ef835744c7afb",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -384,10 +385,9 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n-\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         if \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "b7aef2390f20bd000f453b397d3dd243977aba6c",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n@@ -470,10 +471,10 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Gemma3MultiModalProjector):\n-            module.mm_input_projection_weight.zero_()\n+            init.zeros_(module.mm_input_projection_weight)\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "47e1b49ac7bb92297ce7b60be1dd86ffd92369c7",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n@@ -573,10 +574,10 @@ class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3MultiModalProjector):\n-            module.mm_input_projection_weight.zero_()\n+            init.zeros_(module.mm_input_projection_weight)\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif \"RMSNorm\" in module.__class__.__name__:\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n \n def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, int, int], bool]:"
        },
        {
            "sha": "5111a69cebfef8778444740a2797571645bd683d",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -1605,11 +1606,11 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, Gemma3nAudioAttention):\n-            module.per_dim_scale.zero_()\n+            init.zeros_(module.per_dim_scale)\n         elif isinstance(module, Gemma3nTextAltUp):\n-            module.correct_output_scale.zero_()\n+            init.zeros_(module.correct_output_scale)\n \n \n class Gemma3nRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "375bd93f27230b36bc90b4f456ea199b71c35bae",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n@@ -1879,11 +1880,11 @@ class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, Gemma3nAudioAttention):\n-            module.per_dim_scale.zero_()\n+            init.zeros_(module.per_dim_scale)\n         elif isinstance(module, Gemma3nTextAltUp):\n-            module.correct_output_scale.zero_()\n+            init.zeros_(module.correct_output_scale)\n \n \n @auto_docstring(custom_intro=\"The base Gemma 3n language model without a language modeling head.\")"
        },
        {
            "sha": "66f11594e27ce92ede67013132e4398a7236cfc5",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -392,20 +393,21 @@ class GitPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, GitVisionEmbeddings):\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=self.config.initializer_range)\n-            nn.init.normal_(module.patch_embedding.weight, std=self.config.initializer_range)\n-            nn.init.normal_(module.position_embedding.weight, std=self.config.initializer_range)\n+            init.normal_(module.class_embedding, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.patch_embedding.weight, std=self.config.initializer_range)\n+            init.normal_(module.position_embedding.weight, std=self.config.initializer_range)\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPVisionEmbeddings with CLIP->Git"
        },
        {
            "sha": "1c6575f3420a15ccb9b0b6286e575c8baaa18002",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -496,7 +497,7 @@ class Glm4MoePreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4MoeTopkRouter):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "692ccd2ea9fad7c12ef535014d80309b0e3e2e18",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn.functional as F\n from torch.nn import LayerNorm\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -557,7 +558,7 @@ class Glm4vMoePreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4vMoeTextTopkRouter):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @dataclass"
        },
        {
            "sha": "42059a73bc4ef6dcb8b946b78d8c2e700fb279f4",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -389,21 +389,6 @@ class GLPNPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = []\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class GLPNModel(GLPNPreTrainedModel):"
        },
        {
            "sha": "ad07a2d61e6d6b3c404d6ae09501b3968a95c510",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -30,6 +30,7 @@\n \n from transformers.utils.generic import check_model_inputs\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -291,11 +292,11 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n-                module.rel_pos_h.zero_()\n-                module.rel_pos_w.zero_()\n+                init.zeros_(module.rel_pos_h)\n+                init.zeros_(module.rel_pos_w)\n         elif isinstance(module, GotOcr2VisionEncoder):\n             if module.pos_embed is not None:\n-                module.pos_embed.zero_()\n+                init.zeros_(module.pos_embed)\n \n \n @dataclass"
        },
        {
            "sha": "36f55a80583d5aea6f8d3c0a8e1ac9395998ab15",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_utils import PreTrainedModel\n@@ -294,11 +295,11 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n-                module.rel_pos_h.zero_()\n-                module.rel_pos_w.zero_()\n+                init.zeros_(module.rel_pos_h)\n+                init.zeros_(module.rel_pos_w)\n         elif isinstance(module, GotOcr2VisionEncoder):\n             if module.pos_embed is not None:\n-                module.pos_embed.zero_()\n+                init.zeros_(module.pos_embed)\n \n \n class GotOcr2Model(LlavaModel):"
        },
        {
            "sha": "d2179fa0c33545be09207657cbb72c7beef5f214",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, get_activation\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -484,16 +485,17 @@ def __init__(self, *inputs, **kwargs):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n         if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n@@ -505,7 +507,7 @@ def _init_weights(self, module):\n             for name, p in module.named_parameters():\n                 if name == \"c_proj.weight\":\n                     # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                    p.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n+                    init.normal_(p, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n \n \n @dataclass"
        },
        {
            "sha": "15080b6725214ef3108192fc0845a14c063b5a42",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -365,28 +366,17 @@ def __init__(self, *inputs, **kwargs):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n+        super()._init_weights(module)\n         if isinstance(module, (GPTBigCodeMLP, GPTBigCodeAttention)):\n             # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n             #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n             #   > the weights of residual layers at initialization by a factor of 1/N where N is the # of residual layers.\n             #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n             #\n             # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n-            module.c_proj.weight.normal_(\n-                mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n+            init.normal_(\n+                module.c_proj.weight, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer)\n             )\n-            module.c_proj._is_hf_initialized = True\n-        elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "84bf9f84b11f92dce61f88c62009b76110c47b16",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -381,24 +381,6 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _can_compile_fullgraph = False  # TODO: needs a hybrid cache\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear,)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class GPTNeoModel(GPTNeoPreTrainedModel):"
        },
        {
            "sha": "f723defcd0888d679ff15f64b92c2a2babe86e0b",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -47,26 +48,15 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"gpt_neox_japanese\"\n     _no_split_modules = [\"GPTNeoXJapaneseLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-\n     _can_compile_fullgraph = True\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, GPTNeoXJapaneseAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, GPTNeoXJapaneseAttention):\n             if module.dense_bias is not None:\n-                module.dense_bias.zero_()\n+                init.zeros_(module.dense_bias)\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoXJapanese"
        },
        {
            "sha": "8e1ce9df0b9728b54888c752c578c79303570d54",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n@@ -442,29 +443,18 @@ class GptOssPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n+        super()._init_weights(module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Parameter):\n-            module.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, GptOssRMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, GptOssExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.gate_up_proj_bias.zero_()\n-            module.down_proj.normal_(mean=0.0, std=std)\n-            module.down_proj_bias.zero_()\n+        if isinstance(module, GptOssExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.zeros_(module.gate_up_proj_bias)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n+            init.zeros_(module.down_proj_bias)\n         elif isinstance(module, GptOssAttention):\n-            module.sinks.normal_(mean=0.0, std=std)\n+            init.normal_(module.sinks, mean=0.0, std=std)\n         elif isinstance(module, GptOssTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n-            module.bias.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            init.normal_(module.bias, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "57acfea8df645631331f3c4a81a9f7dfe38a08af",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,14 +19,15 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_outputs import (\n     MoeModelOutputWithPast,\n )\n from ...modeling_rope_utils import dynamic_rope_update\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     TransformersKwargs,\n@@ -358,29 +359,18 @@ class GptOssPreTrainedModel(LlamaPreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n         std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Parameter):\n-            module.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, GptOssRMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, GptOssExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.gate_up_proj_bias.zero_()\n-            module.down_proj.normal_(mean=0.0, std=std)\n-            module.down_proj_bias.zero_()\n+        if isinstance(module, GptOssExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.zeros_(module.gate_up_proj_bias)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n+            init.zeros_(module.down_proj_bias)\n         elif isinstance(module, GptOssAttention):\n-            module.sinks.normal_(mean=0.0, std=std)\n+            init.normal_(module.sinks, mean=0.0, std=std)\n         elif isinstance(module, GptOssTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n-            module.bias.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            init.normal_(module.bias, mean=0.0, std=std)\n \n \n class GptOssModel(MixtralModel):"
        },
        {
            "sha": "2c04b1af0c6783221cf40d2b993b374c79bf4baf",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -444,24 +444,6 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_param_buffer_assignment = False\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear,)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class GPTJModel(GPTJPreTrainedModel):"
        },
        {
            "sha": "7aad1523def9316ca0db06881ff88db451638d70",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n@@ -289,21 +290,9 @@ class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, GraniteSpeechEncoderProjector):\n-            module.query.normal_()\n+        super()._init_weights(module)\n+        if isinstance(module, GraniteSpeechEncoderProjector):\n+            init.normal_(module.query)\n \n \n @auto_docstring("
        },
        {
            "sha": "f722ad416a2fa43ac1090f60d1531d1c3fab2f00",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -465,7 +466,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeParallelExperts):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "94b79494a0f20f2673f27656a956c5275ecd3537",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,6 +18,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n@@ -152,7 +153,7 @@ class GraniteMoePreTrainedModel(LlamaPreTrainedModel, PreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, GraniteMoeParallelExperts):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "760e7372d2209c19dead59e9554e9f25877f2df3",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n \n from transformers.activations import ACT2FN\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n@@ -1205,13 +1206,13 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeHybridParallelExperts):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n         if isinstance(module, GraniteMoeHybridMambaLayer):\n-            module.dt_bias.fill_(1.0)\n-            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n-            module.D.fill_(1.0)\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n         elif isinstance(module, GraniteMoeHybridRMSNormGated):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "6ee9999a0afb316000e9df4c27b0a368f31e4b9b",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast, MoeModelOutputWithPast\n@@ -180,11 +181,11 @@ class GraniteMoeHybridPreTrainedModel(GraniteMoeSharedPreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeHybridMambaLayer):\n-            module.dt_bias.fill_(1.0)\n-            module.A_log.copy_(torch.log(torch.arange(1, module.num_heads + 1)))\n-            module.D.fill_(1.0)\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.log(torch.arange(1, module.num_heads + 1)))\n+            init.ones_(module.D)\n         elif isinstance(module, GraniteMoeHybridRMSNormGated):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n \n \n class GraniteMoeHybridModel(GraniteMoeSharedModel):"
        },
        {
            "sha": "606a59390e6e46c54572fd6abe8d7b98d3932482",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -471,7 +472,7 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, GraniteMoeSharedParallelExperts):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n \n \n class GraniteMoeSharedRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "d1e8ea3e9fb88f6dbef9fbedecd57e11da30f953",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 39,
            "deletions": 37,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...file_utils import ModelOutput, is_timm_available, requires_backends\n from ...integrations import use_kernel_forward_from_hub\n@@ -1374,10 +1375,10 @@ def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, GroundingDinoLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, GroundingDinoMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -1390,50 +1391,51 @@ def _init_weights(self, module):\n             )\n             for i in range(module.n_points):\n                 grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight)\n-            nn.init.constant_(module.value_proj.bias, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight)\n-            nn.init.constant_(module.output_proj.bias, 0.0)\n+\n+            init.copy_(module.sampling_offsets.bias, grid_init.view(-1))\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n+            init.xavier_uniform_(module.value_proj.weight)\n+            init.constant_(module.value_proj.bias, 0.0)\n+            init.xavier_uniform_(module.output_proj.weight)\n+            init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, GroundingDinoBiMultiHeadAttention):\n-            nn.init.xavier_uniform_(module.vision_proj.weight)\n-            module.vision_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.text_proj.weight)\n-            module.text_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.values_vision_proj.weight)\n-            module.values_vision_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.values_text_proj.weight)\n-            module.values_text_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.out_vision_proj.weight)\n-            module.out_vision_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.out_text_proj.weight)\n-            module.out_text_proj.bias.fill_(0)\n+            init.xavier_uniform_(module.vision_proj.weight)\n+            init.zeros_(module.vision_proj.bias)\n+            init.xavier_uniform_(module.text_proj.weight)\n+            init.zeros_(module.text_proj.bias)\n+            init.xavier_uniform_(module.values_vision_proj.weight)\n+            init.zeros_(module.values_vision_proj.bias)\n+            init.xavier_uniform_(module.values_text_proj.weight)\n+            init.zeros_(module.values_text_proj.bias)\n+            init.xavier_uniform_(module.out_vision_proj.weight)\n+            init.zeros_(module.out_vision_proj.bias)\n+            init.xavier_uniform_(module.out_text_proj.weight)\n+            init.zeros_(module.out_text_proj.bias)\n         elif isinstance(module, GroundingDinoFusionLayer):\n-            module.vision_param.fill_(1e-4)\n-            module.text_param.fill_(1e-4)\n+            init.constant_(module.vision_param, 1e-4)\n+            init.constant_(module.text_param, 1e-4)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, GroundingDinoMLPPredictionHead):\n-            nn.init.constant_(module.layers[-1].weight, 0)\n-            nn.init.constant_(module.layers[-1].bias, 0)\n+            init.constant_(module.layers[-1].weight, 0)\n+            init.constant_(module.layers[-1].bias, 0)\n \n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n-            nn.init.normal_(module.level_embed)\n+            init.normal_(module.level_embed)\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, GroundingDinoDecoder):"
        },
        {
            "sha": "7e0877456e604bce6774094f639ef664538185ad",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -754,31 +755,31 @@ def _init_weights(self, module):\n \n         init_range = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=init_range)\n+            init.normal_(module.weight, mean=0.0, std=init_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n         factor = self.config.initializer_factor\n         if isinstance(module, GroupViTTextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, GroupViTAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, GroupViTMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n \n \n class GroupViTVisionEncoder(nn.Module):"
        },
        {
            "sha": "54a6931a64313293d8ce320052f6ee93d7f593ef",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -782,20 +783,20 @@ def _init_weights(self, module) -> None:\n         std = self.config.initializer_range\n \n         if isinstance(module, HieraEmbeddings):\n-            nn.init.trunc_normal_(module.position_embeddings, std=std)\n+            init.trunc_normal_(module.position_embeddings, std=std)\n \n         elif isinstance(module, HieraDecoder):\n-            nn.init.trunc_normal_(module.mask_token, std=std)\n-            nn.init.trunc_normal_(module.decoder_position_embeddings, std=std)\n+            init.trunc_normal_(module.mask_token, std=std)\n+            init.trunc_normal_(module.decoder_position_embeddings, std=std)\n \n         elif isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n-            nn.init.trunc_normal_(module.weight, std=std)\n+            init.trunc_normal_(module.weight, std=std)\n             if module.bias is not None:\n-                nn.init.constant_(module.bias, std)\n+                init.constant_(module.bias, std)\n \n         elif isinstance(module, nn.LayerNorm):\n-            nn.init.constant_(module.bias, std)\n-            nn.init.constant_(module.weight, self.config.layer_norm_init)\n+            init.constant_(module.bias, std)\n+            init.constant_(module.weight, self.config.layer_norm_init)\n \n \n class HieraPooler(nn.Module):"
        },
        {
            "sha": "614492f23ae0d07dbf92ae0f3d85176971761d5f",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -641,33 +642,33 @@ class HubertPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Conv1d):\n             if is_deepspeed_zero3_enabled():\n                 import deepspeed\n \n                 if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n                     with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight)\n+                        init.kaiming_normal_(module.weight)\n                 else:\n                     with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight)\n+                        init.kaiming_normal_(module.weight)\n             else:\n-                nn.init.kaiming_normal_(module.weight)\n+                init.kaiming_normal_(module.weight)\n \n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, HubertModel):\n             if hasattr(module, \"masked_spec_embed\"):\n-                module.masked_spec_embed.uniform_()\n+                init.uniform_(module.masked_spec_embed)\n         elif isinstance(module, HubertForSequenceClassification):\n             if hasattr(module, \"layer_weights\"):\n-                module.layer_weights.fill_(1.0 / (self.config.num_hidden_layers + 1))\n+                init.constant_(module.layer_weights, 1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\""
        },
        {
            "sha": "bc8aadb25e2767e1afeb333b18c61557fdf4eb7a",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_outputs import BaseModelOutput\n@@ -138,33 +139,33 @@ class HubertPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm1d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Conv1d):\n             if is_deepspeed_zero3_enabled():\n                 import deepspeed\n \n                 if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n                     with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight)\n+                        init.kaiming_normal_(module.weight)\n                 else:\n                     with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n-                        nn.init.kaiming_normal_(module.weight)\n+                        init.kaiming_normal_(module.weight)\n             else:\n-                nn.init.kaiming_normal_(module.weight)\n+                init.kaiming_normal_(module.weight)\n \n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, HubertModel):\n             if hasattr(module, \"masked_spec_embed\"):\n-                module.masked_spec_embed.uniform_()\n+                init.uniform_(module.masked_spec_embed)\n         elif isinstance(module, HubertForSequenceClassification):\n             if hasattr(module, \"layer_weights\"):\n-                module.layer_weights.fill_(1.0 / (self.config.num_hidden_layers + 1))\n+                init.constant_(module.layer_weights, 1.0 / (self.config.num_hidden_layers + 1))\n \n     def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\""
        },
        {
            "sha": "556e91b453eaabd31241c270b1719342b9a74bb5",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -290,18 +290,6 @@ class HunYuanDenseV1PreTrainedModel(PreTrainedModel):\n         \"attentions\": HunYuanDenseV1Attention,\n     }\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n \n class HunYuanDenseV1RotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`"
        },
        {
            "sha": "d41b5f236759504acaf07a6765cc8dbbcec6d67f",
            "filename": "src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -120,17 +120,7 @@ def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n \n \n class HunYuanDenseV1PreTrainedModel(LlamaPreTrainedModel):\n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+    pass\n \n \n class HunYuanDenseV1RotaryEmbedding(LlamaRotaryEmbedding):"
        },
        {
            "sha": "1d2fb7d936ca974c63120df56a8fe871f6ffbdb2",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import gelu\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -589,18 +590,19 @@ class IBertPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (QuantLinear, nn.Linear)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (QuantEmbedding, nn.Embedding)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, (IntLayerNorm, nn.LayerNorm)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, IBertLMHead):\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n     def resize_token_embeddings(self, new_num_tokens=None):\n         raise NotImplementedError(\"`resize_token_embeddings` is not supported for I-BERT.\")"
        },
        {
            "sha": "cb57a0fd8d20a09803233593ef52e0e6d7337bac",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -836,34 +837,21 @@ def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only\n         # inference and fine-tuning - so the proper init weights code has been removed - the m4 code\n         # base should be used for training from scratch and it contains the correct code.\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, IdeficsRMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, IdeficsVisionEmbeddings):\n-            module.class_embedding.normal_()\n+        super()._init_weights(module)\n+        if isinstance(module, IdeficsVisionEmbeddings):\n+            init.normal_(module.class_embedding)\n         elif isinstance(module, IdeficsGatedCrossAttentionLayer):\n             if self.config.alpha_initializer == \"zeros\":\n-                module.alpha_cross_attn.zero_()\n-                module.alpha_dense.zero_()\n+                init.zeros_(module.alpha_cross_attn)\n+                init.zeros_(module.alpha_dense)\n             elif self.config.alpha_initializer == \"ones\":\n-                module.alpha_cross_attn.fill_(1.0)\n-                module.alpha_dense.fill_(1.0)\n+                init.ones_(module.alpha_cross_attn)\n+                init.ones_(module.alpha_dense)\n             elif self.config.alpha_initializer in {\"normal\", \"gaussian\", \"random\"}:\n-                module.alpha_cross_attn.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n-                module.alpha_dense.normal_(mean=0.0, std=self.config.alphas_initializer_range)\n+                init.normal_(module.alpha_cross_attn, mean=0.0, std=self.config.alphas_initializer_range)\n+                init.normal_(module.alpha_dense, mean=0.0, std=self.config.alphas_initializer_range)\n         elif isinstance(module, IdeficsPerceiverResampler):\n-            module.latents.normal_()\n+            init.normal_(module.latents)\n \n \n @auto_docstring"
        },
        {
            "sha": "dcfa6d9cd23bfced25e0b77f6d2756b48dc7569a",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 20,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -419,27 +420,11 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, Idefics2RMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, nn.MultiheadAttention):\n-            module._reset_parameters()  # native torch init\n-        elif isinstance(module, Idefics2MultiheadAttentionPoolingHead):\n-            module.probe.normal_()\n+        super()._init_weights(module)\n+        if isinstance(module, Idefics2MultiheadAttentionPoolingHead):\n+            init.normal_(module.probe)\n         elif isinstance(module, Idefics2PerceiverResampler):\n-            module.latents.fill_(1.0)\n+            init.ones_(module.latents)\n \n \n @auto_docstring("
        },
        {
            "sha": "38d6f29c3f041dc15546fb9d0905672a75fd9dde",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -430,27 +430,8 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     _supports_attention_backend = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, Idefics3RMSNorm):\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "c7f2fd39f82452b6c1393ce24f81b23e58cb946c",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 7,
            "deletions": 18,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -11,6 +11,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n@@ -328,28 +329,16 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, IJepaEmbeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.position_embeddings.dtype)\n-            )\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n \n \n class IJepaEncoder(nn.Module):"
        },
        {
            "sha": "9ddde3f87e4b45d2ee69d897efb27f66817f10de",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 7,
            "deletions": 18,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -5,6 +5,7 @@\n \n from transformers.models.ijepa.configuration_ijepa import IJepaConfig\n \n+from ... import initialization as init\n from ...modeling_outputs import BaseModelOutputWithPooling, ImageClassifierOutput\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n@@ -91,28 +92,16 @@ class IJepaPreTrainedModel(ViTPreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, IJepaEmbeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.position_embeddings.dtype)\n-            )\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n \n \n class IJepaModel(IJepaPreTrainedModel, ViTModel):"
        },
        {
            "sha": "c5effca80166abeefba0380474a8977c815c4c3c",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -366,22 +367,10 @@ class ImageGPTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ImageGPTBlock\"]\n \n-    def __init__(self, *inputs, **kwargs):\n-        super().__init__(*inputs, **kwargs)\n-\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, ImageGPTLayerNorm):\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n \n         # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n         #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n@@ -393,7 +382,7 @@ def _init_weights(self, module):\n             for name, p in module.named_parameters():\n                 if \"c_proj\" in name and \"weight\" in name:\n                     # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n-                    p.normal_(mean=0.0, std=(self.config.initializer_range / math.sqrt(2 * self.config.n_layer)))\n+                    init.normal_(p, mean=0.0, std=self.config.initializer_range / math.sqrt(2 * self.config.n_layer))\n \n \n @auto_docstring"
        },
        {
            "sha": "dcb05efb752f71473c7da8dec6cdabc5ebbc0b5c",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...masking_utils import create_bidirectional_mask, create_causal_mask\n@@ -203,9 +204,9 @@ class InformerSinusoidalPositionalEmbedding(nn.Embedding):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n-        super().__init__(num_positions, embedding_dim)\n+        super().__init__(num_positions, embedding_dim, _freeze=True)\n \n-    def _init_weight(self):\n+    def create_weight(self):\n         \"\"\"\n         Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n         the 2nd half of the vector. [dim // 2:]\n@@ -218,7 +219,7 @@ def _init_weight(self):\n         sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n         out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n         out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n-        self.weight = nn.Parameter(out, requires_grad=False)\n+        return out\n \n     @torch.no_grad()\n     def forward(\n@@ -254,7 +255,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module):\n         super()._init_weights(module)\n         if isinstance(module, InformerSinusoidalPositionalEmbedding):\n-            module._init_weight()\n+            init.copy_(module.weight, module.create_weight())\n \n \n def eager_attention_forward("
        },
        {
            "sha": "7c5a4e85f3929af0655291097f5d7f35b6ce7ecc",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -90,7 +91,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module):\n         super()._init_weights(module)\n         if isinstance(module, InformerSinusoidalPositionalEmbedding):\n-            module._init_weight()\n+            init.copy_(module.weight, module.create_weight())\n \n \n class InformerAttention(BartAttention):"
        },
        {
            "sha": "5432283b4b0c7853d343a50bfbe8964fe6a57260",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 14,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -327,22 +328,13 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n         factor = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=factor)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=factor)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, InstructBlipVisionEmbeddings):\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        if isinstance(module, InstructBlipVisionEmbeddings):\n+            init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n         elif isinstance(module, (InstructBlipForConditionalGeneration, InstructBlipModel)):\n-            module.query_tokens.zero_()\n+            init.zeros_(module.query_tokens)\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->InstructBlip"
        },
        {
            "sha": "0375ec5042cff8a7d458c37483b8d93404105c23",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 14,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -150,22 +151,13 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n         factor = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=factor)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=factor)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, InstructBlipVideoVisionEmbeddings):\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        if isinstance(module, InstructBlipVideoVisionEmbeddings):\n+            init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n         elif isinstance(module, (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel)):\n-            module.query_tokens.zero_()\n+            init.zeros_(module.query_tokens)\n \n \n # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBlipVideo doesn't cast attn weights to fp32"
        },
        {
            "sha": "0ec57c60a20cd54b8959f47c2539a30a818674b5",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -28,6 +28,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -416,14 +417,14 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         super()._init_weights(module)\n         if isinstance(module, InternVLVisionEmbeddings):\n-            module.cls_token.zero_()\n+            init.zeros_(module.cls_token)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n             if module.position_embeddings is not None:\n-                module.position_embeddings.zero_()\n+                init.zeros_(module.position_embeddings)\n         elif isinstance(module, InternVLVisionLayer):\n-            module.lambda_1.fill_(self.config.layer_scale_init_value)\n-            module.lambda_2.fill_(self.config.layer_scale_init_value)\n+            init.constant_(module.lambda_1, self.config.layer_scale_init_value)\n+            init.constant_(module.lambda_2, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "f276a49ec5ce2b5ba44e9e04a8e647761954d3d3",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -373,14 +374,14 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         super()._init_weights(module)\n         if isinstance(module, InternVLVisionEmbeddings):\n-            module.cls_token.zero_()\n+            init.zeros_(module.cls_token)\n             if module.mask_token is not None:\n-                module.mask_token.zero_()\n+                init.zeros_(module.mask_token)\n             if module.position_embeddings is not None:\n-                module.position_embeddings.zero_()\n+                init.zeros_(module.position_embeddings)\n         elif isinstance(module, InternVLVisionLayer):\n-            module.lambda_1.fill_(self.config.layer_scale_init_value)\n-            module.lambda_2.fill_(self.config.layer_scale_init_value)\n+            init.constant_(module.lambda_1, self.config.layer_scale_init_value)\n+            init.constant_(module.lambda_2, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "450c06d165a9777a789af5f7205cb923c116ac57",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n@@ -728,8 +729,8 @@ def _init_weights(self, module):\n         if isinstance(module, JambaMambaMixer):\n             A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n-            module.A_log.copy_(torch.log(A))\n-            module.D.fill_(1.0)\n+            init.copy_(module.A_log, torch.log(A))\n+            init.ones_(module.D)\n \n \n ALL_DECODER_LAYER_TYPES = {\"attention\": JambaAttentionDecoderLayer, \"mamba\": JambaMambaDecoderLayer}"
        },
        {
            "sha": "4cbe69d0e1b73b6fa7996c98294447226c8b278b",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n@@ -613,8 +614,8 @@ def _init_weights(self, module):\n         if isinstance(module, JambaMambaMixer):\n             A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n-            module.A_log.copy_(torch.log(A))\n-            module.D.fill_(1.0)\n+            init.copy_(module.A_log, torch.log(A))\n+            init.ones_(module.D)\n \n \n @auto_docstring"
        },
        {
            "sha": "afa139d66f499c441ccd6ded817c6ace3f775c4e",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -585,20 +586,11 @@ class JetMoePreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, JetMoeRMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, JetMoeParallelExperts):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+        super()._init_weights(module)\n+        if isinstance(module, JetMoeParallelExperts):\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, JetMoeMoA | JetMoeMoE):\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "41babe708c63fb09cb0af7b5a6325d1646896a58",
            "filename": "src/transformers/models/jetmoe/modular_jetmoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 14,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -30,7 +31,7 @@\n     GenericForSequenceClassification,\n )\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n@@ -438,20 +439,11 @@ class JetMoePreTrainedModel(MixtralPreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, JetMoeRMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, JetMoeParallelExperts):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, JetMoeParallelExperts):\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, JetMoeMoA | JetMoeMoE):\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "3d2c6486520169f9bbf7f6a1019379c284bb8107",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 23,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -1134,44 +1135,44 @@ def _init_weights(self, module: nn.Module):\n             std = self.config.text_config.init_std\n \n         if isinstance(module, Kosmos2VisionEmbeddings):\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, Kosmos2VisionAttention):\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, Kosmos2VisionMLP):\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, KosmosTextAttention):\n-            nn.init.normal_(module.q_proj.weight, std=std)\n-            nn.init.normal_(module.k_proj.weight, std=std)\n-            nn.init.normal_(module.v_proj.weight, std=std)\n-            nn.init.normal_(module.out_proj.weight, std=std)\n+            init.normal_(module.q_proj.weight, std=std)\n+            init.normal_(module.k_proj.weight, std=std)\n+            init.normal_(module.v_proj.weight, std=std)\n+            init.normal_(module.out_proj.weight, std=std)\n         elif isinstance(module, Kosmos2TextFFN):\n-            nn.init.normal_(module.fc1.weight, std=std)\n-            nn.init.normal_(module.fc2.weight, std=std)\n+            init.normal_(module.fc1.weight, std=std)\n+            init.normal_(module.fc2.weight, std=std)\n         elif isinstance(module, Kosmos2TextForCausalLM):\n-            nn.init.normal_(module.lm_head.weight, std=std)\n+            init.normal_(module.lm_head.weight, std=std)\n         elif isinstance(module, Kosmos2ImageToTextProjection):\n-            nn.init.normal_(module.dense.weight, std=std)\n-            nn.init.normal_(module.latent_query)\n+            init.normal_(module.dense.weight, std=std)\n+            init.normal_(module.latent_query)\n         elif isinstance(module, Kosmos2TextTransformer):\n-            module.embed_tokens.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.embed_tokens.weight, mean=0.0, std=std)\n             if module.embed_tokens.padding_idx is not None:\n-                module.embed_tokens.weight[module.embed_tokens.padding_idx].zero_()\n+                init.zeros_(module.embed_tokens.weight[module.embed_tokens.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class Kosmos2VisionModel(Kosmos2PreTrainedModel):"
        },
        {
            "sha": "a9a456d68d15c0c68e0b05773f5a5ce8eb7a719b",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -1238,19 +1239,20 @@ def _init_weights(self, module):\n         elif isinstance(self, (Kosmos2_5Model, Kosmos2_5ForConditionalGeneration)):\n             std = self.config.text_config.init_std\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, (nn.LayerNorm, Kosmos2_5LayerNorm)):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n             if getattr(module, \"bias\", None) is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, Kosmos2_5ImageToTextProjection):\n-            module.latent_query.normal_(mean=0.0, std=1.0)\n+            init.normal_(module.latent_query, mean=0.0, std=1.0)\n \n \n class Kosmos2_5VisionModel(Kosmos2_5PreTrainedModel):"
        },
        {
            "sha": "af6ffaf046744da5f051494b08af994009a23a81",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 23,
            "deletions": 33,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationConfig, GenerationMixin\n@@ -54,25 +55,6 @@\n logger = logging.get_logger(__name__)\n \n \n-class KyutaiSpeechToTextRMSNorm(nn.Module):\n-    def __init__(self, dim: int, eps: float = 1e-6):\n-        super().__init__()\n-        self.eps = eps\n-        self.weight = nn.Parameter(torch.ones(dim))  # Ignore copy\n-\n-    def _norm(self, x):\n-        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n-\n-    # Ignore copy\n-    def forward(self, x):\n-        output = self._norm(x.float())\n-        output = output * self.weight.float()\n-        return output.type_as(x)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n-\n-\n class KyutaiSpeechToTextFlexibleLinear(nn.Module):\n     def __init__(self, input_size, output_size, num_layers):\n         super().__init__()\n@@ -126,20 +108,9 @@ class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, KyutaiSpeechToTextFlexibleLinear):\n-            module.weight.normal_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, KyutaiSpeechToTextRMSNorm):\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n+        if isinstance(module, KyutaiSpeechToTextFlexibleLinear):\n+            init.normal_(module.weight)\n \n \n class KyutaiSpeechToTextConv1dPaddingCache:\n@@ -251,6 +222,25 @@ def forward(self, input_ids):\n         return inputs_embeds\n \n \n+class KyutaiSpeechToTextRMSNorm(nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.ones(dim))  # Ignore copy\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    # Ignore copy\n+    def forward(self, x):\n+        output = self._norm(x.float())\n+        output = output * self.weight.float()\n+        return output.type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n class KyutaiSpeechToTextLinear(nn.Module):\n     def __init__(self, input_dim, output_dim, num_codebooks, use_flexible_linear=False):\n         super().__init__()"
        },
        {
            "sha": "57537beb1f47b3977477cd5fdedfd8c0124281d5",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -427,19 +428,9 @@ class LayoutLMPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, LayoutLMLayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, LayoutLMLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, LayoutLMLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "c98b84ddd39cd9249b49c4727e6bb89cd4c2cab1",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -461,24 +462,14 @@ class LayoutLMv2PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, LayoutLMv2SelfAttention):\n+        super()._init_weights(module)\n+        if isinstance(module, LayoutLMv2SelfAttention):\n             if self.config.fast_qkv:\n-                module.q_bias.zero_()\n-                module.v_bias.zero_()\n+                init.zeros_(module.q_bias)\n+                init.zeros_(module.v_bias)\n         elif isinstance(module, LayoutLMv2Model):\n             if hasattr(module, \"visual_segment_embedding\"):\n-                module.visual_segment_embedding.normal_(mean=0.0, std=self.config.initializer_range)\n+                init.normal_(module.visual_segment_embedding, mean=0.0, std=self.config.initializer_range)\n \n \n def my_convert_sync_batchnorm(module, process_group=None):"
        },
        {
            "sha": "bb020f1614ab8972d4d34839a85c0155753c380b",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn.functional as F\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -206,21 +207,11 @@ class LayoutLMv3PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, LayoutLMv3Model):\n+        super()._init_weights(module)\n+        if isinstance(module, LayoutLMv3Model):\n             if self.config.visual_embed:\n-                module.cls_token.zero_()\n-                module.pos_embed.zero_()\n+                init.zeros_(module.cls_token)\n+                init.zeros_(module.pos_embed)\n \n \n class LayoutLMv3SelfAttention(nn.Module):\n@@ -600,7 +591,7 @@ def __init__(self, config):\n \n         self.encoder = LayoutLMv3Encoder(config)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.embeddings.word_embeddings\n@@ -890,7 +881,7 @@ def __init__(self, config):\n         else:\n             self.classifier = LayoutLMv3ClassificationHead(config, pool_feature=False)\n \n-        self.init_weights()\n+        self.post_init()\n \n     @auto_docstring\n     def forward(\n@@ -989,7 +980,7 @@ def __init__(self, config):\n         self.layoutlmv3 = LayoutLMv3Model(config)\n         self.qa_outputs = LayoutLMv3ClassificationHead(config, pool_feature=False)\n \n-        self.init_weights()\n+        self.post_init()\n \n     @auto_docstring\n     def forward(\n@@ -1108,7 +1099,7 @@ def __init__(self, config):\n         self.layoutlmv3 = LayoutLMv3Model(config)\n         self.classifier = LayoutLMv3ClassificationHead(config, pool_feature=False)\n \n-        self.init_weights()\n+        self.post_init()\n \n     @auto_docstring\n     def forward("
        },
        {
            "sha": "40d6b82c9ede309f6a1e9119bd959cda4f96b9cc",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -1067,18 +1067,6 @@ class LEDPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"led\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "250070b44b8fde2d4ad0cbfa93f4610e963f6018",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -472,17 +472,6 @@ class LevitPreTrainedModel(PreTrainedModel):\n     input_modalities = \"image\"\n     _no_split_modules = [\"LevitResidualLayer\"]\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class LevitModel(LevitPreTrainedModel):"
        },
        {
            "sha": "78e0546bd51ee555eebc8a8c132af6d267a79ef1",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -500,21 +500,6 @@ class LiltPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class LiltModel(LiltPreTrainedModel):"
        },
        {
            "sha": "355a4f1457793bc9633b9314d0b415ebca484b7e",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 7,
            "deletions": 18,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n \n from transformers.models.llama4.configuration_llama4 import Llama4VisionConfig\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -474,30 +475,18 @@ class Llama4PreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n+        super()._init_weights(module)\n         std = (\n             self.config.initializer_range\n             if hasattr(self.config, \"initializer_range\")\n             else self.config.text_config.initializer_range\n         )\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, Llama4TextRMSNorm):\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, Llama4TextExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+        if isinstance(module, Llama4TextExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, Llama4VisionModel):\n-            module.class_embedding.normal_(std=module.scale)\n-            module.positional_embedding_vlm.normal_(std=module.scale)\n+            init.normal_(module.class_embedding, std=module.scale)\n+            init.normal_(module.positional_embedding_vlm, std=module.scale)\n \n \n @auto_docstring"
        },
        {
            "sha": "cd155dbce452cec190e560de48931c0f4cc6fd3f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -240,12 +241,12 @@ def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, LlavaNextModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.normal_(mean=0.0, std=embed_std)\n+            init.normal_(module.image_newline, mean=0.0, std=embed_std)\n \n \n @auto_docstring("
        },
        {
            "sha": "34feba5e18c7df749168554a036250e264d6181f",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -181,12 +182,12 @@ def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, LlavaNextVideoModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.normal_(mean=0.0, std=embed_std)\n+            init.normal_(module.image_newline, mean=0.0, std=embed_std)\n \n \n def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):"
        },
        {
            "sha": "1bd91aee4715606ef79d565e94ab8e76a86c3a43",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -122,12 +123,12 @@ def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, LlavaOnevisionModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.normal_(mean=0.0, std=embed_std)\n+            init.normal_(module.image_newline, mean=0.0, std=embed_std)\n \n \n class LlavaOnevisionMultiModalProjector(nn.Module):"
        },
        {
            "sha": "8c6b9add8cdb8d109e2f42004c23afa61fed77ab",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -561,10 +562,10 @@ class LongcatFlashPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, LongcatFlashTopkRouter):\n-            module.classifier.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.classifier.weight, mean=0.0, std=self.config.initializer_range)\n         if isinstance(module, LongcatFlashExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=self.config.initializer_range)\n-            module.down_proj.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "9e4b6486631b6d0504ae7d1cd3ce8d8fae4857fa",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n@@ -345,10 +346,10 @@ class LongcatFlashPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, LongcatFlashTopkRouter):\n-            module.classifier.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.classifier.weight, mean=0.0, std=self.config.initializer_range)\n         if isinstance(module, LongcatFlashExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=self.config.initializer_range)\n-            module.down_proj.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n class LongcatFlashModel(DeepseekV3Model):"
        },
        {
            "sha": "f1fbe165808449b407365555a8448cfa01b13d73",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -1292,21 +1292,6 @@ class LongformerPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LongformerSelfAttention\"]\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class LongformerModel(LongformerPreTrainedModel):"
        },
        {
            "sha": "df716432bc2695babf847f6ea6cbb8c7c7ed8044",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 22,
            "deletions": 19,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -1181,40 +1182,42 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, LongT5LayerNorm):\n-            module.weight.fill_(factor * 1.0)\n+            init.constant_(module.weight, factor * 1.0)\n         elif isinstance(module, (LongT5Model, LongT5ForConditionalGeneration, LongT5EncoderModel)):\n-            module.shared.weight.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.shared.weight, mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n-                module.lm_head.weight.normal_(mean=0.0, std=factor * 1.0)\n+                init.normal_(module.lm_head.weight, mean=0.0, std=factor * 1.0)\n         elif isinstance(module, LongT5DenseActDense):\n-            module.wi.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                module.wi.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, LongT5DenseGatedActDense):\n-            module.wi_0.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi_0.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n-                module.wi_0.bias.zero_()\n-            module.wi_1.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+                init.zeros_(module.wi_0.bias)\n+            init.normal_(module.wi_1.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n-                module.wi_1.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi_1.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, (LongT5Attention, LongT5LocalAttention, LongT5TransientGlobalAttention)):\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_kv\n             n_heads = self.config.num_heads\n-            module.q.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.k.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.v.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.o.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.q.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.k.weight, mean=0.0, std=factor * (d_model**-0.5))\n+            init.normal_(module.v.weight, mean=0.0, std=factor * (d_model**-0.5))\n+            init.normal_(module.o.weight, mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n             if module.has_relative_attention_bias:\n-                module.relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n+                init.normal_(module.relative_attention_bias.weight, mean=0.0, std=factor * ((d_model) ** -0.5))\n                 if isinstance(module, LongT5TransientGlobalAttention):\n-                    module.global_relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n+                    init.normal_(\n+                        module.global_relative_attention_bias.weight, mean=0.0, std=factor * ((d_model) ** -0.5)\n+                    )\n \n     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->LongT5\n     def _shift_right(self, input_ids):"
        },
        {
            "sha": "fc6fd7a8081087862febe4d5622765e69b655cb3",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, gelu\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n@@ -770,19 +771,20 @@ class LukePreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n             if module.embedding_dim == 1:  # embedding for bias parameters\n-                module.weight.zero_()\n+                init.zeros_(module.weight)\n             else:\n-                module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+                init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n @auto_docstring("
        },
        {
            "sha": "75d31c0399b312972d9c72d75219a97912f9813b",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss, SmoothL1Loss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, gelu\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -677,19 +678,9 @@ class LxmertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, LxmertLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, LxmertLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "676938c6789fae682ca919053bf69aa0fa467dd6",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -512,25 +512,9 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     # Doesn't support `compile` (dynamic control flow). Can be fixed but low usage model\n     _can_compile_fullgraph = False\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n \n class M2M100Encoder(M2M100PreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "7437082349ac39e5cbc9a6540504aaa3a265c296",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 16,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n@@ -513,14 +514,14 @@ def _init_weights(self, module):\n             # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n             A = torch.arange(1, module.ssm_state_size + 1, dtype=torch.float32)[None, :]\n             A = A.expand(module.intermediate_size, -1).contiguous()\n-            module.A_log.copy_(torch.log(A))\n-            module.D.fill_(1.0)\n+            init.copy_(module.A_log, torch.log(A))\n+            init.ones_(module.D)\n \n             dt_init_std = self.config.time_step_rank**-0.5 * self.config.time_step_scale\n             if self.config.time_step_init_scheme == \"constant\":\n-                nn.init.constant_(module.dt_proj.weight, dt_init_std)\n+                init.constant_(module.dt_proj.weight, dt_init_std)\n             elif self.config.time_step_init_scheme == \"random\":\n-                nn.init.uniform_(module.dt_proj.weight, -dt_init_std, dt_init_std)\n+                init.uniform_(module.dt_proj.weight, -dt_init_std, dt_init_std)\n \n             dt = torch.exp(\n                 torch.rand(self.config.intermediate_size)\n@@ -529,14 +530,12 @@ def _init_weights(self, module):\n             ).clamp(min=self.config.time_step_floor)\n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n-            module.dt_proj.bias.copy_(inv_dt)\n-            module.dt_proj.bias._no_reinit = True\n+            init.copy_(module.dt_proj.bias, inv_dt)\n \n-            nn.init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n             if module.conv1d.bias is not None:\n-                if not getattr(module.conv1d.bias, \"_no_reinit\", False):\n-                    nn.init.zeros_(module.conv1d.bias)\n-            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n+                init.zeros_(module.conv1d.bias)\n+            init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n \n             if self.config.rescale_prenorm_residual:\n                 # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n@@ -553,15 +552,13 @@ def _init_weights(self, module):\n                 p /= math.sqrt(self.config.num_hidden_layers)\n \n         if isinstance(module, nn.Linear):\n-            if not getattr(module.weight, \"_no_reinit\", False):\n-                nn.init.normal_(module.weight, std=std)\n+            init.normal_(module.weight, std=std)\n             if module.bias is not None:\n-                if not getattr(module.bias, \"_no_reinit\", False):\n-                    nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n         elif isinstance(module, MambaRMSNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, std=std)\n+            init.normal_(module.weight, std=std)\n \n \n @dataclass"
        },
        {
            "sha": "d2e7add0d4f317c6dfcea9a43d9d9ba02c16eb1a",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -725,8 +726,8 @@ def _init_weights(self, module):\n             # S4D real initialization. These are not discretized!\n             # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n             A = torch.arange(1, self.config.num_heads + 1)\n-            module.A_log.copy_(torch.log(A))\n-            module.D.fill_(1.0)\n+            init.copy_(module.A_log, torch.log(A))\n+            init.ones_(module.D)\n \n             dt = torch.exp(\n                 torch.rand(self.config.num_heads)\n@@ -736,14 +737,12 @@ def _init_weights(self, module):\n \n             # # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n             inv_dt = dt + torch.log(-torch.expm1(-dt))\n-            module.dt_bias.copy_(inv_dt)\n-            module.dt_bias._no_reinit = True\n+            init.copy_(module.dt_bias, inv_dt)\n \n-            nn.init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n+            init.kaiming_uniform_(module.conv1d.weight, a=math.sqrt(5))\n             if module.conv1d.bias is not None:\n-                if not getattr(module.conv1d.bias, \"_no_reinit\", False):\n-                    nn.init.zeros_(module.conv1d.bias)\n-            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n+                init.zeros_(module.conv1d.bias)\n+            init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n \n             if self.config.rescale_prenorm_residual:\n                 # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n@@ -760,15 +759,13 @@ def _init_weights(self, module):\n                 p /= math.sqrt(self.config.num_hidden_layers)\n \n         if isinstance(module, nn.Linear):\n-            if not getattr(module.weight, \"_no_reinit\", False):\n-                nn.init.normal_(module.weight, std=std)\n+            init.normal_(module.weight, std=std)\n             if module.bias is not None:\n-                if not getattr(module.bias, \"_no_reinit\", False):\n-                    nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n         elif isinstance(module, (Mamba2RMSNorm, MambaRMSNormGated)):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Embedding):\n-            nn.init.normal_(module.weight, std=std)\n+            init.normal_(module.weight, std=std)\n \n \n @dataclass"
        },
        {
            "sha": "3a211ddbe7153beed3d7ad59365d010bc6833adf",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 8,
            "deletions": 18,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -71,9 +72,9 @@ class MarianSinusoidalPositionalEmbedding(nn.Embedding):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n-        super().__init__(num_positions, embedding_dim)\n+        super().__init__(num_positions, embedding_dim, _freeze=True)\n \n-    def _init_weight(self):\n+    def create_weight(self):\n         \"\"\"\n         Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n         the 2nd half of the vector. [dim // 2:]\n@@ -86,7 +87,7 @@ def _init_weight(self):\n         sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n         out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n         out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n-        self.weight = nn.Parameter(out, requires_grad=False)\n+        return out\n \n     @torch.no_grad()\n     def forward(\n@@ -446,21 +447,10 @@ class MarianPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n \n     @torch.no_grad()\n-    def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalPositionalEmbedding]):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, MarianSinusoidalPositionalEmbedding):\n-            module._init_weight()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, MarianSinusoidalPositionalEmbedding):\n+            init.copy_(module.weight, module.create_weight())\n \n     @property\n     def dummy_inputs(self):"
        },
        {
            "sha": "c5daabe2abde9cf86a2e59c362b0c8700429dde9",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -513,19 +514,9 @@ class MarkupLMPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, MarkupLMLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, MarkupLMLMPredictionHead):\n+            init.zeros_(module.bias)\n \n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):"
        },
        {
            "sha": "5ac2b4a5f2eab87512d84d3582dbf8f3f0cf6e3f",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 25,
            "deletions": 23,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...file_utils import ModelOutput, is_scipy_available, requires_backends\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -2111,11 +2112,11 @@ def _init_weights(self, module: nn.Module):\n             if module.input_projections is not None:\n                 for input_projection in module.input_projections:\n                     if not isinstance(input_projection, nn.Sequential):\n-                        nn.init.xavier_uniform_(input_projection.weight, gain=xavier_std)\n-                        nn.init.constant_(input_projection.bias, 0)\n+                        init.xavier_uniform_(input_projection.weight, gain=xavier_std)\n+                        init.constant_(input_projection.bias, 0)\n \n         elif isinstance(module, Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).float() * (2.0 * math.pi / module.n_heads)\n             grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n             grid_init = (\n@@ -2125,42 +2126,43 @@ def _init_weights(self, module: nn.Module):\n             )\n             for i in range(module.n_points):\n                 grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n \n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight)\n-            nn.init.constant_(module.value_proj.bias, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight)\n-            nn.init.constant_(module.output_proj.bias, 0.0)\n+            init.copy_(module.sampling_offsets.bias, grid_init.view(-1))\n+\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n+            init.xavier_uniform_(module.value_proj.weight)\n+            init.constant_(module.value_proj.bias, 0.0)\n+            init.xavier_uniform_(module.output_proj.weight)\n+            init.constant_(module.output_proj.bias, 0.0)\n \n         elif isinstance(module, Mask2FormerMaskedAttentionDecoderLayer):\n             for p in module.parameters():\n                 if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p, gain=xavier_std)\n-            module.cross_attn.in_proj_bias.zero_()\n+                    init.xavier_uniform_(p, gain=xavier_std)\n+            init.zeros_(module.cross_attn.in_proj_bias)\n \n         elif isinstance(module, Mask2FormerPixelDecoder):\n-            nn.init.normal_(module.level_embed, std=0)\n+            init.normal_(module.level_embed, std=0)\n \n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n         if hasattr(module, \"reference_points\"):\n-            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n \n \n @auto_docstring"
        },
        {
            "sha": "22a29fe96b709020db9cbb46348838f9a2806a42",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1442,37 +1443,38 @@ def _init_weights(self, module: nn.Module):\n         std = self.config.init_std\n         if isinstance(module, MaskFormerTransformerModule):\n             if module.input_projection is not None:\n-                nn.init.xavier_uniform_(module.input_projection.weight, gain=xavier_std)\n-                nn.init.constant_(module.input_projection.bias, 0)\n+                init.xavier_uniform_(module.input_projection.weight, gain=xavier_std)\n+                init.constant_(module.input_projection.bias, 0)\n         # FPN\n         elif isinstance(module, MaskFormerFPNModel):\n-            nn.init.xavier_uniform_(module.stem.get_submodule(\"0\").weight, gain=xavier_std)\n+            init.xavier_uniform_(module.stem.get_submodule(\"0\").weight, gain=xavier_std)\n \n         elif isinstance(module, MaskFormerFPNLayer):\n-            nn.init.xavier_uniform_(module.proj[0].weight, gain=xavier_std)\n+            init.xavier_uniform_(module.proj[0].weight, gain=xavier_std)\n \n         elif isinstance(module, MaskFormerFPNConvLayer):\n-            nn.init.xavier_uniform_(module.get_submodule(\"0\").weight, gain=xavier_std)\n+            init.xavier_uniform_(module.get_submodule(\"0\").weight, gain=xavier_std)\n         # The MLP head\n         elif isinstance(module, MaskformerMLPPredictionHead):\n             # I was not able to find the correct initializer in the original implementation\n             # we'll use xavier\n             for submodule in module.modules():\n                 if isinstance(submodule, nn.Linear):\n-                    nn.init.xavier_uniform_(submodule.weight, gain=xavier_std)\n-                    nn.init.constant_(submodule.bias, 0)\n+                    init.xavier_uniform_(submodule.weight, gain=xavier_std)\n+                    init.constant_(submodule.bias, 0)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         # copied from DETR\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n @auto_docstring"
        },
        {
            "sha": "05a58d4527df7a45588fd3109902e5c9e8194303",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...file_utils import ModelOutput\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -704,18 +705,12 @@ class MaskFormerSwinPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, MaskFormerSwinEmbeddings):\n+        super()._init_weights(module)\n+        if isinstance(module, MaskFormerSwinEmbeddings):\n             if module.position_embeddings is not None:\n-                module.position_embeddings.zero_()\n+                init.zeros_(module.position_embeddings)\n         elif isinstance(module, MaskFormerSwinSelfAttention):\n-            module.relative_position_bias_table.zero_()\n+            init.zeros_(module.relative_position_bias_table)\n \n \n class MaskFormerSwinModel(MaskFormerSwinPreTrainedModel):\n@@ -731,6 +726,8 @@ def __init__(self, config, add_pooling_layer=True):\n         self.layernorm = nn.LayerNorm(self.num_features, eps=config.layer_norm_eps)\n         self.pooler = nn.AdaptiveAvgPool1d(1) if add_pooling_layer else None\n \n+        self.post_init()\n+\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n "
        },
        {
            "sha": "b4a3ed8ebd02e7f02526b38f7f68386c88174f9e",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -476,24 +476,8 @@ class MBartPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "8dfea90bbae1d3f77fafeed344e0900e2ed46a36",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -524,15 +525,9 @@ class MegatronBertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Embedding)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, MegatronBertLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, MegatronBertLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @dataclass"
        },
        {
            "sha": "2407a4147fb12cf8e118ef336adf2fff534c1422",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -11,6 +11,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -303,57 +304,57 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, MetaClip2TextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, MetaClip2VisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, MetaClip2Attention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, MetaClip2MLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, MetaClip2Model):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, MetaClip2VisionModelWithProjection):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, MetaClip2TextModelWithProjection):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, MetaClip2ForImageClassification):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.classifier.weight,\n                 std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class MetaClip2Encoder(nn.Module):"
        },
        {
            "sha": "49914ab1e6b3d40bf4c191100f26433774e6abfa",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -3,6 +3,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...processing_utils import Unpack\n@@ -222,57 +223,57 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, MetaClip2TextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, MetaClip2VisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, MetaClip2Attention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, MetaClip2MLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, MetaClip2Model):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, MetaClip2VisionModelWithProjection):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, MetaClip2TextModelWithProjection):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n         elif isinstance(module, MetaClip2ForImageClassification):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.classifier.weight,\n                 std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n             )\n \n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class MetaClip2TextTransformer(CLIPTextTransformer):"
        },
        {
            "sha": "73176c800d1a5d61f5ef559f6f9a7ae01bc56d9e",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -289,15 +290,15 @@ def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, MgpstrEmbeddings):\n-            nn.init.trunc_normal_(module.pos_embed, mean=0.0, std=std)\n-            nn.init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n+            init.trunc_normal_(module.pos_embed, mean=0.0, std=std)\n+            init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n         elif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            nn.init.trunc_normal_(module.weight, mean=0.0, std=std)\n+            init.trunc_normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "7a367e05261abe9bd627e6e0c716bfd2cc17a520",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...masking_utils import create_causal_mask\n@@ -1388,19 +1389,19 @@ class MimiPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n-            nn.init.kaiming_normal_(module.weight)\n+            init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-k, b=k)\n+                init.uniform_(module.bias, a=-k, b=k)\n         elif isinstance(module, MimiLayerScale):\n-            module.scale.fill_(self.config.layer_scale_initial_scale)\n+            init.constant_(module.scale, self.config.layer_scale_initial_scale)\n \n \n @auto_docstring("
        },
        {
            "sha": "004ed68cef23c483820ee21ab7c7cc3f21c89045",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -607,10 +608,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, MiniMaxExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, MiniMaxTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "1faff1f4dceaad0a91b190aa2aa00f1be70554b1",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -33,6 +33,7 @@\n \n from transformers.utils.generic import check_model_inputs\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -414,10 +415,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, MixtralExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, MixtralTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "fabb84db688ec50f0296c6da5c0e116b9ce23787",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n@@ -275,10 +276,10 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         std = self.config.initializer_range\n         if isinstance(module, MixtralExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, MixtralTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n class MixtralModel(MistralModel):"
        },
        {
            "sha": "e1b15cc27192fdcfb4666d8d73eaf6bbeae3023f",
            "filename": "src/transformers/models/mlcd/configuration_mlcd.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconfiguration_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconfiguration_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconfiguration_mlcd.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,7 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PreTrainedConfig\n \n "
        },
        {
            "sha": "72e26db9bd1cd99e0ab8a3c2425a625fd2fcb6aa",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n@@ -421,31 +422,31 @@ def _init_weights(self, module):\n         factor = self.config.initializer_factor\n         if isinstance(module, MLCDVisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, MLCDAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, MLCDMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, MLCDVisionTransformer):\n             factor = self.config.initializer_factor\n             pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n-            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+            init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class MLCDVisionTransformer(nn.Module):"
        },
        {
            "sha": "4cfc743948ef9b956cc5adc007a431825e4cda06",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,6 +18,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...configuration_utils import PreTrainedConfig\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -360,31 +361,31 @@ def _init_weights(self, module):\n         factor = self.config.initializer_factor\n         if isinstance(module, MLCDVisionEmbeddings):\n             factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, MLCDAttention):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, MLCDMLP):\n             factor = self.config.initializer_factor\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, MLCDVisionTransformer):\n             factor = self.config.initializer_factor\n             pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n-            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+            init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n class MLCDVisionTransformer(CLIPVisionTransformer):"
        },
        {
            "sha": "f551312741946d9314470cef3e811258ad7f0d8b",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -818,32 +819,33 @@ def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, MllamaTextRMSNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n         elif isinstance(module, MllamaVisionModel):\n-            nn.init.normal_(module.class_embedding, std=std)\n+            init.normal_(module.class_embedding, std=std)\n         elif isinstance(module, MllamaPrecomputedPositionEmbedding):\n-            nn.init.normal_(module.embedding, std=std)\n-            nn.init.zeros_(module.gate)\n+            init.normal_(module.embedding, std=std)\n+            init.zeros_(module.gate)\n         elif isinstance(module, MllamaVisionEncoderLayer) and module.is_gated:\n-            nn.init.normal_(module.gate_attn, std=std)\n-            nn.init.normal_(module.gate_ffn, std=std)\n+            init.normal_(module.gate_attn, std=std)\n+            init.normal_(module.gate_ffn, std=std)\n         elif isinstance(module, MllamaCrossAttentionDecoderLayer):\n-            module.cross_attn_attn_gate.zero_()\n-            module.cross_attn_mlp_gate.zero_()\n+            init.zeros_(module.cross_attn_attn_gate)\n+            init.zeros_(module.cross_attn_mlp_gate)\n         elif isinstance(module, MllamaPrecomputedAspectRatioEmbedding):\n             if module.is_gated:\n-                module.gate.zero_()\n+                init.zeros_(module.gate)\n \n     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask("
        },
        {
            "sha": "fe153b1622e526a6eb0bc7c0331bf44d123bfb58",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -18,7 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments"
        },
        {
            "sha": "ec65fd185373c94b76674a492e0b0b7d460980df",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 40,
            "deletions": 38,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...file_utils import ModelOutput, is_timm_available, requires_backends\n from ...integrations import use_kernel_forward_from_hub\n@@ -511,10 +512,10 @@ def _init_weights(self, module):\n         std = self.config.init_std\n \n         if isinstance(module, MMGroundingDinoLearnedPositionEmbedding):\n-            nn.init.uniform_(module.row_embeddings.weight)\n-            nn.init.uniform_(module.column_embeddings.weight)\n+            init.uniform_(module.row_embeddings.weight)\n+            init.uniform_(module.column_embeddings.weight)\n         elif isinstance(module, MMGroundingDinoMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             default_dtype = torch.get_default_dtype()\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n                 2.0 * math.pi / module.n_heads\n@@ -527,52 +528,53 @@ def _init_weights(self, module):\n             )\n             for i in range(module.n_points):\n                 grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight)\n-            nn.init.constant_(module.value_proj.bias, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight)\n-            nn.init.constant_(module.output_proj.bias, 0.0)\n+\n+            init.copy_(module.sampling_offsets.bias, grid_init.view(-1))\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n+            init.xavier_uniform_(module.value_proj.weight)\n+            init.constant_(module.value_proj.bias, 0.0)\n+            init.xavier_uniform_(module.output_proj.weight)\n+            init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, MMGroundingDinoBiMultiHeadAttention):\n-            nn.init.xavier_uniform_(module.vision_proj.weight)\n-            module.vision_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.text_proj.weight)\n-            module.text_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.values_vision_proj.weight)\n-            module.values_vision_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.values_text_proj.weight)\n-            module.values_text_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.out_vision_proj.weight)\n-            module.out_vision_proj.bias.fill_(0)\n-            nn.init.xavier_uniform_(module.out_text_proj.weight)\n-            module.out_text_proj.bias.fill_(0)\n+            init.xavier_uniform_(module.vision_proj.weight)\n+            init.zeros_(module.vision_proj.bias)\n+            init.xavier_uniform_(module.text_proj.weight)\n+            init.zeros_(module.text_proj.bias)\n+            init.xavier_uniform_(module.values_vision_proj.weight)\n+            init.zeros_(module.values_vision_proj.bias)\n+            init.xavier_uniform_(module.values_text_proj.weight)\n+            init.zeros_(module.values_text_proj.bias)\n+            init.xavier_uniform_(module.out_vision_proj.weight)\n+            init.zeros_(module.out_vision_proj.bias)\n+            init.xavier_uniform_(module.out_text_proj.weight)\n+            init.zeros_(module.out_text_proj.bias)\n         elif isinstance(module, MMGroundingDinoFusionLayer):\n-            module.vision_param.fill_(1e-4)\n-            module.text_param.fill_(1e-4)\n+            init.constant_(module.vision_param, 1e-4)\n+            init.constant_(module.text_param, 1e-4)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, MMGroundingDinoMLPPredictionHead):\n-            nn.init.constant_(module.layers[-1].weight, 0)\n-            nn.init.constant_(module.layers[-1].bias, 0)\n+            init.constant_(module.layers[-1].weight, 0)\n+            init.constant_(module.layers[-1].bias, 0)\n \n         if hasattr(module, \"reference_points\") and not self.config.two_stage:\n-            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n         if hasattr(module, \"level_embed\"):\n-            nn.init.normal_(module.level_embed)\n+            init.normal_(module.level_embed)\n         if isinstance(module, MMGroundingDinoContrastiveEmbedding):\n-            nn.init.constant_(module.bias, -math.log((1 - 0.01) / 0.01))\n+            init.constant_(module.bias, -math.log((1 - 0.01) / 0.01))\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, MMGroundingDinoDecoder):"
        },
        {
            "sha": "91648343e44058685def7c7c821edebb18f83daa",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -17,6 +17,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n@@ -323,7 +324,7 @@ class MMGroundingDinoPreTrainedModel(GroundingDinoPreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, MMGroundingDinoContrastiveEmbedding):\n-            nn.init.constant_(module.bias, -math.log((1 - 0.01) / 0.01))\n+            init.constant_(module.bias, -math.log((1 - 0.01) / 0.01))\n \n \n class MMGroundingDinoConvEncoder(GroundingDinoConvEncoder):"
        },
        {
            "sha": "6874dfecd1d3d5199911de106e69cd0f9625b10f",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 12,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...masking_utils import create_bidirectional_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -549,19 +550,12 @@ class MobileBertPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, (nn.LayerNorm, NoNorm)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n+        if isinstance(module, NoNorm):\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, MobileBertLMPredictionHead):\n-            module.bias.zero_()\n+            init.zeros_(module.bias)\n \n \n @dataclass"
        },
        {
            "sha": "de8fefa142c35812e4190a748ba1efc786afef03",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -132,17 +132,6 @@ class MobileNetV1PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = []\n \n-    @torch.no_grad()\n-    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d]) -> None:\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.BatchNorm2d):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class MobileNetV1Model(MobileNetV1PreTrainedModel):"
        },
        {
            "sha": "5a107d3fcddca50c62393e9f0ec7d729e45b3c68",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -258,17 +258,6 @@ class MobileNetV2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = []\n \n-    @torch.no_grad()\n-    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d]) -> None:\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.BatchNorm2d):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class MobileNetV2Model(MobileNetV2PreTrainedModel):"
        },
        {
            "sha": "a41155c1bd7133ae93faf633232861fa00d72b2e",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -611,12 +612,12 @@ class MobileViTPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "9eefb9eb77e91c10fa65c12f076c814d30403b28",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -578,12 +579,12 @@ class MobileViTV2PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.GroupNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n @auto_docstring"
        },
        {
            "sha": "8069f2bec2ffacee2dec3c37d689d6bf888c1041",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -628,7 +629,7 @@ def _init_weights(self, module: nn.Module):\n             cutoff_factor = 3\n \n         def init_weight(module: nn.Module, std: float):\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n@@ -638,7 +639,7 @@ def init_weight(module: nn.Module, std: float):\n \n             if isinstance(module, nn.Linear):\n                 if module.bias is not None:\n-                    nn.init.zeros_(module.bias)\n+                    init.zeros_(module.bias)\n \n         stds = {\n             \"in\": self.config.initializer_range,\n@@ -670,9 +671,9 @@ def init_weight(module: nn.Module, std: float):\n         ):\n             init_weight(module.classifier, stds[\"final_out\"])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n     def _check_and_adjust_attn_implementation(\n         self, attn_implementation: Optional[str], is_init_check: bool = False"
        },
        {
            "sha": "8783517edb8a951fcaa52e7ce093a008541bd516",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig, layer_type_validation\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n@@ -809,7 +810,7 @@ def _init_weights(self, module: nn.Module):\n             cutoff_factor = 3\n \n         def init_weight(module: nn.Module, std: float):\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n@@ -819,7 +820,7 @@ def init_weight(module: nn.Module, std: float):\n \n             if isinstance(module, nn.Linear):\n                 if module.bias is not None:\n-                    nn.init.zeros_(module.bias)\n+                    init.zeros_(module.bias)\n \n         stds = {\n             \"in\": self.config.initializer_range,\n@@ -851,9 +852,9 @@ def init_weight(module: nn.Module, std: float):\n         ):\n             init_weight(module.classifier, stds[\"final_out\"])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n     def _check_and_adjust_attn_implementation(\n         self, attn_implementation: Optional[str], is_init_check: bool = False"
        },
        {
            "sha": "7564e375716bd01ea30e99358be4bb828bc3ff3b",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -401,7 +402,7 @@ def _init_weights(self, module: nn.Module):\n             cutoff_factor = 3\n \n         def init_weight(module: nn.Module, std: float):\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n@@ -411,7 +412,7 @@ def init_weight(module: nn.Module, std: float):\n \n             if isinstance(module, nn.Linear):\n                 if module.bias is not None:\n-                    nn.init.zeros_(module.bias)\n+                    init.zeros_(module.bias)\n \n         stds = {\n             \"in\": self.config.initializer_range,\n@@ -437,9 +438,9 @@ def init_weight(module: nn.Module, std: float):\n         elif isinstance(module, ModernBertDecoderForCausalLM):\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "24c63f499bb2e239a4e53de3df1c815687153d19",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n@@ -427,7 +428,7 @@ def _init_weights(self, module: nn.Module):\n             cutoff_factor = 3\n \n         def init_weight(module: nn.Module, std: float):\n-            nn.init.trunc_normal_(\n+            init.trunc_normal_(\n                 module.weight,\n                 mean=0.0,\n                 std=std,\n@@ -437,7 +438,7 @@ def init_weight(module: nn.Module, std: float):\n \n             if isinstance(module, nn.Linear):\n                 if module.bias is not None:\n-                    nn.init.zeros_(module.bias)\n+                    init.zeros_(module.bias)\n \n         stds = {\n             \"in\": self.config.initializer_range,\n@@ -463,9 +464,9 @@ def init_weight(module: nn.Module, std: float):\n         elif isinstance(module, ModernBertDecoderForCausalLM):\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n+            init.ones_(module.weight)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n     def _check_and_adjust_attn_implementation(self, attn_implementation, is_init_check):\n         raise AttributeError(\"No need to inherit!\")"
        },
        {
            "sha": "eb379f0bcf1e04706ac7e9e564ba422ec340e16e",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationConfig, GenerationMixin\n@@ -827,20 +828,9 @@ class MoshiPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, MoshiFlexibleLinear):\n-            module.weight.normal_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, MoshiRMSNorm):\n-            module.weight.fill_(1.0)\n+        super()._init_weights(module)\n+        if isinstance(module, MoshiFlexibleLinear):\n+            init.normal_(module.weight)\n \n \n class MoshiDepthDecoder(MoshiPreTrainedModel, GenerationMixin):"
        },
        {
            "sha": "a585a63d72ad53d44f462672bd21e0020d62566b",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN, gelu\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -48,19 +49,9 @@ class MPNetPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, MPNetLMHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, MPNetLMHead):\n+            init.zeros_(module.bias)\n \n \n class MPNetEmbeddings(nn.Module):"
        },
        {
            "sha": "a0dae399bf7337876f341685ccb4e5c8689075eb",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -223,22 +223,6 @@ class MptPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MptBlock\"]\n \n-    @torch.no_grad()\n-    def _init_weights(self, module: nn.Module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, LayerNorm):\n-            if module.bias is not None:\n-                module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class MptModel(MptPreTrainedModel):"
        },
        {
            "sha": "08bebf0c8766f04b991fba8938c9235a4af5ced1",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -792,20 +793,9 @@ class MraPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, MraLMPredictionHead):\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, MraLMPredictionHead):\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "cf4b1189ee95f333ab80706b0ca39a87e584c5a7",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 27,
            "deletions": 26,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -564,55 +565,55 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, MT5LayerNorm):\n-            module.weight.fill_(factor * 1.0)\n+            init.constant_(module.weight, factor * 1.0)\n         elif isinstance(\n             module,\n             (MT5Model, MT5ForConditionalGeneration, MT5EncoderModel, MT5ForQuestionAnswering),\n         ):\n-            module.shared.weight.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.shared.weight, mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n-                module.lm_head.weight.normal_(mean=0.0, std=factor * 1.0)\n+                init.normal_(module.lm_head.weight, mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"qa_outputs\"):\n-                module.qa_outputs.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n-                module.qa_outputs.bias.zero_()\n+                init.normal_(module.qa_outputs.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+                init.zeros_(module.qa_outputs.bias)\n         elif isinstance(module, MT5ForTokenClassification):\n             if hasattr(module, \"classifier\"):\n-                module.classifier.weight.normal_(mean=0.0, std=factor * 1.0)\n-                module.classifier.bias.zero_()\n+                init.normal_(module.classifier.weight, mean=0.0, std=factor * 1.0)\n+                init.zeros_(module.classifier.bias)\n         elif isinstance(module, MT5ClassificationHead):\n-            module.dense.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.dense.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.dense, \"bias\") and module.dense.bias is not None:\n-                module.dense.bias.zero_()\n-            module.out_proj.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+                init.zeros_(module.dense.bias)\n+            init.normal_(module.out_proj.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.out_proj, \"bias\") and module.out_proj.bias is not None:\n-                module.out_proj.bias.zero_()\n+                init.zeros_(module.out_proj.bias)\n         elif isinstance(module, MT5DenseActDense):\n-            module.wi.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                module.wi.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, MT5DenseGatedActDense):\n-            module.wi_0.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi_0.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n-                module.wi_0.bias.zero_()\n-            module.wi_1.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+                init.zeros_(module.wi_0.bias)\n+            init.normal_(module.wi_1.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n-                module.wi_1.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi_1.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, MT5Attention):\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_kv\n             n_heads = self.config.num_heads\n-            module.q.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.k.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.v.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.o.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.q.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.k.weight, mean=0.0, std=factor * (d_model**-0.5))\n+            init.normal_(module.v.weight, mean=0.0, std=factor * (d_model**-0.5))\n+            init.normal_(module.o.weight, mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n             if module.has_relative_attention_bias:\n-                module.relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n+                init.normal_(module.relative_attention_bias.weight, mean=0.0, std=factor * ((d_model) ** -0.5))\n \n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id"
        },
        {
            "sha": "35969de777ab80a6a4ad109f56a482b4989af0a2",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import (\n@@ -420,16 +421,17 @@ class MusicgenPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n class MusicgenDecoder(MusicgenPreTrainedModel):"
        },
        {
            "sha": "bd8f29a86a5d684eae25a732b1ef1835557a54a2",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import (\n@@ -391,16 +392,17 @@ class MusicgenMelodyPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         std = self.config.initializer_factor\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n \n # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody\n@@ -1312,9 +1314,9 @@ def _init_weights(self, module):\n         # Projection layers still need to be initialized.\n         std = self.decoder.config.initializer_factor\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n     def get_text_encoder(self):\n         return self.text_encoder"
        },
        {
            "sha": "28fbf6960545daa1a4d99972a4d71c917360bb48",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -469,18 +469,6 @@ class MvpPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n     @property\n     def dummy_inputs(self):\n         pad_token = self.config.pad_token_id"
        },
        {
            "sha": "af1d14ee2da08c268c8f6f2eb40da760d349eeb8",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn.functional as F\n from torch import Size, Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n@@ -612,18 +613,10 @@ class NemotronPreTrainedModel(PreTrainedModel):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, NemotronLayerNorm1P):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, NemotronLayerNorm1P):\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "7f0de0ce824c6dfd536a8783ace53ffc09c9c52c",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -665,22 +665,6 @@ class NllbMoePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n-    @torch.no_grad()\n-    def _init_weights(self, module: nn.Module):\n-        \"\"\"Initialize the weights\"\"\"\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n \n class NllbMoeEncoder(NllbMoePreTrainedModel):\n     _can_record_outputs = {"
        },
        {
            "sha": "92e613d9d2fa695bafcef1aaa5b669696db3dec5",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -413,21 +413,6 @@ class NystromformerPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"nystromformer\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @auto_docstring\n class NystromformerModel(NystromformerPreTrainedModel):"
        },
        {
            "sha": "35804ec9c9be9e8b083fee053d85ae2d845a0a8a",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 17,
            "deletions": 16,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch.nn.functional as F\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2CLS, ACT2FN\n from ...file_utils import (\n     ModelOutput,\n@@ -991,36 +992,36 @@ class OmDetTurboPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         def linear_init_(module_to_init):\n             bound = 1 / math.sqrt(module_to_init.weight.shape[0])\n-            nn.init.uniform_(module_to_init.weight, -bound, bound)\n+            init.uniform_(module_to_init.weight, -bound, bound)\n             if hasattr(module_to_init, \"bias\") and module_to_init.bias is not None:\n-                nn.init.uniform_(module_to_init.bias, -bound, bound)\n+                init.uniform_(module_to_init.bias, -bound, bound)\n \n         if isinstance(module, OmDetTurboEncoderLayer):\n             linear_init_(module.fc1)\n             linear_init_(module.fc2)\n         elif isinstance(module, OmDetTurboDecoder):\n-            nn.init.constant_(module.encoder_bbox_head.layers[-1].weight, 0.0)\n-            nn.init.constant_(module.encoder_bbox_head.layers[-1].bias, 0.0)\n+            init.constant_(module.encoder_bbox_head.layers[-1].weight, 0.0)\n+            init.constant_(module.encoder_bbox_head.layers[-1].bias, 0.0)\n             for mlp in module.decoder_bbox_head:\n-                nn.init.constant_(mlp.layers[-1].weight, 0.0)\n-                nn.init.constant_(mlp.layers[-1].bias, 0.0)\n+                init.constant_(mlp.layers[-1].weight, 0.0)\n+                init.constant_(mlp.layers[-1].bias, 0.0)\n             linear_init_(module.encoder_vision_features[0])\n-            nn.init.xavier_uniform_(module.encoder_vision_features[0].weight)\n+            init.xavier_uniform_(module.encoder_vision_features[0].weight)\n             if module.learn_initial_query:\n-                nn.init.xavier_uniform_(module.tgt_embed.weight)\n-            nn.init.xavier_uniform_(module.query_position_head.layers[0].weight)\n-            nn.init.xavier_uniform_(module.query_position_head.layers[1].weight)\n+                init.xavier_uniform_(module.tgt_embed.weight)\n+            init.xavier_uniform_(module.query_position_head.layers[0].weight)\n+            init.xavier_uniform_(module.query_position_head.layers[1].weight)\n             for layer in module.channel_projection_layers:\n-                nn.init.xavier_uniform_(layer[0].weight)\n+                init.xavier_uniform_(layer[0].weight)\n         elif isinstance(module, OmDetTurboLanguageBackbone):\n-            nn.init.normal_(module.text_projection, std=self.config.text_projection_in_dim**-0.5)\n+            init.normal_(module.text_projection, std=self.config.text_projection_in_dim**-0.5)\n         elif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n+            init.normal_(module.weight, mean=0.0, std=self.config.init_std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, OmDetTurboDecoder):"
        },
        {
            "sha": "86d5e8d670ce0f4f69cec0e607c4ce2a3efd3c1b",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 37,
            "deletions": 35,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n@@ -2774,13 +2775,13 @@ def _init_weights(self, module: nn.Module):\n             if module.input_projections is not None:\n                 for input_projection in module.input_projections:\n                     if not isinstance(input_projection, nn.Sequential):\n-                        nn.init.xavier_uniform_(input_projection.weight, gain=xavier_std)\n-                        nn.init.constant_(input_projection.bias, 0)\n+                        init.xavier_uniform_(input_projection.weight, gain=xavier_std)\n+                        init.constant_(input_projection.bias, 0)\n         elif isinstance(module, OneFormerTransformerDecoder):\n-            nn.init.xavier_uniform_(module.query_input_projection.weight, gain=xavier_std)\n-            nn.init.constant_(module.query_input_projection.bias, 0)\n+            init.xavier_uniform_(module.query_input_projection.weight, gain=xavier_std)\n+            init.constant_(module.query_input_projection.bias, 0)\n         elif isinstance(module, OneFormerPixelDecoderEncoderMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight, 0.0)\n+            init.constant_(module.sampling_offsets.weight, 0.0)\n             thetas = torch.arange(module.n_heads, dtype=torch.int64).float() * (2.0 * math.pi / module.n_heads)\n             grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n             grid_init = (\n@@ -2790,56 +2791,57 @@ def _init_weights(self, module: nn.Module):\n             )\n             for i in range(module.n_points):\n                 grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight, 0.0)\n-            nn.init.constant_(module.attention_weights.bias, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight)\n-            nn.init.constant_(module.value_proj.bias, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight)\n-            nn.init.constant_(module.output_proj.bias, 0.0)\n+\n+            init.copy_(module.sampling_offsets.bias, grid_init.view(-1))\n+            init.constant_(module.attention_weights.weight, 0.0)\n+            init.constant_(module.attention_weights.bias, 0.0)\n+            init.xavier_uniform_(module.value_proj.weight)\n+            init.constant_(module.value_proj.bias, 0.0)\n+            init.xavier_uniform_(module.output_proj.weight)\n+            init.constant_(module.output_proj.bias, 0.0)\n         elif isinstance(module, OneFormerPixelDecoder):\n-            nn.init.normal_(module.level_embed, std=0)\n+            init.normal_(module.level_embed, std=0)\n         elif isinstance(module, (OneFormerTransformerDecoderLayer, OneFormerTransformerDecoderQueryTransformer)):\n             for p in module.parameters():\n                 if p.dim() > 1:\n-                    nn.init.xavier_uniform_(p, gain=xavier_std)\n+                    init.xavier_uniform_(p, gain=xavier_std)\n         elif isinstance(module, OneFormerTextTransformer):\n             proj_std = (module.width**-0.5) * ((2 * module.num_layers) ** -0.5)\n             attn_std = module.width**-0.5\n             fc_std = (2 * module.width) ** -0.5\n             for layer in module.layers:\n-                nn.init.normal_(layer.self_attn.in_proj_weight, std=attn_std)\n-                nn.init.normal_(layer.self_attn.out_proj.weight, std=proj_std)\n-                nn.init.normal_(layer.mlp.fc1.weight, std=fc_std)\n-                nn.init.normal_(layer.mlp.fc2.weight, std=proj_std)\n+                init.normal_(layer.self_attn.in_proj_weight, std=attn_std)\n+                init.normal_(layer.self_attn.out_proj.weight, std=proj_std)\n+                init.normal_(layer.mlp.fc1.weight, std=fc_std)\n+                init.normal_(layer.mlp.fc2.weight, std=proj_std)\n         elif isinstance(module, OneFormerTextEncoder):\n-            nn.init.normal_(module.token_embedding.weight, std=0.02)\n-            nn.init.normal_(module.positional_embedding, std=0.01)\n+            init.normal_(module.token_embedding.weight, std=0.02)\n+            init.normal_(module.positional_embedding, std=0.01)\n         if hasattr(module, \"reference_points\"):\n-            nn.init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n-            nn.init.constant_(module.reference_points.bias, 0.0)\n+            init.xavier_uniform_(module.reference_points.weight, gain=1.0)\n+            init.constant_(module.reference_points.bias, 0.0)\n         elif isinstance(module, OneFormerMLPPredictionHead):\n             for submodule in module.modules():\n                 if isinstance(submodule, nn.Linear):\n-                    nn.init.xavier_uniform_(submodule.weight, gain=xavier_std)\n-                    nn.init.constant_(submodule.bias, 0)\n+                    init.xavier_uniform_(submodule.weight, gain=xavier_std)\n+                    init.constant_(submodule.bias, 0)\n         elif isinstance(module, nn.MultiheadAttention):\n-            module.in_proj_weight.normal_(mean=0.0, std=std)\n-            module.in_proj_bias.zero_()\n+            init.normal_(module.in_proj_weight, mean=0.0, std=std)\n+            init.zeros_(module.in_proj_bias)\n         elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+            init.ones_(module.weight)\n+            init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, OneFormerLoss):\n-            module.logit_scale.fill_(np.log(1 / self.config.contrastive_temperature))\n+            init.constant_(module.logit_scale, np.log(1 / self.config.contrastive_temperature))\n \n \n @auto_docstring"
        },
        {
            "sha": "4c15f2719be30a71df6509120697489a5a135f1b",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -259,21 +259,6 @@ class OpenAIGPTPreTrainedModel(PreTrainedModel):\n     config: OpenAIGPTConfig\n     base_model_prefix = \"transformer\"\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear, Conv1D)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "025ea7d2cd8016828531626df470bff0e03c0dc4",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -301,24 +301,8 @@ class OPTPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n \n class OPTDecoder(OPTPreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "16cfd68e49bb119444e8d27ae19f0272cc32b3f7",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 18,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -572,41 +573,41 @@ def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, Owlv2TextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, Owlv2VisionEmbeddings):\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, Owlv2Attention):\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, Owlv2MLP):\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, Owlv2Model):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * factor,\n             )\n-            module.logit_scale.fill_(self.config.logit_scale_init_value)\n+            init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=factor)\n+            init.normal_(module.weight, mean=0.0, std=factor)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTEncoder with OwlViT->Owlv2"
        },
        {
            "sha": "bd8b23ca38eb537250f042e3ec0b835282a364cc",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import Tensor, nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -559,41 +560,41 @@ def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor\n         if isinstance(module, OwlViTTextEmbeddings):\n-            module.token_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n-            module.position_embedding.weight.normal_(mean=0.0, std=factor * 0.02)\n+            init.normal_(module.token_embedding.weight, mean=0.0, std=factor * 0.02)\n+            init.normal_(module.position_embedding.weight, mean=0.0, std=factor * 0.02)\n         elif isinstance(module, OwlViTVisionEmbeddings):\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n         elif isinstance(module, OwlViTAttention):\n             in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+            init.normal_(module.q_proj.weight, std=in_proj_std)\n+            init.normal_(module.k_proj.weight, std=in_proj_std)\n+            init.normal_(module.v_proj.weight, std=in_proj_std)\n+            init.normal_(module.out_proj.weight, std=out_proj_std)\n         elif isinstance(module, OwlViTMLP):\n             in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n             fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+            init.normal_(module.fc1.weight, std=fc_std)\n+            init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, OwlViTModel):\n-            nn.init.normal_(\n+            init.normal_(\n                 module.text_projection.weight,\n                 std=module.text_embed_dim**-0.5 * factor,\n             )\n-            nn.init.normal_(\n+            init.normal_(\n                 module.visual_projection.weight,\n                 std=module.vision_embed_dim**-0.5 * factor,\n             )\n-            module.logit_scale.fill_(self.config.logit_scale_init_value)\n+            init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n         if isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=factor)\n+            init.normal_(module.weight, mean=0.0, std=factor)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n class OwlViTEncoder(nn.Module):\n@@ -1198,6 +1199,8 @@ def __init__(self, config: OwlViTConfig):\n         self.num_patches_width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n         self.box_bias = self.compute_box_bias(self.num_patches_height, self.num_patches_width)\n \n+        self.post_init()\n+\n     @staticmethod\n     def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width: int) -> torch.Tensor:\n         # Create grid coordinates using torch"
        },
        {
            "sha": "509906fa9e410efcd36387a7504d537a3a536f20",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -219,24 +219,12 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-\n     _can_compile_fullgraph = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        # important: this ported version of PaliGemmaisn't meant for training from scratch - only\n-        # inference and fine-tuning\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "ef8841a51b3f7edbf9558bc448e879a1e3ad6a62",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n@@ -467,8 +468,8 @@ def _init_weights(self, module):\n \n         if isinstance(module, ParakeetEncoderAttention):\n             # Initialize positional bias parameters\n-            module.bias_u.normal_(mean=0.0, std=std)\n-            module.bias_v.normal_(mean=0.0, std=std)\n+            init.normal_(module.bias_u, mean=0.0, std=std)\n+            init.normal_(module.bias_v, mean=0.0, std=std)\n \n     def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n         encoder_config = self.config.encoder_config if isinstance(self.config, ParakeetCTCConfig) else self.config"
        },
        {
            "sha": "e1d552cbf43d1c55762c53bf68102f1db3c65301",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n@@ -343,8 +344,8 @@ def _init_weights(self, module):\n \n         if isinstance(module, ParakeetEncoderAttention):\n             # Initialize positional bias parameters\n-            module.bias_u.normal_(mean=0.0, std=std)\n-            module.bias_v.normal_(mean=0.0, std=std)\n+            init.normal_(module.bias_u, mean=0.0, std=std)\n+            init.normal_(module.bias_v, mean=0.0, std=std)\n \n     def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n         encoder_config = self.config.encoder_config if isinstance(self.config, ParakeetCTCConfig) else self.config"
        },
        {
            "sha": "410ef5abb7f4ee53700de0c2d704221616cb4e78",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n from transformers.modeling_utils import PreTrainedModel\n from transformers.utils import ModelOutput\n \n+from ... import initialization as init\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -691,17 +692,17 @@ def _init_weights(self, module):\n         if isinstance(module, PatchTSMixerPositionalEncoding):\n             # initialize positional encoding\n             if self.config.positional_encoding_type == \"random\":\n-                nn.init.normal_(module.position_enc, mean=0.0, std=0.1)\n+                init.normal_(module.position_enc, mean=0.0, std=0.1)\n         elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, PatchTSMixerBatchNorm):\n-            module.batchnorm.bias.zero_()\n-            module.batchnorm.weight.fill_(1.0)\n+            init.zeros_(module.batchnorm.bias)\n+            init.ones_(module.batchnorm.weight)\n         elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n+            init.normal_(module.weight, mean=0.0, std=self.config.init_std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n class PatchTSMixerPretrainHead(nn.Module):"
        },
        {
            "sha": "5317476b1a4273ab93010fe2ac96ab57a569a793",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2CLS\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput\n@@ -567,20 +568,20 @@ def _init_weights(self, module: nn.Module):\n             ) // self.config.patch_stride + 1\n             # initialize cls_token\n             if self.config.use_cls_token:\n-                nn.init.normal_(module.cls_token, std=0.02)\n+                init.normal_(module.cls_token, std=0.02)\n                 num_patches += 1\n             # initialize positional encoding\n-            module.position_enc = module._init_pe(self.config, num_patches)\n+            init.copy_(module.position_enc, module._init_pe(self.config, num_patches))\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, PatchTSTBatchNorm):\n-            module.batchnorm.bias.zero_()\n-            module.batchnorm.weight.fill_(1.0)\n+            init.zeros_(module.batchnorm.bias)\n+            init.ones_(module.batchnorm.weight)\n         elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n+            init.normal_(module.weight, mean=0.0, std=self.config.init_std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, (PatchTSTEncoder)):"
        },
        {
            "sha": "34205b2e597205dde83b2a5b519a1d4051481181",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 9,
            "deletions": 20,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -73,9 +74,9 @@ class PegasusSinusoidalPositionalEmbedding(nn.Embedding):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n-        super().__init__(num_positions, embedding_dim)\n+        super().__init__(num_positions, embedding_dim, _freeze=True)\n \n-    def _init_weight(self):\n+    def create_weight(self):\n         \"\"\"\n         Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n         the 2nd half of the vector. [dim // 2:]\n@@ -88,7 +89,7 @@ def _init_weight(self):\n         sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n         out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n         out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n-        self.weight = nn.Parameter(out, requires_grad=False)\n+        return out\n \n     @torch.no_grad()\n     def forward(\n@@ -435,25 +436,13 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n     _can_compile_fullgraph = True\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, PegasusSinusoidalPositionalEmbedding):\n-            module._init_weight()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n+        super()._init_weights(module)\n+        if isinstance(module, PegasusSinusoidalPositionalEmbedding):\n+            init.copy_(module.weight, module.create_weight())\n \n \n class PegasusEncoder(PegasusPreTrainedModel):\n@@ -512,7 +501,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n             self.config.d_model,\n             self.padding_idx,\n         )\n-        self.embed_positions._init_weight()\n+        init.copy_(self.embed_positions.weight, self.embed_positions.create_weight())\n         self.embed_positions.to(self.device)\n \n     def get_position_embeddings(self) -> nn.Embedding:\n@@ -684,7 +673,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n             self.config.d_model,\n             self.padding_idx,\n         )\n-        self.embed_positions._init_weight()\n+        init.copy_(self.embed_positions.weight, self.embed_positions.create_weight())\n         self.embed_positions.to(self.device)\n \n     def get_position_embeddings(self) -> nn.Embedding:"
        },
        {
            "sha": "53e44ef414d08ee21fa450464a0040b6812523c1",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -744,22 +744,8 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     # Flaky logits\n     _supports_sdpa = False\n     _supports_flex_attn = True\n-\n     _can_compile_fullgraph = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.init_std\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n \n class PegasusXEncoder(PegasusXPreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "1b22c7ca1802caea4aa9f6f2287694eabde66c56",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n@@ -535,23 +536,24 @@ class PerceiverPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif hasattr(module, \"latents\"):\n-            module.latents.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.latents, mean=0.0, std=self.config.initializer_range)\n         elif hasattr(module, \"position_embeddings\") and isinstance(module, PerceiverTrainablePositionEncoding):\n-            module.position_embeddings.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.position_embeddings, mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, nn.ParameterDict):\n             for modality in module:\n-                module[modality].normal_(mean=0.0, std=self.config.initializer_range)\n+                init.normal_(module[modality], mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n @auto_docstring("
        },
        {
            "sha": "4b09a2dd75bf45481e772e928e6581ae5036831c",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -423,27 +423,11 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PersimmonDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-\n     _can_compile_fullgraph = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_attention_backend = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-\n \n @auto_docstring\n class PersimmonModel(PersimmonPreTrainedModel):"
        },
        {
            "sha": "eab15068d25235051ac3e4ab0de115ec5afdc671",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 29,
            "deletions": 94,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,7 +19,6 @@\n # limitations under the License.\n \n import math\n-import warnings\n from collections.abc import Callable\n from typing import Optional, Union\n \n@@ -29,6 +28,7 @@\n from torch import nn\n from torch.nn.init import _calculate_fan_in_and_fan_out\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -209,69 +209,7 @@ def forward(\n         return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n-def _trunc_normal_(tensor, mean, std, a, b):\n-    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n-    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n-    def norm_cdf(x):\n-        # Computes standard normal cumulative distribution function\n-        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n-\n-    if (mean < a - 2 * std) or (mean > b + 2 * std):\n-        warnings.warn(\n-            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n-            \"The distribution of values may be incorrect.\",\n-            stacklevel=2,\n-        )\n-\n-    # Values are generated by using a truncated uniform distribution and\n-    # then using the inverse CDF for the normal distribution.\n-    # Get upper and lower cdf values\n-    l = norm_cdf((a - mean) / std)\n-    u = norm_cdf((b - mean) / std)\n-\n-    # Uniformly fill tensor with values from [l, u], then translate to\n-    # [2l-1, 2u-1].\n-    tensor.uniform_(2 * l - 1, 2 * u - 1)\n-\n-    # Use inverse cdf transform for normal distribution to get truncated\n-    # standard normal\n-    tensor.erfinv_()\n-\n-    # Transform to proper mean, std\n-    tensor.mul_(std * math.sqrt(2.0))\n-    tensor.add_(mean)\n-\n-    # Clamp to ensure it's in the proper range\n-    tensor.clamp_(min=a, max=b)\n-\n-\n-def trunc_normal_tf_(\n-    tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0\n-) -> torch.Tensor:\n-    \"\"\"Fills the input Tensor with values drawn from a truncated\n-    normal distribution. The values are effectively drawn from the\n-    normal distribution :math:`\\\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n-    with values outside :math:`[a, b]` redrawn until they are within\n-    the bounds. The method used for generating the random values works\n-    best when :math:`a \\\\leq \\text{mean} \\\\leq b`.\n-\n-    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the\n-    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0\n-    and the result is subsequently scaled and shifted by the mean and std args.\n-\n-    Args:\n-        tensor: an n-dimensional `torch.Tensor`\n-        mean: the mean of the normal distribution\n-        std: the standard deviation of the normal distribution\n-        a: the minimum cutoff value\n-        b: the maximum cutoff value\n-    \"\"\"\n-    with torch.no_grad():\n-        _trunc_normal_(tensor, 0, 1.0, a, b)\n-        tensor.mul_(std).add_(mean)\n-\n-\n-def variance_scaling_(tensor, scale=1.0, mode=\"fan_in\", distribution=\"normal\"):\n+def variance_scaling_(tensor, mode=\"fan_in\", distribution=\"normal\"):\n     fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n     if mode == \"fan_in\":\n         denom = fan_in\n@@ -280,18 +218,15 @@ def variance_scaling_(tensor, scale=1.0, mode=\"fan_in\", distribution=\"normal\"):\n     elif mode == \"fan_avg\":\n         denom = (fan_in + fan_out) / 2\n \n-    variance = scale / denom\n+    variance = 1.0 / denom\n \n     if distribution == \"truncated_normal\":\n-        # constant is stddev of standard normal truncated to (-2, 2)\n-        trunc_normal_tf_(tensor, std=math.sqrt(variance) / 0.87962566103423978)\n+        init.trunc_normal_(tensor, std=math.sqrt(variance) / 0.87962566103423978)\n     elif distribution == \"normal\":\n-        with torch.no_grad():\n-            tensor.normal_(std=math.sqrt(variance))\n+        init.normal_(tensor, std=math.sqrt(variance))\n     elif distribution == \"uniform\":\n         bound = math.sqrt(3 * variance)\n-        with torch.no_grad():\n-            tensor.uniform_(-bound, bound)\n+        init.uniform_(tensor, -bound, bound)\n     else:\n         raise ValueError(f\"invalid distribution {distribution}\")\n \n@@ -331,34 +266,34 @@ def _init_weights(self, module):\n                 if isinstance(self.config, Phi4MultimodalVisionConfig)\n                 else self.config.hidden_size\n             )\n-            nn.init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+            init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n         elif isinstance(module, nn.Embedding):\n             default_flax_embed_init(module.weight)\n         elif isinstance(module, Phi4MultimodalVisionAttention):\n-            nn.init.normal_(module.q_proj.weight)\n-            nn.init.normal_(module.k_proj.weight)\n-            nn.init.normal_(module.v_proj.weight)\n-            nn.init.normal_(module.out_proj.weight)\n-            nn.init.zeros_(module.q_proj.bias)\n-            nn.init.zeros_(module.k_proj.bias)\n-            nn.init.zeros_(module.v_proj.bias)\n-            nn.init.zeros_(module.out_proj.bias)\n+            init.normal_(module.q_proj.weight)\n+            init.normal_(module.k_proj.weight)\n+            init.normal_(module.v_proj.weight)\n+            init.normal_(module.out_proj.weight)\n+            init.zeros_(module.q_proj.bias)\n+            init.zeros_(module.k_proj.bias)\n+            init.zeros_(module.v_proj.bias)\n+            init.zeros_(module.out_proj.bias)\n         elif isinstance(module, Phi4MultimodalVisionMLP):\n-            nn.init.normal_(module.fc1.weight)\n-            nn.init.normal_(module.fc2.weight)\n-            nn.init.normal_(module.fc1.bias, std=1e-6)\n-            nn.init.normal_(module.fc2.bias, std=1e-6)\n+            init.normal_(module.fc1.weight)\n+            init.normal_(module.fc2.weight)\n+            init.normal_(module.fc1.bias, std=1e-6)\n+            init.normal_(module.fc2.bias, std=1e-6)\n         elif isinstance(module, Phi4MultimodalVisionMultiheadAttentionPoolingHead):\n-            nn.init.normal_(module.probe)\n-            nn.init.normal_(module.attention.in_proj_weight)\n-            nn.init.zeros_(module.attention.in_proj_bias)\n+            init.normal_(module.probe)\n+            init.normal_(module.attention.in_proj_weight)\n+            init.zeros_(module.attention.in_proj_bias)\n         elif isinstance(module, (nn.Linear, nn.Conv2d)):\n             lecun_normal_(module.weight)\n             if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n class Phi4MultimodalVisionEmbeddings(nn.Module):\n@@ -944,8 +879,8 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n-            module.b1.zero_()\n-            module.b2.zero_()\n+            init.zeros_(module.b1)\n+            init.zeros_(module.b2)\n \n \n def unfold_tensor(tensor, max_seq_len):\n@@ -1503,8 +1438,8 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Phi4MultimodalImageEmbedding):\n-            module.global_img_feature_extensor.zero_()\n-            module.sub_img_feature_extensor.zero_()\n+            init.zeros_(module.global_img_feature_extensor)\n+            init.zeros_(module.sub_img_feature_extensor)\n \n \n class Phi4MultimodalRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "58cb33d62320dabfd2d485307314b618c6495ce3",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 24,
            "deletions": 23,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PreTrainedConfig\n@@ -555,34 +556,34 @@ def _init_weights(self, module):\n                 if isinstance(self.config, Phi4MultimodalVisionConfig)\n                 else self.config.hidden_size\n             )\n-            nn.init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+            init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n         elif isinstance(module, nn.Embedding):\n             default_flax_embed_init(module.weight)\n         elif isinstance(module, Phi4MultimodalVisionAttention):\n-            nn.init.normal_(module.q_proj.weight)\n-            nn.init.normal_(module.k_proj.weight)\n-            nn.init.normal_(module.v_proj.weight)\n-            nn.init.normal_(module.out_proj.weight)\n-            nn.init.zeros_(module.q_proj.bias)\n-            nn.init.zeros_(module.k_proj.bias)\n-            nn.init.zeros_(module.v_proj.bias)\n-            nn.init.zeros_(module.out_proj.bias)\n+            init.normal_(module.q_proj.weight)\n+            init.normal_(module.k_proj.weight)\n+            init.normal_(module.v_proj.weight)\n+            init.normal_(module.out_proj.weight)\n+            init.zeros_(module.q_proj.bias)\n+            init.zeros_(module.k_proj.bias)\n+            init.zeros_(module.v_proj.bias)\n+            init.zeros_(module.out_proj.bias)\n         elif isinstance(module, Phi4MultimodalVisionMLP):\n-            nn.init.normal_(module.fc1.weight)\n-            nn.init.normal_(module.fc2.weight)\n-            nn.init.normal_(module.fc1.bias, std=1e-6)\n-            nn.init.normal_(module.fc2.bias, std=1e-6)\n+            init.normal_(module.fc1.weight)\n+            init.normal_(module.fc2.weight)\n+            init.normal_(module.fc1.bias, std=1e-6)\n+            init.normal_(module.fc2.bias, std=1e-6)\n         elif isinstance(module, Phi4MultimodalVisionMultiheadAttentionPoolingHead):\n-            nn.init.normal_(module.probe)\n-            nn.init.normal_(module.attention.in_proj_weight)\n-            nn.init.zeros_(module.attention.in_proj_bias)\n+            init.normal_(module.probe)\n+            init.normal_(module.attention.in_proj_weight)\n+            init.zeros_(module.attention.in_proj_bias)\n         elif isinstance(module, (nn.Linear, nn.Conv2d)):\n             lecun_normal_(module.weight)\n             if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n \n \n class Phi4MultimodalVisionEmbeddings(SiglipVisionEmbeddings):\n@@ -1124,8 +1125,8 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Phi4MultimodalAudioGluPointWiseConv):\n-            module.b1.zero_()\n-            module.b2.zero_()\n+            init.zeros_(module.b1)\n+            init.zeros_(module.b2)\n \n \n class Phi4MultimodalAudioModel(Phi4MultimodalAudioPreTrainedModel):\n@@ -1447,8 +1448,8 @@ class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Phi4MultimodalImageEmbedding):\n-            module.global_img_feature_extensor.zero_()\n-            module.sub_img_feature_extensor.zero_()\n+            init.zeros_(module.global_img_feature_extensor)\n+            init.zeros_(module.sub_img_feature_extensor)\n \n \n class Phi4MultimodalModel(Phi3Model):"
        },
        {
            "sha": "12e41214094dfa554c1135e4bdcb71012c2f5d08",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -628,10 +629,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, PhimoeExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, PhimoeTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "8a58a353617fe8ffd698b3f6a19bc47f1ded667c",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 25,
            "deletions": 28,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -355,7 +356,7 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, Pix2StructLayerNorm):\n-            module.weight.fill_(factor * 1.0)\n+            init.constant_(module.weight, factor * 1.0)\n         elif isinstance(module, Pix2StructTextDenseGatedActDense):\n             hidden_size = (\n                 self.config.text_config.hidden_size\n@@ -364,15 +365,15 @@ def _init_weights(self, module):\n             )\n             d_ff = self.config.text_config.d_ff if isinstance(self.config, Pix2StructConfig) else self.config.d_ff\n \n-            module.wi_0.weight.normal_(mean=0.0, std=factor * ((hidden_size) ** -0.5))\n+            init.normal_(module.wi_0.weight, mean=0.0, std=factor * ((hidden_size) ** -0.5))\n             if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n-                module.wi_0.bias.zero_()\n-            module.wi_1.weight.normal_(mean=0.0, std=factor * ((hidden_size) ** -0.5))\n+                init.zeros_(module.wi_0.bias)\n+            init.normal_(module.wi_1.weight, mean=0.0, std=factor * ((hidden_size) ** -0.5))\n             if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n-                module.wi_1.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((d_ff) ** -0.5))\n+                init.zeros_(module.wi_1.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, Pix2StructTextAttention):\n             hidden_size = (\n                 self.config.text_config.hidden_size\n@@ -388,47 +389,43 @@ def _init_weights(self, module):\n                 else self.config.num_heads\n             )\n \n-            module.query.weight.normal_(mean=0.0, std=factor * ((hidden_size * key_value_proj_dim) ** -0.5))\n-            module.key.weight.normal_(mean=0.0, std=factor * (hidden_size**-0.5))\n-            module.value.weight.normal_(mean=0.0, std=factor * (hidden_size**-0.5))\n-            module.output.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.query.weight, mean=0.0, std=factor * ((hidden_size * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.key.weight, mean=0.0, std=factor * (hidden_size**-0.5))\n+            init.normal_(module.value.weight, mean=0.0, std=factor * (hidden_size**-0.5))\n+            init.normal_(module.output.weight, mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n             if module.has_relative_attention_bias:\n-                module.relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((hidden_size) ** -0.5))\n+                init.normal_(module.relative_attention_bias.weight, mean=0.0, std=factor * ((hidden_size) ** -0.5))\n         elif isinstance(module, nn.Embedding):\n             hidden_size = (\n                 self.config.text_config.hidden_size\n                 if isinstance(self.config, Pix2StructConfig)\n                 else self.config.hidden_size\n             )\n \n-            module.weight.normal_(mean=0.0, std=factor * ((hidden_size) ** -0.5))\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=factor * ((hidden_size) ** -0.5))\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n         elif isinstance(module, Pix2StructTextModel):\n             hidden_size = (\n                 self.config.text_config.hidden_size\n                 if isinstance(self.config, Pix2StructConfig)\n                 else self.config.hidden_size\n             )\n \n-            module.lm_head.weight.normal_(mean=0.0, std=factor * ((hidden_size) ** -0.5))\n+            init.normal_(module.lm_head.weight, mean=0.0, std=factor * ((hidden_size) ** -0.5))\n         elif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(\n-                nn.init.trunc_normal_(module.weight.to(torch.float32), mean=0.0, std=self.config.initializer_range).to(\n-                    module.weight.dtype\n-                )\n-            )\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, Pix2StructLayerNorm):\n             if module.weight is not None:\n-                module.weight.fill_(1.0)\n+                init.ones_(module.weight)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n     # Copied from transformers.models.t5.modeling_t5.T5PreTrainedModel._shift_right with T5->Pix2Struct\n     def _shift_right(self, input_ids):"
        },
        {
            "sha": "3e6d468a8184e0276ffc525f0401c96c15511c25",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -441,16 +441,6 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, PixtralRMSNorm):\n-            module.weight.fill_(1.0)\n-\n \n def generate_block_attention_mask(patch_embeds_list, tensor):\n     dtype = tensor.dtype"
        },
        {
            "sha": "e1b9f375c5429b5f2b6403c4044203ff874fca81",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -841,7 +841,7 @@ def __init__(self, config: PLBartConfig):\n         self.encoder = PLBartEncoder(config)\n         self.decoder = PLBartDecoder(config)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.shared\n@@ -970,7 +970,7 @@ def __init__(self, config: PLBartConfig):\n         self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n         self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_encoder(self):\n         return self.model.get_encoder()"
        },
        {
            "sha": "796f63d5463291b70bb7a650b5e77b90db08f832",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -82,7 +82,7 @@ def __init__(self, config: PLBartConfig):\n         self.encoder = PLBartEncoder(config)\n         self.decoder = PLBartDecoder(config)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_input_embeddings(self):\n         return self.shared\n@@ -211,7 +211,7 @@ def __init__(self, config: PLBartConfig):\n         self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n         self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n \n-        self.init_weights()\n+        self.post_init()\n \n     def get_encoder(self):\n         return self.model.get_encoder()"
        },
        {
            "sha": "21f01878ef87d7c9c305457e9b91f1f72d2e1cda",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -20,6 +20,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n from ...modeling_utils import PreTrainedModel\n@@ -248,17 +249,11 @@ class PoolFormerPreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.GroupNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n-        elif isinstance(module, PoolFormerLayer):\n+        super()._init_weights(module)\n+        if isinstance(module, PoolFormerLayer):\n             if hasattr(module, \"layer_scale_1\"):\n-                module.layer_scale_1.fill_(self.config.layer_scale_init_value)\n-                module.layer_scale_2.fill_(self.config.layer_scale_init_value)\n+                init.constant_(module.layer_scale_1, self.config.layer_scale_init_value)\n+                init.constant_(module.layer_scale_2, self.config.layer_scale_init_value)\n \n \n @auto_docstring"
        },
        {
            "sha": "493b9d4776ecdc91d66b6a02bd1cefaabdfa74e1",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -24,6 +24,7 @@\n \n from transformers.generation import GenerationConfig\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -549,40 +550,40 @@ def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_factor  # Used for testing weights initialization\n         if isinstance(module, Pop2PianoLayerNorm):\n-            module.weight.fill_(factor * 1.0)\n+            init.constant_(module.weight, factor * 1.0)\n         elif isinstance(module, Pop2PianoConcatEmbeddingToMel):\n-            module.embedding.weight.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.embedding.weight, mean=0.0, std=factor * 1.0)\n         elif isinstance(module, Pop2PianoForConditionalGeneration):\n-            module.shared.weight.normal_(mean=0.0, std=factor * 1.0)\n+            init.normal_(module.shared.weight, mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\"):\n-                module.lm_head.weight.normal_(mean=0.0, std=factor * 1.0)\n+                init.normal_(module.lm_head.weight, mean=0.0, std=factor * 1.0)\n         elif isinstance(module, Pop2PianoDenseActDense):\n-            module.wi.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n-                module.wi.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, Pop2PianoDenseGatedActDense):\n-            module.wi_0.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+            init.normal_(module.wi_0.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n-                module.wi_0.bias.zero_()\n-            module.wi_1.weight.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n+                init.zeros_(module.wi_0.bias)\n+            init.normal_(module.wi_1.weight, mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n-                module.wi_1.bias.zero_()\n-            module.wo.weight.normal_(mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n+                init.zeros_(module.wi_1.bias)\n+            init.normal_(module.wo.weight, mean=0.0, std=factor * ((self.config.d_ff) ** -0.5))\n             if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n-                module.wo.bias.zero_()\n+                init.zeros_(module.wo.bias)\n         elif isinstance(module, Pop2PianoAttention):\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_kv\n             n_heads = self.config.num_heads\n-            module.q.weight.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n-            module.k.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.v.weight.normal_(mean=0.0, std=factor * (d_model**-0.5))\n-            module.o.weight.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.q.weight, mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n+            init.normal_(module.k.weight, mean=0.0, std=factor * (d_model**-0.5))\n+            init.normal_(module.v.weight, mean=0.0, std=factor * (d_model**-0.5))\n+            init.normal_(module.o.weight, mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n             if module.has_relative_attention_bias:\n-                module.relative_attention_bias.weight.normal_(mean=0.0, std=factor * ((d_model) ** -0.5))\n+                init.normal_(module.relative_attention_bias.weight, mean=0.0, std=factor * ((d_model) ** -0.5))\n \n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id"
        },
        {
            "sha": "619769b9d2b7eca3321eeb76d67f87072638b9c1",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -332,17 +332,6 @@ class ProphetNetPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"prophetnet\"\n     supports_gradient_checkpointing = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        if isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.init_std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n     def _shift_right(self, input_ids):\n         decoder_start_token_id = self.config.decoder_start_token_id\n         pad_token_id = self.config.pad_token_id"
        },
        {
            "sha": "caee0f0dfb78dafd80aae8670bd3259454dae7c3",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 20,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -426,30 +427,16 @@ def _init_weights(self, module: nn.Module) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         std = self.config.initializer_range\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            nn.init.trunc_normal_(module.weight, mean=0.0, std=std)\n+            init.trunc_normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, PvtPatchEmbeddings):\n-            module.position_embeddings.copy_(\n-                nn.init.trunc_normal_(\n-                    module.position_embeddings,\n-                    mean=0.0,\n-                    std=std,\n-                )\n-            )\n+            init.trunc_normal_(module.position_embeddings, mean=0.0, std=std)\n             if module.cls_token is not None:\n-                module.cls_token.copy_(\n-                    nn.init.trunc_normal_(\n-                        module.cls_token,\n-                        mean=0.0,\n-                        std=std,\n-                    )\n-                )\n+                init.trunc_normal_(module.cls_token, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "9973c6dfcad6912132c7050f9127af2859b28677",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, ImageClassifierOutput\n@@ -372,20 +373,18 @@ class PvtV2PreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.copy_(nn.init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range))\n+            init.trunc_normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+            init.zeros_(module.bias)\n+            init.ones_(module.weight)\n         elif isinstance(module, nn.Conv2d):\n             fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n             fan_out //= module.groups\n-            module.weight.normal_(0, math.sqrt(2.0 / fan_out))\n+            init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))\n             if module.bias is not None:\n-                module.bias.zero_()\n+                init.zeros_(module.bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "7cad197e7e72694c47b3eb6dfd7a85ee49a3957d",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -257,28 +257,6 @@ class Qwen2AudioPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n \n-    @torch.no_grad()\n-    def _init_weights(self, module):\n-        # important: this ported version of Qwen2Audio isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed\n-        std = (\n-            self.config.initializer_range\n-            if hasattr(self.config, \"initializer_range\")\n-            else self.config.audio_config.initializer_range\n-        )\n-\n-        if isinstance(module, (nn.Linear, nn.Conv1d)):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.fill_(1.0)\n-            module.bias.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "8bda140d3cdb10e8868347ac64028011c8fe69c0",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -31,6 +31,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -443,10 +444,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, Qwen2MoeExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, Qwen2MoeTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n @auto_docstring"
        },
        {
            "sha": "477694d5fb2b587052646a15e8a1320fc84bf6c8",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -376,10 +377,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, Qwen3MoeExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, Qwen3MoeTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n class Qwen3MoeRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "362c8fab007fa240329fdeb712c3f0ae1e0f837d",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -26,6 +26,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -992,16 +993,16 @@ class Qwen3NextPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Qwen3NextGatedDeltaNet):\n-            module.dt_bias.fill_(1.0)\n-            module.A_log.uniform_(0, 16).log_()\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.empty_like(module.A_log).uniform_(0, 16).log_())\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif isinstance(module, Qwen3NextRMSNorm):\n-            module.weight.zero_()\n-        if isinstance(module, Qwen3NextExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=self.config.initializer_range)\n-            module.down_proj.normal_(mean=0.0, std=self.config.initializer_range)\n-        if isinstance(module, Qwen3NextSparseMoeBlock):\n-            module.gate.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.weight)\n+        elif isinstance(module, Qwen3NextExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Qwen3NextSparseMoeBlock):\n+            init.normal_(module.gate.weight, mean=0.0, std=self.config.initializer_range)\n \n \n class Qwen3NextModel(Qwen3NextPreTrainedModel):"
        },
        {
            "sha": "7deedb9c868ba7da7bc84d964d99b99bd5059cf9",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...masking_utils import create_causal_mask\n@@ -740,16 +741,16 @@ class Qwen3NextPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Qwen3NextGatedDeltaNet):\n-            module.dt_bias.fill_(1.0)\n-            module.A_log.uniform_(0, 16).log_()\n+            init.ones_(module.dt_bias)\n+            init.copy_(module.A_log, torch.empty_like(module.A_log).uniform_(0, 16).log_())\n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif isinstance(module, Qwen3NextRMSNorm):\n-            module.weight.zero_()\n-        if isinstance(module, Qwen3NextExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=self.config.initializer_range)\n-            module.down_proj.normal_(mean=0.0, std=self.config.initializer_range)\n-        if isinstance(module, Qwen3NextSparseMoeBlock):\n-            module.gate.weight.normal_(mean=0.0, std=self.config.initializer_range)\n+            init.zeros_(module.weight)\n+        elif isinstance(module, Qwen3NextExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Qwen3NextSparseMoeBlock):\n+            init.normal_(module.gate.weight, mean=0.0, std=self.config.initializer_range)\n \n \n class Qwen3NextModel(Qwen3NextPreTrainedModel):"
        },
        {
            "sha": "1be0487cea98084db73c53535088e21f72a1a68d",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -31,6 +31,7 @@\n from torch.nn import Parameter\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -81,9 +82,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, Qwen3OmniMoeThinkerTextSparseMoeBlock):\n-            module.experts.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.experts.down_proj.normal_(mean=0.0, std=std)\n-            module.router.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.experts.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.experts.down_proj, mean=0.0, std=std)\n+            init.normal_(module.router.weight, mean=0.0, std=std)\n \n \n def _get_feat_extract_output_lengths(input_lengths):\n@@ -1607,10 +1608,10 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         std = self.config.initializer_range\n         if isinstance(module, Qwen3OmniMoeThinkerTextExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n         elif isinstance(module, Qwen3OmniMoeThinkerTextTopKRouter):\n-            module.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")"
        },
        {
            "sha": "ea6ac6860133cb631dd8b90dc3e20218f1e4fe18",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...audio_utils import AudioInput\n from ...cache_utils import Cache, DynamicCache\n@@ -796,9 +797,9 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         std = self.config.initializer_range\n         if isinstance(module, Qwen3OmniMoeThinkerTextSparseMoeBlock):\n-            module.experts.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.experts.down_proj.normal_(mean=0.0, std=std)\n-            module.router.weight.normal_(mean=0.0, std=std)\n+            init.normal_(module.experts.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.experts.down_proj, mean=0.0, std=std)\n+            init.normal_(module.router.weight, mean=0.0, std=std)\n \n \n class Qwen3OmniMoePreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration):"
        },
        {
            "sha": "efd0e8d2492671e15dbddfb054e2d7f418581f6f",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -27,6 +27,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -413,8 +414,8 @@ def _init_weights(self, module):\n         else:\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n         if isinstance(module, Qwen3VLMoeTextExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n \n \n class Qwen3VLMoeVisionMLP(nn.Module):"
        },
        {
            "sha": "006fa186fe44aafff0fd058ecc0d03bed6ee0608",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n import torch.nn as nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n@@ -367,8 +368,8 @@ def _init_weights(self, module):\n         else:\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n         if isinstance(module, Qwen3VLMoeTextExperts):\n-            module.gate_up_proj.normal_(mean=0.0, std=std)\n-            module.down_proj.normal_(mean=0.0, std=std)\n+            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n+            init.normal_(module.down_proj, mean=0.0, std=std)\n \n \n class Qwen3VLMoeVisionModel(Qwen3VLVisionModel):"
        },
        {
            "sha": "dc1a3d4951e2ffb41e080d8afb3e2cda452380c5",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 28,
            "deletions": 26,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -22,6 +22,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -557,49 +558,50 @@ class RecurrentGemmaPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         std = math.sqrt(self.config.w_init_variance_scale / self.config.conv1d_width)\n         if isinstance(module, nn.Conv1d):\n-            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n-            torch.nn.init.zeros_(module.bias)\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            init.zeros_(module.bias)\n         elif isinstance(module, RecurrentGemmaSdpaAttention):\n-            torch.nn.init.normal_(module.q_proj.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n-            torch.nn.init.normal_(module.k_proj.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n-            torch.nn.init.normal_(module.v_proj.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n+            init.normal_(module.q_proj.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n+            init.normal_(module.k_proj.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n+            init.normal_(module.v_proj.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n \n             std = math.sqrt(self.config.final_w_init_variance_scale / self.config.hidden_size)\n-            torch.nn.init.normal_(module.o_proj.weight, mean=0.0, std=std)\n+            init.normal_(module.o_proj.weight, mean=0.0, std=std)\n         elif isinstance(module, RecurrentGemmaRecurrentBlock):\n-            torch.nn.init.zeros_(module.linear_x.bias)\n-            torch.nn.init.normal_(module.linear_x.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n+            init.zeros_(module.linear_x.bias)\n+            init.normal_(module.linear_x.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n \n-            torch.nn.init.zeros_(module.linear_y.bias)\n-            torch.nn.init.normal_(module.linear_y.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n+            init.zeros_(module.linear_y.bias)\n+            init.normal_(module.linear_y.weight, mean=0.0, std=math.sqrt(1.0 / self.config.hidden_size))\n \n             std = math.sqrt(self.config.final_w_init_variance_scale / self.config.lru_width)\n-            torch.nn.init.normal_(module.linear_out.weight, mean=0.0, std=std)\n-            torch.nn.init.zeros_(module.linear_out.bias)\n+            init.normal_(module.linear_out.weight, mean=0.0, std=std)\n+            init.zeros_(module.linear_out.bias)\n         elif isinstance(module, RecurrentGemmaRglru):\n             std = math.sqrt(\n                 self.config.w_init_variance_scale / (self.config.lru_width // self.config.num_attention_heads)\n             )\n-            torch.nn.init.normal_(module.input_gate_weight, mean=0.0, std=std)\n-            torch.nn.init.normal_(module.recurrent_gate_weight, mean=0.0, std=std)\n-            torch.nn.init.zeros_(module.input_gate_bias)\n-            torch.nn.init.zeros_(module.recurrent_gate_bias)\n-\n-            module.recurrent_param.uniform_(0.9**2 + 1e-8, 0.999**2 + 1e-8)\n-            module.recurrent_param.log_().mul_(0.5)\n-            module.recurrent_param.neg_().exp_().sub_(1.0).log_()\n+            init.normal_(module.input_gate_weight, mean=0.0, std=std)\n+            init.normal_(module.recurrent_gate_weight, mean=0.0, std=std)\n+            init.zeros_(module.input_gate_bias)\n+            init.zeros_(module.recurrent_gate_bias)\n+\n+            recurrent_param = torch.empty_like(module.recurrent_param).uniform_(0.9**2 + 1e-8, 0.999**2 + 1e-8)\n+            recurrent_param.log_().mul_(0.5).neg_().exp_().sub_(1.0).log_()\n+            init.copy_(module.recurrent_param, recurrent_param)\n         elif isinstance(module, nn.Linear):\n-            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n+            init.normal_(module.weight, mean=0.0, std=std)\n             if getattr(module, \"bias\", None) is not None:\n-                torch.nn.init.zeros_(module.bias)\n+                init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n+            init.normal_(module.weight, mean=0.0, std=std)\n+            # Here we need the check explicitly, as we slice the weight in the `zeros_` call, so it looses the flag\n+            if module.padding_idx is not None and not getattr(module.weight, \"_is_hf_initialized\", False):\n+                init.zeros_(module.weight[module.padding_idx])\n \n         # We initialize with 0s to be 1 centered as the RMSNorm here does (1 + weight)\n         elif isinstance(module, RecurrentGemmaRMSNorm):\n-            module.weight.zero_()\n+            init.zeros_(module.weight)\n \n     def _setup_cache(self, config, batch, device, dtype):\n         layers = getattr(self, \"model\", self).layers"
        },
        {
            "sha": "5eccfcd7b8700f2908b26a449af6c3f02e501cb5",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8598421b5120a6a57efe9f57fbfda9bfed29a4dc/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=8598421b5120a6a57efe9f57fbfda9bfed29a4dc",
            "patch": "@@ -29,6 +29,7 @@\n from torch.autograd.function import Function\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutput, MaskedLMOutput, QuestionAnsweringModelOutput, SequenceClassifierOutput\n@@ -1846,20 +1847,10 @@ def dummy_inputs(self):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n         if isinstance(module, AxialPositionEmbeddings):\n             for weight in module.weights:\n-                nn.init.normal_(weight, std=self.config.axial_norm_std)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight[module.padding_idx].zero_()\n-        elif isinstance(module, nn.Linear):\n-            module.weight.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.zero_()\n-            module.weight.fill_(1.0)\n+                init.normal_(weight, std=self.config.axial_norm_std)\n \n \n @dataclass"
        }
    ],
    "stats": {
        "total": 8784,
        "additions": 3341,
        "deletions": 5443
    }
}