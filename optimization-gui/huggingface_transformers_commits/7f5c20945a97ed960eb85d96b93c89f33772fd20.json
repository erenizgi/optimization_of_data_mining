{
    "author": "khushali9",
    "message": "logic to select tf32 API as per Pytorch version (#42428)\n\n* logic to select tf32 API as per Pytorch version\n\n* new method added into __all__\n\n* make style and quality ran\n\n* added global setting for tf32\n\n* added support for MUSA as well\n\n* make style and quality run\n\n* cleared >= 2.9.0 torch version logic",
    "sha": "7f5c20945a97ed960eb85d96b93c89f33772fd20",
    "files": [
        {
            "sha": "1f94b70b84b8df3274384854350728a245bd6ae9",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5c20945a97ed960eb85d96b93c89f33772fd20/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5c20945a97ed960eb85d96b93c89f33772fd20/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=7f5c20945a97ed960eb85d96b93c89f33772fd20",
            "patch": "@@ -31,6 +31,7 @@\n     patch_testing_methods_to_collect_info,\n     patch_torch_compile_force_graph,\n )\n+from transformers.utils import enable_tf32\n \n \n NOT_DEVICE_TESTS = {\n@@ -137,11 +138,9 @@ def check_output(self, want, got, optionflags):\n doctest.DocTestParser = HfDocTestParser\n \n if is_torch_available():\n-    import torch\n-\n     # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n     # We set it to `False` for CI. See https://github.com/pytorch/pytorch/issues/157274#issuecomment-3090791615\n-    torch.backends.cudnn.allow_tf32 = False\n+    enable_tf32(False)\n \n     # patch `torch.compile`: if `TORCH_COMPILE_FORCE_FULLGRAPH=1` (or values considered as true, e.g. yes, y, etc.),\n     # the patched version will always run with `fullgraph=True`."
        },
        {
            "sha": "04c54343404b564abfeb7da1b98e9d92d544d78d",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5c20945a97ed960eb85d96b93c89f33772fd20/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5c20945a97ed960eb85d96b93c89f33772fd20/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=7f5c20945a97ed960eb85d96b93c89f33772fd20",
            "patch": "@@ -53,7 +53,7 @@\n     requires_backends,\n )\n from .utils.generic import strtobool\n-from .utils.import_utils import is_optimum_neuron_available\n+from .utils.import_utils import enable_tf32, is_optimum_neuron_available\n \n \n logger = logging.get_logger(__name__)\n@@ -379,7 +379,7 @@ class TrainingArguments:\n             metric values.\n         tf32 (`bool`, *optional*):\n             Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n-            on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n+            on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32` and For PyTorch 2.9+ torch.backends.cuda.matmul.fp32_precision. For more details please refer to\n             the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n             experimental API and it may change.\n         ddp_backend (`str`, *optional*):\n@@ -1601,32 +1601,20 @@ def __post_init__(self):\n                         f\"Setting TF32 in {device_str} backends to speedup torch compile, you won't see any improvement\"\n                         \" otherwise.\"\n                     )\n-                    if is_torch_musa_available():\n-                        torch.backends.mudnn.allow_tf32 = True\n-                    else:\n-                        torch.backends.cuda.matmul.allow_tf32 = True\n-                        torch.backends.cudnn.allow_tf32 = True\n+                    enable_tf32(True)\n             else:\n                 logger.warning(\n                     \"The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\"\n                 )\n         if is_torch_available() and self.tf32 is not None:\n             if self.tf32:\n                 if is_torch_tf32_available():\n-                    if is_torch_musa_available():\n-                        torch.backends.mudnn.allow_tf32 = True\n-                    else:\n-                        torch.backends.cuda.matmul.allow_tf32 = True\n-                        torch.backends.cudnn.allow_tf32 = True\n+                    enable_tf32(True)\n                 else:\n                     raise ValueError(\"--tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7\")\n             else:\n                 if is_torch_tf32_available():\n-                    if is_torch_musa_available():\n-                        torch.backends.mudnn.allow_tf32 = False\n-                    else:\n-                        torch.backends.cuda.matmul.allow_tf32 = False\n-                        torch.backends.cudnn.allow_tf32 = False\n+                    enable_tf32(False)\n                 # no need to assert on else\n \n         if self.report_to == \"all\" or self.report_to == [\"all\"]:"
        },
        {
            "sha": "debe1f69f940b55157e0a772f913e92fd85be719",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5c20945a97ed960eb85d96b93c89f33772fd20/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5c20945a97ed960eb85d96b93c89f33772fd20/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=7f5c20945a97ed960eb85d96b93c89f33772fd20",
            "patch": "@@ -108,6 +108,7 @@\n     _LazyModule,\n     check_torch_load_is_safe,\n     direct_transformers_import,\n+    enable_tf32,\n     get_torch_version,\n     is_accelerate_available,\n     is_apex_available,"
        },
        {
            "sha": "45e793814e8509ae6a7b88e83bcddf51d6ca34e8",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5c20945a97ed960eb85d96b93c89f33772fd20/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5c20945a97ed960eb85d96b93c89f33772fd20/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=7f5c20945a97ed960eb85d96b93c89f33772fd20",
            "patch": "@@ -508,6 +508,29 @@ def is_torch_tf32_available() -> bool:\n     return True\n \n \n+@lru_cache\n+def enable_tf32(enable: bool) -> None:\n+    \"\"\"\n+    Set TF32 mode using the appropriate PyTorch API.\n+    For PyTorch 2.9+, uses the new fp32_precision API.\n+    For older versions, uses the legacy allow_tf32 flags.\n+    Args:\n+        enable: Whether to enable TF32 mode\n+    \"\"\"\n+    import torch\n+\n+    pytorch_version = version.parse(get_torch_version())\n+    if pytorch_version >= version.parse(\"2.9.0\"):\n+        precision_mode = \"tf32\" if enable else \"ieee\"\n+        torch.backends.fp32_precision = precision_mode\n+    else:\n+        if is_torch_musa_available():\n+            torch.backends.mudnn.allow_tf32 = enable\n+        else:\n+            torch.backends.cuda.matmul.allow_tf32 = enable\n+            torch.backends.cudnn.allow_tf32 = enable\n+\n+\n @lru_cache\n def is_torch_flex_attn_available() -> bool:\n     return is_torch_available() and version.parse(get_torch_version()) >= version.parse(\"2.5.0\")"
        },
        {
            "sha": "33a71e4f1fd75f5bf625634bda87b682199cf5e2",
            "filename": "utils/modular_model_detector.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f5c20945a97ed960eb85d96b93c89f33772fd20/utils%2Fmodular_model_detector.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f5c20945a97ed960eb85d96b93c89f33772fd20/utils%2Fmodular_model_detector.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_detector.py?ref=7f5c20945a97ed960eb85d96b93c89f33772fd20",
            "patch": "@@ -117,6 +117,7 @@\n \n import transformers\n from transformers import AutoModel, AutoTokenizer\n+from transformers.utils import enable_tf32\n from transformers.utils import logging as transformers_logging\n \n \n@@ -247,7 +248,7 @@ def __init__(self, hub_dataset: str):\n             logging.getLogger(name).setLevel(logging.ERROR)\n         huggingface_hub_logging.set_verbosity_error()\n         transformers_logging.set_verbosity_error()\n-        torch.backends.cuda.matmul.allow_tf32 = True\n+        enable_tf32(True)\n         torch.set_grad_enabled(False)\n \n         self.models_root = MODELS_ROOT"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 33,
        "deletions": 21
    }
}