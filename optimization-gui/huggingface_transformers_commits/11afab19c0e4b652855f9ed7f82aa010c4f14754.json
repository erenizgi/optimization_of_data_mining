{
    "author": "faaany",
    "message": "[docs] update awq doc (#36079)\n\n* update awq doc\n\n* Update docs/source/en/quantization/awq.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/awq.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/awq.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/awq.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* add note for inference\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "11afab19c0e4b652855f9ed7f82aa010c4f14754",
    "files": [
        {
            "sha": "f581c161392fd3799c9044a04a12e4f52722eac5",
            "filename": "docs/source/en/quantization/awq.md",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/11afab19c0e4b652855f9ed7f82aa010c4f14754/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/11afab19c0e4b652855f9ed7f82aa010c4f14754/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fawq.md?ref=11afab19c0e4b652855f9ed7f82aa010c4f14754",
            "patch": "@@ -31,6 +31,8 @@ Make sure you have autoawq installed:\n ```bash\n pip install autoawq\n ```\n+> [!WARNING]\n+> AutoAWQ downgrades Transformers to version 4.47.1. If you want to do inference with AutoAWQ, you may need to reinstall your Transformers' version after installing AutoAWQ.\n \n AWQ-quantized models can be identified by checking the `quantization_config` attribute in the model's [config.json](https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json) file:\n \n@@ -59,13 +61,14 @@ A quantized model is loaded with the [`~PreTrainedModel.from_pretrained`] method\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n ```\n \n Loading an AWQ-quantized model automatically sets other weights to fp16 by default for performance reasons. If you want to load these other weights in a different format, use the `torch_dtype` parameter:\n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer\n+import torch\n \n model_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\n model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)\n@@ -175,7 +178,7 @@ quantization_config = AwqConfig(\n     }\n )\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)\n+model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, trust_remote_code=True).to(0)\n ```\n \n The parameter `modules_to_fuse` should include:\n@@ -232,12 +235,12 @@ Note this feature is supported on AMD GPUs.\n </Tip>\n \n \n-## CPU support\n+## Intel CPU/GPU support\n \n-Recent versions of `autoawq` supports CPU with ipex op optimizations. To get started, first install the latest version of `autoawq` by running:\n+Recent versions of autoawq supports Intel CPU/GPU with IPEX op optimizations. To get started, install the latest version of autoawq.\n \n ```bash\n-pip install intel-extension-for-pytorch\n+pip install intel-extension-for-pytorch # for IPEX-GPU refer to https://intel.github.io/intel-extension-for-pytorch/xpu/2.5.10+xpu/ \n pip install git+https://github.com/casper-hansen/AutoAWQ.git\n ```\n \n@@ -247,27 +250,28 @@ Get started by passing an `AwqConfig()` with `version=\"ipex\"`.\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n \n+device = \"cpu\" # set to \"xpu\" for Intel GPU\n quantization_config = AwqConfig(version=\"ipex\")\n \n model = AutoModelForCausalLM.from_pretrained(\n     \"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\",\n     quantization_config=quantization_config,\n-    device_map=\"cpu\",\n+    device_map=device,\n )\n \n-input_ids = torch.randint(0, 100, (1, 128), dtype=torch.long, device=\"cpu\")\n+input_ids = torch.randint(0, 100, (1, 128), dtype=torch.long, device=device)\n output = model(input_ids)\n print(output.logits)\n \n tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\")\n-input_ids = tokenizer.encode(\"How to make a cake\", return_tensors=\"pt\")\n+input_ids = tokenizer.encode(\"How to make a cake\", return_tensors=\"pt\").to(device)\n pad_token_id = tokenizer.eos_token_id\n output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=pad_token_id)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n <Tip warning={true}>\n \n-Note this feature is supported on Intel CPUs.\n+This feature is supported on Intel CPUs/GPUs.\n \n-</Tip>\n\\ No newline at end of file\n+</Tip>"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 14,
        "deletions": 10
    }
}