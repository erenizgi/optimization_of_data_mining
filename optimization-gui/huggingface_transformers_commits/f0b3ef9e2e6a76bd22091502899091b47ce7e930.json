{
    "author": "blueingman",
    "message": "translated gguf.md into chinese (#34163)\n\n* translated gguf.md into chinese\r\n\r\n* Apply suggestions from code review\r\n\r\nI have updated the PR accordingly.Thank you very much for detailed guidance,and I 'll pay more attention to the details next time.\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>",
    "sha": "f0b3ef9e2e6a76bd22091502899091b47ce7e930",
    "files": [
        {
            "sha": "07c97e51550cb7e2f6e001461b2da61833656731",
            "filename": "docs/source/zh/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0b3ef9e2e6a76bd22091502899091b47ce7e930/docs%2Fsource%2Fzh%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0b3ef9e2e6a76bd22091502899091b47ce7e930/docs%2Fsource%2Fzh%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2F_toctree.yml?ref=f0b3ef9e2e6a76bd22091502899091b47ce7e930",
            "patch": "@@ -50,6 +50,8 @@\n     title: 导出为 TFLite\n   - local: torchscript\n     title: 导出为 TorchScript\n+  - local: gguf\n+    title: 与 GGUF 格式的互操作性\n   title: 开发者指南\n - sections:\n   - local: performance"
        },
        {
            "sha": "3da64a5d9956a18e185517e05ad754e502d08529",
            "filename": "docs/source/zh/gguf.md",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0b3ef9e2e6a76bd22091502899091b47ce7e930/docs%2Fsource%2Fzh%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0b3ef9e2e6a76bd22091502899091b47ce7e930/docs%2Fsource%2Fzh%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fgguf.md?ref=f0b3ef9e2e6a76bd22091502899091b47ce7e930",
            "patch": "@@ -0,0 +1,104 @@\n+<!--\n+Copyright 2023 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# GGUF 和 Transformers 的交互\n+\n+GGUF文件格式用于存储模型，以便通过[GGML](https://github.com/ggerganov/ggml)和其他依赖它的库进行推理，例如非常流行的[llama.cpp](https://github.com/ggerganov/llama.cpp)或[whisper.cpp](https://github.com/ggerganov/whisper.cpp)。\n+\n+该文件格式[由抱抱脸支持](https://huggingface.co/docs/hub/en/gguf)，可用于快速检查文件中张量和元数据。\n+\n+该文件格式是一种“单文件格式”，通常单个文件就包含了配置属性、分词器词汇表和其他属性，同时还有模型中要加载的所有张量。这些文件根据文件的量化类型有不同的格式。我们在[这里](https://huggingface.co/docs/hub/en/gguf#quantization-types)进行了简要介绍。\n+\n+## 在 Transformers 中的支持\n+\n+我们在 transformers 中添加了加载 gguf 文件的功能，这样可以对 GGUF 模型进行进一步的训练或微调，然后再将模型转换回 GGUF 格式，以便在 ggml 生态系统中使用。加载模型时，我们首先将其反量化为 FP32，然后再加载权重以在 PyTorch 中使用。\n+\n+>    [!注意]\n+>    目前这个功能还处于探索阶段，欢迎大家贡献力量，以便在不同量化类型和模型架构之间更好地完善这一功能。\n+\n+目前，支持的模型架构和量化类型如下：\n+\n+### 支持的量化类型\n+\n+根据分享在 Hub 上的较为热门的量化文件，初步支持以下量化类型：\n+\n+- F32\n+- F16\n+- BF16\n+- Q4_0\n+- Q4_1\n+- Q5_0\n+- Q5_1\n+- Q8_0\n+- Q2_K\n+- Q3_K\n+- Q4_K\n+- Q5_K\n+- Q6_K\n+- IQ1_S\n+- IQ1_M\n+- IQ2_XXS\n+- IQ2_XS\n+- IQ2_S\n+- IQ3_XXS\n+- IQ3_S\n+- IQ4_XS\n+- IQ4_NL\n+\n+>    [!注意]\n+>    为了支持 gguf 反量化，需要安装 `gguf>=0.10.0`。\n+\n+### 支持的模型架构\n+\n+目前支持以下在 Hub 上非常热门的模型架构：\n+\n+- LLaMa\n+- Mistral\n+- Qwen2\n+- Qwen2Moe\n+- Phi3\n+- Bloom\n+- Falcon\n+- StableLM\n+- GPT2\n+- Starcoder2\n+\n+## 使用示例\n+\n+为了在`transformers`中加载`gguf`文件，你需要在 `from_pretrained`方法中为分词器和模型指定 `gguf_file`参数。下面是从同一个文件中加载分词器和模型的示例：\n+\n+```py\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n+filename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n+model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n+```\n+\n+现在，你就已经可以结合 PyTorch 生态系统中的一系列其他工具，来使用完整的、未量化的模型了。\n+\n+为了将模型转换回`gguf`文件，我们建议使用`llama.cpp`中的[`convert-hf-to-gguf.py`文件](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py)。\n+\n+以下是如何补充上面的脚本，以保存模型并将其导出回 `gguf`的示例：\n+\n+```py\n+tokenizer.save_pretrained('directory')\n+model.save_pretrained('directory')\n+\n+!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}\n+```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 106,
        "additions": 106,
        "deletions": 0
    }
}