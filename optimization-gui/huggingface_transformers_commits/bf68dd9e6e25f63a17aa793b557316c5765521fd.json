{
    "author": "zucchini-nlp",
    "message": "[tests] expand flex-attn test for vision models (#38434)\n\n* expand the test for VLMs\n\n* typo\n\n* mark models `supports_flex` + expand test for additional kwargs\n\n* flex attn for refactored vision models\n\n* fix copies\n\n* fix\n\n* unskip\n\n* style\n\n* address comments",
    "sha": "bf68dd9e6e25f63a17aa793b557316c5765521fd",
    "files": [
        {
            "sha": "e618c9a1c62dd00f74dd88a426125ed885b02641",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 297,
            "deletions": 147,
            "changes": 444,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -12,24 +12,52 @@\n \n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    replace_return_docstrings,\n-)\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ..auto import AutoModel\n from .configuration_new_task_model import NewTaskModelConfig\n \n \n-_CONFIG_FOR_DOC = \"NewTaskModelConfig\"\n+@dataclass\n+class NewTaskModelModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for NewTaskModel outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n-    Base class for NewTaskModelcausal language model (or autoregressive) outputs.\n+    Base class for NewTaskModel causal language model (or autoregressive) outputs.\n \n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n@@ -77,30 +105,10 @@ def forward(self, image_features):\n         return hidden_states\n \n \n-NEW_TASK_MODEL_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`NewTaskModelConfig`] or [`NewTaskModelVisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n-    NEW_TASK_MODEL_START_DOCSTRING,\n-)\n+@auto_docstring\n class NewTaskModelPreTrainedModel(PreTrainedModel):\n     config_class = NewTaskModelConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"NewTaskModelMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -109,6 +117,8 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         # important: this ported version of NewTaskModelisn't meant for training from scratch - only\n@@ -121,102 +131,24 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n \n-NEW_TASK_MODEL_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`NewTaskModelProcessor`] uses\n-            [`SiglipImageProcessor`] for processing images).\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"The NEW_TASK_MODEL model which consists of a vision backbone and a language model.\"\"\",\n-    NEW_TASK_MODEL_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Base NewTaskModel model which consists of a vision backbone and a language model withou language modeling head.,\n+    \"\"\"\n )\n-class NewTaskModelForNewTask(NewTaskModelPreTrainedModel, GenerationMixin):\n-    main_input_name: ClassVar[str] = \"doc_input_ids\"  # transformers-related\n+class NewTaskModelModel(NewTaskModelPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n \n-    def __init__(self, config):\n+    def __init__(self, config: NewTaskModelConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config=config.vision_config)\n         self.multi_modal_projector = NewTaskModelMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n \n-        language_model = AutoModelForCausalLM.from_config(config=config.text_config)\n-\n-        if language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n+        language_model = AutoModel.from_config(config=config.text_config)\n         self.language_model = language_model\n \n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-\n-        self.embedding_dim = self.config.embedding_dim\n-        self.custom_text_proj = nn.Linear(self.config.text_config.hidden_size, self.embedding_dim)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"model.language_model.{k}\" for k in self.language_model._tied_weights_keys]\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -225,18 +157,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def _update_causal_mask(\n         self,\n         attention_mask,\n@@ -321,8 +241,191 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n-    @add_start_docstrings_to_model_forward(NEW_TASK_MODEL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=NewTaskModelCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, NewTaskModelModelOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, NewTaskModelForConditionalGeneration\n+\n+        >>> model = NewTaskModelForConditionalGeneration.from_pretrained(\"google/new_task_model2-3b-mix-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/new_task_model2-3b-mix-224\")\n+\n+        >>> prompt = \"Where is the cat standing?\"\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs,)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Where is the cat standing?\\nsnow\"\n+        ```\"\"\"\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        is_training = token_type_ids is not None and labels is not None\n+\n+        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            llm_input_ids = input_ids.clone()\n+            llm_input_ids[special_image_mask] = 0\n+        else:\n+            llm_input_ids = input_ids\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0) + 1  # NewTaskModel positions are 1-indexed\n+\n+        # Merge text and images\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                raise ValueError(\n+                    f\"Number of images does not match number of special image tokens in the input text. \"\n+                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n+                    \"tokens from image embeddings.\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n+        )\n+        outputs = self.language_model(\n+            attention_mask=causal_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return NewTaskModelModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Base NewTaskModel model which consists of a vision backbone and a language model without language modeling head.,\n+    \"\"\"\n+)\n+class NewTaskModelForNewTask(NewTaskModelPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    main_input_name: ClassVar[str] = \"doc_input_ids\"  # transformers-related\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = NewTaskModelModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        self.embedding_dim = self.config.embedding_dim\n+        self.custom_text_proj = nn.Linear(self.config.text_config.hidden_size, self.embedding_dim)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"model.language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -341,19 +444,10 @@ def forward(\n         num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n \n         Example:\n \n@@ -400,7 +494,8 @@ def forward(\n         # L2 normalization\n         embeddings = proj / proj.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n \n-        embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+        if attention_mask is not None:\n+            embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n \n         return (embeddings,) + vlm_outputs\n \n@@ -420,7 +515,7 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -443,13 +538,68 @@ def prepare_inputs_for_generation(\n         is_training = token_type_ids is not None and labels is not None\n         if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n             input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self._update_causal_mask(\n+            causal_mask = self.model._update_causal_mask(\n                 attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n             )\n             model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n     def resize_token_embeddings(\n         self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None, mean_resizing=True\n     ) -> nn.Embedding:"
        },
        {
            "sha": "53fa5a3f09c6bd1e904b3490bca971a15803058a",
            "filename": "examples/modular-transformers/modular_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/examples%2Fmodular-transformers%2Fmodular_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_new_task_model.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -65,7 +65,8 @@ def forward(\n         # L2 normalization\n         embeddings = proj / proj.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n \n-        embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+        if attention_mask is not None:\n+            embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n \n         return (embeddings,) + vlm_outputs\n "
        },
        {
            "sha": "ef7bf6ff665f5d09a842a00253d1e497a7c9aeba",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -384,6 +384,8 @@ class ASTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "1d723f1b5aee0dc6fdaeaf1dd0f2fbc5efad7544",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -97,6 +97,7 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = False\n     _supports_static_cache = False\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "5e39d2515e0d76988ed72beb20016a52d1f0e52f",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -830,6 +830,7 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "d50efcb7556d4276f4e8f637d1b4bb8bd74f54a5",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -450,6 +450,8 @@ class CLIPPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "79cb24a8a502f24aa0ff4105abbe7749ef85beee",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -162,7 +162,8 @@ def forward(\n         # L2 normalization\n         embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)\n \n-        embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n+        if attention_mask is not None:\n+            embeddings = embeddings * attention_mask.unsqueeze(-1)  # (batch_size, sequence_length, dim)\n \n         return ColPaliForRetrievalOutput(\n             embeddings=embeddings,"
        },
        {
            "sha": "05bacf81552f92bc6eaa0e37ef7698346a8dcf2a",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -450,6 +450,8 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"DeiTLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "30bf56d3524b2d8dfa1a58532f55bbc4346d1241",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -494,6 +494,8 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Dinov2SwiGLUFFN\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "82d8169c38d473f623458e0aae7c396afb57ee79",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -512,6 +512,8 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"Dinov2WithRegistersSwiGLUFFN\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "18cc873b552d72f4a932b2fd5af8e9a60384a9cd",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -826,6 +826,8 @@ class DPTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "995e9cac7d6050ec1538eaef7c7589b848e670ad",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -1142,8 +1142,8 @@ class Emu3PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_param_buffer_assignment = False\n-    _supports_attention_backend = True\n     _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range"
        },
        {
            "sha": "fc3ab807df828c8e909c055baeaf6082534b3ff2",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -593,6 +593,7 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "dafef6d5666fa63bc5091eae7ded386c29d0ab76",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -151,6 +151,8 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "2f6064acbb111def3db44042cc7f9867f34bb2a4",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -95,6 +95,8 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "4c747a2394e57f754901ff717292c5738e9335c1",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -178,6 +178,8 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -537,6 +539,7 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "248f58f3b78be719a139af68897c39c0e9f1c0d7",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -140,6 +140,8 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "4321dea59d9404734e73ca07e10679edbf6ee359",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -141,6 +141,7 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "d0ea1bb233cde557c24e0366056f116a4b97f736",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -252,6 +252,7 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "368fd09ef32f0fd1d5451f2507c4ac03d217ed46",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -195,6 +195,7 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "5a1157edebf3f9c28b168fba69b307e85bae3e22",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -308,6 +308,7 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "082020b3afd0847d54b366bafb08b3839d946481",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -206,6 +206,7 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "4c73295cab65c9507cfba0fb4e064ebaf1450525",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -860,6 +860,7 @@ class MllamaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_quantized_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "addba2b30fef6a800de678bcaeda591a826c7f03",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -131,6 +131,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "d484f9255a8de321ba5e93932b02a563d0f5cee2",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -375,6 +375,7 @@ class Phi4MultimodalVisionPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "fa5e3f6dc84a690cf989c18681e24de5223bb59c",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -517,6 +517,8 @@ class SiglipPreTrainedModel(PreTrainedModel):\n     ]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "eb3bf5d4a340334f8c4d9334b1f5ece264f188cd",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -749,6 +749,8 @@ class Siglip2PreTrainedModel(PreTrainedModel):\n     ]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "e6a114a522b31d91712457777a24b356f89305b6",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -491,6 +491,8 @@ class VideoMAEPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "c4a20aef91485429a4b9f2da2911f4fa0d4238d2",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -142,6 +142,7 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n+    _supports_flex_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "e8a9958fc5b50be2dd673670c276b1e583977451",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -448,6 +448,8 @@ class ViTPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ViTEmbeddings\", \"ViTLayer\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "326f193713f0f3c4cbe4a67154980721385f0c44",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -633,6 +633,8 @@ class ViTMAEPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "7684eb2b277012fec33d2230b6c774c9cf05b160",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -452,6 +452,8 @@ class ViTMSNPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"ViTMSNAttention\", \"ViTMSNSdpaAttention\"]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     # todo: Resort to https://github.com/facebookresearch/msn/blob/main/src/deit.py#L200-#L211\n     # when creating pre-training scripts."
        },
        {
            "sha": "2ca76996ad1849dd1da5a8894a5c9a06faa632a8",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -456,6 +456,8 @@ class VivitPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "a42982fb932c1fb48c5fbba3ea9e5a674a8ccbbb",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -532,6 +532,8 @@ class YolosPreTrainedModel(PreTrainedModel):\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "82e04fc4541b2f0f9caada38b516fdf0d05f582c",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -535,6 +535,7 @@ class CLIPModelTest(CLIPModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     pipeline_model_mapping = (\n         {\"feature-extraction\": CLIPModel, \"image-feature-extraction\": CLIPVisionModel} if is_torch_available() else {}\n     )\n+    additional_model_inputs = [\"pixel_values\"]\n     fx_compatible = True\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "bec6f0fc1fbcc9422e1dc35a9f1cc2f93a320323",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -401,10 +401,6 @@ def test_initialization(self):\n     def test_generate_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\"Emu3 doesn't support Flex attn yet!\")\n-    def test_flex_attention_with_grads(self):\n-        pass\n-\n \n @require_torch\n class Emu3IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "7fed1157756cbc61e09c8f884272bc7c175d564c",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -351,12 +351,6 @@ def test_eager_matches_fa2_generate(self):\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"Siglip has no FLEX attention, and we don't have a proper way to set/test attn in VLMs. TODO @raushan\"\n-    )\n-    def test_flex_attention_with_grads(self):\n-        pass\n-\n     def test_automodelforcausallm(self):\n         \"\"\"\n         Regression test for #36741/#36917 -- make sure `AutoModelForCausalLM` works with a Gemma3 config, i.e. that"
        },
        {
            "sha": "047d4a0da9b7f3fe8c716cfdbcfeb9af9dba9dd7",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -236,10 +236,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @unittest.skip(reason=\"Vision backbone doesn't support FLEX yet!\")\n-    def test_flex_attention_with_grads(self):\n-        pass\n-\n \n @require_torch\n class GotOcr2IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "b051fa9657bb50fd323df84737cf28e5f0f6566f",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -569,6 +569,8 @@ class MusicgenTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     all_generative_model_classes = ()\n     greedy_sample_model_classes = (MusicgenForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-to-audio\": MusicgenForConditionalGeneration} if is_torch_available() else {}\n+    # Addition keys that are required for forward. MusicGen isn't encoder-decoder in config so we have to pass decoder ids as additional\n+    additional_model_inputs = [\"decoder_input_ids\"]\n     test_pruning = False  # training is not supported yet for MusicGen\n     test_headmasking = False\n     test_resize_embeddings = False"
        },
        {
            "sha": "86fe12b324a88b15bef0a12ef91e0a825b8b006b",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -589,6 +589,8 @@ class MusicgenMelodyTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n     all_generative_model_classes = ()\n     greedy_sample_model_classes = (MusicgenMelodyForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-to-audio\": MusicgenMelodyForConditionalGeneration} if is_torch_available() else {}\n+    # Addition keys that are required for forward. MusicGen isn't encoder-decoder in config so we have to pass decoder ids as additional\n+    additional_model_inputs = [\"decoder_input_ids\"]\n     test_pruning = False  # training is not supported yet for MusicGen\n     test_headmasking = False\n     test_resize_embeddings = False"
        },
        {
            "sha": "8a5b1037eb667d8ee95204fdbe1143232e4d1789",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -103,7 +103,7 @@ def __init__(\n         patch_size=2,\n         num_channels=3,\n         is_training=True,\n-        hidden_size=32,\n+        hidden_size=64,\n         num_hidden_layers=2,\n         num_attention_heads=4,\n         intermediate_size=37,\n@@ -274,7 +274,7 @@ def __init__(\n         use_input_mask=True,\n         use_labels=True,\n         vocab_size=99,\n-        hidden_size=32,\n+        hidden_size=64,\n         num_hidden_layers=2,\n         num_attention_heads=4,\n         intermediate_size=37,"
        },
        {
            "sha": "b1c005a3cc1806a979ab2313418804fd051a1f3c",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -180,7 +180,7 @@ def __init__(\n         patch_size=2,\n         num_channels=3,\n         is_training=True,\n-        hidden_size=32,\n+        hidden_size=64,\n         num_hidden_layers=2,\n         num_attention_heads=4,\n         intermediate_size=37,\n@@ -363,7 +363,7 @@ def __init__(\n         use_input_mask=True,\n         use_labels=True,\n         vocab_size=99,\n-        hidden_size=32,\n+        hidden_size=64,\n         num_hidden_layers=2,\n         num_attention_heads=4,\n         intermediate_size=37,"
        },
        {
            "sha": "0b85c31f8a9aa2fff26058006e2bf88e3f0ea1de",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -190,7 +190,8 @@ class VideoMAEModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else {}\n     )\n-\n+    # Addition keys that are required for forward, used in tests where we manipulate and create new input dict from scratch\n+    additional_model_inputs = [\"bool_masked_pos\"]\n     test_pruning = False\n     test_torchscript = False\n     test_resize_embeddings = False"
        },
        {
            "sha": "07d9ab3c53e297cfff2609e9e48c6f6df773e072",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -322,10 +322,6 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @unittest.skip(\"LLaVA vision backbones doesn't support flex attention yet\")\n-    def test_flex_attention_with_grads(self):\n-        pass\n-\n \n @require_torch\n class VipLlavaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "441c99267b6ef8a4dd4cfb777d08b5c74c03b16b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 68,
            "deletions": 23,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf68dd9e6e25f63a17aa793b557316c5765521fd/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=bf68dd9e6e25f63a17aa793b557316c5765521fd",
            "patch": "@@ -3637,7 +3637,10 @@ def test_eager_matches_sdpa_inference(\n                 processed_inputs[model.main_input_name] = inputs_dict[model.main_input_name]\n \n                 for key in getattr(self, \"additional_model_inputs\", []):\n-                    processed_inputs[key] = inputs_dict[key]\n+                    # Some models don't have all `additional_model_inputs`, especially when we\n+                    # craft cases to test model in different settings\n+                    if key in inputs_dict:\n+                        processed_inputs[key] = inputs_dict[key]\n \n                 for key, value in processed_inputs.items():\n                     if torch.is_floating_point(value):\n@@ -4012,19 +4015,21 @@ def test_flash_attn_2_can_dispatch_composite_models(self):\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n \n                 sub_models_supporting_fa2 = [\n-                    (module._supports_flash_attn_2 or module._supports_attention_backend)\n+                    module._supports_flash_attn_2\n                     for name, module in model.named_modules()\n                     if isinstance(module, PreTrainedModel) and name != \"\"\n                 ]\n                 supports_fa2_all_modules = (\n                     all(sub_models_supporting_fa2)\n                     if len(sub_models_supporting_fa2) > 0\n-                    else (model._supports_flash_attn_2 or model._supports_attention_backend)\n+                    else model._supports_flash_attn_2\n                 )\n                 if not supports_fa2_all_modules:\n                     with self.assertRaises(ValueError):\n                         model_fa2 = model_class.from_pretrained(\n-                            tmpdirname, torch_dtype=torch_dtype, attn_implementation=\"flash_attention_2\"\n+                            tmpdirname,\n+                            torch_dtype=torch_dtype,\n+                            attn_implementation=\"flash_attention_2\",\n                         )\n                 else:\n                     model_fa2 = model_class.from_pretrained(\n@@ -4572,33 +4577,73 @@ def recursively_check(eager_outputs, exported_outputs):\n     @require_torch_gpu\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:\n-            # TODO: raushan, fix for composite models after making VLMs support new attn API\n-            if not model_class._supports_flex_attn or self._is_composite:\n-                self.skipTest(reason=\"This model does not support flex attention\")\n-\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            config._attn_implementation = \"flex_attention\"\n-            # Flex Attention cannot use dropout\n-            if hasattr(config, \"attention_dropout\"):\n-                config.attention_dropout = 0\n-            if hasattr(config, \"attention_probs_dropout_prob\"):\n-                config.attention_probs_dropout_prob = 0\n-\n-            # Flex attention relies on triton on compilation\n-            # However, triton cannot handle hidden dimensions of less than 16\n-            # --> forcing at least a hidden dim of 16\n-            config.hidden_size *= max(\n-                16 // getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads), 1\n+            inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n+            model = model_class(config).to(device=torch_device)\n+\n+            # If not all sub-models support flex, skip the test\n+            sub_models_supporting_flex = [\n+                module._supports_flex_attn\n+                for name, module in model.named_modules()\n+                if isinstance(module, PreTrainedModel) and name != \"\"\n+            ]\n+            supports_flex_all_modules = (all(sub_models_supporting_flex) and len(sub_models_supporting_flex) > 0) or (\n+                model._supports_flex_attn and len(sub_models_supporting_flex) == 0\n             )\n-            if hasattr(config, \"head_dim\"):\n-                config.head_dim = max(16, config.head_dim)\n+            if not supports_flex_all_modules:\n+                self.skipTest(reason=\"This model's submodels does not support flex attention\")\n+\n+            def update_config_for_flex(config):\n+                # Flex Attention cannot use dropout\n+                if hasattr(config, \"attention_dropout\"):\n+                    config.attention_dropout = 0\n+                if hasattr(config, \"attention_probs_dropout_prob\"):\n+                    config.attention_probs_dropout_prob = 0\n+\n+                # Flex attention relies on triton on compilation\n+                # However, triton cannot handle hidden dimensions of less than 16\n+                # --> forcing at least a hidden dim of 16\n+\n+                # Update the head dim and try to update hidden size as well if present in config\n+                # NOTE: some models may have none if the values in sub-config, thus we check for `Noneness`\n+                head_dim = None\n+                if hasattr(config, \"head_dim\") and config.head_dim is not None:\n+                    head_dim = config.head_dim\n+                    config.head_dim = max(16, config.head_dim)\n+\n+                if (\n+                    getattr(config, \"hidden_size\", None) is not None\n+                    and getattr(config, \"num_attention_heads\", None) is not None\n+                ):\n+                    head_dim = head_dim if head_dim is not None else config.hidden_size // config.num_attention_heads\n+                    config.hidden_size *= max(16 // head_dim, 1)\n+\n+                if (\n+                    getattr(config, \"decoder_hidden_size\", None) is not None\n+                    and getattr(config, \"decoder_num_attention_heads\", None) is not None\n+                ):\n+                    decoder_head_dim = config.decoder_hidden_size // config.decoder_num_attention_heads\n+                    config.decoder_hidden_size *= max(16 // decoder_head_dim, 1)\n \n+            # Set default attention to flex and update config values\n+            update_config_for_flex(config)\n+            for key in config.sub_configs:\n+                sub_config = getattr(config, key)\n+                update_config_for_flex(sub_config)\n+\n+            config._attn_implementation = \"flex_attention\"\n             model = model_class(config).to(device=torch_device)\n             self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n \n             # Elaborate workaround for encoder-decoder models as some do not specify their main input\n             dummy_inputs = {model.main_input_name: inputs_dict[model.main_input_name].to(torch_device)}\n-            if config.is_encoder_decoder:\n+            for key in getattr(self, \"additional_model_inputs\", []):\n+                # Some models don't have all `additional_model_inputs`, especially when we\n+                # craft cases to test model in different settings\n+                if key in inputs_dict:\n+                    dummy_inputs[key] = inputs_dict[key].to(torch_device)\n+\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 dummy_inputs[\"decoder_input_ids\"] = inputs_dict[\"decoder_input_ids\"].to(torch_device)\n                 dummy_inputs[\"decoder_attention_mask\"] = inputs_dict[\"decoder_attention_mask\"].to(torch_device)\n "
        }
    ],
    "stats": {
        "total": 626,
        "additions": 430,
        "deletions": 196
    }
}