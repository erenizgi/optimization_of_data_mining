{
    "author": "maximizemaxwell",
    "message": "ğŸŒ [i18n-KO] Translated quark.md to Korean (#39268)\n\n* initial translation\n\n* removed english parts\n\n* maintain consistency\n\n* Update docs/source/ko/quantization/quark.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/quantization/quark.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/quantization/quark.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Update docs/source/ko/quantization/quark.md\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* add toctree\n\n* fixed indentation\n\n---------\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>",
    "sha": "4652677c89ecb664ee06bf141d1b7b648798e122",
    "files": [
        {
            "sha": "75632c1f59a51d1875afaab3112aa685217a35c7",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4652677c89ecb664ee06bf141d1b7b648798e122/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4652677c89ecb664ee06bf141d1b7b648798e122/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=4652677c89ecb664ee06bf141d1b7b648798e122",
            "patch": "@@ -157,6 +157,8 @@\n     title: (ë²ˆì—­ì¤‘) VPTQ\n   - local: quantization/quanto\n     title: Quanto\n+  - local: quantization/quark\n+    title: Quark\n   - local: quantization/eetq\n     title: EETQ\n   - local: in_translation"
        },
        {
            "sha": "eb1d18bf0f90a61d89e6431ba77d22c5a6b02091",
            "filename": "docs/source/ko/quantization/quark.md",
            "status": "added",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/4652677c89ecb664ee06bf141d1b7b648798e122/docs%2Fsource%2Fko%2Fquantization%2Fquark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4652677c89ecb664ee06bf141d1b7b648798e122/docs%2Fsource%2Fko%2Fquantization%2Fquark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquantization%2Fquark.md?ref=4652677c89ecb664ee06bf141d1b7b648798e122",
            "patch": "@@ -0,0 +1,85 @@\n+<!--Copyright 2025 Advanced Micro Devices, Inc. and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Quark[[quark]]\n+\n+[Quark](https://quark.docs.amd.com/latest/)ëŠ” íŠ¹ì • ë°ì´í„° íƒ€ì…, ì•Œê³ ë¦¬ì¦˜, í•˜ë“œì›¨ì–´ì— êµ¬ì• ë°›ì§€ ì•Šë„ë¡ ì„¤ê³„ëœ ë”¥ëŸ¬ë‹ ì–‘ìí™” íˆ´í‚·ì…ë‹ˆë‹¤. Quarkì—ì„œëŠ” ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ì „ëµ, ì•Œê³ ë¦¬ì¦˜, ë°ì´í„° íƒ€ì…ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ğŸ¤— Transformersë¥¼ í†µí•´ í†µí•©ëœ PyTorch ì§€ì›ì€ ì£¼ë¡œ AMD CPU ë° GPUë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ë©°, ì£¼ë¡œ í‰ê°€ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)ë¥¼ ğŸ¤— Transformers ë°±ì—”ë“œì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ Quarkë¡œ ì–‘ìí™”ëœ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì›í™œí•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+Quarkì— ê´€ì‹¬ì´ ìˆëŠ” ì‚¬ìš©ìëŠ” [ë¬¸ì„œ](https://quark.docs.amd.com/latest/)ë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë¸ ì–‘ìí™”ë¥¼ ì‹œì‘í•˜ê³  ì§€ì›ë˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n+\n+QuarkëŠ” ìì²´ ì²´í¬í¬ì¸íŠ¸/[ì„¤ì • í¬ë§·](https://huggingface.co/amd/Llama-3.1-8B-Instruct-FP8-KV-Quark-test/blob/main/config.json#L26)ë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ, ë‹¤ë¥¸ ì–‘ìí™”/ëŸ°íƒ€ì„ êµ¬í˜„ì²´ ([AutoAWQ](https://huggingface.co/docs/transformers/quantization/awq), [ë„¤ì´í‹°ë¸Œ fp8](https://huggingface.co/docs/transformers/quantization/finegrained_fp8))ì™€ í˜¸í™˜ë˜ëŠ” ì§ë ¬í™” ë ˆì´ì•„ì›ƒìœ¼ë¡œ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ê²ƒë„ ì§€ì›í•©ë‹ˆë‹¤.\n+\n+Transformerì—ì„œ Quark ì–‘ìí™” ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ ë¨¼ì € ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:\n+\n+```bash\n+pip install amd-quark\n+```\n+\n+## ì§€ì› ë§¤íŠ¸ë¦­ìŠ¤[[Support matrix]]\n+\n+Quarkë¥¼ í†µí•´ ì–‘ìí™”ëœ ëª¨ë¸ì€ í•¨ê»˜ ì¡°í•©í•  ìˆ˜ ìˆëŠ” ê´‘ë²”ìœ„í•œ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. êµ¬ì„±ì— ê´€ê³„ì—†ì´ ëª¨ë“  ì–‘ìí™”ëœ ëª¨ë¸ì€ `PretrainedModel.from_pretrained`ë¥¼ í†µí•´ ì›í™œí•˜ê²Œ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì•„ë˜ í‘œëŠ” Quarkì—ì„œ ì§€ì›í•˜ëŠ” ëª‡ ê°€ì§€ ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:\n+\n+| **ê¸°ëŠ¥**                        | **Quarkì—ì„œ ì§€ì›í•˜ëŠ” í•­ëª©**                                                                             |   |\n+|---------------------------------|-----------------------------------------------------------------------------------------------------------|---|\n+| ë°ì´í„° íƒ€ì…                     | int8, int4, int2, bfloat16, float16, fp8_e5m2, fp8_e4m3, fp6_e3m2, fp6_e2m3, fp4, OCP MX, MX6, MX9, bfp16 |   |\n+| ì–‘ìí™” ì „ ëª¨ë¸ ë³€í™˜ | SmoothQuant, QuaRot, SpinQuant, AWQ                                                                       |   |\n+| ì–‘ìí™” ì•Œê³ ë¦¬ì¦˜                 | GPTQ                                                                                                      |   |\n+| ì§€ì› ì—°ì‚°ì                     | ``nn.Linear``, ``nn.Conv2d``, ``nn.ConvTranspose2d``, ``nn.Embedding``, ``nn.EmbeddingBag``               |   |\n+| ì„¸ë¶„ì„±(Granularity)             | per-tensor, per-channel, per-block, per-layer, per-layer type                                             |   |\n+| KV ìºì‹œ                         | fp8                                                                                                       |   |\n+| í™œì„±í™” ìº˜ë¦¬ë¸Œë ˆì´ì…˜             | MinMax / Percentile / MSE                                                                                 |   |\n+| ì–‘ìí™” ì „ëµ                     | weight-only, static, dynamic, with or without output quantization                                         |   |\n+\n+## Hugging Face Hubì˜ ëª¨ë¸[[Models on Hugging Face Hub]]\n+\n+Quark ë„¤ì´í‹°ë¸Œ ì§ë ¬í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê³µê°œ ëª¨ë¸ì€ https://huggingface.co/models?other=quark ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+QuarkëŠ” [`quant_method=\"fp8\"`ì„ ì´ìš©í•˜ëŠ” ëª¨ë¸](https://huggingface.co/models?other=fp8)ê³¼ [`quant_method=\"awq\"`ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸](https://huggingface.co/models?other=awq)ë„ ì§€ì›í•˜ì§€ë§Œ, TransformersëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì„ [AutoAWQ](https://huggingface.co/docs/transformers/quantization/awq)ë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ \n+[ğŸ¤— Transformersì˜ ë„¤ì´í‹°ë¸Œ fp8 ì§€ì›](https://huggingface.co/docs/transformers/quantization/finegrained_fp8)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+\n+## Transformersì—ì„œ Quarkëª¨ë¸ ì‚¬ìš©í•˜ê¸°[[Using Quark models in Transformers]]\n+\n+ë‹¤ìŒì€ Transformersì—ì„œ Quark ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤:\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_id = \"EmbeddedLLM/Llama-3.1-8B-Instruct-w_fp8_per_channel_sym\"\n+model = AutoModelForCausalLM.from_pretrained(model_id)\n+model = model.to(\"cuda\")\n+\n+print(model.model.layers[0].self_attn.q_proj)\n+# QParamsLinear(\n+#   (weight_quantizer): ScaledRealQuantizer()\n+#   (input_quantizer): ScaledRealQuantizer()\n+#   (output_quantizer): ScaledRealQuantizer()\n+# )\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+inp = tokenizer(\"Where is a good place to cycle around Tokyo?\", return_tensors=\"pt\")\n+inp = inp.to(\"cuda\")\n+\n+res = model.generate(**inp, min_new_tokens=50, max_new_tokens=100)\n+\n+print(tokenizer.batch_decode(res)[0])\n+# <|begin_of_text|>Where is a good place to cycle around Tokyo? There are several places in Tokyo that are suitable for cycling, depending on your skill level and interests. Here are a few suggestions:\n+# 1. Yoyogi Park: This park is a popular spot for cycling and has a wide, flat path that's perfect for beginners. You can also visit the Meiji Shrine, a famous Shinto shrine located in the park.\n+# 2. Imperial Palace East Garden: This beautiful garden has a large, flat path that's perfect for cycling. You can also visit the\n+```"
        }
    ],
    "stats": {
        "total": 87,
        "additions": 87,
        "deletions": 0
    }
}