{
    "author": "ylacombe",
    "message": "Fix Bark saving (#33266)",
    "sha": "979f4774f619e43a7c121c49b3c9cdc0b48d687a",
    "files": [
        {
            "sha": "5aad7b23a8a6727f2302aee819c6f030908601ce",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/979f4774f619e43a7c121c49b3c9cdc0b48d687a/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/979f4774f619e43a7c121c49b3c9cdc0b48d687a/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=979f4774f619e43a7c121c49b3c9cdc0b48d687a",
            "patch": "@@ -1248,6 +1248,17 @@ def resize_token_embeddings(\n \n         return model_embeds\n \n+    def _tie_weights(self):\n+        if getattr(self.config, \"tie_word_embeddings\", True):\n+            self._tied_weights_keys = []\n+            output_embeddings = self.get_output_embeddings()\n+            input_embeddings = self.get_input_embeddings()\n+\n+            for i in range(self.config.n_codes_total - self.config.n_codes_given):\n+                # self.input_embeds_layers[i + 1].weight = self.lm_heads[i].weight\n+                self._tie_or_clone_weights(output_embeddings[i], input_embeddings[i + 1])\n+                self._tied_weights_keys.append(f\"lm_heads.{i}.weight\")\n+\n     def tie_weights(self):\n         \"\"\"\n         Tie the weights between the input embeddings list and the output embeddings list."
        }
    ],
    "stats": {
        "total": 11,
        "additions": 11,
        "deletions": 0
    }
}