{
    "author": "ArthurZucker",
    "message": "Add CB (#38085)\n\n* stash for now\n\n* initial commit\n\n* small updated\n\n* up\n\n* up\n\n* works!\n\n* nits and fixes\n\n* don't loop too much\n\n* finish working example\n\n* update\n\n* fix the small freeblocks issue\n\n* feat: stream inputs to continuous batch\n\n* fix: update attn from `eager` to `sdpa`\n\n* refactor: fmt\n\n* refactor: cleanup unnecessary code\n\n* feat: add `update` fn to `PagedAttentionCache`\n\n* feat: broken optimal block size computation\n\n* fix: debugging invalid cache logic\n\n* fix: attention mask\n\n* refactor: use custom prompts for example\n\n* feat: add streaming output\n\n* fix: prefill split\n\nrefactor: add doc strings and unsound/redundant logic\nfix: compute optimal blocks logic\n\n* fix: send decoded tokens when `prefilling_split` -> `decoding`\n\n* refactor: move logic to appropriate parent class\n\n* fix: remove truncation as we split prefilling anyways\n\nrefactor: early return when we have enough selected requests\n\n* feat: add paged attention forward\n\n* push Ggraoh>\n\n* add paged sdpa\n\n* update\n\n* btter mps defaults\n\n* feat: add progress bar for `generate_batch`\n\n* feat: add opentelemetry metrics (ttft + batch fill %age)\n\n* feat: add tracing\n\n* Add cuda graphs (#38059)\n\n* draft cudagraphs addition\n\n* nits\n\n* styling\n\n* update\n\n* fix\n\n* kinda draft of what it should look like\n\n* fixes\n\n* lol\n\n* not sure why inf everywhere\n\n* can generate but output is shit\n\n* some fixes\n\n* we should have a single device synch\n\n* broken outputs but it does run\n\n* refactor\n\n* updates\n\n* updates with some fixes\n\n* fix mask causality\n\n* another commit that casts after\n\n* add error\n\n* simplify example\n\n* update\n\n* updates\n\n* revert llama changes\n\n* fix merge conflicts\n\n* fix: tracing and metrics\n\n* my updates\n\n* update script default values\n\n* fix block allocation issue\n\n* fix prefill split attnetion mask\n\n* no bugs\n\n* add paged eager\n\n* fix\n\n* update\n\n* style\n\n* feat: add pytorch traces\n\n* fix\n\n* fix\n\n* refactor: remove pytorch profiler data\n\n* style\n\n* nits\n\n* cleanup\n\n* draft test file\n\n* fix\n\n* fix\n\n* fix paged and graphs\n\n* small renamings\n\n* cleanups and push\n\n* refactor: move tracing and metrics logic to utils\n\n* refactor: trace more blocks of code\n\n* nits\n\n* nits\n\n* update\n\n* to profile or not to profile\n\n* refactor: create new output object\n\n* causal by default\n\n* cleanup but generations are still off for IDK what reason\n\n* simplifications but not running still\n\n* this does work.\n\n* small quality of life updates\n\n* nits\n\n* updaet\n\n* fix the scheduler\n\n* fix warning\n\n* ol\n\n* fully fixed\n\n* nits\n\n* different generation parameters\n\n* nice\n\n* just style\n\n* feat: add cache memory usage\n\n* feat: add kv cache free memory\n\n* feat: add active/waiting count & req latency\n\n* do the sampling\n\n* fix: synchronize CUDA only if available and improve error handling in ContinuousBatchingManager\n\n* fix on mps\n\n* feat: add dashboard & histogram buckets\n\n* perf: improve waiting reqs data structures\n\n* attempt to compile, but we should only do it on mps AFAIK\n\n* feat: decouple scheduling logic\n\n* just a draft\n\n* c;eanup and fixup\n\n* optional\n\n* style\n\n* update\n\n* update\n\n* remove the draft documentation\n\n* fix import as well\n\n* update\n\n* fix the test\n\n* style doomed\n\n---------\n\nCo-authored-by: Luc Georges <luc.sydney.georges@gmail.com>",
    "sha": "211f2b08751cdee4bd6b84892b91f4a18dbfe305",
    "files": [
        {
            "sha": "64ef1160c66bc2c148fc5263364fc36bdd7f727b",
            "filename": "examples/metrics-monitoring/README.md",
            "status": "added",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2FREADME.md?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,4 @@\n+# Metrics Monitoring\n+\n+## Continuous Batching Metrics in Transformers\n+"
        },
        {
            "sha": "e0a293d06295e58d0694d85806a7ddd76def7216",
            "filename": "examples/metrics-monitoring/continuous-batching-dashboard.json",
            "status": "added",
            "additions": 974,
            "deletions": 0,
            "changes": 974,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fcontinuous-batching-dashboard.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fcontinuous-batching-dashboard.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Fcontinuous-batching-dashboard.json?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,974 @@\n+{\n+    \"annotations\": {\n+        \"list\": [\n+            {\n+                \"builtIn\": 1,\n+                \"datasource\": {\n+                    \"type\": \"grafana\",\n+                    \"uid\": \"-- Grafana --\"\n+                },\n+                \"enable\": true,\n+                \"hide\": true,\n+                \"iconColor\": \"rgba(0, 211, 255, 1)\",\n+                \"name\": \"Annotations & Alerts\",\n+                \"target\": {\n+                    \"limit\": 100,\n+                    \"matchAny\": false,\n+                    \"tags\": [],\n+                    \"type\": \"dashboard\"\n+                },\n+                \"type\": \"dashboard\"\n+            }\n+        ]\n+    },\n+    \"editable\": true,\n+    \"fiscalYearStartMonth\": 0,\n+    \"graphTooltip\": 0,\n+    \"id\": 2,\n+    \"links\": [],\n+    \"panels\": [\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"description\": \"Memory usage of the PagedAttentionCache\",\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"thresholds\"\n+                    },\n+                    \"mappings\": [],\n+                    \"max\": 10737418240,\n+                    \"min\": 0,\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"yellow\",\n+                                \"value\": 5368709120\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 8589934592\n+                            }\n+                        ]\n+                    },\n+                    \"unit\": \"bytes\"\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 6,\n+                \"x\": 0,\n+                \"y\": 0\n+            },\n+            \"id\": 2,\n+            \"options\": {\n+                \"minVizHeight\": 75,\n+                \"minVizWidth\": 75,\n+                \"orientation\": \"auto\",\n+                \"reduceOptions\": {\n+                    \"calcs\": [\n+                        \"lastNotNull\"\n+                    ],\n+                    \"fields\": \"\",\n+                    \"values\": false\n+                },\n+                \"showThresholdLabels\": false,\n+                \"showThresholdMarkers\": true,\n+                \"sizing\": \"auto\"\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"kv_cache_memory_bytes\",\n+                    \"fullMetaSearch\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"__auto\",\n+                    \"range\": true,\n+                    \"refId\": \"A\",\n+                    \"useBackend\": false\n+                }\n+            ],\n+            \"title\": \"KV Cache Memory Usage\",\n+            \"transparent\": true,\n+            \"type\": \"gauge\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"thresholds\"\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"dark-blue\"\n+                            }\n+                        ]\n+                    }\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 6,\n+                \"x\": 6,\n+                \"y\": 0\n+            },\n+            \"id\": 13,\n+            \"options\": {\n+                \"colorMode\": \"value\",\n+                \"graphMode\": \"area\",\n+                \"justifyMode\": \"auto\",\n+                \"orientation\": \"auto\",\n+                \"percentChangeColorMode\": \"standard\",\n+                \"reduceOptions\": {\n+                    \"calcs\": [\n+                        \"lastNotNull\"\n+                    ],\n+                    \"fields\": \"\",\n+                    \"values\": false\n+                },\n+                \"showPercentChange\": false,\n+                \"textMode\": \"auto\",\n+                \"wideLayout\": true\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"active_requests_count\",\n+                    \"fullMetaSearch\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"__auto\",\n+                    \"range\": true,\n+                    \"refId\": \"A\",\n+                    \"useBackend\": false\n+                }\n+            ],\n+            \"title\": \"Active Requests\",\n+            \"transparent\": true,\n+            \"type\": \"stat\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"thresholds\"\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"dark-orange\"\n+                            }\n+                        ]\n+                    }\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 6,\n+                \"x\": 12,\n+                \"y\": 0\n+            },\n+            \"id\": 14,\n+            \"options\": {\n+                \"colorMode\": \"value\",\n+                \"graphMode\": \"area\",\n+                \"justifyMode\": \"auto\",\n+                \"orientation\": \"auto\",\n+                \"percentChangeColorMode\": \"standard\",\n+                \"reduceOptions\": {\n+                    \"calcs\": [\n+                        \"lastNotNull\"\n+                    ],\n+                    \"fields\": \"\",\n+                    \"values\": false\n+                },\n+                \"showPercentChange\": false,\n+                \"textMode\": \"auto\",\n+                \"wideLayout\": true\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"waiting_requests_count\",\n+                    \"fullMetaSearch\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"__auto\",\n+                    \"range\": true,\n+                    \"refId\": \"A\",\n+                    \"useBackend\": false\n+                }\n+            ],\n+            \"title\": \"Waiting Requests\",\n+            \"transparent\": true,\n+            \"type\": \"stat\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"description\": \"Ratio of decode tokens to prefill tokens in a batch\",\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"thresholds\"\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"blue\"\n+                            }\n+                        ]\n+                    }\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 6,\n+                \"x\": 18,\n+                \"y\": 0\n+            },\n+            \"id\": 6,\n+            \"options\": {\n+                \"colorMode\": \"value\",\n+                \"graphMode\": \"none\",\n+                \"justifyMode\": \"auto\",\n+                \"orientation\": \"auto\",\n+                \"percentChangeColorMode\": \"standard\",\n+                \"reduceOptions\": {\n+                    \"calcs\": [\n+                        \"lastNotNull\"\n+                    ],\n+                    \"fields\": \"\",\n+                    \"values\": false\n+                },\n+                \"showPercentChange\": false,\n+                \"textMode\": \"auto\",\n+                \"wideLayout\": true\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"decode_prefill_ratio\",\n+                    \"fullMetaSearch\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"__auto\",\n+                    \"range\": true,\n+                    \"refId\": \"A\",\n+                    \"useBackend\": false\n+                }\n+            ],\n+            \"title\": \"Decode/Prefill Ratio\",\n+            \"transparent\": true,\n+            \"type\": \"stat\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"palette-classic\"\n+                    },\n+                    \"custom\": {\n+                        \"axisBorderShow\": false,\n+                        \"axisCenteredZero\": false,\n+                        \"axisColorMode\": \"text\",\n+                        \"axisLabel\": \"\",\n+                        \"axisPlacement\": \"auto\",\n+                        \"barAlignment\": 0,\n+                        \"barWidthFactor\": 0.6,\n+                        \"drawStyle\": \"line\",\n+                        \"fillOpacity\": 0,\n+                        \"gradientMode\": \"none\",\n+                        \"hideFrom\": {\n+                            \"legend\": false,\n+                            \"tooltip\": false,\n+                            \"viz\": false\n+                        },\n+                        \"insertNulls\": false,\n+                        \"lineInterpolation\": \"linear\",\n+                        \"lineWidth\": 1,\n+                        \"pointSize\": 5,\n+                        \"scaleDistribution\": {\n+                            \"type\": \"linear\"\n+                        },\n+                        \"showPoints\": \"auto\",\n+                        \"spanNulls\": false,\n+                        \"stacking\": {\n+                            \"group\": \"A\",\n+                            \"mode\": \"none\"\n+                        },\n+                        \"thresholdsStyle\": {\n+                            \"mode\": \"off\"\n+                        }\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 80\n+                            }\n+                        ]\n+                    }\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 12,\n+                \"x\": 0,\n+                \"y\": 8\n+            },\n+            \"id\": 10,\n+            \"options\": {\n+                \"legend\": {\n+                    \"calcs\": [],\n+                    \"displayMode\": \"list\",\n+                    \"placement\": \"bottom\",\n+                    \"showLegend\": true\n+                },\n+                \"tooltip\": {\n+                    \"hideZeros\": false,\n+                    \"mode\": \"single\",\n+                    \"sort\": \"none\"\n+                }\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"rate(decode_tokens_processed_total[$__rate_interval])\",\n+                    \"legendFormat\": \"__auto\",\n+                    \"range\": true,\n+                    \"refId\": \"A\"\n+                }\n+            ],\n+            \"title\": \"Decode tokens throupught tok/s\",\n+            \"type\": \"timeseries\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"palette-classic\"\n+                    },\n+                    \"custom\": {\n+                        \"axisBorderShow\": false,\n+                        \"axisCenteredZero\": false,\n+                        \"axisColorMode\": \"text\",\n+                        \"axisLabel\": \"\",\n+                        \"axisPlacement\": \"auto\",\n+                        \"barAlignment\": 0,\n+                        \"barWidthFactor\": 0.6,\n+                        \"drawStyle\": \"line\",\n+                        \"fillOpacity\": 0,\n+                        \"gradientMode\": \"none\",\n+                        \"hideFrom\": {\n+                            \"legend\": false,\n+                            \"tooltip\": false,\n+                            \"viz\": false\n+                        },\n+                        \"insertNulls\": false,\n+                        \"lineInterpolation\": \"linear\",\n+                        \"lineWidth\": 1,\n+                        \"pointSize\": 5,\n+                        \"scaleDistribution\": {\n+                            \"type\": \"linear\"\n+                        },\n+                        \"showPoints\": \"auto\",\n+                        \"spanNulls\": false,\n+                        \"stacking\": {\n+                            \"group\": \"A\",\n+                            \"mode\": \"none\"\n+                        },\n+                        \"thresholdsStyle\": {\n+                            \"mode\": \"off\"\n+                        }\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 80\n+                            }\n+                        ]\n+                    }\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 12,\n+                \"x\": 12,\n+                \"y\": 8\n+            },\n+            \"id\": 11,\n+            \"options\": {\n+                \"legend\": {\n+                    \"calcs\": [],\n+                    \"displayMode\": \"list\",\n+                    \"placement\": \"bottom\",\n+                    \"showLegend\": true\n+                },\n+                \"tooltip\": {\n+                    \"hideZeros\": false,\n+                    \"mode\": \"single\",\n+                    \"sort\": \"none\"\n+                }\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"rate(prefill_tokens_processed_total[$__rate_interval])\",\n+                    \"legendFormat\": \"__auto\",\n+                    \"range\": true,\n+                    \"refId\": \"A\"\n+                }\n+            ],\n+            \"title\": \"Prefill rate tok/s\",\n+            \"type\": \"timeseries\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"palette-classic\"\n+                    },\n+                    \"custom\": {\n+                        \"axisBorderShow\": false,\n+                        \"axisCenteredZero\": false,\n+                        \"axisColorMode\": \"text\",\n+                        \"axisLabel\": \"\",\n+                        \"axisPlacement\": \"auto\",\n+                        \"barAlignment\": 0,\n+                        \"barWidthFactor\": 0.6,\n+                        \"drawStyle\": \"line\",\n+                        \"fillOpacity\": 0,\n+                        \"gradientMode\": \"none\",\n+                        \"hideFrom\": {\n+                            \"legend\": false,\n+                            \"tooltip\": false,\n+                            \"viz\": false\n+                        },\n+                        \"insertNulls\": false,\n+                        \"lineInterpolation\": \"linear\",\n+                        \"lineWidth\": 1,\n+                        \"pointSize\": 5,\n+                        \"scaleDistribution\": {\n+                            \"type\": \"linear\"\n+                        },\n+                        \"showPoints\": \"auto\",\n+                        \"spanNulls\": false,\n+                        \"stacking\": {\n+                            \"group\": \"A\",\n+                            \"mode\": \"none\"\n+                        },\n+                        \"thresholdsStyle\": {\n+                            \"mode\": \"off\"\n+                        }\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 80\n+                            }\n+                        ]\n+                    }\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 12,\n+                \"x\": 0,\n+                \"y\": 16\n+            },\n+            \"id\": 9,\n+            \"options\": {\n+                \"legend\": {\n+                    \"calcs\": [],\n+                    \"displayMode\": \"list\",\n+                    \"placement\": \"bottom\",\n+                    \"showLegend\": true\n+                },\n+                \"tooltip\": {\n+                    \"hideZeros\": false,\n+                    \"mode\": \"single\",\n+                    \"sort\": \"none\"\n+                }\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"histogram_quantile(0.95, sum by(le) (rate(batch_fill_percentage_percent_bucket[$__rate_interval])))\",\n+                    \"legendFormat\": \"p95\",\n+                    \"range\": true,\n+                    \"refId\": \"A\"\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"histogram_quantile(0.99, sum by(le) (rate(batch_fill_percentage_percent_bucket[$__rate_interval])))\",\n+                    \"hide\": false,\n+                    \"instant\": false,\n+                    \"legendFormat\": \"p99\",\n+                    \"range\": true,\n+                    \"refId\": \"B\"\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"histogram_quantile(0.5, sum by(le) (rate(batch_fill_percentage_percent_bucket[$__rate_interval])))\",\n+                    \"hide\": false,\n+                    \"instant\": false,\n+                    \"legendFormat\": \"p50\",\n+                    \"range\": true,\n+                    \"refId\": \"C\"\n+                }\n+            ],\n+            \"title\": \"Batch fill percentage percentiles\",\n+            \"type\": \"timeseries\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"description\": \"KV Cache Memory Usage Over Time\",\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"palette-classic\"\n+                    },\n+                    \"custom\": {\n+                        \"axisBorderShow\": false,\n+                        \"axisCenteredZero\": false,\n+                        \"axisColorMode\": \"text\",\n+                        \"axisLabel\": \"\",\n+                        \"axisPlacement\": \"auto\",\n+                        \"barAlignment\": 0,\n+                        \"barWidthFactor\": 0.6,\n+                        \"drawStyle\": \"line\",\n+                        \"fillOpacity\": 20,\n+                        \"gradientMode\": \"none\",\n+                        \"hideFrom\": {\n+                            \"legend\": false,\n+                            \"tooltip\": false,\n+                            \"viz\": false\n+                        },\n+                        \"insertNulls\": false,\n+                        \"lineInterpolation\": \"linear\",\n+                        \"lineWidth\": 2,\n+                        \"pointSize\": 5,\n+                        \"scaleDistribution\": {\n+                            \"type\": \"linear\"\n+                        },\n+                        \"showPoints\": \"auto\",\n+                        \"spanNulls\": false,\n+                        \"stacking\": {\n+                            \"group\": \"A\",\n+                            \"mode\": \"none\"\n+                        },\n+                        \"thresholdsStyle\": {\n+                            \"mode\": \"off\"\n+                        }\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 80\n+                            }\n+                        ]\n+                    },\n+                    \"unit\": \"bytes\"\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 12,\n+                \"x\": 12,\n+                \"y\": 16\n+            },\n+            \"id\": 4,\n+            \"options\": {\n+                \"legend\": {\n+                    \"calcs\": [],\n+                    \"displayMode\": \"list\",\n+                    \"placement\": \"bottom\",\n+                    \"showLegend\": true\n+                },\n+                \"tooltip\": {\n+                    \"hideZeros\": false,\n+                    \"mode\": \"single\",\n+                    \"sort\": \"none\"\n+                }\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"kv_cache_memory_bytes\",\n+                    \"fullMetaSearch\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"Used memory\",\n+                    \"range\": true,\n+                    \"refId\": \"A\",\n+                    \"useBackend\": false\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"kv_cache_free_memory_bytes\",\n+                    \"fullMetaSearch\": false,\n+                    \"hide\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"instant\": false,\n+                    \"legendFormat\": \"free memory\",\n+                    \"range\": true,\n+                    \"refId\": \"B\",\n+                    \"useBackend\": false\n+                }\n+            ],\n+            \"title\": \"KV Cache Memory Usage Over Time\",\n+            \"type\": \"timeseries\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"thresholds\"\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 80\n+                            }\n+                        ]\n+                    },\n+                    \"unit\": \"ms\"\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 12,\n+                \"x\": 0,\n+                \"y\": 24\n+            },\n+            \"id\": 8,\n+            \"options\": {\n+                \"displayMode\": \"gradient\",\n+                \"legend\": {\n+                    \"calcs\": [],\n+                    \"displayMode\": \"list\",\n+                    \"placement\": \"bottom\",\n+                    \"showLegend\": false\n+                },\n+                \"maxVizHeight\": 300,\n+                \"minVizHeight\": 10,\n+                \"minVizWidth\": 0,\n+                \"namePlacement\": \"auto\",\n+                \"orientation\": \"auto\",\n+                \"reduceOptions\": {\n+                    \"calcs\": [\n+                        \"lastNotNull\"\n+                    ],\n+                    \"fields\": \"\",\n+                    \"values\": false\n+                },\n+                \"showUnfilled\": true,\n+                \"sizing\": \"auto\",\n+                \"valueMode\": \"color\"\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"histogram_quantile(0.95, sum by(le) (rate(ttft_milliseconds_bucket[$__rate_interval])))\",\n+                    \"fullMetaSearch\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"p95\",\n+                    \"range\": true,\n+                    \"refId\": \"A\",\n+                    \"useBackend\": false\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"histogram_quantile(0.5, sum by(le) (rate(ttft_milliseconds_bucket[$__rate_interval])))\",\n+                    \"fullMetaSearch\": false,\n+                    \"hide\": false,\n+                    \"includeNullMetadata\": true,\n+                    \"legendFormat\": \"p50\",\n+                    \"range\": true,\n+                    \"refId\": \"B\",\n+                    \"useBackend\": false\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"disableTextWrap\": false,\n+                    \"editorMode\": \"builder\",\n+                    \"expr\": \"histogram_quantile(0.99, sum by(le) (rate(ttft_milliseconds_bucket[$__rate_interval])))\",\n+                    \"fullMetaSearch\": false,\n+                    \"hide\": false,\n+                    \"includeNullMetadata\": false,\n+                    \"instant\": false,\n+                    \"legendFormat\": \"p99\",\n+                    \"range\": true,\n+                    \"refId\": \"C\",\n+                    \"useBackend\": false\n+                }\n+            ],\n+            \"title\": \"Time to First Token (TTFT)\",\n+            \"type\": \"bargauge\"\n+        },\n+        {\n+            \"datasource\": {\n+                \"type\": \"prometheus\",\n+                \"uid\": \"PBFA97CFB590B2093\"\n+            },\n+            \"fieldConfig\": {\n+                \"defaults\": {\n+                    \"color\": {\n+                        \"mode\": \"palette-classic\"\n+                    },\n+                    \"custom\": {\n+                        \"axisBorderShow\": false,\n+                        \"axisCenteredZero\": false,\n+                        \"axisColorMode\": \"text\",\n+                        \"axisLabel\": \"\",\n+                        \"axisPlacement\": \"auto\",\n+                        \"barAlignment\": 0,\n+                        \"barWidthFactor\": 0.6,\n+                        \"drawStyle\": \"line\",\n+                        \"fillOpacity\": 0,\n+                        \"gradientMode\": \"none\",\n+                        \"hideFrom\": {\n+                            \"legend\": false,\n+                            \"tooltip\": false,\n+                            \"viz\": false\n+                        },\n+                        \"insertNulls\": false,\n+                        \"lineInterpolation\": \"linear\",\n+                        \"lineWidth\": 1,\n+                        \"pointSize\": 5,\n+                        \"scaleDistribution\": {\n+                            \"type\": \"linear\"\n+                        },\n+                        \"showPoints\": \"auto\",\n+                        \"spanNulls\": false,\n+                        \"stacking\": {\n+                            \"group\": \"A\",\n+                            \"mode\": \"none\"\n+                        },\n+                        \"thresholdsStyle\": {\n+                            \"mode\": \"off\"\n+                        }\n+                    },\n+                    \"mappings\": [],\n+                    \"thresholds\": {\n+                        \"mode\": \"absolute\",\n+                        \"steps\": [\n+                            {\n+                                \"color\": \"green\"\n+                            },\n+                            {\n+                                \"color\": \"red\",\n+                                \"value\": 80\n+                            }\n+                        ]\n+                    },\n+                    \"unit\": \"ms\"\n+                },\n+                \"overrides\": []\n+            },\n+            \"gridPos\": {\n+                \"h\": 8,\n+                \"w\": 12,\n+                \"x\": 12,\n+                \"y\": 24\n+            },\n+            \"id\": 12,\n+            \"options\": {\n+                \"legend\": {\n+                    \"calcs\": [],\n+                    \"displayMode\": \"list\",\n+                    \"placement\": \"bottom\",\n+                    \"showLegend\": true\n+                },\n+                \"tooltip\": {\n+                    \"hideZeros\": false,\n+                    \"mode\": \"single\",\n+                    \"sort\": \"none\"\n+                }\n+            },\n+            \"pluginVersion\": \"12.0.0\",\n+            \"targets\": [\n+                {\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"histogram_quantile(0.5, sum by(le) (rate(request_latency_milliseconds_bucket[$__rate_interval])))\",\n+                    \"legendFormat\": \"p50\",\n+                    \"range\": true,\n+                    \"refId\": \"A\"\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"histogram_quantile(0.95, sum by(le) (rate(request_latency_milliseconds_bucket[$__rate_interval])))\",\n+                    \"hide\": false,\n+                    \"instant\": false,\n+                    \"legendFormat\": \"p95\",\n+                    \"range\": true,\n+                    \"refId\": \"B\"\n+                },\n+                {\n+                    \"datasource\": {\n+                        \"type\": \"prometheus\",\n+                        \"uid\": \"PBFA97CFB590B2093\"\n+                    },\n+                    \"editorMode\": \"code\",\n+                    \"expr\": \"histogram_quantile(0.99, sum by(le) (rate(request_latency_milliseconds_bucket[$__rate_interval])))\",\n+                    \"hide\": false,\n+                    \"instant\": false,\n+                    \"legendFormat\": \"p99\",\n+                    \"range\": true,\n+                    \"refId\": \"C\"\n+                }\n+            ],\n+            \"title\": \"Request latency percentiles\",\n+            \"type\": \"timeseries\"\n+        }\n+    ],\n+    \"preload\": false,\n+    \"refresh\": \"5s\",\n+    \"schemaVersion\": 41,\n+    \"tags\": [],\n+    \"templating\": {\n+        \"list\": []\n+    },\n+    \"time\": {\n+        \"from\": \"now-15m\",\n+        \"to\": \"now\"\n+    },\n+    \"timepicker\": {},\n+    \"timezone\": \"\",\n+    \"title\": \"Transformers Continuous Batching Metrics\",\n+    \"uid\": \"Lw6CTvVSz\",\n+    \"version\": 5\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "936f4a894ced25a1ed7a93fc71b240a947fd44c9",
            "filename": "examples/metrics-monitoring/docker-compose.yml",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fdocker-compose.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fdocker-compose.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Fdocker-compose.yml?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,55 @@\n+services:\n+  memcached:\n+    image: memcached:1.6.29\n+    container_name: memcached\n+    ports:\n+      - \"11211:11211\"\n+    environment:\n+      - MEMCACHED_MAX_MEMORY=64m # Set the maximum memory usage\n+      - MEMCACHED_THREADS=4 # Number of threads to use\n+\n+  prometheus:\n+    image: prom/prometheus:latest\n+    command:\n+      - \"--config.file=/etc/prometheus/prometheus.yml\"\n+      - --web.enable-otlp-receiver # Enable OTLP receiver\n+      - --web.enable-remote-write-receiver\n+      - --enable-feature=exemplar-storage\n+      - --enable-feature=native-histograms\n+    volumes:\n+      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n+    ports:\n+      - \"9090:9090\"\n+\n+  tempo:\n+    image: grafana/tempo:latest\n+    command: [ \"-config.file=/etc/tempo.yaml\" ]\n+    volumes:\n+      - ./tempo.yaml:/etc/tempo.yaml\n+    ports:\n+      - \"14268:14268\" # jaeger ingest\n+      - \"3200:3200\" # tempo\n+      - \"9095:9095\" # tempo grpc\n+      - \"4317:4317\" # otlp grpc\n+      - \"4318:4318\" # otlp http\n+      - \"9411:9411\" # zipkin\n+    depends_on:\n+      - memcached\n+\n+  grafana:\n+    image: grafana/grafana:latest\n+    volumes:\n+      - ./continuous-batching-dashboard.json:/etc/grafana/provisioning/dashboards/continuous-batching-dashboard.json\n+      - ./grafana-dashboard.yaml:/etc/grafana/provisioning/dashboards/grafana-dashboard.yaml\n+      - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml\n+    environment:\n+      - GF_AUTH_ANONYMOUS_ENABLED=true\n+      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin\n+      - GF_AUTH_DISABLE_LOGIN_FORM=true\n+      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor metricsSummary\n+      - GF_INSTALL_PLUGINS=https://storage.googleapis.com/integration-artifacts/grafana-exploretraces-app/grafana-exploretraces-app-latest.zip;grafana-traces-app\n+    ports:\n+      - \"3000:3000\"\n+    depends_on:\n+      - prometheus\n+      - tempo"
        },
        {
            "sha": "6dd396d00e16aec028837b9ce9a3f23f61454659",
            "filename": "examples/metrics-monitoring/grafana-dashboard.yaml",
            "status": "added",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fgrafana-dashboard.yaml",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fgrafana-dashboard.yaml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Fgrafana-dashboard.yaml?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,11 @@\n+apiVersion: 1\n+\n+providers:\n+  - name: 'Transformers Dashboards'\n+    orgId: 1\n+    folder: 'Transformers'\n+    type: file\n+    disableDeletion: false\n+    editable: true\n+    options:\n+      path: /etc/grafana/provisioning/dashboards"
        },
        {
            "sha": "e3f2e78becea840e921037484189e2ca10b571f1",
            "filename": "examples/metrics-monitoring/grafana-datasources.yaml",
            "status": "added",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fgrafana-datasources.yaml",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fgrafana-datasources.yaml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Fgrafana-datasources.yaml?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,14 @@\n+apiVersion: 1\n+\n+datasources:\n+  - name: Prometheus\n+    type: prometheus\n+    access: proxy\n+    url: http://prometheus:9090\n+    isDefault: true\n+\n+  - name: Tempo\n+    type: tempo\n+    access: proxy\n+    url: http://tempo:3200\n+    uid: tempo"
        },
        {
            "sha": "df3551b68d4819992c1ac704a2533aafd28a68a5",
            "filename": "examples/metrics-monitoring/metrics_example.py",
            "status": "added",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fmetrics_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fmetrics_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Fmetrics_example.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,48 @@\n+# Example usage of the trace and attach_tracer decorators\n+\n+from transformers.utils.metrics import attach_tracer, traced\n+\n+\n+@attach_tracer()\n+class ExampleClass:\n+    def __init__(self, name):\n+        # The attach_tracer decorator has already created self.tracer for us\n+        self.name = name\n+\n+    @traced  # This method will use the tracer from the class instance\n+    def process_data(self, data):\n+        # This method is traced and can use self.tracer\n+        return f\"Processed {data} with {self.name}\"\n+\n+    @traced(span_name=\"custom_operation\")  # With custom span name\n+    def special_operation(self, value):\n+        # Also traced, with a custom span name\n+        return value * 2\n+\n+    @traced(\n+        additional_attributes=[\n+            (\"name\", \"object.name\", lambda x: x.upper()),  # Using a transform function\n+            (\"name\", \"object.fixed_value\", \"static_value\"),  # Using a fixed value\n+        ]\n+    )\n+    def operation_with_attributes(self):\n+        # This will add the specified attributes to the span\n+        return \"Operation completed\"\n+\n+\n+# For functions without a class, the traced decorator still works\n+@traced\n+def standalone_function(arg1, arg2):\n+    # For functions, a tracer is created based on the module name\n+    return arg1 + arg2\n+\n+\n+# Usage:\n+if __name__ == \"__main__\":\n+    # With OpenTelemetry configured, these will produce traces\n+    example = ExampleClass(\"test_object\")\n+    example.process_data(\"sample\")\n+    example.special_operation(42)\n+    example.operation_with_attributes()\n+\n+    result = standalone_function(1, 2)"
        },
        {
            "sha": "6c578ad89f518a839d7992028599ef578bf56f1a",
            "filename": "examples/metrics-monitoring/prometheus.yml",
            "status": "added",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fprometheus.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Fprometheus.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Fprometheus.yml?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,3 @@\n+global:\n+  scrape_interval: 15s\n+"
        },
        {
            "sha": "353b83e1cccf60393e9c2c72e0460ea70468a894",
            "filename": "examples/metrics-monitoring/tempo.yaml",
            "status": "added",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Ftempo.yaml",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fmetrics-monitoring%2Ftempo.yaml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmetrics-monitoring%2Ftempo.yaml?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,90 @@\n+stream_over_http_enabled: true\n+server:\n+  http_listen_port: 3200\n+  log_level: info\n+\n+\n+cache:\n+  background:\n+    writeback_goroutines: 5\n+  caches:\n+  - roles:\n+    - frontend-search\n+    memcached:\n+      addresses: dns+memcached:11211\n+\n+query_frontend:\n+  search:\n+    duration_slo: 5s\n+    throughput_bytes_slo: 1.073741824e+09\n+    metadata_slo:\n+        duration_slo: 5s\n+        throughput_bytes_slo: 1.073741824e+09\n+  trace_by_id:\n+    duration_slo: 100ms\n+  metrics:\n+    max_duration: 200h                # maximum duration of a metrics query, increase for local setups\n+    query_backend_after: 5m\n+    duration_slo: 5s\n+    throughput_bytes_slo: 1.073741824e+09\n+\n+distributor:\n+  receivers:                           # this configuration will listen on all ports and protocols that tempo is capable of.\n+    jaeger:                            # the receives all come from the OpenTelemetry collector.  more configuration information can\n+      protocols:                       # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver\n+        thrift_http:                   #\n+          endpoint: \"tempo:14268\"      # for a production deployment you should only enable the receivers you need!\n+        grpc:\n+          endpoint: \"tempo:14250\"\n+        thrift_binary:\n+          endpoint: \"tempo:6832\"\n+        thrift_compact:\n+          endpoint: \"tempo:6831\"\n+    zipkin:\n+      endpoint: \"tempo:9411\"\n+    otlp:\n+      protocols:\n+        grpc:\n+          endpoint: \"tempo:4317\"\n+        http:\n+          endpoint: \"tempo:4318\"\n+    opencensus:\n+      endpoint: \"tempo:55678\"\n+\n+ingester:\n+  max_block_duration: 5m               # cut the headblock when this much time passes. this is being set for demo purposes and should probably be left alone normally\n+\n+compactor:\n+  compaction:\n+    block_retention: 720h                # overall Tempo trace retention. set for demo purposes\n+\n+metrics_generator:\n+  registry:\n+    external_labels:\n+      source: tempo\n+      cluster: docker-compose\n+  storage:\n+    path: /var/tempo/generator/wal\n+    remote_write:\n+      - url: http://prometheus:9090/api/v1/write\n+        send_exemplars: true\n+  traces_storage:\n+    path: /var/tempo/generator/traces\n+  processor:\n+    local_blocks:\n+      filter_server_spans: false\n+      flush_to_storage: true\n+\n+storage:\n+  trace:\n+    backend: local                     # backend configuration to use\n+    wal:\n+      path: /var/tempo/wal             # where to store the wal locally\n+    local:\n+      path: /var/tempo/blocks\n+\n+overrides:\n+  defaults:\n+    metrics_generator:\n+      processors: [service-graphs, span-metrics, local-blocks] # enables metrics generator\n+      generate_native_histograms: both"
        },
        {
            "sha": "9aaa836f7baee55340a2daedba2070225f8e61e6",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "added",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,109 @@\n+import time\n+\n+import datasets\n+import torch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.generation import GenerationConfig\n+\n+\n+torch.set_float32_matmul_precision(\"high\")\n+\n+model_id = \"meta-llama/Llama-3.2-3b-Instruct\"\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id, attn_implementation=\"sdpa_paged\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n+).eval()\n+tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n+\n+generation_config = GenerationConfig(\n+    max_new_tokens=512,\n+    eos_token_id=tokenizer.eos_token_id,\n+    pad_token_id=tokenizer.pad_token_id,\n+    use_cache=False,\n+    num_blocks=2048,\n+    block_size=128,\n+    do_sample=True,\n+    max_batch_tokens=1024,  # Maximum number of tokens to process in a single batch\n+    scheduler=\"prefill_first\",\n+)\n+\n+train_dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n+\n+# --- Example 1: Simple Version using generate_batch ---\n+print(\"--- Running CB Generation Example ---\")\n+\n+\n+def tokenize_function(examples):\n+    return tokenizer(examples[\"question\"])\n+\n+\n+tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n+simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n+\n+start_time_simple = time.time()\n+# model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\", fullgraph=True)\n+batch_outputs = model.generate_batch(\n+    inputs=simple_batch_inputs,\n+    generation_config=generation_config,\n+)\n+end_time_simple = time.time()\n+\n+for request in batch_outputs:\n+    input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=False)\n+    try:\n+        output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=False)\n+    except Exception as e:\n+        print(f\"Decoding failed for request {request}: {e}\")\n+        output_text = tokenizer.decode(batch_outputs[request].generated_tokens[1:], skip_special_tokens=False)\n+    if len(output_text) > 0:\n+        print(\"-\" * 20)\n+        print(f\"{request} Input:  {input_text}\")\n+        print(f\"{request} Output: {output_text}\")\n+    else:\n+        print(\"\", end=\"\\r\\r\\r\\r\")\n+print(\"-\" * 20)\n+print(\"--- Finished CB Generation Example ---\\n\\n\")\n+\n+\n+print(f\"CB generation took: {end_time_simple - start_time_simple:.2f} seconds\")\n+\n+\n+# train_dataset = train_dataset.select(range(5))  # Use only 5 examples for the simple version\n+\n+# tokenized_test_prompts = tokenizer(_TEST_PROMPTS, padding=True, padding_side=\"left\", truncation=True, max_length=512)\n+# simple_batch_inputs = list(tokenized_test_prompts[\"input_ids\"])\n+\n+# def tokenize_function(examples):\n+#     # Truncate to avoid overly long prompts exceeding max context length\n+#     return tokenizer(examples[\"question\"], padding=True, truncation=True, max_length=512)\n+\n+\n+# tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n+# simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n+\n+\n+# model.config.attn_implementation = \"sdpa\"\n+# start_time_simple = time.time()\n+# batch_size = 64\n+# full_outputs = []\n+# from tqdm import tqdm\n+\n+# for i in tqdm(range(0, len(simple_batch_inputs)-batch_size, batch_size)):\n+#     outputs = model.generate(\n+#         torch.tensor(simple_batch_inputs[i:i+batch_size], device=model.device),\n+#         generation_config=GenerationConfig(\n+#             max_new_tokens=16, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id\n+#         ),\n+#     )\n+#     full_outputs.extend(outputs.tolist())\n+\n+# end_time_simple = time.time()\n+# print(f\"\\nSimple batch generation took: {end_time_simple - start_time_simple:.2f} seconds\")\n+\n+# print(\"\\nResults from simple generate_batch:\")\n+# for i, request in enumerate(full_outputs):\n+#     output_text = tokenizer.decode(request, skip_special_tokens=False)\n+#     print(\"-\" * 20)\n+#     print(f\"  Output: {output_text}\")\n+# print(\"-\" * 20)\n+# print(\"--- Finished Simple Batch Generation Example ---\\n\\n\")"
        },
        {
            "sha": "2b74308081ef1f5af5df23013808c5c13c046a84",
            "filename": "setup.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -201,6 +201,9 @@\n     \"pytest-rich\",\n     \"libcst\",\n     \"rich\",\n+    \"opentelemetry-api\",\n+    \"opentelemetry-exporter-otlp\",\n+    \"opentelemetry-sdk\",\n ]\n \n \n@@ -435,6 +438,9 @@ def run(self):\n \n extras[\"benchmark\"] = deps_list(\"optimum-benchmark\")\n \n+# OpenTelemetry dependencies for metrics collection in continuous batching\n+extras[\"open-telemetry\"] = deps_list(\"opentelemetry-api\", \"opentelemetry-exporter-otlp\", \"opentelemetry-sdk\")\n+\n # when modifying the following list, make sure to update src/transformers/dependency_versions_check.py\n install_requires = [\n     deps[\"filelock\"],  # filesystem locks, e.g., to prevent parallel downloads"
        },
        {
            "sha": "5c0ae6b772f37d456a12e9b732a0d7cb1328ab5f",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -103,4 +103,7 @@\n     \"pytest-rich\": \"pytest-rich\",\n     \"libcst\": \"libcst\",\n     \"rich\": \"rich\",\n+    \"opentelemetry-api\": \"opentelemetry-api\",\n+    \"opentelemetry-exporter-otlp\": \"opentelemetry-exporter-otlp\",\n+    \"opentelemetry-sdk\": \"opentelemetry-sdk\",\n }"
        },
        {
            "sha": "64ebfe6fc7c3fdfbb973b9112dbec8aaddc34309",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -97,6 +97,9 @@\n         \"validate_stopping_criteria\",\n         \"StopStringCriteria\",\n     ]\n+    _import_structure[\"continuous_batching\"] = [\n+        \"ContinuousMixin\",\n+    ]\n     _import_structure[\"utils\"] = [\n         \"GenerationMixin\",\n         \"GreedySearchEncoderDecoderOutput\",\n@@ -213,6 +216,7 @@\n             EarlyExitCandidateGenerator,\n             PromptLookupCandidateGenerator,\n         )\n+        from .continuous_batching import ContinuousMixin\n         from .logits_process import (\n             AlternatingCodebooksLogitsProcessor,\n             ClassifierFreeGuidanceLogitsProcessor,"
        },
        {
            "sha": "a1aa6fa2ec9bd41c1252912a0885be6dfa528eed",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "added",
            "additions": 1446,
            "deletions": 0,
            "changes": 1446,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,1446 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import logging\n+import queue\n+import statistics\n+import threading\n+import time\n+from abc import ABC, abstractmethod\n+from collections import deque\n+from dataclasses import dataclass, field\n+from enum import Enum\n+from functools import partial\n+from typing import Deque, Dict, List, Optional, Set, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.profiler import profile, schedule, tensorboard_trace_handler\n+from tqdm import tqdm\n+\n+from ..cache_utils import Cache\n+from ..configuration_utils import PretrainedConfig\n+from ..generation.configuration_utils import GenerationConfig\n+from ..utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n+\n+\n+class RequestStatus(Enum):\n+    \"\"\"Status of a generation request through its lifecycle.\"\"\"\n+\n+    PENDING = \"pending\"\n+    PREFILLING = \"prefilling\"\n+    PREFILLING_SPLIT = \"prefilling_split\"\n+    SPLIT_PENDING_REMAINDER = \"split_pending_remainder\"\n+    DECODING = \"decoding\"\n+    FINISHED = \"finished\"\n+    FAILED = \"failed\"\n+\n+\n+# Setup your logger\n+logger = logging.getLogger(__name__)\n+logger.setLevel(logging.WARNING)\n+\n+\n+@dataclass\n+class GenerationOutput:\n+    \"\"\"Tracks the output of a generation request.\n+\n+    Attributes:\n+        request_id (str): The ID of the generation request.\n+        prompt_ids (List[int]): The IDs of the prompt tokens.\n+        generated_tokens (List[int]): The generated tokens.\n+        logprobs (List[float]): The log probabilities of the generated tokens.\n+        error (Optional[str]): Any error message associated with the request. When None, the request was successful.\n+    \"\"\"\n+\n+    request_id: str\n+    prompt_ids: List[int] = field(default_factory=list)\n+    generated_tokens: List[int] = field(default_factory=list)\n+    logprobs: List[float] = field(default_factory=list)\n+    error: Optional[str] = None\n+    status: RequestStatus = RequestStatus.PENDING\n+    created_time: float = field(default_factory=time.time)\n+\n+\n+@dataclass\n+class RequestState:\n+    \"\"\"Tracks the state of a generation request through its lifecycle.\n+\n+    Attributes:\n+        status (RequestStatus): can be one of PENDING, PREFILLING, PREFILLING_SPLIT,\n+                                SPLIT_PENDING_REMAINDER, DECODING, FINISHED, FAILED\n+    \"\"\"\n+\n+    # Required fields\n+    request_id: str\n+    prompt_ids: Optional[List[int]] = None  # the one being processed\n+    full_prompt_ids: Optional[List[int]] = None  # the full prompt\n+    remaining_prompt_ids: List[int] = field(default_factory=list)  # For split requests\n+    static_outputs: List[int] = field(default_factory=list)\n+    allocated_blocks: List[int] = field(default_factory=list)\n+    position_offset: int = 0  # Current position in the sequence for position_ids\n+    status: RequestStatus = RequestStatus.PENDING\n+    max_new_tokens: int = 20\n+    eos_token_id: int = -1\n+    created_time: float = field(default_factory=time.time)\n+    error: Optional[str] = None\n+\n+    def current_len(self) -> int:\n+        \"\"\"Get the current length of the sequence (prompt + generated tokens).\"\"\"\n+        return self.position_offset\n+\n+    def generated_len(self) -> int:\n+        \"\"\"Get the number of tokens generated so far.\"\"\"\n+        return len(self.static_outputs)\n+\n+    @traced\n+    def update_with_token(self, token_id: int) -> bool:\n+        \"\"\"Update the request with a newly generated token and check for completion.\n+\n+        Args:\n+            token_id: The token ID to add to the output sequence\n+\n+        Returns:\n+            bool: True if the request is now complete, False otherwise\n+        \"\"\"\n+        # Only update if we're in decoding state\n+        if self.status != RequestStatus.DECODING:\n+            return False\n+\n+        is_eos = token_id == self.eos_token_id and self.eos_token_id != -1\n+        is_max_len = self.generated_len() >= self.max_new_tokens\n+\n+        if is_eos or is_max_len:\n+            self.status = RequestStatus.FINISHED\n+            return True\n+        return False\n+\n+    def __repr__(self):\n+        return f\"RequestState(\\n\\trequest_id={self.request_id},\\n\\tstatus={self.status},\\n\\tout_tokens={self.generated_len()},\\n\\tquery_length={len(self.prompt_ids)}, \\n\\tremaining_tokens={len(self.remaining_prompt_ids)}, \\n\\tkv_length={self.position_offset}\\n\\tfull_prompt_lenght={len(self.full_prompt_ids)},\\n\\tallocated_blocks={self.allocated_blocks},\\n\\tgenerated_tokens={self.static_outputs}\\n)\"\n+\n+    def to_generation_output(self):\n+        \"\"\"Convert the request state to a GenerationOutput object.\"\"\"\n+        return GenerationOutput(\n+            request_id=self.request_id,\n+            prompt_ids=self.full_prompt_ids,\n+            status=self.status,\n+            generated_tokens=self.static_outputs,\n+            logprobs=[],\n+            error=self.error,\n+        )\n+\n+\n+@attach_tracer()\n+class PagedAttentionCache(Cache):\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        generation_config: GenerationConfig,\n+        device: torch.device,\n+        dtype: torch.dtype = torch.float16,\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n+        initial_prompt_shapes: Optional[List[List[int]]] = None,\n+    ) -> None:\n+        \"\"\"Initialize a paged attention cache for efficient memory usage.\n+\n+        Args:\n+            config: Model configuration\n+            generation_config: Generation configuration containing cache parameters\n+            device: Device for the cache tensors\n+            dtype: Data type for the cache tensors\n+            layer_device_map: Optional mapping of layer indices to devices\n+            initial_prompt_shapes: Optional sample prompts to help calculate optimal cache size\n+        \"\"\"\n+        # Extract model dimensions\n+        self.num_key_value_heads = (\n+            config.num_attention_heads\n+            if getattr(config, \"num_key_value_heads\", None) is None\n+            else config.num_key_value_heads\n+        )\n+        self.head_dim = (\n+            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n+        )\n+        self.num_hidden_layers = config.num_hidden_layers\n+\n+        # Calculate optimal block size and number if not provided\n+        num_blocks = getattr(generation_config, \"num_blocks\", None)\n+        block_size = getattr(generation_config, \"block_size\", None)\n+        if num_blocks is None or block_size is None:\n+            logger.info(\"Calculating optimal block size and number...\")\n+            num_blocks, block_size = compute_optimal_blocks(\n+                device, config, generation_config, initial_prompt_shapes or [], dtype, median_prefill_length=200\n+            )\n+            logger.info(f\"Using calculated num_blocks={num_blocks}, block_size={block_size}\")\n+\n+        self.block_size = block_size\n+        self.num_blocks = num_blocks\n+        self.cache_shape = (self.num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n+\n+        self.dtype = dtype\n+        self.device = device\n+\n+        self.key_cache: List[torch.Tensor] = []\n+        self.value_cache: List[torch.Tensor] = []\n+        for idx in range(config.num_hidden_layers):\n+            layer_device = layer_device_map[idx] if layer_device_map is not None else device\n+            new_layer_key_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n+            new_layer_value_cache = torch.zeros(self.cache_shape, dtype=self.dtype, device=layer_device)\n+            # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n+            # preventing compiled graph breaks when updating the cache.\n+            torch._dynamo.mark_static_address(new_layer_key_cache)\n+            torch._dynamo.mark_static_address(new_layer_value_cache)\n+            self.key_cache.append(new_layer_key_cache)\n+            self.value_cache.append(new_layer_value_cache)\n+\n+        # Block management data structures\n+        self._free_blocks = deque(range(num_blocks))\n+        self._block_tables: Dict[str, List[int]] = {}\n+\n+    @traced\n+    def allocate_blocks(self, n_blocks: int, request_id: str) -> List[int]:\n+        \"\"\"Allocates n_blocks for a given request_id.\"\"\"\n+        if len(self._free_blocks) < n_blocks:\n+            return False\n+\n+        allocated = []\n+        for _ in range(n_blocks):\n+            allocated.append(self._free_blocks.popleft())\n+\n+        if request_id not in self._block_tables:\n+            self._block_tables[request_id] = []\n+        self._block_tables[request_id].extend(allocated)\n+        return allocated\n+\n+    @traced\n+    def free_blocks(self, request_id: str) -> None:\n+        \"\"\"Frees all blocks associated with a request_id.\"\"\"\n+        if request_id in self._block_tables:\n+            blocks_to_free = self._block_tables.pop(request_id)\n+            self._free_blocks.extend(blocks_to_free)\n+        else:\n+            logger.warning(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n+\n+    def get_num_free_blocks(self) -> int:\n+        \"\"\"Returns the number of free blocks available.\"\"\"\n+        return len(self._free_blocks)\n+\n+    def get_block_table(self, request_id: str) -> List[int]:\n+        \"\"\"Returns the block table for a request.\"\"\"\n+        return self._block_tables.get(request_id, [])\n+\n+    @traced\n+    def _get_physical_indices(self, state: RequestState, logical_indices: List[int]) -> List[int]:\n+        \"\"\"\n+        Maps logical sequence indices to physical cache indices using the block table, using PyTorch.\n+\n+        Args:\n+            request_id: The request ID.\n+            logical_indices: A list of logical indices.\n+\n+        Returns:\n+            A list of physical indices.\n+\n+        Raises:\n+            ValueError: If no block table is found for the request ID.\n+            IndexError: If a logical index maps to a block index that is out of bounds.\n+        \"\"\"\n+        request_id = state.request_id\n+        block_table = self._block_tables.get(request_id)\n+        if not block_table:\n+            raise ValueError(f\"No block table found for request {request_id}\")\n+\n+        block_size = self.block_size\n+        physical_indices = []\n+\n+        for idx in logical_indices:\n+            block_idx = idx // block_size\n+            block_offset = idx % block_size\n+\n+            if block_idx >= len(block_table):\n+                raise IndexError(\n+                    f\"Logical index {idx} maps to block index {block_idx} which is out of bounds \"\n+                    f\"for request {request_id}\"\n+                )\n+\n+            physical_block_num = block_table[block_idx]\n+            physical_index = physical_block_num * block_size + block_offset\n+            physical_indices.append(physical_index)\n+\n+        return physical_indices\n+\n+    @traced\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        read_index,\n+        write_index,\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        # Reshape cache for easier indexing\n+        total_slots = self.num_blocks * self.block_size\n+        k_cache_flat = self.key_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n+        v_cache_flat = self.value_cache[layer_idx].view(self.num_key_value_heads, total_slots, self.head_dim)\n+        k_cache_flat[:, write_index, :] = key_states[0]\n+        v_cache_flat[:, write_index, :] = value_states[0]\n+        return k_cache_flat[None, :, read_index, :], v_cache_flat[None, :, read_index, :]\n+\n+\n+class Scheduler(ABC):\n+    \"\"\"\n+    Abstract base class for scheduling requests in the continuous batch processor.\n+    It is expected that cache allocation and scheduling logic will be implemented in subclasses.\n+    \"\"\"\n+\n+    def __init__(self, cache: PagedAttentionCache):\n+        self.active_requests: Dict[str, RequestState] = {}\n+        self.waiting_requests: Dict[str, RequestState] = {}\n+        self.waiting_requests_order: Deque[str] = deque()\n+        self.cache = cache\n+\n+    @abstractmethod\n+    def add_waiting_request(self, state: RequestState):\n+        \"\"\"Add a request to the waiting list.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def schedule_batch(self, token_budget: int) -> List[RequestState]:\n+        pass\n+\n+    @traced\n+    def has_pending_requests(self) -> bool:\n+        \"\"\"Check if there are requests ready to be processed.\"\"\"\n+        return self.active_requests or self.waiting_requests\n+\n+    @abstractmethod\n+    def finish_request(self, state: RequestState):\n+        \"\"\"Finish processing a request and free its allocated blocks.\"\"\"\n+        pass\n+\n+    @traced\n+    def get_active_request_static_outputs(self, request_id: str) -> List[int]:\n+        if request_id in self.active_requests:\n+            return self.active_requests[request_id].static_outputs\n+        return []\n+\n+\n+@attach_tracer()\n+class FIFOScheduler(Scheduler):\n+    @traced\n+    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n+        # 1. we check that the occupancy is less than the requested length\n+        # 2. we allocate enough blocks to cover the requested length\n+        current_len = state.current_len()\n+        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n+        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n+            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n+            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n+            if not allocated:\n+                return False\n+            state.allocated_blocks.extend(allocated)\n+        return True\n+\n+    @traced(span_name=\"prepare_request\")\n+    def _prepare_request_for_processing(\n+        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: Set[str]\n+    ):\n+        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n+        request_tokens = (\n+            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n+        )\n+        if len(request_tokens) < token_budget:\n+            # Can process the entire prompt/remainder\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING\n+                state.prompt_ids = state.remaining_prompt_ids\n+                state.remaining_prompt_ids = []\n+        else:\n+            # Need to split the request\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+            state.remaining_prompt_ids = request_tokens[token_budget:]\n+            state.prompt_ids = request_tokens[:token_budget]\n+\n+    @traced\n+    def add_waiting_request(self, state: RequestState):\n+        \"\"\"Add a request to the waiting list.\"\"\"\n+        self.waiting_requests[state.request_id] = state\n+        self.waiting_requests_order.append(state.request_id)\n+\n+    @traced\n+    def schedule_batch(self, token_budget: int) -> List[RequestState]:\n+        priority_states: List[RequestState] = []\n+        second_priority_states: List[RequestState] = []\n+        scheduled_requests = []\n+\n+        for state in self.active_requests.values():\n+            if state.status == RequestStatus.DECODING:\n+                priority_states.append(state)\n+            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                second_priority_states.append(state)\n+\n+        # Add waiting requests to second priority\n+        for req_id in self.waiting_requests_order:\n+            second_priority_states.append(self.waiting_requests[req_id])\n+\n+        candidates = priority_states + second_priority_states\n+        request_ids_to_remove_from_waiting = set()\n+\n+        for state in candidates:\n+            self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n+            request_len = len(state.prompt_ids)\n+            if not self._allocate_blocks_if_needed(\n+                state, len(state.prompt_ids)\n+            ):  # don't schedule if we can't allocate blocks\n+                if len(self.cache._free_blocks) == 0:\n+                    break\n+                continue\n+\n+            @traced\n+            def _add_to_scheduled_requests(state: RequestState):\n+                scheduled_requests.append(state)\n+\n+            _add_to_scheduled_requests(state)\n+\n+            token_budget -= request_len\n+\n+            @traced\n+            def _remove_from_waiting_requests(state: RequestState):\n+                req_id = state.request_id\n+                if req_id in self.waiting_requests:\n+                    del self.waiting_requests[req_id]\n+                    request_ids_to_remove_from_waiting.add(req_id)\n+\n+            _remove_from_waiting_requests(state)\n+\n+            if token_budget == 0:\n+                break\n+\n+        self.waiting_requests_order = deque(\n+            [req_id for req_id in self.waiting_requests_order if req_id not in request_ids_to_remove_from_waiting]\n+        )\n+\n+        return scheduled_requests\n+\n+    @traced\n+    def finish_request(self, state: RequestState):\n+        request_id = state.request_id\n+        self.cache.free_blocks(request_id)\n+        if request_id in self.active_requests:\n+            del self.active_requests[request_id]\n+\n+\n+@attach_tracer()\n+class PrefillFirstScheduler(Scheduler):\n+    @traced\n+    def _allocate_blocks_if_needed(self, state: RequestState, len_next_tokens: int):\n+        # 1. we check that the occupancy is less than the requested length\n+        # 2. we allocate enough blocks to cover the requested length\n+        current_len = state.current_len()\n+        occupancy = len(state.allocated_blocks) * self.cache.block_size - current_len\n+        if occupancy < len_next_tokens or (len(state.allocated_blocks) == 0):\n+            blocks_needed = ((len_next_tokens - occupancy + 1) // self.cache.block_size) + 1\n+            allocated = self.cache.allocate_blocks(blocks_needed, state.request_id)\n+            if not allocated:\n+                return False\n+            state.allocated_blocks.extend(allocated)\n+        return True\n+\n+    @traced(span_name=\"prepare_request\")\n+    def _prepare_request_for_processing(\n+        self, state: RequestState, token_budget: int, request_ids_to_remove_from_waiting: Set[str]\n+    ):\n+        \"\"\"Prepare a request for processing in the current batch.\"\"\"\n+        request_tokens = (\n+            state.remaining_prompt_ids if state.status == RequestStatus.SPLIT_PENDING_REMAINDER else state.prompt_ids\n+        )\n+        if len(request_tokens) < token_budget:\n+            # Can process the entire prompt/remainder\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING\n+                state.prompt_ids = state.remaining_prompt_ids\n+                state.remaining_prompt_ids = []\n+        else:\n+            # Need to split the request\n+            if state.status == RequestStatus.PENDING:\n+                self.active_requests[state.request_id] = state\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+                request_ids_to_remove_from_waiting.add(state.request_id)\n+            elif state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                state.status = RequestStatus.PREFILLING_SPLIT\n+            state.remaining_prompt_ids = request_tokens[token_budget:]\n+            state.prompt_ids = request_tokens[:token_budget]\n+\n+    @traced\n+    def add_waiting_request(self, state: RequestState):\n+        \"\"\"Add a request to the waiting list.\"\"\"\n+        self.waiting_requests[state.request_id] = state\n+        self.waiting_requests_order.append(state.request_id)\n+\n+    @traced\n+    def schedule_batch(self, token_budget: int) -> List[RequestState]:\n+        priority_states: List[RequestState] = []\n+        second_priority_states: List[RequestState] = []\n+        scheduled_requests = []\n+\n+        for state in self.active_requests.values():\n+            if state.status == RequestStatus.SPLIT_PENDING_REMAINDER:\n+                priority_states.append(state)\n+            elif state.status == RequestStatus.DECODING:\n+                second_priority_states.append(state)\n+\n+        for req_id in self.waiting_requests_order:\n+            second_priority_states.append(self.waiting_requests[req_id])\n+\n+        candidates = priority_states + second_priority_states\n+\n+        request_ids_to_remove_from_waiting = set()\n+\n+        for state in candidates:\n+            self._prepare_request_for_processing(state, token_budget, request_ids_to_remove_from_waiting)\n+            request_len = len(state.prompt_ids)\n+            if not self._allocate_blocks_if_needed(\n+                state, len(state.prompt_ids)\n+            ):  # don't schedule if we can't allocate blocks\n+                if len(self.cache._free_blocks) == 0:\n+                    break\n+                continue\n+\n+            @traced\n+            def _add_to_scheduled_requests(state: RequestState):\n+                scheduled_requests.append(state)\n+\n+            _add_to_scheduled_requests(state)\n+\n+            token_budget -= request_len\n+\n+            @traced\n+            def _remove_from_waiting_requests(state: RequestState):\n+                req_id = state.request_id\n+                if req_id in self.waiting_requests:\n+                    del self.waiting_requests[req_id]\n+                    request_ids_to_remove_from_waiting.add(req_id)\n+\n+            _remove_from_waiting_requests(state)\n+\n+            if token_budget == 0:\n+                break\n+\n+        self.waiting_requests_order = deque(\n+            [req_id for req_id in self.waiting_requests_order if req_id not in request_ids_to_remove_from_waiting]\n+        )\n+\n+        return scheduled_requests\n+\n+    @traced\n+    def finish_request(self, state: RequestState):\n+        request_id = state.request_id\n+        self.cache.free_blocks(request_id)\n+        if request_id in self.active_requests:\n+            del self.active_requests[request_id]\n+\n+\n+@traced(standalone=True)\n+def compute_optimal_blocks(\n+    device: torch.device,\n+    config: PretrainedConfig,\n+    generation_config: GenerationConfig,\n+    inputs: List[List[int]],\n+    dtype: torch.dtype = torch.bfloat16,\n+    safety_margin: float = 0.9,\n+    median_prefill_length: Optional[int] = None,\n+):\n+    \"\"\"Calculate optimal number and size of blocks for the KV cache.\n+\n+    Args:\n+        device: The device where the model runs\n+        config: The model configuration\n+        generation_config: The generation configuration\n+        inputs: Sample input sequences to estimate memory requirements\n+        dtype: Data type for cache tensors\n+        safety_margin: Fraction of available memory to use\n+        median_prefill_length: Override for median prefill length calculation\n+\n+    Returns:\n+        Tuple of (num_blocks, block_size)\n+    \"\"\"\n+    # Extract model dimensions\n+    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+    num_kv_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+    num_hidden_layers = getattr(config, \"num_hidden_layers\", 40)\n+\n+    # Get available device memory\n+    if device.type == \"cuda\":\n+        device_properties = torch.cuda.get_device_properties(device)\n+        total_memory = device_properties.total_memory\n+        allocated_memory = torch.cuda.memory_allocated(device)\n+        reserved_memory = torch.cuda.memory_reserved(device)\n+        available_memory = total_memory - max(allocated_memory, reserved_memory)\n+    elif device.type == \"mps\":\n+        logger.warning(\"MPS memory estimation is approximate. Using conservative defaults.\")\n+        return 2048, 256\n+    else:\n+        logger.warning(f\"Unsupported device type {device.type} for optimal block calculation. Using defaults.\")\n+        return 32, 128\n+\n+    # Apply safety margin\n+    available_memory = int(available_memory * safety_margin)\n+    if available_memory <= 0:\n+        logger.warning(\"Not enough available memory. Using minimum configuration.\")\n+        return 8, 128  # Minimum viable configuration\n+\n+    # Calculate memory per token\n+    dtype_size = torch.tensor([], dtype=dtype).element_size()\n+    memory_per_token = 2 * num_kv_heads * head_dim * dtype_size * num_hidden_layers  # For K and V caches\n+\n+    # Estimate sequence length requirements\n+    tokens_to_generate = getattr(generation_config, \"max_new_tokens\", 20)\n+\n+    if median_prefill_length is None and inputs:\n+        non_empty_inputs = [len(seq) for seq in inputs if seq]\n+        median_prefill_length = int(statistics.median(non_empty_inputs)) if non_empty_inputs else 64\n+    elif median_prefill_length is None:\n+        median_prefill_length = 64  # Reasonable default if no inputs provided\n+\n+    # Total sequence length including generated tokens\n+    seq_length = median_prefill_length + tokens_to_generate\n+\n+    # Calculate block parameters\n+    MIN_BLOCK_SIZE = 16\n+\n+    # Estimate number of concurrent sequences\n+    per_sequence_memory = seq_length * memory_per_token\n+    max_concurrent_sequences = max(1, int(available_memory // per_sequence_memory))\n+\n+    # Total tokens that can fit in memory\n+    total_tokens = available_memory // memory_per_token\n+\n+    # Calculate block size (rounded to power of 2)\n+    initial_block_size = max(MIN_BLOCK_SIZE, total_tokens // (max_concurrent_sequences * 2))\n+    block_size = 1 << (initial_block_size - 1).bit_length()  # Round to power of 2\n+\n+    # Calculate number of blocks\n+    num_blocks = max(1, total_tokens // block_size)\n+\n+    logger.info(\n+        f\"Optimal cache: {num_blocks} blocks of size {block_size} \"\n+        f\"(can handle ~{num_blocks * block_size // seq_length} sequences of length {seq_length})\"\n+    )\n+\n+    return int(num_blocks), int(block_size)\n+\n+\n+@dataclass\n+class PagedAttentionArgs:\n+    input_ids: torch.Tensor\n+    attention_mask: torch.Tensor\n+    position_ids: torch.Tensor\n+    cumulative_seqlens_q: torch.Tensor\n+    cumulative_seqlens_k: torch.Tensor\n+    max_seqlen_q: int\n+    max_seqlen_k: int\n+    write_index: torch.Tensor\n+    read_index: torch.Tensor\n+    logits_indices: torch.Tensor\n+    block_tables: Dict[str, List[int]]\n+    cache: PagedAttentionCache\n+    use_cache: bool = False\n+\n+\n+@traced\n+def create_document_mask(cumulative_seqlens_q, cumulative_seqlens_k):\n+    # Number of documents\n+    valid_docs_q = cumulative_seqlens_q[1:] > cumulative_seqlens_q[:-1]\n+    valid_docs_k = cumulative_seqlens_k[1:] > cumulative_seqlens_k[:-1]\n+    num_valid_docs = min(valid_docs_q.sum(), valid_docs_k.sum())\n+\n+    # Trim to valid docs\n+    cumulative_seqlens_q = cumulative_seqlens_q[: num_valid_docs + 1]\n+    cumulative_seqlens_k = cumulative_seqlens_k[: num_valid_docs + 1]\n+\n+    total_q = cumulative_seqlens_q[-1]\n+    total_k = cumulative_seqlens_k[-1]\n+\n+    q_indices = torch.arange(total_q, device=cumulative_seqlens_q.device)\n+    k_indices = torch.arange(total_k, device=cumulative_seqlens_k.device)\n+\n+    q_doc_ids = torch.bucketize(q_indices, cumulative_seqlens_q[1:], right=True)\n+    k_doc_ids = torch.bucketize(k_indices, cumulative_seqlens_k[1:], right=False)\n+    doc_mask = q_doc_ids[:, None] == k_doc_ids[None, :]\n+    # apply causal mask where no decoding (same nb of q than k)\n+\n+    is_causal = ~(cumulative_seqlens_q[1:] - cumulative_seqlens_q[:-1] == 1) * cumulative_seqlens_q[1:]\n+    apply_causal = torch.bucketize(q_indices, is_causal, right=True)[:, None] == k_doc_ids\n+    # TODO don't apply on prefill splitting\n+    causal_mask = torch.triu(torch.ones(total_q, total_k, device=q_doc_ids.device), diagonal=1).bool()\n+    doc_mask.masked_fill_((apply_causal & causal_mask), False)\n+    return doc_mask\n+\n+\n+# Continuous Batch Processor (Internal Logic)\n+@attach_tracer()\n+class ContinuousBatchProcessor:\n+    def __init__(\n+        self,\n+        cache: PagedAttentionCache,\n+        config: PretrainedConfig,\n+        generation_config: GenerationConfig,\n+        input_queue: queue.Queue,\n+        output_queue: queue.Queue,\n+        stop_event: threading.Event,\n+        model_device: torch.device,\n+        model_dtype: torch.dtype,\n+        scheduler: Scheduler,\n+        streaming: bool = False,\n+    ):\n+        \"\"\"Initialize the continuous batch processor.\n+\n+        Args:\n+            cache: The paged attention cache to use\n+            generation_config: The generation configuration\n+            input_queue: Queue for incoming requests\n+            output_queue: Queue for outgoing results\n+            stop_event: Event to signal processing should stop\n+            model_device: Device for model inputs/outputs\n+            model_dtype: Data type for model inputs/outputs\n+            streaming: Whether to stream tokens as they're generated\n+        \"\"\"\n+        self.cache = cache\n+        self.config = config\n+        self.generation_config = generation_config\n+        self.input_queue = input_queue\n+        self.output_queue = output_queue\n+        self.stop_event = stop_event\n+        self.model_device = model_device\n+        self.model_dtype = model_dtype\n+        self.scheduler = scheduler\n+        self.streaming = streaming\n+\n+        self.requests_in_batch: List[RequestState] = []\n+\n+        # Get batch size parameters from generation config\n+        self._configure_batch_parameters()\n+\n+        # Set up metrics collector\n+        self.metrics = ContinuousBatchProcessorMetrics(self.max_batch_tokens)\n+\n+        self.setup_static_tensors()\n+\n+    @traced(standalone=True)\n+    def setup_static_tensors(self):\n+        T = self.max_batch_tokens\n+        max_token_budget = self.cache.num_blocks * self.cache.block_size\n+        tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n+        self.tensor_metadata = tensor_metadata\n+        self.input_ids = torch.zeros((1, T), **tensor_metadata)\n+        self.position_ids = torch.zeros((1, T), **tensor_metadata)\n+        self.attention_mask = torch.zeros(\n+            (1, 1, T, max_token_budget), dtype=self.model_dtype, device=self.model_device\n+        )\n+        self.cumulative_seqlens_q = torch.zeros((T + 1,), **tensor_metadata)\n+        self.cumulative_seqlens_k = torch.zeros((T + 1,), **tensor_metadata)\n+        self.write_index = torch.zeros((T,), **tensor_metadata)\n+        self.read_index = torch.zeros((max_token_budget,), **tensor_metadata)\n+        self.logits_indices = torch.full((T,), -1, **tensor_metadata)\n+        self.max_seqlen_q = 0\n+        self.max_seqlen_k = 0\n+        self.output_ids = torch.full((1, T), -1, **tensor_metadata)\n+\n+    @traced\n+    @torch.no_grad()\n+    @torch.compile()\n+    def reset_static_tensors(self):\n+        \"\"\"Reset static tensors for the next batch.\"\"\"\n+        self.input_ids.zero_()\n+        self.position_ids.zero_()\n+        self.attention_mask.fill_(torch.finfo(self.model_dtype).min)\n+        self.cumulative_seqlens_q.zero_()\n+        self.cumulative_seqlens_k.zero_()\n+        self.write_index.fill_(-1)\n+        self.read_index.fill_(-1)\n+        self.logits_indices.fill_(-1)\n+        self.max_seqlen_q = 0\n+        self.max_seqlen_k = 0\n+        self.output_ids.zero_()\n+\n+    def get_model_kwargs(self) -> PagedAttentionArgs:\n+        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n+        # torch.set_printoptions(threshold=100000,linewidth=10000)\n+        return {\n+            \"input_ids\": self.input_ids,\n+            \"position_ids\": self.position_ids,\n+            \"attention_mask\": self.attention_mask,\n+            \"cumulative_seqlens_q\": self.cumulative_seqlens_q,\n+            \"cumulative_seqlens_k\": self.cumulative_seqlens_k,\n+            \"write_index\": self.write_index,\n+            \"read_index\": self.read_index,\n+            \"logits_indices\": self.logits_indices,\n+            \"max_seqlen_q\": self.max_seqlen_q,\n+            \"max_seqlen_k\": self.max_seqlen_k,\n+            \"block_tables\": self.cache._block_tables,\n+            \"cache\": self.cache,\n+            \"use_cache\": False,\n+        }\n+\n+    def __repr__(self):\n+        return (\n+            f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n+            + self.get_model_kwargs().__repr__()\n+        )\n+\n+    @traced(standalone=True)\n+    def _configure_batch_parameters(self):\n+        \"\"\"Set up batch processing parameters based on generation config.\"\"\"\n+        # Calculate total cache capacity\n+        total_cache_tokens = self.cache.num_blocks * self.cache.block_size\n+\n+        # Get or calculate max tokens per batch\n+        user_batch_tokens = getattr(self.generation_config, \"max_batch_tokens\", None)\n+        if user_batch_tokens is not None:\n+            self.max_batch_tokens = user_batch_tokens\n+        else:\n+            # Default to 1/8 of total cache capacity, adjusted for context\n+            self.max_context_len = getattr(self.generation_config, \"max_position_embeddings\", 2048)\n+            recommended_batch_size = min(total_cache_tokens // 8, self.max_context_len)\n+            self.max_batch_tokens = max(64, recommended_batch_size)\n+\n+        # Context length and EOS token\n+        self.max_context_len = getattr(self.generation_config, \"max_position_embeddings\", 2048)\n+\n+    @traced\n+    def _get_new_requests(self):\n+        \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n+        while not self.input_queue.empty():\n+            try:\n+                state = self.input_queue.get_nowait()\n+                if state is None:  # Sentinel value\n+                    continue\n+                self.scheduler.add_waiting_request(state)\n+\n+            except queue.Empty:\n+                break\n+            except Exception as e:\n+                logger.error(f\"Error processing new request: {e}\", exc_info=True)\n+                state: RequestState = locals().get(\"state\")\n+                if state is not None:\n+                    self._handle_request_error(e, state)\n+\n+    @traced\n+    def _handle_request_error(self, error, state: RequestState):\n+        \"\"\"Handle general request processing error.\"\"\"\n+        state.status = RequestStatus.FAILED\n+        state.error = str(error)\n+\n+        # Include any generated tokens if this is an active request\n+        if isinstance(state.request_id, str):\n+            state.static_outputs = self.scheduler.get_active_request_static_outputs(state.request_id)\n+        else:\n+            state.static_outputs = []\n+\n+        self.metrics.record_request_completion(state.created_time, state.request_id)\n+        self.output_queue.put(state.to_generation_output())\n+\n+    @traced\n+    def prepare_next_batch(self):\n+        \"\"\"Prepare tensors and metadata for the next model forward pass.\"\"\"\n+        # Get new requests from the queue\n+        self._get_new_requests()\n+        if not self.scheduler.has_pending_requests():\n+            return None\n+\n+        self.metrics.record_queue_metrics(len(self.scheduler.active_requests), len(self.scheduler.waiting_requests))\n+\n+        self.requests_in_batch = self.scheduler.schedule_batch(self.max_batch_tokens)\n+        if not self.requests_in_batch:\n+            return None\n+\n+        # Get the request objects for this batch\n+        self.reset_static_tensors()\n+        position_ids = []\n+        input_ids = []\n+        read_index = []\n+        write_index = []\n+        cumulative_seqlens_q = [0]\n+        cumulative_seqlens_k = [0]\n+        logits_indices = []\n+        self.metrics.record_batch_metrics(self.requests_in_batch)\n+\n+        for state in self.requests_in_batch:\n+            next_input_ids = state.prompt_ids\n+            input_ids.extend(next_input_ids)\n+            past_length = state.position_offset\n+            query_length = len(next_input_ids)\n+            key_length = query_length + past_length\n+            cache_index = list(range(key_length))\n+\n+            positions_to_add = cache_index[past_length:]\n+            read_indices = self.cache._get_physical_indices(state, cache_index)\n+            write_indices = read_indices[-query_length:]\n+\n+            position_ids.extend(positions_to_add)\n+            read_index.extend(read_indices)\n+            write_index.extend(write_indices)\n+            cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n+            cumulative_seqlens_k.append(cumulative_seqlens_k[-1] + key_length)\n+            if len(state.remaining_prompt_ids) == 0:\n+                logits_indices.append(cumulative_seqlens_q[-1] - 1)\n+            self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n+            self.max_seqlen_k = max(self.max_seqlen_k, key_length)\n+            state.position_offset += query_length\n+\n+        logger.warning(\n+            f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. cum KV: {cumulative_seqlens_k[-1]}, free blocks: {self.cache.get_num_free_blocks()}\"\n+        )\n+        self._build_tensors(\n+            input_ids,\n+            position_ids,\n+            read_index,\n+            write_index,\n+            cumulative_seqlens_q,\n+            cumulative_seqlens_k,\n+            logits_indices,\n+        )\n+\n+        self.metrics.record_kv_cache_memory_metrics(self.cache)\n+\n+    @traced\n+    def _build_tensors(\n+        self,\n+        input_ids,\n+        position_ids,\n+        read_index,\n+        write_index,\n+        cumulative_seqlens_q,\n+        cumulative_seqlens_k,\n+        logits_indices,\n+    ):\n+        to_tensor = partial(torch.tensor, **self.tensor_metadata)\n+        self.input_ids[:, : len(input_ids)] = to_tensor(input_ids)\n+        self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n+        self.write_index[: len(write_index)] = to_tensor(write_index)\n+        self.read_index[: len(read_index)] = to_tensor(read_index)\n+        self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n+        self.cumulative_seqlens_k[: len(cumulative_seqlens_k)] = to_tensor(cumulative_seqlens_k)\n+        self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n+        min_value = torch.finfo(self.model_dtype).min\n+        if self.config._attn_implementation != \"paged_attention\":  # we set `is_causal` to True in paged call`\n+            for i in range(len(cumulative_seqlens_q) - 1):\n+                if (\n+                    cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n+                    < cumulative_seqlens_k[i + 1] - cumulative_seqlens_k[i]\n+                    and cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i] >= 1\n+                ):\n+                    diagonal = (\n+                        cumulative_seqlens_k[i + 1] - (cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]) + 1\n+                    )\n+                    diagonal = diagonal - cumulative_seqlens_k[i]\n+                else:\n+                    diagonal = 1\n+                query_range = slice(cumulative_seqlens_q[i], cumulative_seqlens_q[i + 1])\n+                key_range = slice(cumulative_seqlens_k[i], cumulative_seqlens_k[i + 1])\n+\n+                mask = torch.triu(\n+                    torch.full(\n+                        self.attention_mask[..., query_range, key_range].shape,\n+                        min_value,\n+                        dtype=self.model_dtype,\n+                        device=self.model_device,\n+                    ),\n+                    diagonal=diagonal,\n+                )\n+                self.attention_mask[..., query_range, key_range] = mask\n+\n+    @traced\n+    def _sync(self):\n+        return self.output_ids.tolist()[0]  # should be the only synch we do\n+\n+    @traced\n+    def _maybe_send_output(self, state: RequestState, token: int):\n+        \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n+        if self.streaming:\n+            state.next_token = token\n+            self.output_queue.put(state.to_generation_output())\n+        elif state.status == RequestStatus.FINISHED:\n+            self.output_queue.put(state.to_generation_output())\n+\n+    @traced\n+    def update_batch(self):\n+        \"\"\"Update request states based on generated tokens.\"\"\"\n+        out_tokens = self._sync()\n+        finished_request_ids = []\n+        for i, state in enumerate(self.requests_in_batch):\n+            req_id = state.request_id\n+            if len(state.remaining_prompt_ids) == 0:\n+                self.metrics.record_ttft_metric(state.created_time, state.request_id)\n+                state.status = RequestStatus.DECODING\n+                token = out_tokens[self.logits_indices[i]]\n+                state.static_outputs.extend([token])\n+                state.prompt_ids = [token]\n+                if state.update_with_token(token):\n+                    self.metrics.record_request_completion(state.created_time, state.request_id)\n+                    self.scheduler.finish_request(state)\n+                    finished_request_ids.append(req_id)\n+                self._maybe_send_output(state, token)\n+            elif state.status == RequestStatus.PREFILLING_SPLIT:\n+                state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n+\n+    @traced\n+    def has_pending_requests(self) -> bool:\n+        \"\"\"Check if there are any active or waiting requests.\"\"\"\n+        return self.scheduler.has_pending_requests()\n+\n+    @traced\n+    def handle_batch_error(self, error):\n+        \"\"\"Handle errors during batch processing.\"\"\"\n+        failed_reqs = self.requests_in_batch\n+        for req in failed_reqs:\n+            self._handle_request_error(error, req)\n+            self.scheduler.finish_request(req)\n+\n+    @traced\n+    def fail_all_requests(self, error):\n+        \"\"\"Fail all active requests with the given error.\n+\n+        Args:\n+            error: The error to report in the failure message\n+        \"\"\"\n+        for state in self.scheduler.active_requests.values():\n+            self._handle_request_error(error, state)\n+            self.scheduler.finish_request(state)\n+\n+        # Also fail any requests in the waiting queue\n+        for req_id in list(self.scheduler.waiting_requests.keys()):\n+            state = self.scheduler.waiting_requests.pop(req_id)\n+            self._handle_request_error(error, state)\n+\n+        # Clear the ordering queue\n+        self.scheduler.waiting_requests_order.clear()\n+\n+\n+SCHEDULER_MAPPING = {\n+    \"fifo\": FIFOScheduler,\n+    \"prefill_first\": PrefillFirstScheduler,\n+}\n+\n+\n+# Manager Class (User Interface)\n+@attach_tracer()\n+class ContinuousBatchingManager:\n+    \"\"\"Manager for handling continuous batching of generation requests.\n+\n+    This class provides the user interface for submitting generation requests,\n+    retrieving results, and managing the background generation thread.\n+    \"\"\"\n+\n+    def __init__(self, model, generation_config: GenerationConfig, max_queue_size=0, streaming: bool = True):\n+        \"\"\"Initialize the continuous batching manager.\n+\n+        Args:\n+            model: The language model for generation\n+            generation_config: Configuration for generation parameters\n+            max_queue_size: Maximum size of the request queue (0 = unlimited)\n+            streaming: Whether to stream tokens as they are generated\n+        \"\"\"\n+        self.model = model\n+        self.generation_config = generation_config\n+        self.input_queue = queue.Queue(maxsize=max_queue_size)\n+        self.output_queue = queue.Queue()\n+        self.stop_event = threading.Event()\n+        self.streaming = streaming\n+        self.log_prob_generation = getattr(generation_config, \"log_prob_generation\", False)\n+        self._generation_thread = None\n+        self._request_counter = 0\n+        self._request_lock = threading.Lock()\n+        self.model.generation_config.top_p = None\n+        self.do_sample = getattr(generation_config, \"do_sample\", True)\n+        self.logit_processor = self.model._get_logits_processor(self.model.generation_config)\n+        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n+        self.profile = getattr(generation_config, \"profile\", False)\n+\n+    @traced\n+    def start(self):\n+        \"\"\"Start the background generation thread.\"\"\"\n+        if self._generation_thread is not None and self._generation_thread.is_alive():\n+            logger.warning(\"Manager thread is already running.\")\n+            return\n+\n+        self._result_queue = queue.Queue()\n+        self._generation_thread = threading.Thread(target=self._run_generation_loop)\n+        self._generation_thread.start()\n+        logger.info(\"Continuous batching manager started.\")\n+\n+    def is_running(self):\n+        \"\"\"Check if the background generation thread is running.\"\"\"\n+        return self._generation_thread is not None and self._generation_thread.is_alive()\n+\n+    def stop(self, block: bool = False, timeout: Optional[float] = None):\n+        \"\"\"Signal the background thread to stop.\n+\n+        Args:\n+            block: Whether to wait for the thread to stop\n+            timeout: Maximum time to wait for the thread to stop\n+        \"\"\"\n+        if self._generation_thread is None:\n+            logger.warning(\"Manager not started.\")\n+            return\n+\n+        if not self.stop_event.is_set():\n+            self.stop_event.set()\n+            logger.info(\"Stopping continuous batching manager...\")\n+\n+        if block:\n+            self.join(timeout)\n+\n+    def join(self, timeout: Optional[float] = None):\n+        \"\"\"Wait for the background thread to finish.\n+\n+        Args:\n+            timeout: Maximum time to wait for the thread to stop\n+        \"\"\"\n+        if self._generation_thread is not None:\n+            self._generation_thread.join(timeout=timeout)\n+            if self._generation_thread.is_alive():\n+                logger.warning(\"Generation thread did not exit after join timeout.\")\n+            else:\n+                logger.info(\"Continuous Batching Manager stopped.\")\n+                self._generation_thread = None\n+\n+    def add_request(\n+        self, input_ids: List[int], request_id: Optional[str] = None, max_new_tokens: Optional[int] = None\n+    ) -> str:\n+        \"\"\"Add a new generation request to the queue.\n+\n+        Args:\n+            input_ids: Input token IDs to use as prompt\n+            request_id: Optional custom request ID (auto-generated if None)\n+            **kwargs: Additional generation parameters\n+\n+        Returns:\n+            str: The request ID\n+        \"\"\"\n+        if request_id is None:\n+            with self._request_lock:\n+                request_id = f\"req_{self._request_counter}\"\n+                self._request_counter += 1\n+\n+        max_new_tokens = self.generation_config.max_new_tokens if max_new_tokens is None else max_new_tokens\n+\n+        state = RequestState(\n+            request_id=request_id,\n+            prompt_ids=list(input_ids),\n+            full_prompt_ids=list(input_ids),\n+            max_new_tokens=max_new_tokens,\n+            eos_token_id=self.generation_config.eos_token_id,\n+        )\n+\n+        # Use block=True with timeout to handle backpressure if queue is full\n+        self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n+        logger.debug(f\"Added request {request_id} to queue.\")\n+        return request_id\n+\n+    def add_requests(self, inputs: List[List[int]], **kwargs):\n+        for i, input_ids in enumerate(inputs):\n+            # Assign a predictable request ID for ordering results later\n+            req_id = f\"batch_req_{i}\"\n+            self.add_request(input_ids, request_id=req_id, **kwargs)\n+\n+    def get_result(self, timeout=None) -> Optional[GenerationOutput]:\n+        \"\"\"Retrieve one result from the output queue.\n+\n+        Args:\n+            timeout: Maximum time to wait for a result\n+\n+        Returns:\n+            Optional[Dict]: The result data or None if timeout\n+        \"\"\"\n+        if self._generation_thread is None and self.output_queue.empty():\n+            return None\n+        try:\n+            result = self.output_queue.get(block=True, timeout=timeout)\n+            logger.debug(f\"Retrieved result for request {result.request_id}\")\n+            return result\n+        except queue.Empty:\n+            return None\n+\n+    def __iter__(self):\n+        \"\"\"Iterate over results as they become available.\"\"\"\n+        while (\n+            self._generation_thread is not None and self._generation_thread.is_alive() or not self.output_queue.empty()\n+        ):\n+            result = self.get_result(timeout=0.1)  # allow the model to run for 10 seconds\n+            if result is not None:\n+                yield result\n+\n+    @traced\n+    def warmup(self, batch_processor):\n+        stream = torch.cuda.Stream()\n+        stream.wait_stream(torch.cuda.current_stream())\n+        with torch.cuda.stream(stream):\n+            # Warmup the model with a dummy forward pass\n+            self._generation_step(batch_processor)\n+        torch.cuda.current_stream().wait_stream(stream)\n+\n+        self.graph = torch.cuda.CUDAGraph()\n+        with torch.cuda.graph(self.graph):\n+            self._generation_step(batch_processor)\n+\n+    @traced\n+    # @torch.compile\n+    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n+        \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n+        batch_data = batch_processor.get_model_kwargs()\n+        with torch.no_grad():\n+            logits = self._model_forward(batch_data)\n+            if self.log_prob_generation:\n+                batch_processor.output_probs.copy_(logits)  # TODO\n+            probs = self._process_logit(batch_data, logits)\n+            self._sample(batch_processor, probs)\n+\n+    @traced(span_name=\"model_forward\")\n+    def _model_forward(self, batch_data):\n+        return self.model(**batch_data).logits\n+\n+    @traced(span_name=\"logit_processing\")\n+    @torch.compile()\n+    def _process_logit(self, batch_data, logits):\n+        return self.logit_processor(batch_data[\"input_ids\"], logits)\n+\n+    @traced(span_name=\"sampling\")\n+    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n+        if self.do_sample:  # sample\n+            probs = nn.functional.softmax(probs, dim=-1)\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(1)\n+        else:\n+            next_tokens = torch.argmax(probs, dim=-1)\n+        batch_processor.output_ids.copy_(next_tokens)\n+\n+    def _run_generation_loop(self):\n+        \"\"\"Main processing loop running in the background thread.\"\"\"\n+        batch_processor = None\n+        try:\n+            paged_attention_cache = PagedAttentionCache(\n+                self.model.config,\n+                self.generation_config,\n+                self.model.device,\n+                self.model.dtype,\n+            )\n+\n+            scheduler = SCHEDULER_MAPPING.get(self.generation_config.scheduler)\n+            if scheduler is None:\n+                logger.warning(f\"Scheduler '{scheduler}' not found. Defaulting to FIFO.\")\n+                scheduler = FIFOScheduler\n+\n+            batch_processor = ContinuousBatchProcessor(\n+                paged_attention_cache,\n+                self.model.config,\n+                self.generation_config,\n+                self.input_queue,\n+                self.output_queue,\n+                self.stop_event,\n+                self.model.device,\n+                self.model.dtype,\n+                scheduler(paged_attention_cache),\n+                self.streaming,\n+            )\n+            is_first = True\n+\n+            if self.profile:\n+                tracing_schedule = schedule(skip_first=2, warmup=3, active=200, repeat=100, wait=1)\n+                trace_handler = tensorboard_trace_handler(\n+                    dir_name=\"/fsx/arthur/transformers\", use_gzip=True, worker_name=\"paged_compile\"\n+                )\n+                activities = [\n+                    torch.profiler.ProfilerActivity.CPU,\n+                    torch.profiler.ProfilerActivity.CUDA,\n+                ]\n+                with profile(\n+                    activities=activities,\n+                    schedule=tracing_schedule,\n+                    on_trace_ready=trace_handler,\n+                    record_shapes=False,\n+                    with_stack=True,\n+                ) as prof:\n+                    while not self.stop_event.is_set() or batch_processor.has_pending_requests():\n+                        self._inner_generation_loop(batch_processor, is_first)\n+                        if is_first:\n+                            is_first = False\n+                        prof.step()\n+            else:\n+                while not self.stop_event.is_set() or batch_processor.has_pending_requests():\n+                    self._inner_generation_loop(batch_processor, is_first)\n+                    if is_first:\n+                        is_first = False\n+\n+        except Exception as e:\n+            logger.error(f\"Error in generation loop: {e}\", exc_info=True)\n+            self._handle_critical_error(e, batch_processor)\n+        finally:\n+            logger.info(\"Generation loop finished.\")\n+\n+    @traced(span_name=\"generation_loop\")\n+    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor, is_first: bool = False):\n+        if torch.cuda.is_available():\n+            torch.cuda.synchronize()\n+        batch_processor.prepare_next_batch()\n+        if torch.cuda.is_available() and self.use_cuda_graph:\n+            if is_first:\n+                self.warmup(batch_processor)\n+            elif hasattr(self, \"graph\"):\n+                try:\n+                    self._graph_replay()\n+                except Exception as e:\n+                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n+                    batch_processor.handle_batch_error(e)\n+                    return\n+            else:\n+                self._generation_step(batch_processor)\n+        else:\n+            self._generation_step(batch_processor)\n+        if torch.cuda.is_available():\n+            torch.cuda.synchronize()\n+        batch_processor.update_batch()\n+\n+    @traced(span_name=\"graph_replay\")\n+    def _graph_replay(self):\n+        self.graph.replay()\n+\n+    @traced\n+    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n+        \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n+        # Signal stop\n+        self.stop_event.set()\n+\n+        # Fail pending requests in input queue\n+        try:\n+            while True:\n+                req_data = self.input_queue.get_nowait()\n+                if batch_processor is not None:\n+                    batch_processor._handle_request_error(error, req_data)\n+        except queue.Empty:\n+            pass\n+\n+        # Fail active requests\n+        if batch_processor is not None:\n+            batch_processor.fail_all_requests(error)\n+\n+\n+class ContinuousMixin:\n+    \"\"\"Mixin class for models to add continuous batching capabilities.\"\"\"\n+\n+    def init_continuous_batching(\n+        self,\n+        generation_config: Optional[GenerationConfig] = None,\n+        max_queue_size: int = 0,\n+        scheduler: str = \"fifo\",\n+        streaming: bool = False,\n+    ) -> ContinuousBatchingManager:\n+        \"\"\"Initialize a manager for continuous batching inference.\n+\n+        Args:\n+            generation_config: Custom generation configuration\n+            max_queue_size: Maximum size of the input request queue\n+            streaming: Whether to stream tokens as they are generated\n+\n+        Returns:\n+            `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n+        \"\"\"\n+        if not hasattr(self, \"config\") or not hasattr(self, \"device\") or not hasattr(self, \"dtype\"):\n+            raise AttributeError(\"Model must have 'config', 'device', and 'dtype' attributes.\")\n+\n+        gen_config = generation_config if generation_config is not None else self.generation_config\n+        if gen_config is None:\n+            raise ValueError(\"A GenerationConfig must be provided or set in the model.\")\n+\n+        if gen_config.eos_token_id is None:\n+            logger.warning(\"`eos_token_id` not set in GenerationConfig. Setting to -1 (disabled).\")\n+            gen_config.eos_token_id = -1\n+\n+        # Create and return the manager\n+        return ContinuousBatchingManager(\n+            model=self, generation_config=gen_config, max_queue_size=max_queue_size, streaming=streaming\n+        )\n+\n+    @traced\n+    @torch.inference_mode()\n+    def generate_batch(\n+        self,\n+        inputs: List[List[int]],\n+        generation_config: Optional[GenerationConfig] = None,\n+        progress_bar: bool = True,\n+        **kwargs,\n+    ) -> List[List[int]]:\n+        \"\"\"Generate sequences for a batch of prompts using continuous batching.\n+\n+        Args:\n+            inputs: List of input token sequences (prompts)\n+            generation_config: Optional generation configuration\n+            **kwargs: Additional generation parameters\n+\n+        Returns:\n+            `List[List[int]]`: A list containing the generated sequences (including prompt tokens\n+                                if not handled otherwise) for each input prompt, in the same order.\n+                                Returns an empty list `[]` for requests that failed.\n+        \"\"\"\n+        if not inputs:\n+            return []\n+\n+        # Initialize manager with the batch inputs\n+        manager = self.init_continuous_batching(generation_config=generation_config)\n+        manager.start()\n+        results = {}\n+        num_requests = len(inputs)\n+        try:\n+            from tqdm.contrib.logging import logging_redirect_tqdm\n+\n+            with logging_redirect_tqdm([logger]):\n+                with tqdm(\n+                    total=num_requests,\n+                    disable=(not progress_bar),\n+                    desc=f\"Solving {num_requests} requests\",\n+                    unit=\"request\",\n+                ) as pbar:\n+                    manager.add_requests(inputs, **kwargs)\n+                    finished_count = 0\n+                    while finished_count < num_requests:\n+                        result = manager.get_result(timeout=1)\n+                        if result:\n+                            req_id = result.request_id\n+                            if result.status == RequestStatus.FINISHED:\n+                                results[req_id] = result\n+                                finished_count += 1\n+                                pbar.update(1)\n+                        else:\n+                            if not manager.is_running():\n+                                logger.error(\"Generation thread terminated unexpectedly.\")\n+                                break\n+\n+        except Exception as e:\n+            logger.error(f\"Error during batch generation: {e}\", exc_info=True)\n+        finally:\n+            manager.stop(block=True, timeout=5.0)\n+        return results"
        },
        {
            "sha": "914069ce6b5dba0299a191453ab4e8f4c526748c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -79,6 +79,7 @@\n     GenerationConfig,\n     GenerationMode,\n )\n+from .continuous_batching import ContinuousMixin\n from .logits_process import (\n     EncoderNoRepeatNGramLogitsProcessor,\n     EncoderRepetitionPenaltyLogitsProcessor,\n@@ -352,7 +353,7 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]\n \n \n-class GenerationMixin:\n+class GenerationMixin(ContinuousMixin):\n     \"\"\"\n     A class containing all functions for auto-regressive text generation, to be used as a mixin in model classes.\n     Inheriting from this class causes the model to have special generation-related behavior, such as loading a\n@@ -1099,10 +1100,10 @@ def _get_candidate_generator(\n     def _get_logits_processor(\n         self,\n         generation_config: GenerationConfig,\n-        input_ids_seq_length: int,\n-        encoder_input_ids: torch.LongTensor,\n-        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]],\n-        logits_processor: Optional[LogitsProcessorList],\n+        input_ids_seq_length: Optional[int] = None,\n+        encoder_input_ids: torch.LongTensor = None,\n+        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n+        logits_processor: Optional[LogitsProcessorList] = None,\n         device: Optional[str] = None,\n         model_kwargs: Optional[Dict[str, Any]] = None,\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n@@ -1114,6 +1115,8 @@ def _get_logits_processor(\n         \"\"\"\n         # instantiate processors list\n         processors = LogitsProcessorList()\n+        if logits_processor is None:\n+            logits_processor = []\n \n         if generation_config.guidance_scale is not None and generation_config.guidance_scale != 1:\n             processors.append(\n@@ -1183,7 +1186,7 @@ def _get_logits_processor(\n             )\n         if (\n             generation_config.min_length is not None\n-            and generation_config._eos_token_tensor is not None\n+            and getattr(generation_config, \"_eos_token_tensor\", None) is not None\n             and generation_config.min_length > 0\n         ):\n             processors.append(\n@@ -1195,7 +1198,7 @@ def _get_logits_processor(\n             )\n         if (\n             generation_config.min_new_tokens is not None\n-            and generation_config._eos_token_tensor is not None\n+            and getattr(generation_config, \"_eos_token_tensor\", None) is not None\n             and generation_config.min_new_tokens > 0\n         ):\n             processors.append("
        },
        {
            "sha": "9893e10c89ae3fead9ebf1cd3814751ba3b83dbb",
            "filename": "src/transformers/integrations/eager_paged.py",
            "status": "added",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feager_paged.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,45 @@\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_paged_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    cache = kwargs.pop(\"cache\", None)\n+    if cache is not None:\n+        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights"
        },
        {
            "sha": "b0463d952487a442782531cdd7fa9f59afaeb76e",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "added",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,64 @@\n+import torch\n+\n+from ..generation.continuous_batching import PagedAttentionCache\n+from ..utils import is_flash_attn_2_available\n+\n+\n+if is_flash_attn_2_available():\n+    from flash_attn import flash_attn_varlen_func\n+\n+\n+def paged_attention_forward(\n+    module: torch.nn.Module,\n+    q: torch.Tensor,\n+    k: torch.Tensor,\n+    v: torch.Tensor,\n+    attention_mask: torch.Tensor = None,\n+    cache: PagedAttentionCache = None,\n+    cumulative_seqlens_q=None,\n+    cumulative_seqlens_k=None,\n+    max_seqlen_q=None,\n+    max_seqlen_k=None,\n+    block_tables=None,\n+    **kwargs,\n+) -> torch.Tensor:\n+    r\"\"\"Perform the forward pass of attention with paged key-value cache.\n+\n+    This function handles the cache updates and performs the attention computation\n+    using the flash_attn_varlen_func for efficient processing.\n+\n+    Args:\n+        q: (total_q, nheads, headdim), where total_q = total number of query tokens in the batch.\n+        k: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.  but if there is a block table it can be the full k\n+        v: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.  but if there is a block table it can be the full v\n+        cumulative_seqlens_q: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n+           of the sequences in the batch, used to index into q.\n+        cumulative_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n+           of the sequences in the batch, used to index into kv.\n+        max_seqlen_q: int. Maximum query sequence length in the batch.\n+        max_seqlen_k: int. Maximum key sequence length in the batch.\n+        dropout_p: float. Dropout probability.\n+        softmax_scale: float. The scaling of QK^T before applying softmax.\n+            Default to 1 / sqrt(headdim).\n+        causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n+        window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n+        softcap: float. Anything > 0 activates softcapping attention.\n+    \"\"\"\n+    k, v = cache.update(k, v, module.layer_idx, cumulative_seqlens_k=cumulative_seqlens_k, **kwargs)\n+\n+    attn_output = flash_attn_varlen_func(\n+        q.transpose(1, 2).squeeze(0),\n+        k.transpose(1, 2).squeeze(0),\n+        v.transpose(1, 2).squeeze(0),\n+        cumulative_seqlens_q.to(torch.int32),\n+        cumulative_seqlens_k.to(torch.int32),\n+        max_seqlen_q,\n+        max_seqlen_k,\n+        softmax_scale=module.scaling,\n+        causal=True,  # kind of a must, it automatically aligns the mask for q < k\n+        window_size=(-1, -1),  # -1 means infinite context window\n+        # block_table=block_tables, -> torch.Tensor\n+        # **kwargs,\n+    )\n+\n+    return attn_output, None"
        },
        {
            "sha": "640db16d0dec23f273629c58bd0c3ac90406fb8b",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "added",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,51 @@\n+from typing import Optional, Tuple\n+\n+import torch\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def sdpa_attention_paged_forward(\n+    module: torch.nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    dropout: float = 0.0,\n+    scaling: Optional[float] = None,\n+    is_causal: Optional[bool] = None,\n+    **kwargs,\n+) -> Tuple[torch.Tensor, None]:\n+    cache = kwargs.pop(\"cache\", None)\n+    if cache is not None:\n+        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+    if hasattr(module, \"num_key_value_groups\"):\n+        key = repeat_kv(key, module.num_key_value_groups)\n+        value = repeat_kv(value, module.num_key_value_groups)\n+\n+    causal_mask = attention_mask\n+    query = query.contiguous()\n+    key = key.contiguous()\n+    value = value.contiguous()\n+    attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        query,\n+        key,\n+        value,\n+        attn_mask=causal_mask,\n+        dropout_p=dropout,\n+        scale=scaling,\n+        is_causal=False,\n+    )\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, None"
        },
        {
            "sha": "2f00d9b6c0e1e93de5b2ff3243144e82a2f0fe7b",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -427,17 +427,17 @@ class FlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for Flash Attention with Compile.\n \n     Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`, *optional*)\n+        cumulative_seqlens_q (`torch.LongTensor`, *optional*)\n             Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`, *optional*)\n+        cumulative_seqlens_k (`torch.LongTensor`, *optional*)\n             Gets cumulative sequence length for key state.\n         max_length_q (`int`, *optional*):\n             Maximum sequence length for query state.\n         max_length_k (`int`, *optional*):\n             Maximum sequence length for key state.\n     \"\"\"\n \n-    cu_seq_lens_q: Optional[torch.LongTensor]\n-    cu_seq_lens_k: Optional[torch.LongTensor]\n+    cumulative_seqlens_q: Optional[torch.LongTensor]\n+    cumulative_seqlens_k: Optional[torch.LongTensor]\n     max_length_q: Optional[int]\n     max_length_k: Optional[int]"
        },
        {
            "sha": "ced2231d1311f75b37a8146d520655f4f7116df1",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -57,9 +57,12 @@\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n from .integrations.accelerate import find_tied_parameters, init_empty_weights\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n+from .integrations.eager_paged import eager_paged_attention_forward\n from .integrations.flash_attention import flash_attention_forward\n+from .integrations.flash_paged import paged_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.sdpa_attention import sdpa_attention_forward\n+from .integrations.sdpa_paged import sdpa_attention_paged_forward\n from .integrations.tensor_parallel import (\n     ALL_PARALLEL_STYLES,\n     _get_parameter_tp_plan,\n@@ -6089,7 +6092,10 @@ class AttentionInterface(GeneralInterface):\n     _global_mapping = {\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n+        \"paged_attention\": paged_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n+        \"sdpa_paged\": sdpa_attention_paged_forward,\n+        \"eager_paged\": eager_paged_attention_forward,\n     }\n \n "
        },
        {
            "sha": "16379831a97bee8097e33b1339df51252dd37275",
            "filename": "src/transformers/utils/metrics.py",
            "status": "added",
            "additions": 434,
            "deletions": 0,
            "changes": 434,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Futils%2Fmetrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/src%2Ftransformers%2Futils%2Fmetrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmetrics.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,434 @@\n+import functools\n+import logging\n+import time\n+from enum import Enum\n+from typing import Any, Callable, List, Optional, Tuple, Union\n+\n+import torch\n+\n+\n+class RequestStatus(Enum):\n+    \"\"\"Status of a generation request through its lifecycle.\"\"\"\n+\n+    PENDING = \"pending\"\n+    PREFILLING = \"prefilling\"\n+    PREFILLING_SPLIT = \"prefilling_split\"\n+    SPLIT_PENDING_REMAINDER = \"split_pending_remainder\"\n+    DECODING = \"decoding\"\n+    FINISHED = \"finished\"\n+    FAILED = \"failed\"\n+\n+\n+try:\n+    from opentelemetry import metrics, trace\n+    from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter\n+    from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n+    from opentelemetry.sdk.metrics import MeterProvider\n+    from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n+    from opentelemetry.sdk.resources import Resource\n+    from opentelemetry.sdk.trace import TracerProvider\n+    from opentelemetry.sdk.trace.export import BatchSpanProcessor\n+    from opentelemetry.trace import Status, StatusCode, get_tracer\n+\n+    resource = Resource.create({\"service.name\": \"transformers\"})\n+\n+    metrics_exporter = PeriodicExportingMetricReader(OTLPMetricExporter(), export_interval_millis=1000)\n+    meter_provider = MeterProvider(resource=resource, metric_readers=[metrics_exporter])\n+    metrics.set_meter_provider(meter_provider)\n+\n+    trace_exporter = OTLPSpanExporter()\n+    tracer_provider = TracerProvider(resource=resource)\n+    tracer_provider.add_span_processor(BatchSpanProcessor(trace_exporter))\n+    trace.set_tracer_provider(tracer_provider)\n+\n+    _has_opentelemetry = True\n+except ImportError:\n+    _has_opentelemetry = False\n+\n+\n+def attach_tracer(tracer_name_template=None):\n+    \"\"\"\n+    Decorator that attaches a tracer to a class.\n+\n+    This decorator should be applied to classes that need OpenTelemetry tracing.\n+    It adds a tracer attribute to the class instance that can be used by the traced decorator.\n+\n+    Args:\n+        tracer_name_template: Optional template string for the tracer name.\n+            If provided, it should contain {module} which will be replaced with the class's full module path\n+            and {class_name} for the class name.\n+            If None, a default naming scheme will be used where:\n+              - If the module already starts with \"transformers.\", it will use that directly\n+              - Otherwise, it will prepend \"transformers.\" to the module name\n+\n+    Returns:\n+        Class decorator function\n+    \"\"\"\n+    if not _has_opentelemetry:\n+        return lambda cls: cls\n+\n+    def decorator(cls):\n+        original_init = cls.__init__\n+\n+        @functools.wraps(original_init)\n+        def init_with_tracer(self, *args, **kwargs):\n+            original_init(self, *args, **kwargs)\n+\n+            module_name = cls.__module__\n+            class_name = cls.__qualname__\n+\n+            if tracer_name_template is None:\n+                if module_name.startswith(\"transformers.\"):\n+                    tracer_name = f\"{module_name}.{class_name}\"\n+                else:\n+                    tracer_name = f\"transformers.{module_name}.{class_name}\"\n+            else:\n+                tracer_name = tracer_name_template.format(module=module_name, class_name=class_name)\n+\n+            self.tracer = get_tracer(tracer_name)\n+\n+        cls.__init__ = init_with_tracer\n+        return cls\n+\n+    return decorator\n+\n+\n+def traced(\n+    func=None,\n+    *,\n+    span_name=None,\n+    standalone=False,\n+    additional_attributes: Optional[List[Tuple[str, str, Union[Any, Callable[[Any], Any]]]]] = None,\n+):\n+    \"\"\"\n+    Decorator to trace function calls with OpenTelemetry.\n+\n+    Can be used as @traced or @traced(span_name=\"custom_name\")\n+\n+    Args:\n+        func: The function to trace\n+        span_name: Optional custom name for the span (defaults to function name)\n+        standalone: If True, creates a parentless span\n+        additional_attributes: Optional list of additional attributes to set on the span.\n+          Each item is a tuple of (instance_attribute_name, span_attribute_key, value_or_transform_function)\n+          where:\n+            - instance_attribute_name: Name of the attribute to get from the class instance\n+            - span_attribute_key: Key to use when setting the attribute on the span\n+            - value_or_transform_function: Either a raw value to use directly, or a function to transform\n+              the attribute value before setting it on the span\n+\n+    Returns:\n+        Decorated function with tracing\n+    \"\"\"\n+\n+    def decorator(func):\n+        if not _has_opentelemetry:\n+            return func\n+\n+        import functools\n+\n+        @functools.wraps(func)\n+        def wrapper(*args, **kwargs):\n+            instance = args[0] if args and (hasattr(func, \"__self__\") and func.__self__ is not None) else None\n+            is_method = instance is not None\n+\n+            if is_method and hasattr(instance, \"tracer\"):\n+                tracer = instance.tracer\n+            else:\n+                tracer = get_tracer(f\"transformers.{func.__module__}.{func.__name__}\")\n+\n+            name = span_name or func.__name__\n+            span_fn = tracer.start_span if standalone else tracer.start_as_current_span\n+            with span_fn(name) as span:\n+                span.set_attribute(\"function.name\", func.__name__)\n+                span.set_attribute(\"function.module\", func.__module__)\n+                span.set_attribute(\"function.is_method\", is_method)\n+\n+                if args:\n+                    for i, arg in enumerate(args):\n+                        if isinstance(arg, (str, int, float, bool)) or arg is None:\n+                            span.set_attribute(f\"args.{i}\", str(arg))\n+                        else:\n+                            span.set_attribute(f\"args.{i}\", str(type(arg)))\n+                if kwargs:\n+                    for key, value in kwargs.items():\n+                        if isinstance(value, (str, int, float, bool)) or value is None:\n+                            span.set_attribute(f\"kwargs.{key}\", str(value))\n+                        else:\n+                            span.set_attribute(f\"kwargs.{key}\", str(type(value)))\n+\n+                if additional_attributes and is_method:\n+                    for attr_config in additional_attributes:\n+                        instance_attribute_name, span_attribute_key, value_or_transform_function = attr_config\n+                        if hasattr(instance, instance_attribute_name):\n+                            attribute_value = getattr(instance, instance_attribute_name)\n+                            if callable(value_or_transform_function):\n+                                transformed_value = value_or_transform_function(attribute_value)\n+                            else:\n+                                transformed_value = value_or_transform_function\n+                            span.set_attribute(span_attribute_key, transformed_value)\n+\n+                try:\n+                    result = func(*args, **kwargs)\n+                    return result\n+                except Exception as e:\n+                    span.set_status(Status(StatusCode.ERROR))\n+                    span.record_exception(e)\n+                    raise\n+\n+        return wrapper\n+\n+    if func is None:\n+        return decorator\n+    return decorator(func)\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@attach_tracer()\n+class ContinuousBatchProcessorMetrics:\n+    \"\"\"Metrics collection for ContinuousBatchProcessor.\"\"\"\n+\n+    def __init__(self, max_batch_tokens: int):\n+        \"\"\"Initialize metrics for continuous batch processor.\n+\n+        Args:\n+            max_batch_tokens: Maximum number of tokens in a batch\n+        \"\"\"\n+        self.max_batch_tokens = max_batch_tokens\n+\n+        self._setup_metrics()\n+\n+    def _setup_metrics(self):\n+        \"\"\"Initialize OpenTelemetry metrics and tracing if the library is available.\"\"\"\n+\n+        if not _has_opentelemetry:\n+            logger.info(\"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\")\n+            return\n+\n+        self.meter = metrics.get_meter(\"transformers.generation.continuous_batch_processor\")\n+\n+        # Define appropriate buckets for TTFT (typically ranges from ~50ms to several seconds)\n+        ttft_buckets = [10, 25, 50, 75, 100, 150, 200, 300, 500, 750, 1000, 2000, 5000, 10000]\n+\n+        self.ttft_histogram = self.meter.create_histogram(\n+            name=\"ttft_milliseconds\",\n+            description=\"Time to first token in milliseconds\",\n+            unit=\"ms\",\n+            explicit_bucket_boundaries_advisory=ttft_buckets,\n+        )\n+\n+        self.active_requests_gauge = self.meter.create_gauge(\n+            name=\"active_requests_count\",\n+            description=\"Number of active requests currently being processed\",\n+            unit=\"requests\",\n+        )\n+\n+        self.waiting_requests_gauge = self.meter.create_gauge(\n+            name=\"waiting_requests_count\",\n+            description=\"Number of requests waiting to be processed\",\n+            unit=\"requests\",\n+        )\n+\n+        # Define appropriate buckets for request latency (similar to TTFT but with higher upper bounds)\n+        latency_buckets = [50, 100, 250, 500, 1000, 2000, 5000, 10000, 20000, 30000, 60000]\n+\n+        self.request_latency_histogram = self.meter.create_histogram(\n+            name=\"request_latency_milliseconds\",\n+            description=\"End-to-end latency for completed requests in milliseconds\",\n+            unit=\"ms\",\n+            explicit_bucket_boundaries_advisory=latency_buckets,\n+        )\n+\n+        self.decode_prefill_ratio_gauge = self.meter.create_gauge(\n+            name=\"decode_prefill_ratio\",\n+            description=\"Ratio of decode tokens to prefill tokens in a batch\",\n+            unit=\"ratio\",\n+        )\n+\n+        self.prefill_tokens_counter = self.meter.create_counter(\n+            name=\"prefill_tokens_processed\",\n+            description=\"Number of prefill tokens processed\",\n+            unit=\"tokens\",\n+        )\n+\n+        self.decode_tokens_counter = self.meter.create_counter(\n+            name=\"decode_tokens_processed\",\n+            description=\"Number of decode tokens processed\",\n+            unit=\"tokens\",\n+        )\n+\n+        # Define appropriate buckets for batch fill percentage (0-100%)\n+        batch_fill_buckets = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 98, 100]\n+\n+        self.batch_fill_percentage_histogram = self.meter.create_histogram(\n+            name=\"batch_fill_percentage\",\n+            description=\"Percentage of max_batch_tokens utilized in each batch\",\n+            unit=\"percent\",\n+            explicit_bucket_boundaries_advisory=batch_fill_buckets,\n+        )\n+\n+        self.kv_cache_free_memory_gauge = self.meter.create_gauge(\n+            name=\"kv_cache_free_memory_bytes\",\n+            description=\"Free memory of the PagedAttentionCache in bytes\",\n+            unit=\"bytes\",\n+        )\n+\n+        self.kv_cache_memory_gauge = self.meter.create_gauge(\n+            name=\"kv_cache_memory_bytes\",\n+            description=\"Memory usage of the PagedAttentionCache in bytes\",\n+            unit=\"bytes\",\n+        )\n+\n+    @traced\n+    def record_ttft_metric(self, created_time: float, request_id: str) -> None:\n+        \"\"\"Record Time to First Token (TTFT).\n+\n+        Args:\n+            created_time: The time the request was created\n+            request_id: The ID of the request\n+        \"\"\"\n+        if not _has_opentelemetry:\n+            return\n+\n+        ttft_ms = (time.time() - created_time) * 1000.0\n+\n+        try:\n+            self.ttft_histogram.record(ttft_ms)\n+            logger.debug(f\"Recorded TTFT for request {request_id}: {ttft_ms:.2f}ms\")\n+        except Exception as e:\n+            logger.warning(f\"Failed to record TTFT metric: {e}\")\n+\n+    @traced\n+    def record_batch_metrics(self, requests_in_batch: List) -> None:\n+        \"\"\"Record metrics about the batch composition including decode/prefill ratio and batch fill percentage.\n+\n+        Args:\n+            requests_in_batch: List of request states in the current batch\n+        \"\"\"\n+        if not _has_opentelemetry or not requests_in_batch:\n+            return\n+\n+        decode_tokens = 0\n+        prefill_tokens = 0\n+\n+        for state in requests_in_batch:\n+            if state.status == RequestStatus.DECODING:\n+                decode_tokens += 1\n+            elif state.status in [RequestStatus.PREFILLING, RequestStatus.PREFILLING_SPLIT]:\n+                prefill_tokens += len(state.prompt_ids)\n+\n+        total_batch_tokens = decode_tokens + prefill_tokens\n+\n+        try:\n+            if prefill_tokens > 0:\n+                self.prefill_tokens_counter.add(prefill_tokens)\n+\n+            if decode_tokens > 0:\n+                self.decode_tokens_counter.add(decode_tokens)\n+\n+            if prefill_tokens > 0:\n+                ratio = decode_tokens / prefill_tokens\n+                self.decode_prefill_ratio_gauge.set(ratio)\n+\n+            fill_percentage = (total_batch_tokens / self.max_batch_tokens) * 100.0\n+            self.batch_fill_percentage_histogram.record(fill_percentage)\n+            logger.debug(\n+                f\"Batch metrics: {decode_tokens} decode tokens, {prefill_tokens} prefill tokens, \"\n+                f\"batch fill: {fill_percentage:.2f}% ({total_batch_tokens}/{self.max_batch_tokens})\"\n+            )\n+        except Exception as e:\n+            logger.warning(f\"Failed to record batch metrics: {e}\")\n+\n+    @traced\n+    def record_kv_cache_memory_metrics(self, cache) -> None:\n+        \"\"\"Record memory usage of the PagedAttentionCache without GPU synchronization.\n+\n+        This calculates the theoretical memory usage based on cache configuration\n+        and the number of blocks currently in use.\n+\n+        Args:\n+            cache: The PagedAttentionCache object to measure\n+        \"\"\"\n+        if not _has_opentelemetry:\n+            return\n+\n+        try:\n+            # Calculate memory usage based on cache configuration\n+            num_used_blocks = cache.num_blocks - len(cache._free_blocks)\n+            num_layers = len(cache.key_cache)\n+\n+            # Each used block stores key and value states\n+            # Each with shape: (num_kv_heads, block_size, head_dim)\n+            bytes_per_parameter = 2 if cache.dtype in [torch.float16, torch.bfloat16] else 4  # Size in bytes\n+\n+            # Total bytes = num_layers * num_used_blocks * block_size *\n+            #               num_kv_heads * head_dim * 2 (both K and V) * bytes_per_parameter\n+            memory_bytes = (\n+                num_layers\n+                * num_used_blocks\n+                * cache.block_size\n+                * cache.num_key_value_heads\n+                * cache.head_dim\n+                * 2  # For both key and value caches\n+                * bytes_per_parameter\n+            )\n+\n+            free_memory_bytes = (\n+                num_layers\n+                * len(cache._free_blocks)\n+                * cache.block_size\n+                * cache.num_key_value_heads\n+                * cache.head_dim\n+                * 2  # For both key and value caches\n+                * bytes_per_parameter\n+            )\n+\n+            self.kv_cache_memory_gauge.set(memory_bytes)\n+            self.kv_cache_free_memory_gauge.set(free_memory_bytes)\n+            logger.debug(\n+                f\"KV Cache memory: {memory_bytes / (1024 * 1024):.2f}MB, \"\n+                f\"Used blocks: {num_used_blocks}/{cache.num_blocks} \"\n+                f\"({num_used_blocks / cache.num_blocks * 100:.1f}%)\"\n+            )\n+        except Exception as e:\n+            logger.warning(f\"Failed to record KV cache memory metrics: {e}\")\n+\n+    @traced\n+    def record_queue_metrics(self, active_requests: int, waiting_requests: int) -> None:\n+        \"\"\"Record metrics about active and waiting requests.\n+\n+        Args:\n+            active_requests: Number of active requests\n+            waiting_requests: Number of waiting requests\n+        \"\"\"\n+        if not _has_opentelemetry:\n+            return\n+\n+        try:\n+            self.active_requests_gauge.set(active_requests)\n+            self.waiting_requests_gauge.set(waiting_requests)\n+            logger.debug(f\"Queue metrics: {active_requests} active requests, {waiting_requests} waiting requests\")\n+        except Exception as e:\n+            logger.warning(f\"Failed to record queue metrics: {e}\")\n+\n+    @traced\n+    def record_request_completion(self, created_time: float, request_id: str) -> None:\n+        \"\"\"Record metrics about a completed request.\n+\n+        Args:\n+            created_time: The time the request was created\n+            request_id: The ID of the request\n+        \"\"\"\n+        if not _has_opentelemetry:\n+            return\n+\n+        latency_ms = (time.time() - created_time) * 1000.0\n+\n+        try:\n+            self.request_latency_histogram.record(latency_ms)\n+\n+            logger.debug(f\"Recorded request completion for {request_id}: {latency_ms:.2f}ms\")\n+        except Exception as e:\n+            logger.warning(f\"Failed to record request completion metric: {e}\")"
        },
        {
            "sha": "c06975844bfcd401cbcf1736b23d6cabdeb49c0b",
            "filename": "tests/generation/test_paged_attention.py",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/211f2b08751cdee4bd6b84892b91f4a18dbfe305/tests%2Fgeneration%2Ftest_paged_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/211f2b08751cdee4bd6b84892b91f4a18dbfe305/tests%2Fgeneration%2Ftest_paged_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_paged_attention.py?ref=211f2b08751cdee4bd6b84892b91f4a18dbfe305",
            "patch": "@@ -0,0 +1,86 @@\n+import time\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n+from transformers.testing_utils import require_flash_attn, require_torch_gpu, run_slow\n+\n+\n+_TEST_PROMPTS = [\n+    \"A man is a walking his dog down the street, and a the turn he sees\",\n+    \"Describe a fruit that is of orange color and round. It is a sweet fruit and a great source of Vitamine C. The fruit I'm thinking of is an\",\n+    \"A plane is flying high in the sky, out of the window are clouds and mountains. Where could the plane be located?\",\n+    \"Please fill in the form to\",\n+    \"For safety reasons, the train is stopped in the middle of the\",\n+]\n+\n+_EXPECTED_OUTPUTS = [\n+    \"a woman standing on the sidewalk, looking at him. He is immediately drawn to her and feels a strong attraction. He walks up to her and strikes up a conversation, and they quickly discover that they have a lot in common. They exchange numbers and\",\n+    \"orange.\\n\\n## Step 1: Identify the key characteristics of the fruit\\nThe fruit is described as being orange in color and round in shape.\\n\\n## Step 2: Determine the taste and nutritional value of the fruit\\nThe fruit is described as sweet\",\n+    \"This riddle is a classic example of a lateral thinking puzzle, which requires the test-taker to think creatively and consider multiple possibilities. The answer is not a straightforward one, and it requires some lateral thinking to arrive at the correct solution.\",\n+    \"get in touch with us. We will respond to your message as soon as possible.\\n\\n[Your Name]\\n[Your Email]\\n[Your Phone Number]\\n[Your Message]\\n\\nWe are looking forward to hearing from you!\\n\\n[Insert Contact Information]\\n\\nNote:\",\n+    \"track. The train is stopped for 30 minutes. The train is moving at a speed of 60 km/h. How many kilometers does the train travel in 30 minutes?\\n## Step 1: Convert the speed from km/h to km/min\",\n+]\n+\n+\n+@run_slow\n+@require_torch_gpu\n+@require_flash_attn\n+class TestBatchGeneration(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model = AutoModelForCausalLM.from_pretrained(\n+            \"meta-llama/Llama-3.2-3b-Instruct\", torch_dtype=\"bfloat16\", device_map=\"auto\"\n+        ).eval()\n+\n+        cls.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3b-Instruct\", padding_side=\"left\")\n+\n+        if cls.tokenizer.pad_token is None:\n+            cls.tokenizer.pad_token = cls.tokenizer.eos_token\n+            cls.model.config.pad_token_id = cls.model.config.eos_token_id\n+\n+        cls.model.use_cache = False\n+\n+    @parameterized.expand(\n+        [\n+            (\"eager_paged\", 64, 128, 64),\n+            (\"sdpa_paged\", 32, 256, 128),\n+            (\"paged_attention\", 16, 512, 256),\n+            (\"flex_paged\", 64, 128, 64),\n+        ]\n+    )\n+    def test_generate_batch_consistency(self, attn_impl, num_blocks, block_size, max_batch_tokens):\n+        self.model.config.attn_implementation = attn_impl\n+\n+        generation_config = GenerationConfig(\n+            max_new_tokens=50,\n+            top_k=0,\n+            eos_token_id=self.tokenizer.eos_token_id,\n+            pad_token_id=self.tokenizer.pad_token_id,\n+            use_cache=False,\n+            num_blocks=num_blocks,\n+            block_size=block_size,\n+            max_batch_tokens=max_batch_tokens,\n+        )\n+\n+        tokenized = self.tokenizer(_TEST_PROMPTS, truncation=True, max_length=512)\n+        batch_inputs = list(tokenized[\"input_ids\"])\n+\n+        start = time.time()\n+        batch_outputs = self.model.generate_batch(\n+            inputs=batch_inputs,\n+            generation_config=generation_config,\n+        )\n+        end = time.time()\n+        print(\n+            f\"\\n[{attn_impl}] Batch took {end - start:.2f}s with config: blocks={num_blocks}, block_size={block_size}, max_batch_tokens={max_batch_tokens}\"\n+        )\n+\n+        for i, req_id in enumerate(batch_outputs):\n+            generated = self.tokenizer.decode(batch_outputs[req_id].static_outputs, skip_special_tokens=False).strip()\n+            expected = _EXPECTED_OUTPUTS[i].strip()\n+            self.assertTrue(\n+                generated.startswith(expected),\n+                msg=f\"[{attn_impl}] Mismatch in request {i}:\\nExpected start: {expected}\\nGot: {generated}\",\n+            )"
        }
    ],
    "stats": {
        "total": 3478,
        "additions": 3467,
        "deletions": 11
    }
}