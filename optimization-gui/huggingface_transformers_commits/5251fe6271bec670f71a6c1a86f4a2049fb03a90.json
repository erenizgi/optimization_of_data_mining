{
    "author": "VladOS95-cyber",
    "message": "Add GGUF for Mamba (#34200)\n\n* add mamba architecture for gguf\r\n\r\n* add logic for weights conversion, some fixes and refactoring\r\n\r\n* add lm_head layers, unit test refactoring\r\n\r\n* more fixes for tests\r\n\r\n* remove lm_head creation\r\n\r\n* remove unused comments",
    "sha": "5251fe6271bec670f71a6c1a86f4a2049fb03a90",
    "files": [
        {
            "sha": "2da721b28986af0bc3b519c6aca7e7a8114e2d8f",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5251fe6271bec670f71a6c1a86f4a2049fb03a90/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5251fe6271bec670f71a6c1a86f4a2049fb03a90/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=5251fe6271bec670f71a6c1a86f4a2049fb03a90",
            "patch": "@@ -86,6 +86,7 @@ For now the supported model architectures are the architectures that have been v\n - GPT2\n - Starcoder2\n - T5\n+- Mamba\n \n ## Example usage\n "
        },
        {
            "sha": "f4545f2698c017e7c038a06a468e5ac39fe80d85",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5251fe6271bec670f71a6c1a86f4a2049fb03a90/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5251fe6271bec670f71a6c1a86f4a2049fb03a90/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=5251fe6271bec670f71a6c1a86f4a2049fb03a90",
            "patch": "@@ -235,6 +235,19 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"model.norm\",\n     },\n+    \"mamba\": {\n+        \"token_embd\": \"backbone.embeddings\",\n+        \"blk\": \"backbone.layers\",\n+        \"ssm_a\": \"mixer.A_log\",\n+        \"ssm_conv1d\": \"mixer.conv1d\",\n+        \"ssm_in\": \"mixer.in_proj\",\n+        \"ssm_out\": \"mixer.out_proj\",\n+        \"ssm_x\": \"mixer.x_proj\",\n+        \"ssm_dt\": \"mixer.dt_proj\",\n+        \"attn_norm\": \"norm\",\n+        \"output_norm\": \"backbone.norm_f\",\n+        \"output.weight\": \"lm_head.weight\",\n+    },\n }\n \n \n@@ -373,6 +386,17 @@\n         \"attention.head_count_kv\": \"num_key_value_heads\",\n         \"attention.layer_norm_epsilon\": \"norm_epsilon\",\n     },\n+    \"mamba\": {\n+        \"vocab_size\": \"vocab_size\",\n+        \"context_length\": \"max_position_embeddings\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"attention.layer_norm_rms_epsilon\": \"layer_norm_epsilon\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"ssm.conv_kernel\": \"conv_kernel\",\n+        \"ssm.state_size\": \"state_size\",\n+        \"ssm.time_step_rank\": \"time_step_rank\",\n+        \"ssm.inner_size\": \"intermediate_size\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -768,6 +792,7 @@ def converted(self) -> Tokenizer:\n     \"gpt2\": GGUFGPTConverter,\n     \"starcoder2\": GGUFGPTConverter,\n     \"t5\": GGUFT5Converter,\n+    \"mamba\": GGUFGPTConverter,\n }\n \n "
        },
        {
            "sha": "c784ca0eb4ca2cd5e91bfb19fc1d84a990a98f80",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5251fe6271bec670f71a6c1a86f4a2049fb03a90/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5251fe6271bec670f71a6c1a86f4a2049fb03a90/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=5251fe6271bec670f71a6c1a86f4a2049fb03a90",
            "patch": "@@ -220,6 +220,19 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                     name = \"lm_head.weight\"\n                     parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n                     continue\n+            if architecture == \"mamba\":\n+                if \"ssm_d\" in name and \"bias\" not in name and \"weight\" not in name:\n+                    # ssm_d has conflicts with ssm_dt in name checking\n+                    # we have to explicitly check that name is exactly ssm_d\n+                    name = name.replace(\"ssm_d\", \"mixer.D\")\n+                if \"ssm_conv1d.weight\" in name:\n+                    # for compatibility tensor ssm_conv1d must be (5120, 1, 4]) dim,\n+                    # quantized one is (5120, 4)\n+                    weights = np.expand_dims(weights, axis=1)\n+                if \"ssm_a\" in name:\n+                    # Original exponential implementation\n+                    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L2975-L2977\n+                    weights = np.log(-weights)\n \n             for tensor_name in tensor_key_mapping:\n                 if tensor_name.format(bid=bid) in name:"
        },
        {
            "sha": "da1af9bff8df90aeab10900557a1010149efeba2",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 54,
            "deletions": 2,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5251fe6271bec670f71a6c1a86f4a2049fb03a90/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5251fe6271bec670f71a6c1a86f4a2049fb03a90/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=5251fe6271bec670f71a6c1a86f4a2049fb03a90",
            "patch": "@@ -59,6 +59,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     starcoder2_model_id = \"QuantFactory/starcoder2-3b-GGUF\"\n     starcoder2_fp16_model_id = \"brittlewis12/starcoder2-3b-GGUF\"\n     starcoder2_original_model_id = \"bigcode/starcoder2-3b\"\n+    mamba_original_model_id = \"state-spaces/mamba-2.8b-hf\"\n+    mamba_model_id = \"jpodivin/mamba-2.8b-hf-GGUF\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -102,6 +104,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     q6_k_gpt2_xl_model_id = \"gpt2-xl.Q6_K.gguf\"\n     q6_k_starcoder2_model_id = \"starcoder2-3b.Q6_K.gguf\"\n     fp16_starcoder2_gguf_model_id = \"starcoder2-3b.fp16.gguf\"\n+    q6_k_mamba_model_id = \"ggml-model-Q6_K.gguf\"\n+    fp16_mamba_model_id = \"ggml-model-f16.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -573,6 +577,8 @@ def test_gpt2_weights_conversion_fp16(self):\n             if layer_name in quantized_state_dict:\n                 self.assertTrue(original_params.shape == quantized_state_dict[layer_name].shape)\n                 torch.testing.assert_close(original_params, quantized_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n \n     def test_gpt2_xl_Q6_K(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.gpt2_xl_model_id, gguf_file=self.q6_k_gpt2_xl_model_id)\n@@ -639,6 +645,8 @@ def test_falcon7b_weights_conversion_fp16(self):\n             if layer_name in quantized_state_dict:\n                 self.assertTrue(original_params.shape == quantized_state_dict[layer_name].shape)\n                 torch.testing.assert_close(original_params, quantized_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n \n     def test_stablelm_q4_k_m(self):\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -708,6 +716,8 @@ def test_stablelm_weights_conversion_fp16(self):\n             if layer_name in converted_state_dict:\n                 self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n                 torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n \n     def test_starcoder2_weights_conversion_fp16(self):\n         original_model = AutoModelForCausalLM.from_pretrained(\n@@ -727,10 +737,11 @@ def test_starcoder2_weights_conversion_fp16(self):\n         original_state_dict = original_model.state_dict()\n \n         for layer_name, original_params in original_state_dict.items():\n-            if layer_name in converted_state_dict and layer_name != \"lm_head.weight\":\n-                # quantized models do not contain \"lm_head.weight\" layer\n+            if layer_name in converted_state_dict:\n                 self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n                 torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n \n     def test_starcoder2_q6_k(self):\n         example_function_text = \"def print_hello_world():\"\n@@ -748,6 +759,47 @@ def test_starcoder2_q6_k(self):\n         EXPECTED_TEXT = 'def print_hello_world():\\n    print(\"Hello World\")\\n\\ndef print'\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_mamba_weights_conversion_fp16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.mamba_original_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.mamba_model_id,\n+            gguf_file=self.fp16_mamba_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                if \"mixer.A_log\" in layer_name:\n+                    # we should increase tolerance after exponential reversing\n+                    # and performing np.log(-weights) operation as numbers are slightly different\n+                    torch.testing.assert_close(original_params, converted_state_dict[layer_name], atol=1e-3, rtol=1e-3)\n+                else:\n+                    torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n+    def test_mamba_q6_k(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.mamba_model_id,\n+            gguf_file=self.q6_k_mamba_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.mamba_model_id, gguf_file=self.q6_k_mamba_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello,I answerthe question.\\n\\nA\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 93,
        "deletions": 2
    }
}