{
    "author": "IlyasMoutawwakil",
    "message": "Fix onnx non-expotable inplace aten op (#34376)\n\n* fix onnx non-expotable inplace op\r\n\r\n* mistral, qwen2, qwen2_vl, starcoder2\r\n\r\n* fixup copies",
    "sha": "5779bac4c45b2c881603cafd20663892869d5860",
    "files": [
        {
            "sha": "cbdd2c663c584479dbabc4e9ea990ff9bdce8de4",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1156,7 +1156,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "321d3dc0daf378b18e9bde140205c0e2fb2c9e7b",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -961,7 +961,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "78a17178ecdda80c91eeefd29985e9e031714fd4",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1174,7 +1174,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "9975996d21d144b34c443f3df448913bd3821b64",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1385,7 +1385,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n@@ -1689,7 +1689,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "bae3f6d4cdaeaa30abc4426ffb2ceaa92dcd0f66",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1136,7 +1136,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "f3690e5f686fbb8f90575e1614dd676ab0d2be4d",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1305,7 +1305,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "0883fac1aebafc22e0436e986ce71d92126c172a",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1059,7 +1059,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "7f4f19aba1f3eb5d8772ec6933c172e091bd0914",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1239,7 +1239,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "90bf29c8b5d66a0855acf05c4d5df350ad439bee",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1321,7 +1321,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        },
        {
            "sha": "1a8b6412e738e1ad1acfe505d7dbf6c6e17b60b3",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5779bac4c45b2c881603cafd20663892869d5860/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=5779bac4c45b2c881603cafd20663892869d5860",
            "patch": "@@ -1033,7 +1033,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                     sliding_attend_mask = torch.arange(target_length, device=device) <= (\n                         cache_position.reshape(-1, 1) - config.sliding_window\n                     )\n-                    diagonal_attend_mask |= sliding_attend_mask\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n             causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 11,
        "deletions": 11
    }
}