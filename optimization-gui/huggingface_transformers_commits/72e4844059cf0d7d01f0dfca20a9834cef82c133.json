{
    "author": "NanoCode012",
    "message": "fix: learning_rate logged as tensor causing save issue with deepspeed (#37704)\n\n* fix: learning_rate logged as tensor causing save issue with deepspeed\n\n* chore: lint\n\n---------\n\nCo-authored-by: NanoCode012 <chanvichet@Chanvichets-MacBook-Pro.local>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "72e4844059cf0d7d01f0dfca20a9834cef82c133",
    "files": [
        {
            "sha": "fb31e921cfaf8326f538f6a91c5a3b2cb45b4ad8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72e4844059cf0d7d01f0dfca20a9834cef82c133/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72e4844059cf0d7d01f0dfca20a9834cef82c133/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=72e4844059cf0d7d01f0dfca20a9834cef82c133",
            "patch": "@@ -3074,7 +3074,9 @@ def _maybe_log_save_evaluate(\n             if grad_norm is not None:\n                 logs[\"grad_norm\"] = grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm\n             if learning_rate is not None:\n-                logs[\"learning_rate\"] = learning_rate\n+                logs[\"learning_rate\"] = (\n+                    learning_rate.item() if isinstance(learning_rate, torch.Tensor) else learning_rate\n+                )\n             else:\n                 logs[\"learning_rate\"] = self._get_learning_rate()\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}