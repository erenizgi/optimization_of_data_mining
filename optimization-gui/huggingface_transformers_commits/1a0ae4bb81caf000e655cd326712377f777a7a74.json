{
    "author": "ydshieh",
    "message": "Remove some custom datasets defined in codebase (#41511)\n\n* how bad it woud be anyway?\n\n* let's break all\n\n* delete\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "1a0ae4bb81caf000e655cd326712377f777a7a74",
    "files": [
        {
            "sha": "eaf64f624637778d9b07fe3e034c30ca0acb70e9",
            "filename": "examples/legacy/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2FREADME.md?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,21 +0,0 @@\n-<!---\n-Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Legacy examples\n-\n-This folder contains examples which are not actively maintained (mostly contributed by the community).\n-\n-Using these examples together with a recent version of the library usually requires to make small (sometimes big) adaptations to get the scripts working."
        },
        {
            "sha": "63cf4e367c3d31f355b2c5282f93a3d7de34e09b",
            "filename": "examples/legacy/benchmarking/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fbenchmarking%2FREADME.md?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,26 +0,0 @@\n-<!---\n-Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# ðŸ¤— Benchmark results\n-\n-Here, you can find a list of the different benchmark results created by the community.\n-\n-If you would like to list benchmark results on your favorite models of the [model hub](https://huggingface.co/models) here, please open a Pull Request and add it below.\n-\n-| Benchmark description | Results | Environment info |      Author      |\n-|:----------|:-------------|:-------------|------:|\n-| PyTorch Benchmark on inference for `google-bert/bert-base-cased` |[memory](https://github.com/patrickvonplaten/files_to_link_to/blob/master/bert_benchmark/inference_memory.csv) | [env](https://github.com/patrickvonplaten/files_to_link_to/blob/master/bert_benchmark/env.csv) | [Patrick von Platen](https://github.com/patrickvonplaten) | \n-| PyTorch Benchmark on inference for `google-bert/bert-base-cased` |[time](https://github.com/patrickvonplaten/files_to_link_to/blob/master/bert_benchmark/inference_time.csv) | [env](https://github.com/patrickvonplaten/files_to_link_to/blob/master/bert_benchmark/env.csv) | [Patrick von Platen](https://github.com/patrickvonplaten) | "
        },
        {
            "sha": "d802dbe67e83aaaddbacb4a90320bfe999a24c67",
            "filename": "examples/legacy/benchmarking/plot_csv_file.py",
            "status": "removed",
            "additions": 0,
            "deletions": 178,
            "changes": 178,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2Fplot_csv_file.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2Fplot_csv_file.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fbenchmarking%2Fplot_csv_file.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,178 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import csv\n-from collections import defaultdict\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-from matplotlib.ticker import ScalarFormatter\n-\n-from transformers import HfArgumentParser\n-\n-\n-def list_field(default=None, metadata=None):\n-    return field(default_factory=lambda: default, metadata=metadata)\n-\n-\n-@dataclass\n-class PlotArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    csv_file: str = field(\n-        metadata={\"help\": \"The csv file to plot.\"},\n-    )\n-    plot_along_batch: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to plot along batch size or sequence length. Defaults to sequence length.\"},\n-    )\n-    is_time: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether the csv file has time results or memory results. Defaults to memory results.\"},\n-    )\n-    no_log_scale: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Disable logarithmic scale when plotting\"},\n-    )\n-    is_train: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"Whether the csv file has training results or inference results. Defaults to inference results.\"\n-        },\n-    )\n-    figure_png_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Filename under which the plot will be saved. If unused no plot is saved.\"},\n-    )\n-    short_model_names: Optional[list[str]] = list_field(\n-        default=None, metadata={\"help\": \"List of model names that are used instead of the ones in the csv file.\"}\n-    )\n-\n-\n-def can_convert_to_int(string):\n-    try:\n-        int(string)\n-        return True\n-    except ValueError:\n-        return False\n-\n-\n-def can_convert_to_float(string):\n-    try:\n-        float(string)\n-        return True\n-    except ValueError:\n-        return False\n-\n-\n-class Plot:\n-    def __init__(self, args):\n-        self.args = args\n-        self.result_dict = defaultdict(lambda: {\"bsz\": [], \"seq_len\": [], \"result\": {}})\n-\n-        with open(self.args.csv_file, newline=\"\") as csv_file:\n-            reader = csv.DictReader(csv_file)\n-            for row in reader:\n-                model_name = row[\"model\"]\n-                self.result_dict[model_name][\"bsz\"].append(int(row[\"batch_size\"]))\n-                self.result_dict[model_name][\"seq_len\"].append(int(row[\"sequence_length\"]))\n-                if can_convert_to_int(row[\"result\"]):\n-                    # value is not None\n-                    self.result_dict[model_name][\"result\"][(int(row[\"batch_size\"]), int(row[\"sequence_length\"]))] = (\n-                        int(row[\"result\"])\n-                    )\n-                elif can_convert_to_float(row[\"result\"]):\n-                    # value is not None\n-                    self.result_dict[model_name][\"result\"][(int(row[\"batch_size\"]), int(row[\"sequence_length\"]))] = (\n-                        float(row[\"result\"])\n-                    )\n-\n-    def plot(self):\n-        fig, ax = plt.subplots()\n-        title_str = \"Time usage\" if self.args.is_time else \"Memory usage\"\n-        title_str = title_str + \" for training\" if self.args.is_train else title_str + \" for inference\"\n-\n-        if not self.args.no_log_scale:\n-            # set logarithm scales\n-            ax.set_xscale(\"log\")\n-            ax.set_yscale(\"log\")\n-\n-        for axis in [ax.xaxis, ax.yaxis]:\n-            axis.set_major_formatter(ScalarFormatter())\n-\n-        for model_name_idx, model_name in enumerate(self.result_dict.keys()):\n-            batch_sizes = sorted(set(self.result_dict[model_name][\"bsz\"]))\n-            sequence_lengths = sorted(set(self.result_dict[model_name][\"seq_len\"]))\n-            results = self.result_dict[model_name][\"result\"]\n-\n-            (x_axis_array, inner_loop_array) = (\n-                (batch_sizes, sequence_lengths) if self.args.plot_along_batch else (sequence_lengths, batch_sizes)\n-            )\n-\n-            label_model_name = (\n-                model_name if self.args.short_model_names is None else self.args.short_model_names[model_name_idx]\n-            )\n-\n-            for inner_loop_value in inner_loop_array:\n-                if self.args.plot_along_batch:\n-                    y_axis_array = np.asarray(\n-                        [results[(x, inner_loop_value)] for x in x_axis_array if (x, inner_loop_value) in results],\n-                        dtype=int,\n-                    )\n-                else:\n-                    y_axis_array = np.asarray(\n-                        [results[(inner_loop_value, x)] for x in x_axis_array if (inner_loop_value, x) in results],\n-                        dtype=np.float32,\n-                    )\n-\n-                (x_axis_label, inner_loop_label) = (\n-                    (\"batch_size\", \"len\") if self.args.plot_along_batch else (\"in #tokens\", \"bsz\")\n-                )\n-\n-                x_axis_array = np.asarray(x_axis_array, int)[: len(y_axis_array)]\n-                plt.scatter(\n-                    x_axis_array, y_axis_array, label=f\"{label_model_name} - {inner_loop_label}: {inner_loop_value}\"\n-                )\n-                plt.plot(x_axis_array, y_axis_array, \"--\")\n-\n-            title_str += f\" {label_model_name} vs.\"\n-\n-        title_str = title_str[:-4]\n-        y_axis_label = \"Time in s\" if self.args.is_time else \"Memory in MB\"\n-\n-        # plot\n-        plt.title(title_str)\n-        plt.xlabel(x_axis_label)\n-        plt.ylabel(y_axis_label)\n-        plt.legend()\n-\n-        if self.args.figure_png_file is not None:\n-            plt.savefig(self.args.figure_png_file)\n-        else:\n-            plt.show()\n-\n-\n-def main():\n-    parser = HfArgumentParser(PlotArguments)\n-    plot_args = parser.parse_args_into_dataclasses()[0]\n-    plot = Plot(args=plot_args)\n-    plot.plot()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "68c56b321909d91ccabb62ebc9a6bca869e9c288",
            "filename": "examples/legacy/benchmarking/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fbenchmarking%2Frequirements.txt?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1 +0,0 @@\n-torch >= 1.3\n\\ No newline at end of file"
        },
        {
            "sha": "1bdd69bbe29d6e8626f5046359619d0b168f0507",
            "filename": "examples/legacy/benchmarking/run_benchmark.py",
            "status": "removed",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2Frun_benchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fbenchmarking%2Frun_benchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fbenchmarking%2Frun_benchmark.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,47 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Benchmarking the library on inference and training\"\"\"\n-\n-from transformers import HfArgumentParser, PyTorchBenchmark, PyTorchBenchmarkArguments\n-\n-\n-def main():\n-    parser = HfArgumentParser(PyTorchBenchmarkArguments)\n-    try:\n-        benchmark_args = parser.parse_args_into_dataclasses()[0]\n-    except ValueError as e:\n-        arg_error_msg = \"Arg --no_{0} is no longer used, please use --no-{0} instead.\"\n-        begin_error_msg = \" \".join(str(e).split(\" \")[:-1])\n-        full_error_msg = \"\"\n-        depreciated_args = eval(str(e).split(\" \")[-1])\n-        wrong_args = []\n-        for arg in depreciated_args:\n-            # arg[2:] removes '--'\n-            if arg[2:] in PyTorchBenchmarkArguments.deprecated_args:\n-                # arg[5:] removes '--no_'\n-                full_error_msg += arg_error_msg.format(arg[5:])\n-            else:\n-                wrong_args.append(arg)\n-        if len(wrong_args) > 0:\n-            full_error_msg = full_error_msg + begin_error_msg + str(wrong_args)\n-        raise ValueError(full_error_msg)\n-\n-    benchmark = PyTorchBenchmark(args=benchmark_args)\n-    benchmark.run()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "0b33e47c03c8706da9048c46f3652bdd3bb36fe5",
            "filename": "examples/legacy/multiple_choice/run_multiple_choice.py",
            "status": "removed",
            "additions": 0,
            "deletions": 232,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,232 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Finetuning the library models for multiple choice (Bert, Roberta, XLNet).\"\"\"\n-\n-import logging\n-import os\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import numpy as np\n-from utils_multiple_choice import MultipleChoiceDataset, Split, processors\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoModelForMultipleChoice,\n-    AutoTokenizer,\n-    DataCollatorWithPadding,\n-    EvalPrediction,\n-    HfArgumentParser,\n-    Trainer,\n-    TrainingArguments,\n-    set_seed,\n-)\n-from transformers.trainer_utils import is_main_process\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def simple_accuracy(preds, labels):\n-    return (preds == labels).mean()\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    task_name: str = field(metadata={\"help\": \"The name of the task to train on: \" + \", \".join(processors.keys())})\n-    data_dir: str = field(metadata={\"help\": \"Should contain the data files for the task.\"})\n-    max_seq_length: int = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_process_index,\n-        training_args.device,\n-        training_args.n_gpu,\n-        bool(training_args.parallel_mode.value == \"distributed\"),\n-        training_args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_process_index):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Set seed\n-    set_seed(training_args.seed)\n-\n-    try:\n-        processor = processors[data_args.task_name]()\n-        label_list = processor.get_labels()\n-        num_labels = len(label_list)\n-    except KeyError:\n-        raise ValueError(\"Task not found: %s\" % (data_args.task_name))\n-\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=data_args.task_name,\n-        cache_dir=model_args.cache_dir,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-    )\n-    model = AutoModelForMultipleChoice.from_pretrained(\n-        model_args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    # Get datasets\n-    train_dataset = (\n-        MultipleChoiceDataset(\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            task=data_args.task_name,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-            mode=Split.train,\n-        )\n-        if training_args.do_train\n-        else None\n-    )\n-    eval_dataset = (\n-        MultipleChoiceDataset(\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            task=data_args.task_name,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-            mode=Split.dev,\n-        )\n-        if training_args.do_eval\n-        else None\n-    )\n-\n-    def compute_metrics(p: EvalPrediction) -> dict:\n-        preds = np.argmax(p.predictions, axis=1)\n-        return {\"acc\": simple_accuracy(preds, p.label_ids)}\n-\n-    # Data collator\n-    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) if training_args.fp16 else None\n-\n-    # Initialize our Trainer\n-    trainer = Trainer(\n-        model=model,\n-        args=training_args,\n-        train_dataset=train_dataset,\n-        eval_dataset=eval_dataset,\n-        compute_metrics=compute_metrics,\n-        data_collator=data_collator,\n-    )\n-\n-    # Training\n-    if training_args.do_train:\n-        trainer.train(\n-            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n-        )\n-        trainer.save_model()\n-        # For convenience, we also re-save the tokenizer to the same directory,\n-        # so that you can share your model easily on huggingface.co/models =)\n-        if trainer.is_world_master():\n-            tokenizer.save_pretrained(training_args.output_dir)\n-\n-    # Evaluation\n-    results = {}\n-    if training_args.do_eval:\n-        logger.info(\"*** Evaluate ***\")\n-\n-        result = trainer.evaluate()\n-\n-        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n-        if trainer.is_world_master():\n-            with open(output_eval_file, \"w\") as writer:\n-                logger.info(\"***** Eval results *****\")\n-                for key, value in result.items():\n-                    logger.info(\"  %s = %s\", key, value)\n-                    writer.write(\"{} = {}\\n\".format(key, value))\n-\n-                results.update(result)\n-\n-    return results\n-\n-\n-def _mp_fn(index):\n-    # For xla_spawn (TPUs)\n-    main()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "64d3604f9ca4b37a8b1bac584ed623e6f9b5a661",
            "filename": "examples/legacy/multiple_choice/utils_multiple_choice.py",
            "status": "removed",
            "additions": 0,
            "deletions": 483,
            "changes": 483,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Futils_multiple_choice.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,483 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Multiple choice fine-tuning: utilities to work with multiple choice tasks of reading comprehension\"\"\"\n-\n-import csv\n-import glob\n-import json\n-import logging\n-import os\n-from dataclasses import dataclass\n-from enum import Enum\n-from typing import Optional\n-\n-import tqdm\n-from filelock import FileLock\n-\n-from transformers import PreTrainedTokenizer, is_torch_available\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass(frozen=True)\n-class InputExample:\n-    \"\"\"\n-    A single training/test example for multiple choice\n-\n-    Args:\n-        example_id: Unique id for the example.\n-        question: string. The untokenized text of the second sequence (question).\n-        contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n-        endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n-        label: (Optional) string. The label of the example. This should be\n-        specified for train and dev examples, but not for test examples.\n-    \"\"\"\n-\n-    example_id: str\n-    question: str\n-    contexts: list[str]\n-    endings: list[str]\n-    label: Optional[str]\n-\n-\n-@dataclass(frozen=True)\n-class InputFeatures:\n-    \"\"\"\n-    A single set of features of data.\n-    Property names are the same names as the corresponding inputs to a model.\n-    \"\"\"\n-\n-    example_id: str\n-    input_ids: list[list[int]]\n-    attention_mask: Optional[list[list[int]]]\n-    token_type_ids: Optional[list[list[int]]]\n-    label: Optional[int]\n-\n-\n-class Split(Enum):\n-    train = \"train\"\n-    dev = \"dev\"\n-    test = \"test\"\n-\n-\n-if is_torch_available():\n-    import torch\n-    from torch.utils.data import Dataset\n-\n-    class MultipleChoiceDataset(Dataset):\n-        features: list[InputFeatures]\n-\n-        def __init__(\n-            self,\n-            data_dir: str,\n-            tokenizer: PreTrainedTokenizer,\n-            task: str,\n-            max_seq_length: Optional[int] = None,\n-            overwrite_cache=False,\n-            mode: Split = Split.train,\n-        ):\n-            processor = processors[task]()\n-\n-            cached_features_file = os.path.join(\n-                data_dir,\n-                \"cached_{}_{}_{}_{}\".format(\n-                    mode.value,\n-                    tokenizer.__class__.__name__,\n-                    str(max_seq_length),\n-                    task,\n-                ),\n-            )\n-\n-            # Make sure only the first process in distributed training processes the dataset,\n-            # and the others will use the cache.\n-            lock_path = cached_features_file + \".lock\"\n-            with FileLock(lock_path):\n-                if os.path.exists(cached_features_file) and not overwrite_cache:\n-                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n-                    self.features = torch.load(cached_features_file, weights_only=True)\n-                else:\n-                    logger.info(f\"Creating features from dataset file at {data_dir}\")\n-                    label_list = processor.get_labels()\n-                    if mode == Split.dev:\n-                        examples = processor.get_dev_examples(data_dir)\n-                    elif mode == Split.test:\n-                        examples = processor.get_test_examples(data_dir)\n-                    else:\n-                        examples = processor.get_train_examples(data_dir)\n-                    logger.info(\"Training examples: %s\", len(examples))\n-                    self.features = convert_examples_to_features(\n-                        examples,\n-                        label_list,\n-                        max_seq_length,\n-                        tokenizer,\n-                    )\n-                    logger.info(\"Saving features into cached file %s\", cached_features_file)\n-                    torch.save(self.features, cached_features_file)\n-\n-        def __len__(self):\n-            return len(self.features)\n-\n-        def __getitem__(self, i) -> InputFeatures:\n-            return self.features[i]\n-\n-\n-class DataProcessor:\n-    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n-\n-    def get_train_examples(self, data_dir):\n-        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n-        raise NotImplementedError()\n-\n-    def get_dev_examples(self, data_dir):\n-        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n-        raise NotImplementedError()\n-\n-    def get_test_examples(self, data_dir):\n-        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n-        raise NotImplementedError()\n-\n-    def get_labels(self):\n-        \"\"\"Gets the list of labels for this data set.\"\"\"\n-        raise NotImplementedError()\n-\n-\n-class RaceProcessor(DataProcessor):\n-    \"\"\"Processor for the RACE data set.\"\"\"\n-\n-    def get_train_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} train\")\n-        high = os.path.join(data_dir, \"train/high\")\n-        middle = os.path.join(data_dir, \"train/middle\")\n-        high = self._read_txt(high)\n-        middle = self._read_txt(middle)\n-        return self._create_examples(high + middle, \"train\")\n-\n-    def get_dev_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} dev\")\n-        high = os.path.join(data_dir, \"dev/high\")\n-        middle = os.path.join(data_dir, \"dev/middle\")\n-        high = self._read_txt(high)\n-        middle = self._read_txt(middle)\n-        return self._create_examples(high + middle, \"dev\")\n-\n-    def get_test_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} test\")\n-        high = os.path.join(data_dir, \"test/high\")\n-        middle = os.path.join(data_dir, \"test/middle\")\n-        high = self._read_txt(high)\n-        middle = self._read_txt(middle)\n-        return self._create_examples(high + middle, \"test\")\n-\n-    def get_labels(self):\n-        \"\"\"See base class.\"\"\"\n-        return [\"0\", \"1\", \"2\", \"3\"]\n-\n-    def _read_txt(self, input_dir):\n-        lines = []\n-        files = glob.glob(input_dir + \"/*txt\")\n-        for file in tqdm.tqdm(files, desc=\"read files\"):\n-            with open(file, encoding=\"utf-8\") as fin:\n-                data_raw = json.load(fin)\n-                data_raw[\"race_id\"] = file\n-                lines.append(data_raw)\n-        return lines\n-\n-    def _create_examples(self, lines, set_type):\n-        \"\"\"Creates examples for the training and dev sets.\"\"\"\n-        examples = []\n-        for _, data_raw in enumerate(lines):\n-            race_id = \"{}-{}\".format(set_type, data_raw[\"race_id\"])\n-            article = data_raw[\"article\"]\n-            for i in range(len(data_raw[\"answers\"])):\n-                truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n-                question = data_raw[\"questions\"][i]\n-                options = data_raw[\"options\"][i]\n-\n-                examples.append(\n-                    InputExample(\n-                        example_id=race_id,\n-                        question=question,\n-                        contexts=[article, article, article, article],  # this is not efficient but convenient\n-                        endings=[options[0], options[1], options[2], options[3]],\n-                        label=truth,\n-                    )\n-                )\n-        return examples\n-\n-\n-class SynonymProcessor(DataProcessor):\n-    \"\"\"Processor for the Synonym data set.\"\"\"\n-\n-    def get_train_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} train\")\n-        return self._create_examples(self._read_csv(os.path.join(data_dir, \"mctrain.csv\")), \"train\")\n-\n-    def get_dev_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} dev\")\n-        return self._create_examples(self._read_csv(os.path.join(data_dir, \"mchp.csv\")), \"dev\")\n-\n-    def get_test_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} dev\")\n-\n-        return self._create_examples(self._read_csv(os.path.join(data_dir, \"mctest.csv\")), \"test\")\n-\n-    def get_labels(self):\n-        \"\"\"See base class.\"\"\"\n-        return [\"0\", \"1\", \"2\", \"3\", \"4\"]\n-\n-    def _read_csv(self, input_file):\n-        with open(input_file, encoding=\"utf-8\") as f:\n-            return list(csv.reader(f))\n-\n-    def _create_examples(self, lines: list[list[str]], type: str):\n-        \"\"\"Creates examples for the training and dev sets.\"\"\"\n-\n-        examples = [\n-            InputExample(\n-                example_id=line[0],\n-                question=\"\",  # in the swag dataset, the\n-                # common beginning of each\n-                # choice is stored in \"sent2\".\n-                contexts=[line[1], line[1], line[1], line[1], line[1]],\n-                endings=[line[2], line[3], line[4], line[5], line[6]],\n-                label=line[7],\n-            )\n-            for line in lines  # we skip the line with the column names\n-        ]\n-\n-        return examples\n-\n-\n-class SwagProcessor(DataProcessor):\n-    \"\"\"Processor for the SWAG data set.\"\"\"\n-\n-    def get_train_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} train\")\n-        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n-\n-    def get_dev_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} dev\")\n-        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n-\n-    def get_test_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} dev\")\n-        raise ValueError(\n-            \"For swag testing, the input file does not contain a label column. It can not be tested in current code \"\n-            \"setting!\"\n-        )\n-        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n-\n-    def get_labels(self):\n-        \"\"\"See base class.\"\"\"\n-        return [\"0\", \"1\", \"2\", \"3\"]\n-\n-    def _read_csv(self, input_file):\n-        with open(input_file, encoding=\"utf-8\") as f:\n-            return list(csv.reader(f))\n-\n-    def _create_examples(self, lines: list[list[str]], type: str):\n-        \"\"\"Creates examples for the training and dev sets.\"\"\"\n-        if type == \"train\" and lines[0][-1] != \"label\":\n-            raise ValueError(\"For training, the input file must contain a label column.\")\n-\n-        examples = [\n-            InputExample(\n-                example_id=line[2],\n-                question=line[5],  # in the swag dataset, the\n-                # common beginning of each\n-                # choice is stored in \"sent2\".\n-                contexts=[line[4], line[4], line[4], line[4]],\n-                endings=[line[7], line[8], line[9], line[10]],\n-                label=line[11],\n-            )\n-            for line in lines[1:]  # we skip the line with the column names\n-        ]\n-\n-        return examples\n-\n-\n-class ArcProcessor(DataProcessor):\n-    \"\"\"Processor for the ARC data set (request from allennlp).\"\"\"\n-\n-    def get_train_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} train\")\n-        return self._create_examples(self._read_json(os.path.join(data_dir, \"train.jsonl\")), \"train\")\n-\n-    def get_dev_examples(self, data_dir):\n-        \"\"\"See base class.\"\"\"\n-        logger.info(f\"LOOKING AT {data_dir} dev\")\n-        return self._create_examples(self._read_json(os.path.join(data_dir, \"dev.jsonl\")), \"dev\")\n-\n-    def get_test_examples(self, data_dir):\n-        logger.info(f\"LOOKING AT {data_dir} test\")\n-        return self._create_examples(self._read_json(os.path.join(data_dir, \"test.jsonl\")), \"test\")\n-\n-    def get_labels(self):\n-        \"\"\"See base class.\"\"\"\n-        return [\"0\", \"1\", \"2\", \"3\"]\n-\n-    def _read_json(self, input_file):\n-        with open(input_file, encoding=\"utf-8\") as fin:\n-            lines = fin.readlines()\n-            return lines\n-\n-    def _create_examples(self, lines, type):\n-        \"\"\"Creates examples for the training and dev sets.\"\"\"\n-\n-        # There are two types of labels. They should be normalized\n-        def normalize(truth):\n-            if truth in \"ABCD\":\n-                return ord(truth) - ord(\"A\")\n-            elif truth in \"1234\":\n-                return int(truth) - 1\n-            else:\n-                logger.info(\"truth ERROR! %s\", str(truth))\n-                return None\n-\n-        examples = []\n-        three_choice = 0\n-        four_choice = 0\n-        five_choice = 0\n-        other_choices = 0\n-        # we deleted example which has more than or less than four choices\n-        for line in tqdm.tqdm(lines, desc=\"read arc data\"):\n-            data_raw = json.loads(line.strip(\"\\n\"))\n-            if len(data_raw[\"question\"][\"choices\"]) == 3:\n-                three_choice += 1\n-                continue\n-            elif len(data_raw[\"question\"][\"choices\"]) == 5:\n-                five_choice += 1\n-                continue\n-            elif len(data_raw[\"question\"][\"choices\"]) != 4:\n-                other_choices += 1\n-                continue\n-            four_choice += 1\n-            truth = str(normalize(data_raw[\"answerKey\"]))\n-            assert truth != \"None\"\n-            question_choices = data_raw[\"question\"]\n-            question = question_choices[\"stem\"]\n-            id = data_raw[\"id\"]\n-            options = question_choices[\"choices\"]\n-            if len(options) == 4:\n-                examples.append(\n-                    InputExample(\n-                        example_id=id,\n-                        question=question,\n-                        contexts=[\n-                            options[0][\"para\"].replace(\"_\", \"\"),\n-                            options[1][\"para\"].replace(\"_\", \"\"),\n-                            options[2][\"para\"].replace(\"_\", \"\"),\n-                            options[3][\"para\"].replace(\"_\", \"\"),\n-                        ],\n-                        endings=[options[0][\"text\"], options[1][\"text\"], options[2][\"text\"], options[3][\"text\"]],\n-                        label=truth,\n-                    )\n-                )\n-\n-        if type == \"train\":\n-            assert len(examples) > 1\n-            assert examples[0].label is not None\n-        logger.info(\"len examples: %s}\", str(len(examples)))\n-        logger.info(\"Three choices: %s\", str(three_choice))\n-        logger.info(\"Five choices: %s\", str(five_choice))\n-        logger.info(\"Other choices: %s\", str(other_choices))\n-        logger.info(\"four choices: %s\", str(four_choice))\n-\n-        return examples\n-\n-\n-def convert_examples_to_features(\n-    examples: list[InputExample],\n-    label_list: list[str],\n-    max_length: int,\n-    tokenizer: PreTrainedTokenizer,\n-) -> list[InputFeatures]:\n-    \"\"\"\n-    Loads a data file into a list of `InputFeatures`\n-    \"\"\"\n-\n-    label_map = {label: i for i, label in enumerate(label_list)}\n-\n-    features = []\n-    for ex_index, example in tqdm.tqdm(enumerate(examples), desc=\"convert examples to features\"):\n-        if ex_index % 10000 == 0:\n-            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n-        choices_inputs = []\n-        for ending_idx, (context, ending) in enumerate(zip(example.contexts, example.endings)):\n-            text_a = context\n-            if example.question.find(\"_\") != -1:\n-                # this is for cloze question\n-                text_b = example.question.replace(\"_\", ending)\n-            else:\n-                text_b = example.question + \" \" + ending\n-\n-            inputs = tokenizer(\n-                text_a,\n-                text_b,\n-                add_special_tokens=True,\n-                max_length=max_length,\n-                padding=\"max_length\",\n-                truncation=True,\n-                return_overflowing_tokens=True,\n-            )\n-            if \"num_truncated_tokens\" in inputs and inputs[\"num_truncated_tokens\"] > 0:\n-                logger.info(\n-                    \"Attention! you are cropping tokens (swag task is ok). \"\n-                    \"If you are training ARC and RACE and you are popping question + options, \"\n-                    \"you need to try to use a bigger max seq length!\"\n-                )\n-\n-            choices_inputs.append(inputs)\n-\n-        label = label_map[example.label]\n-\n-        input_ids = [x[\"input_ids\"] for x in choices_inputs]\n-        attention_mask = (\n-            [x[\"attention_mask\"] for x in choices_inputs] if \"attention_mask\" in choices_inputs[0] else None\n-        )\n-        token_type_ids = (\n-            [x[\"token_type_ids\"] for x in choices_inputs] if \"token_type_ids\" in choices_inputs[0] else None\n-        )\n-\n-        features.append(\n-            InputFeatures(\n-                example_id=example.example_id,\n-                input_ids=input_ids,\n-                attention_mask=attention_mask,\n-                token_type_ids=token_type_ids,\n-                label=label,\n-            )\n-        )\n-\n-    for f in features[:2]:\n-        logger.info(\"*** Example ***\")\n-        logger.info(\"feature: %s\" % f)\n-\n-    return features\n-\n-\n-processors = {\"race\": RaceProcessor, \"swag\": SwagProcessor, \"arc\": ArcProcessor, \"syn\": SynonymProcessor}\n-MULTIPLE_CHOICE_TASKS_NUM_LABELS = {\"race\", 4, \"swag\", 4, \"arc\", 4, \"syn\", 5}"
        },
        {
            "sha": "64d28135943f2f486a9e9980a62d5d57e0ec661e",
            "filename": "examples/legacy/pytorch-lightning/lightning_base.py",
            "status": "removed",
            "additions": 0,
            "deletions": 397,
            "changes": 397,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,397 +0,0 @@\n-import argparse\n-import logging\n-import os\n-from pathlib import Path\n-from typing import Any\n-\n-import pytorch_lightning as pl\n-from pytorch_lightning.utilities import rank_zero_info\n-\n-from transformers import (\n-    AutoConfig,\n-    AutoModel,\n-    AutoModelForPreTraining,\n-    AutoModelForQuestionAnswering,\n-    AutoModelForSeq2SeqLM,\n-    AutoModelForSequenceClassification,\n-    AutoModelForTokenClassification,\n-    AutoModelWithLMHead,\n-    AutoTokenizer,\n-    PreTrainedConfig,\n-    PreTrainedTokenizer,\n-    is_torch_available,\n-)\n-from transformers.optimization import (\n-    Adafactor,\n-    get_cosine_schedule_with_warmup,\n-    get_cosine_with_hard_restarts_schedule_with_warmup,\n-    get_linear_schedule_with_warmup,\n-    get_polynomial_decay_schedule_with_warmup,\n-)\n-from transformers.utils.versions import require_version\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-require_version(\"pytorch_lightning>=1.0.4\")\n-\n-MODEL_MODES = {\n-    \"base\": AutoModel,\n-    \"sequence-classification\": AutoModelForSequenceClassification,\n-    \"question-answering\": AutoModelForQuestionAnswering,\n-    \"pretraining\": AutoModelForPreTraining,\n-    \"token-classification\": AutoModelForTokenClassification,\n-    \"language-modeling\": AutoModelWithLMHead,\n-    \"summarization\": AutoModelForSeq2SeqLM,\n-    \"translation\": AutoModelForSeq2SeqLM,\n-}\n-\n-\n-# update this and the import above to support new schedulers from transformers.optimization\n-arg_to_scheduler = {\n-    \"linear\": get_linear_schedule_with_warmup,\n-    \"cosine\": get_cosine_schedule_with_warmup,\n-    \"cosine_w_restarts\": get_cosine_with_hard_restarts_schedule_with_warmup,\n-    \"polynomial\": get_polynomial_decay_schedule_with_warmup,\n-    # '': get_constant_schedule,             # not supported for now\n-    # '': get_constant_schedule_with_warmup, # not supported for now\n-}\n-arg_to_scheduler_choices = sorted(arg_to_scheduler.keys())\n-arg_to_scheduler_metavar = \"{\" + \", \".join(arg_to_scheduler_choices) + \"}\"\n-\n-\n-class BaseTransformer(pl.LightningModule):\n-    def __init__(\n-        self,\n-        hparams: argparse.Namespace,\n-        num_labels=None,\n-        mode=\"base\",\n-        config=None,\n-        tokenizer=None,\n-        model=None,\n-        **config_kwargs,\n-    ):\n-        \"\"\"Initialize a model, tokenizer and config.\"\"\"\n-        super().__init__()\n-        # TODO: move to self.save_hyperparameters()\n-        # self.save_hyperparameters()\n-        # can also expand arguments into trainer signature for easier reading\n-\n-        self.save_hyperparameters(hparams)\n-        self.step_count = 0\n-        self.output_dir = Path(self.hparams.output_dir)\n-        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n-        if config is None:\n-            self.config = AutoConfig.from_pretrained(\n-                self.hparams.config_name if self.hparams.config_name else self.hparams.model_name_or_path,\n-                **({\"num_labels\": num_labels} if num_labels is not None else {}),\n-                cache_dir=cache_dir,\n-                **config_kwargs,\n-            )\n-        else:\n-            self.config: PreTrainedConfig = config\n-\n-        extra_model_params = (\"encoder_layerdrop\", \"decoder_layerdrop\", \"dropout\", \"attention_dropout\")\n-        for p in extra_model_params:\n-            if getattr(self.hparams, p, None):\n-                assert hasattr(self.config, p), f\"model config doesn't have a `{p}` attribute\"\n-                setattr(self.config, p, getattr(self.hparams, p))\n-\n-        if tokenizer is None:\n-            self.tokenizer = AutoTokenizer.from_pretrained(\n-                self.hparams.tokenizer_name if self.hparams.tokenizer_name else self.hparams.model_name_or_path,\n-                cache_dir=cache_dir,\n-            )\n-        else:\n-            self.tokenizer: PreTrainedTokenizer = tokenizer\n-        self.model_type = MODEL_MODES[mode]\n-        if model is None:\n-            self.model = self.model_type.from_pretrained(\n-                self.hparams.model_name_or_path,\n-                from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path),\n-                config=self.config,\n-                cache_dir=cache_dir,\n-            )\n-        else:\n-            self.model = model\n-\n-    def load_hf_checkpoint(self, *args, **kwargs):\n-        self.model = self.model_type.from_pretrained(*args, **kwargs)\n-\n-    def get_lr_scheduler(self):\n-        get_schedule_func = arg_to_scheduler[self.hparams.lr_scheduler]\n-        scheduler = get_schedule_func(\n-            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.total_steps()\n-        )\n-        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n-        return scheduler\n-\n-    def configure_optimizers(self):\n-        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n-        model = self.model\n-        no_decay = [\"bias\", \"LayerNorm.weight\"]\n-        optimizer_grouped_parameters = [\n-            {\n-                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n-                \"weight_decay\": self.hparams.weight_decay,\n-            },\n-            {\n-                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n-                \"weight_decay\": 0.0,\n-            },\n-        ]\n-        if self.hparams.adafactor:\n-            optimizer = Adafactor(\n-                optimizer_grouped_parameters, lr=self.hparams.learning_rate, scale_parameter=False, relative_step=False\n-            )\n-\n-        else:\n-            optimizer = torch.optim.AdamW(\n-                optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon\n-            )\n-        self.opt = optimizer\n-\n-        scheduler = self.get_lr_scheduler()\n-\n-        return [optimizer], [scheduler]\n-\n-    def test_step(self, batch, batch_nb):\n-        return self.validation_step(batch, batch_nb)\n-\n-    def test_epoch_end(self, outputs):\n-        return self.validation_end(outputs)\n-\n-    def total_steps(self) -> int:\n-        \"\"\"The number of total training steps that will be run. Used for lr scheduler purposes.\"\"\"\n-        num_devices = max(1, self.hparams.gpus)  # TODO: consider num_tpu_cores\n-        effective_batch_size = self.hparams.train_batch_size * self.hparams.accumulate_grad_batches * num_devices\n-        return (self.dataset_size / effective_batch_size) * self.hparams.max_epochs\n-\n-    def setup(self, mode):\n-        if mode == \"test\":\n-            self.dataset_size = len(self.test_dataloader().dataset)\n-        else:\n-            self.train_loader = self.get_dataloader(\"train\", self.hparams.train_batch_size, shuffle=True)\n-            self.dataset_size = len(self.train_dataloader().dataset)\n-\n-    def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False):\n-        raise NotImplementedError(\"You must implement this for your task\")\n-\n-    def train_dataloader(self):\n-        return self.train_loader\n-\n-    def val_dataloader(self):\n-        return self.get_dataloader(\"dev\", self.hparams.eval_batch_size, shuffle=False)\n-\n-    def test_dataloader(self):\n-        return self.get_dataloader(\"test\", self.hparams.eval_batch_size, shuffle=False)\n-\n-    def _feature_file(self, mode):\n-        return os.path.join(\n-            self.hparams.data_dir,\n-            \"cached_{}_{}_{}\".format(\n-                mode,\n-                list(filter(None, self.hparams.model_name_or_path.split(\"/\"))).pop(),\n-                str(self.hparams.max_seq_length),\n-            ),\n-        )\n-\n-    @pl.utilities.rank_zero_only\n-    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\n-        save_path = self.output_dir.joinpath(\"best_tfmr\")\n-        self.model.config.save_step = self.step_count\n-        self.model.save_pretrained(save_path)\n-        self.tokenizer.save_pretrained(save_path)\n-\n-    @staticmethod\n-    def add_model_specific_args(parser, root_dir):\n-        parser.add_argument(\n-            \"--model_name_or_path\",\n-            default=None,\n-            type=str,\n-            required=True,\n-            help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-        )\n-        parser.add_argument(\n-            \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n-        )\n-        parser.add_argument(\n-            \"--tokenizer_name\",\n-            default=None,\n-            type=str,\n-            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-        )\n-        parser.add_argument(\n-            \"--cache_dir\",\n-            default=\"\",\n-            type=str,\n-            help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n-        )\n-        parser.add_argument(\n-            \"--encoder_layerdrop\",\n-            type=float,\n-            help=\"Encoder layer dropout probability (Optional). Goes into model.config\",\n-        )\n-        parser.add_argument(\n-            \"--decoder_layerdrop\",\n-            type=float,\n-            help=\"Decoder layer dropout probability (Optional). Goes into model.config\",\n-        )\n-        parser.add_argument(\n-            \"--dropout\",\n-            type=float,\n-            help=\"Dropout probability (Optional). Goes into model.config\",\n-        )\n-        parser.add_argument(\n-            \"--attention_dropout\",\n-            type=float,\n-            help=\"Attention dropout probability (Optional). Goes into model.config\",\n-        )\n-        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n-        parser.add_argument(\n-            \"--lr_scheduler\",\n-            default=\"linear\",\n-            choices=arg_to_scheduler_choices,\n-            metavar=arg_to_scheduler_metavar,\n-            type=str,\n-            help=\"Learning rate scheduler\",\n-        )\n-        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-        parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-        parser.add_argument(\"--num_workers\", default=4, type=int, help=\"kwarg passed to DataLoader\")\n-        parser.add_argument(\"--num_train_epochs\", dest=\"max_epochs\", default=3, type=int)\n-        parser.add_argument(\"--train_batch_size\", default=32, type=int)\n-        parser.add_argument(\"--eval_batch_size\", default=32, type=int)\n-        parser.add_argument(\"--adafactor\", action=\"store_true\")\n-\n-\n-class LoggingCallback(pl.Callback):\n-    def on_batch_end(self, trainer, pl_module):\n-        lr_scheduler = trainer.lr_schedulers[0][\"scheduler\"]\n-        lrs = {f\"lr_group_{i}\": lr for i, lr in enumerate(lr_scheduler.get_lr())}\n-        pl_module.logger.log_metrics(lrs)\n-\n-    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n-        rank_zero_info(\"***** Validation results *****\")\n-        metrics = trainer.callback_metrics\n-        # Log results\n-        for key in sorted(metrics):\n-            if key not in [\"log\", \"progress_bar\"]:\n-                rank_zero_info(f\"{key} = {str(metrics[key])}\\n\")\n-\n-    def on_test_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n-        rank_zero_info(\"***** Test results *****\")\n-        metrics = trainer.callback_metrics\n-        # Log and save results to file\n-        output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n-        with open(output_test_results_file, \"w\") as writer:\n-            for key in sorted(metrics):\n-                if key not in [\"log\", \"progress_bar\"]:\n-                    rank_zero_info(f\"{key} = {str(metrics[key])}\\n\")\n-                    writer.write(f\"{key} = {str(metrics[key])}\\n\")\n-\n-\n-def add_generic_args(parser, root_dir) -> None:\n-    #  To allow all pl args uncomment the following line\n-    #  parser = pl.Trainer.add_argparse_args(parser)\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model predictions and checkpoints will be written.\",\n-    )\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O2\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\"--n_tpu_cores\", dest=\"tpu_cores\", type=int)\n-    parser.add_argument(\"--max_grad_norm\", dest=\"gradient_clip_val\", default=1.0, type=float, help=\"Max gradient norm\")\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        dest=\"accumulate_grad_batches\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\",\n-    )\n-\n-\n-def generic_train(\n-    model: BaseTransformer,\n-    args: argparse.Namespace,\n-    early_stopping_callback=None,\n-    logger=True,  # can pass WandbLogger() here\n-    extra_callbacks=[],\n-    checkpoint_callback=None,\n-    logging_callback=None,\n-    **extra_train_kwargs,\n-):\n-    pl.seed_everything(args.seed)\n-\n-    # init model\n-    odir = Path(model.hparams.output_dir)\n-    odir.mkdir(exist_ok=True)\n-\n-    # add custom checkpoints\n-    if checkpoint_callback is None:\n-        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n-            filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=1\n-        )\n-    if early_stopping_callback:\n-        extra_callbacks.append(early_stopping_callback)\n-    if logging_callback is None:\n-        logging_callback = LoggingCallback()\n-\n-    train_params = {}\n-\n-    # TODO: remove with PyTorch 1.6 since pl uses native amp\n-    if args.fp16:\n-        train_params[\"precision\"] = 16\n-        train_params[\"amp_level\"] = args.fp16_opt_level\n-\n-    if args.gpus > 1:\n-        train_params[\"distributed_backend\"] = \"ddp\"\n-\n-    train_params[\"accumulate_grad_batches\"] = args.accumulate_grad_batches\n-    train_params[\"accelerator\"] = extra_train_kwargs.get(\"accelerator\")\n-    train_params[\"profiler\"] = extra_train_kwargs.get(\"profiler\")\n-\n-    trainer = pl.Trainer.from_argparse_args(\n-        args,\n-        weights_summary=None,\n-        callbacks=[logging_callback] + extra_callbacks,\n-        logger=logger,\n-        checkpoint_callback=checkpoint_callback,\n-        **train_params,\n-    )\n-\n-    if args.do_train:\n-        trainer.fit(model)\n-\n-    return trainer"
        },
        {
            "sha": "a6f2d6dce5a9d53013a3c124da6198901486867d",
            "filename": "examples/legacy/pytorch-lightning/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frequirements.txt?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,21 +0,0 @@\n-tensorboard\n-scikit-learn\n-seqeval\n-psutil\n-sacrebleu\n-rouge-score\n-tensorflow_datasets\n-matplotlib\n-git-python==1.0.3\n-faiss-cpu\n-streamlit\n-elasticsearch\n-nltk\n-pandas\n-datasets >= 1.1.3\n-fire\n-pytest<8.0.1\n-conllu\n-sentencepiece != 0.1.92\n-protobuf\n-ray"
        },
        {
            "sha": "388e69728b79600089d7a6107ecd98eb4d8683c3",
            "filename": "examples/legacy/pytorch-lightning/run_glue.py",
            "status": "removed",
            "additions": 0,
            "deletions": 201,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,201 +0,0 @@\n-import argparse\n-import glob\n-import logging\n-import os\n-import time\n-from argparse import Namespace\n-\n-import numpy as np\n-import torch\n-from lightning_base import BaseTransformer, add_generic_args, generic_train\n-from torch.utils.data import DataLoader, TensorDataset\n-\n-from transformers import glue_compute_metrics as compute_metrics\n-from transformers import glue_convert_examples_to_features as convert_examples_to_features\n-from transformers import glue_output_modes, glue_tasks_num_labels\n-from transformers import glue_processors as processors\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class GLUETransformer(BaseTransformer):\n-    mode = \"sequence-classification\"\n-\n-    def __init__(self, hparams):\n-        if isinstance(hparams, dict):\n-            hparams = Namespace(**hparams)\n-        hparams.glue_output_mode = glue_output_modes[hparams.task]\n-        num_labels = glue_tasks_num_labels[hparams.task]\n-\n-        super().__init__(hparams, num_labels, self.mode)\n-\n-    def forward(self, **inputs):\n-        return self.model(**inputs)\n-\n-    def training_step(self, batch, batch_idx):\n-        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n-\n-        if self.config.model_type not in [\"distilbert\", \"bart\"]:\n-            inputs[\"token_type_ids\"] = batch[2] if self.config.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n-\n-        outputs = self(**inputs)\n-        loss = outputs[0]\n-\n-        lr_scheduler = self.trainer.lr_schedulers[0][\"scheduler\"]\n-        tensorboard_logs = {\"loss\": loss, \"rate\": lr_scheduler.get_last_lr()[-1]}\n-        return {\"loss\": loss, \"log\": tensorboard_logs}\n-\n-    def prepare_data(self):\n-        \"Called to initialize data. Use the call to construct features\"\n-        args = self.hparams\n-        processor = processors[args.task]()\n-        self.labels = processor.get_labels()\n-\n-        for mode in [\"train\", \"dev\"]:\n-            cached_features_file = self._feature_file(mode)\n-            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n-                logger.info(\"Loading features from cached file %s\", cached_features_file)\n-            else:\n-                logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n-                examples = (\n-                    processor.get_dev_examples(args.data_dir)\n-                    if mode == \"dev\"\n-                    else processor.get_train_examples(args.data_dir)\n-                )\n-                features = convert_examples_to_features(\n-                    examples,\n-                    self.tokenizer,\n-                    max_length=args.max_seq_length,\n-                    label_list=self.labels,\n-                    output_mode=args.glue_output_mode,\n-                )\n-                logger.info(\"Saving features into cached file %s\", cached_features_file)\n-                torch.save(features, cached_features_file)\n-\n-    def get_dataloader(self, mode: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n-        \"Load datasets. Called after prepare data.\"\n-\n-        # We test on dev set to compare to benchmarks without having to submit to GLUE server\n-        mode = \"dev\" if mode == \"test\" else mode\n-\n-        cached_features_file = self._feature_file(mode)\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file, weights_only=True)\n-        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n-        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n-        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n-        if self.hparams.glue_output_mode == \"classification\":\n-            all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n-        elif self.hparams.glue_output_mode == \"regression\":\n-            all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n-\n-        return DataLoader(\n-            TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels),\n-            batch_size=batch_size,\n-            shuffle=shuffle,\n-        )\n-\n-    def validation_step(self, batch, batch_idx):\n-        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n-\n-        if self.config.model_type not in [\"distilbert\", \"bart\"]:\n-            inputs[\"token_type_ids\"] = batch[2] if self.config.model_type in [\"bert\", \"xlnet\", \"albert\"] else None\n-\n-        outputs = self(**inputs)\n-        tmp_eval_loss, logits = outputs[:2]\n-        preds = logits.detach().cpu().numpy()\n-        out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n-\n-        return {\"val_loss\": tmp_eval_loss.detach().cpu(), \"pred\": preds, \"target\": out_label_ids}\n-\n-    def _eval_end(self, outputs) -> tuple:\n-        val_loss_mean = torch.stack([x[\"val_loss\"] for x in outputs]).mean().detach().cpu().item()\n-        preds = np.concatenate([x[\"pred\"] for x in outputs], axis=0)\n-\n-        if self.hparams.glue_output_mode == \"classification\":\n-            preds = np.argmax(preds, axis=1)\n-        elif self.hparams.glue_output_mode == \"regression\":\n-            preds = np.squeeze(preds)\n-\n-        out_label_ids = np.concatenate([x[\"target\"] for x in outputs], axis=0)\n-        out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n-        preds_list = [[] for _ in range(out_label_ids.shape[0])]\n-\n-        results = {\"val_loss\": val_loss_mean, **compute_metrics(self.hparams.task, preds, out_label_ids)}\n-\n-        ret = dict(results.items())\n-        ret[\"log\"] = results\n-        return ret, preds_list, out_label_list\n-\n-    def validation_epoch_end(self, outputs: list) -> dict:\n-        ret, preds, targets = self._eval_end(outputs)\n-        logs = ret[\"log\"]\n-        return {\"val_loss\": logs[\"val_loss\"], \"log\": logs, \"progress_bar\": logs}\n-\n-    def test_epoch_end(self, outputs) -> dict:\n-        ret, predictions, targets = self._eval_end(outputs)\n-        logs = ret[\"log\"]\n-        # `val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`\n-        return {\"avg_test_loss\": logs[\"val_loss\"], \"log\": logs, \"progress_bar\": logs}\n-\n-    @staticmethod\n-    def add_model_specific_args(parser, root_dir):\n-        BaseTransformer.add_model_specific_args(parser, root_dir)\n-        parser.add_argument(\n-            \"--max_seq_length\",\n-            default=128,\n-            type=int,\n-            help=(\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            ),\n-        )\n-\n-        parser.add_argument(\n-            \"--task\",\n-            default=\"\",\n-            type=str,\n-            required=True,\n-            help=\"The GLUE task to run\",\n-        )\n-        parser.add_argument(\n-            \"--gpus\",\n-            default=0,\n-            type=int,\n-            help=\"The number of GPUs allocated for this, it is by default 0 meaning none\",\n-        )\n-\n-        parser.add_argument(\n-            \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-        )\n-\n-        return parser\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    add_generic_args(parser, os.getcwd())\n-    parser = GLUETransformer.add_model_specific_args(parser, os.getcwd())\n-    args = parser.parse_args()\n-\n-    # If output_dir not provided, a folder will be generated in pwd\n-    if args.output_dir is None:\n-        args.output_dir = os.path.join(\n-            \"./results\",\n-            f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",\n-        )\n-        os.makedirs(args.output_dir)\n-\n-    model = GLUETransformer(args)\n-    trainer = generic_train(model, args)\n-\n-    # Optionally, predict on dev set and write to output_dir\n-    if args.do_predict:\n-        checkpoints = sorted(glob.glob(os.path.join(args.output_dir, \"checkpoint-epoch=*.ckpt\"), recursive=True))\n-        model = model.load_from_checkpoint(checkpoints[-1])\n-        return trainer.test(model)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "7cd57306d4e18596b5d7ea70847b5ed5fdcdd7dd",
            "filename": "examples/legacy/pytorch-lightning/run_glue.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_glue.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,34 +0,0 @@\n-# Install example requirements\n-pip install -r ../requirements.txt\n-\n-# Download glue data\n-python3 ../../utils/download_glue_data.py\n-\n-export TASK=mrpc\n-export DATA_DIR=./glue_data/MRPC/\n-export MAX_LENGTH=128\n-export LEARNING_RATE=2e-5\n-export BERT_MODEL=bert-base-cased\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SEED=2\n-export OUTPUT_DIR_NAME=mrpc-pl-bert\n-export CURRENT_DIR=${PWD}\n-export OUTPUT_DIR=${CURRENT_DIR}/${OUTPUT_DIR_NAME}\n-\n-# Make output directory if it doesn't exist\n-mkdir -p $OUTPUT_DIR\n-# Add parent directory to python path to access lightning_base.py\n-export PYTHONPATH=\"../\":\"${PYTHONPATH}\"\n-\n-python3 run_glue.py --gpus 1 --data_dir $DATA_DIR \\\n---task $TASK \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---learning_rate $LEARNING_RATE \\\n---num_train_epochs $NUM_EPOCHS \\\n---train_batch_size $BATCH_SIZE \\\n---seed $SEED \\\n---do_train \\\n---do_predict"
        },
        {
            "sha": "6cbb138f023fb47873f7c9d833ae3ef9005e51d4",
            "filename": "examples/legacy/pytorch-lightning/run_ner.py",
            "status": "removed",
            "additions": 0,
            "deletions": 216,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,216 +0,0 @@\n-import argparse\n-import glob\n-import logging\n-import os\n-from argparse import Namespace\n-from importlib import import_module\n-\n-import numpy as np\n-import torch\n-from lightning_base import BaseTransformer, add_generic_args, generic_train\n-from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n-from torch.nn import CrossEntropyLoss\n-from torch.utils.data import DataLoader, TensorDataset\n-from utils_ner import TokenClassificationTask\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class NERTransformer(BaseTransformer):\n-    \"\"\"\n-    A training module for NER. See BaseTransformer for the core options.\n-    \"\"\"\n-\n-    mode = \"token-classification\"\n-\n-    def __init__(self, hparams):\n-        if isinstance(hparams, dict):\n-            hparams = Namespace(**hparams)\n-        module = import_module(\"tasks\")\n-        try:\n-            token_classification_task_clazz = getattr(module, hparams.task_type)\n-            self.token_classification_task: TokenClassificationTask = token_classification_task_clazz()\n-        except AttributeError:\n-            raise ValueError(\n-                f\"Task {hparams.task_type} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n-                f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n-            )\n-        self.labels = self.token_classification_task.get_labels(hparams.labels)\n-        self.pad_token_label_id = CrossEntropyLoss().ignore_index\n-        super().__init__(hparams, len(self.labels), self.mode)\n-\n-    def forward(self, **inputs):\n-        return self.model(**inputs)\n-\n-    def training_step(self, batch, batch_num):\n-        \"Compute loss and log.\"\n-        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n-        if self.config.model_type != \"distilbert\":\n-            inputs[\"token_type_ids\"] = (\n-                batch[2] if self.config.model_type in [\"bert\", \"xlnet\"] else None\n-            )  # XLM and RoBERTa don\"t use token_type_ids\n-\n-        outputs = self(**inputs)\n-        loss = outputs[0]\n-        # tensorboard_logs = {\"loss\": loss, \"rate\": self.lr_scheduler.get_last_lr()[-1]}\n-        return {\"loss\": loss}\n-\n-    def prepare_data(self):\n-        \"Called to initialize data. Use the call to construct features\"\n-        args = self.hparams\n-        for mode in [\"train\", \"dev\", \"test\"]:\n-            cached_features_file = self._feature_file(mode)\n-            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n-                logger.info(\"Loading features from cached file %s\", cached_features_file)\n-                features = torch.load(cached_features_file, weights_only=True)\n-            else:\n-                logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n-                examples = self.token_classification_task.read_examples_from_file(args.data_dir, mode)\n-                features = self.token_classification_task.convert_examples_to_features(\n-                    examples,\n-                    self.labels,\n-                    args.max_seq_length,\n-                    self.tokenizer,\n-                    cls_token_at_end=bool(self.config.model_type == \"xlnet\"),\n-                    cls_token=self.tokenizer.cls_token,\n-                    cls_token_segment_id=2 if self.config.model_type == \"xlnet\" else 0,\n-                    sep_token=self.tokenizer.sep_token,\n-                    sep_token_extra=False,\n-                    pad_on_left=bool(self.config.model_type == \"xlnet\"),\n-                    pad_token=self.tokenizer.pad_token_id,\n-                    pad_token_segment_id=self.tokenizer.pad_token_type_id,\n-                    pad_token_label_id=self.pad_token_label_id,\n-                )\n-                logger.info(\"Saving features into cached file %s\", cached_features_file)\n-                torch.save(features, cached_features_file)\n-\n-    def get_dataloader(self, mode: int, batch_size: int, shuffle: bool = False) -> DataLoader:\n-        \"Load datasets. Called after prepare data.\"\n-        cached_features_file = self._feature_file(mode)\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file, weights_only=True)\n-        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n-        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n-        if features[0].token_type_ids is not None:\n-            all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n-        else:\n-            all_token_type_ids = torch.tensor([0 for f in features], dtype=torch.long)\n-            # HACK(we will not use this anymore soon)\n-        all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n-        return DataLoader(\n-            TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label_ids), batch_size=batch_size\n-        )\n-\n-    def validation_step(self, batch, batch_nb):\n-        \"\"\"Compute validation\"\"\" \"\"\n-        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n-        if self.config.model_type != \"distilbert\":\n-            inputs[\"token_type_ids\"] = (\n-                batch[2] if self.config.model_type in [\"bert\", \"xlnet\"] else None\n-            )  # XLM and RoBERTa don\"t use token_type_ids\n-        outputs = self(**inputs)\n-        tmp_eval_loss, logits = outputs[:2]\n-        preds = logits.detach().cpu().numpy()\n-        out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n-        return {\"val_loss\": tmp_eval_loss.detach().cpu(), \"pred\": preds, \"target\": out_label_ids}\n-\n-    def _eval_end(self, outputs):\n-        \"Evaluation called for both Val and Test\"\n-        val_loss_mean = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n-        preds = np.concatenate([x[\"pred\"] for x in outputs], axis=0)\n-        preds = np.argmax(preds, axis=2)\n-        out_label_ids = np.concatenate([x[\"target\"] for x in outputs], axis=0)\n-\n-        label_map = dict(enumerate(self.labels))\n-        out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n-        preds_list = [[] for _ in range(out_label_ids.shape[0])]\n-\n-        for i in range(out_label_ids.shape[0]):\n-            for j in range(out_label_ids.shape[1]):\n-                if out_label_ids[i, j] != self.pad_token_label_id:\n-                    out_label_list[i].append(label_map[out_label_ids[i][j]])\n-                    preds_list[i].append(label_map[preds[i][j]])\n-\n-        results = {\n-            \"val_loss\": val_loss_mean,\n-            \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n-            \"precision\": precision_score(out_label_list, preds_list),\n-            \"recall\": recall_score(out_label_list, preds_list),\n-            \"f1\": f1_score(out_label_list, preds_list),\n-        }\n-\n-        ret = dict(results.items())\n-        ret[\"log\"] = results\n-        return ret, preds_list, out_label_list\n-\n-    def validation_epoch_end(self, outputs):\n-        # when stable\n-        ret, preds, targets = self._eval_end(outputs)\n-        logs = ret[\"log\"]\n-        return {\"val_loss\": logs[\"val_loss\"], \"log\": logs, \"progress_bar\": logs}\n-\n-    def test_epoch_end(self, outputs):\n-        # updating to test_epoch_end instead of deprecated test_end\n-        ret, predictions, targets = self._eval_end(outputs)\n-\n-        # Converting to the dict required by pl\n-        # https://github.com/PyTorchLightning/pytorch-lightning/blob/master/\\\n-        # pytorch_lightning/trainer/logging.py#L139\n-        logs = ret[\"log\"]\n-        # `val_loss` is the key returned by `self._eval_end()` but actually refers to `test_loss`\n-        return {\"avg_test_loss\": logs[\"val_loss\"], \"log\": logs, \"progress_bar\": logs}\n-\n-    @staticmethod\n-    def add_model_specific_args(parser, root_dir):\n-        # Add NER specific options\n-        BaseTransformer.add_model_specific_args(parser, root_dir)\n-        parser.add_argument(\n-            \"--task_type\", default=\"NER\", type=str, help=\"Task type to fine tune in training (e.g. NER, POS, etc)\"\n-        )\n-        parser.add_argument(\n-            \"--max_seq_length\",\n-            default=128,\n-            type=int,\n-            help=(\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            ),\n-        )\n-\n-        parser.add_argument(\n-            \"--labels\",\n-            default=\"\",\n-            type=str,\n-            help=\"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\",\n-        )\n-        parser.add_argument(\n-            \"--gpus\",\n-            default=0,\n-            type=int,\n-            help=\"The number of GPUs allocated for this, it is by default 0 meaning none\",\n-        )\n-\n-        parser.add_argument(\n-            \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-        )\n-\n-        return parser\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser()\n-    add_generic_args(parser, os.getcwd())\n-    parser = NERTransformer.add_model_specific_args(parser, os.getcwd())\n-    args = parser.parse_args()\n-    model = NERTransformer(args)\n-    trainer = generic_train(model, args)\n-\n-    if args.do_predict:\n-        # See https://github.com/huggingface/transformers/issues/3159\n-        # pl use this default format to create a checkpoint:\n-        # https://github.com/PyTorchLightning/pytorch-lightning/blob/master\\\n-        # /pytorch_lightning/callbacks/model_checkpoint.py#L322\n-        checkpoints = sorted(glob.glob(os.path.join(args.output_dir, \"checkpoint-epoch=*.ckpt\"), recursive=True))\n-        model = model.load_from_checkpoint(checkpoints[-1])\n-        trainer.test(model)"
        },
        {
            "sha": "a5b185aa960d09394d7caa12fa5d5dda959cdd61",
            "filename": "examples/legacy/pytorch-lightning/run_ner.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_ner.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,44 +0,0 @@\n-#!/usr/bin/env bash\n-\n-# for seqeval metrics import\n-pip install -r ../requirements.txt\n-\n-## The relevant files are currently on a shared Google\n-## drive at https://drive.google.com/drive/folders/1kC0I2UGl2ltrluI9NqDjaQJGw5iliw_J\n-## Monitor for changes and eventually migrate to use the `datasets` library\n-curl -L 'https://drive.google.com/uc?export=download&id=1Jjhbal535VVz2ap4v4r_rN1UEHTdLK5P' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > train.txt.tmp\n-curl -L 'https://drive.google.com/uc?export=download&id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > dev.txt.tmp\n-curl -L 'https://drive.google.com/uc?export=download&id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > test.txt.tmp\n-\n-export MAX_LENGTH=128\n-export BERT_MODEL=bert-base-multilingual-cased\n-python3 scripts/preprocess.py train.txt.tmp $BERT_MODEL $MAX_LENGTH > train.txt\n-python3 scripts/preprocess.py dev.txt.tmp $BERT_MODEL $MAX_LENGTH > dev.txt\n-python3 scripts/preprocess.py test.txt.tmp $BERT_MODEL $MAX_LENGTH > test.txt\n-cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SEED=1\n-\n-export OUTPUT_DIR_NAME=germeval-model\n-export CURRENT_DIR=${PWD}\n-export OUTPUT_DIR=${CURRENT_DIR}/${OUTPUT_DIR_NAME}\n-mkdir -p $OUTPUT_DIR\n-\n-# Add parent directory to python path to access lightning_base.py\n-export PYTHONPATH=\"../\":\"${PYTHONPATH}\"\n-\n-python3 run_ner.py --data_dir ./ \\\n---labels ./labels.txt \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---train_batch_size $BATCH_SIZE \\\n---seed $SEED \\\n---gpus 1 \\\n---do_train \\\n---do_predict"
        },
        {
            "sha": "93765366cf3123af5e361c236b46cf36680d90e2",
            "filename": "examples/legacy/pytorch-lightning/run_pos.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_pos.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fpytorch-lightning%2Frun_pos.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Frun_pos.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,39 +0,0 @@\n-#!/usr/bin/env bash\n-if ! [ -f ./dev.txt ]; then\n-  echo \"Download dev dataset....\"\n-  curl -L -o ./dev.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-dev.conllu'\n-fi\n-\n-if ! [ -f ./test.txt ]; then\n-  echo \"Download test dataset....\"\n-  curl -L -o ./test.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-test.conllu'\n-fi\n-\n-if ! [ -f ./train.txt ]; then\n-  echo \"Download train dataset....\"\n-  curl -L -o ./train.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-train.conllu'\n-fi\n-\n-export MAX_LENGTH=200\n-export BERT_MODEL=bert-base-uncased\n-export OUTPUT_DIR=postagger-model\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SAVE_STEPS=750\n-export SEED=1\n-\n-\n-# Add parent directory to python path to access lightning_base.py\n-export PYTHONPATH=\"../\":\"${PYTHONPATH}\"\n-\n-python3 run_ner.py --data_dir ./ \\\n---task_type POS \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---train_batch_size $BATCH_SIZE \\\n---seed $SEED \\\n---gpus 1 \\\n---do_train \\\n---do_predict"
        },
        {
            "sha": "39d65e917661072233cd4f9fa480f9faa9a97a84",
            "filename": "examples/legacy/question-answering/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fquestion-answering%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fquestion-answering%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2FREADME.md?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,126 +0,0 @@\n-#### Fine-tuning BERT on SQuAD1.0 with relative position embeddings\n-\n-The following examples show how to fine-tune BERT models with different relative position embeddings. The BERT model \n-`google-bert/bert-base-uncased` was pretrained with default absolute position embeddings. We provide the following pretrained \n-models which were pre-trained on the same training data (BooksCorpus and English Wikipedia) as in the BERT model \n-training, but with different relative position embeddings. \n-\n-* `zhiheng-huang/bert-base-uncased-embedding-relative-key`, trained from scratch with relative embedding proposed by \n-Shaw et al., [Self-Attention with Relative Position Representations](https://huggingface.co/papers/1803.02155)\n-* `zhiheng-huang/bert-base-uncased-embedding-relative-key-query`, trained from scratch with relative embedding method 4 \n-in Huang et al. [Improve Transformer Models with Better Relative Position Embeddings](https://huggingface.co/papers/2009.13658)\n-* `zhiheng-huang/bert-large-uncased-whole-word-masking-embedding-relative-key-query`, fine-tuned from model \n-`google-bert/bert-large-uncased-whole-word-masking` with 3 additional epochs with relative embedding method 4 in Huang et al. \n-[Improve Transformer Models with Better Relative Position Embeddings](https://huggingface.co/papers/2009.13658)\n-\n-\n-##### Base models fine-tuning\n-\n-```bash\n-export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n-torchrun --nproc_per_node=8 ./examples/question-answering/run_squad.py \\\n-    --model_name_or_path zhiheng-huang/bert-base-uncased-embedding-relative-key-query \\\n-    --dataset_name squad \\\n-    --do_train \\\n-    --do_eval \\\n-    --learning_rate 3e-5 \\\n-    --num_train_epochs 2 \\\n-    --max_seq_length 512 \\\n-    --doc_stride 128 \\\n-    --output_dir relative_squad \\\n-    --per_device_eval_batch_size=60 \\\n-    --per_device_train_batch_size=6\n-```\n-Training with the above command leads to the following results. It boosts the BERT default from f1 score of 88.52 to 90.54.\n-\n-```bash\n-'exact': 83.6802270577105, 'f1': 90.54772098174814\n-```\n-\n-The change of `max_seq_length` from 512 to 384 in the above command leads to the f1 score of 90.34. Replacing the above \n-model `zhiheng-huang/bert-base-uncased-embedding-relative-key-query` with \n-`zhiheng-huang/bert-base-uncased-embedding-relative-key` leads to the f1 score of 89.51. The changing of 8 gpus to one \n-gpu training leads to the f1 score of 90.71.\n-\n-##### Large models fine-tuning\n-\n-```bash\n-export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n-torchrun --nproc_per_node=8 ./examples/question-answering/run_squad.py \\\n-    --model_name_or_path zhiheng-huang/bert-large-uncased-whole-word-masking-embedding-relative-key-query \\\n-    --dataset_name squad \\\n-    --do_train \\\n-    --do_eval \\\n-    --learning_rate 3e-5 \\\n-    --num_train_epochs 2 \\\n-    --max_seq_length 512 \\\n-    --doc_stride 128 \\\n-    --output_dir relative_squad \\\n-    --per_gpu_eval_batch_size=6 \\\n-    --per_gpu_train_batch_size=2 \\\n-    --gradient_accumulation_steps 3\n-```\n-Training with the above command leads to the f1 score of 93.52, which is slightly better than the f1 score of 93.15 for \n-`google-bert/bert-large-uncased-whole-word-masking`.\n-\n-#### Distributed training\n-\n-Here is an example using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 > 93 on SQuAD1.1:\n-\n-```bash\n-torchrun --nproc_per_node=8 ./examples/question-answering/run_squad.py \\\n-    --model_name_or_path google-bert/bert-large-uncased-whole-word-masking \\\n-    --dataset_name squad \\\n-    --do_train \\\n-    --do_eval \\\n-    --learning_rate 3e-5 \\\n-    --num_train_epochs 2 \\\n-    --max_seq_length 384 \\\n-    --doc_stride 128 \\\n-    --output_dir ./examples/models/wwm_uncased_finetuned_squad/ \\\n-    --per_device_eval_batch_size=3   \\\n-    --per_device_train_batch_size=3   \\\n-```\n-\n-Training with the previously defined hyper-parameters yields the following results:\n-\n-```bash\n-f1 = 93.15\n-exact_match = 86.91\n-```\n-\n-This fine-tuned model is available as a checkpoint under the reference\n-[`google-bert/bert-large-uncased-whole-word-masking-finetuned-squad`](https://huggingface.co/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad).\n-\n-## Results\n-\n-Larger batch size may improve the performance while costing more memory.\n-\n-##### Results for SQuAD1.0 with the previously defined hyper-parameters:\n-\n-```python\n-{\n-\"exact\": 85.45884578997162,\n-\"f1\": 92.5974600601065,\n-\"total\": 10570,\n-\"HasAns_exact\": 85.45884578997162,\n-\"HasAns_f1\": 92.59746006010651,\n-\"HasAns_total\": 10570\n-}\n-```\n-\n-##### Results for SQuAD2.0 with the previously defined hyper-parameters:\n-\n-```python\n-{\n-\"exact\": 80.4177545691906,\n-\"f1\": 84.07154997729623,\n-\"total\": 11873,\n-\"HasAns_exact\": 76.73751686909581,\n-\"HasAns_f1\": 84.05558584352873,\n-\"HasAns_total\": 5928,\n-\"NoAns_exact\": 84.0874684608915,\n-\"NoAns_f1\": 84.0874684608915,\n-\"NoAns_total\": 5945\n-}\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "126fa197ee27e9d39d942dafd801619d7355ca46",
            "filename": "examples/legacy/question-answering/run_squad.py",
            "status": "removed",
            "additions": 0,
            "deletions": 824,
            "changes": 824,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,824 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\"\n-\n-import argparse\n-import glob\n-import logging\n-import os\n-import random\n-import timeit\n-\n-import numpy as np\n-import torch\n-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm, trange\n-\n-import transformers\n-from transformers import (\n-    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n-    WEIGHTS_NAME,\n-    AutoConfig,\n-    AutoModelForQuestionAnswering,\n-    AutoTokenizer,\n-    get_linear_schedule_with_warmup,\n-    squad_convert_examples_to_features,\n-)\n-from transformers.data.metrics.squad_metrics import (\n-    compute_predictions_log_probs,\n-    compute_predictions_logits,\n-    squad_evaluate,\n-)\n-from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n-from transformers.trainer_utils import is_main_process\n-\n-\n-try:\n-    from torch.utils.tensorboard import SummaryWriter\n-except ImportError:\n-    from tensorboardX import SummaryWriter\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-def set_seed(args):\n-    random.seed(args.seed)\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    if args.n_gpu > 0:\n-        torch.cuda.manual_seed_all(args.seed)\n-\n-\n-def to_list(tensor):\n-    return tensor.tolist()\n-\n-\n-def train(args, train_dataset, model, tokenizer):\n-    \"\"\"Train the model\"\"\"\n-    if args.local_rank in [-1, 0]:\n-        tb_writer = SummaryWriter()\n-\n-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n-\n-    if args.max_steps > 0:\n-        t_total = args.max_steps\n-        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n-    else:\n-        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n-\n-    # Prepare optimizer and schedule (linear warmup and decay)\n-    no_decay = [\"bias\", \"LayerNorm.weight\"]\n-    optimizer_grouped_parameters = [\n-        {\n-            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n-            \"weight_decay\": args.weight_decay,\n-        },\n-        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n-    ]\n-    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n-    scheduler = get_linear_schedule_with_warmup(\n-        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n-    )\n-\n-    # Check if saved optimizer or scheduler states exist\n-    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n-        os.path.join(args.model_name_or_path, \"scheduler.pt\")\n-    ):\n-        # Load in optimizer and scheduler states\n-        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\"), weights_only=True))\n-        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\"), weights_only=True))\n-\n-    if args.fp16:\n-        try:\n-            from apex import amp\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-\n-        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n-\n-    # multi-gpu training (should be after apex fp16 initialization)\n-    if args.n_gpu > 1:\n-        model = torch.nn.DataParallel(model)\n-\n-    # Distributed training (should be after apex fp16 initialization)\n-    if args.local_rank != -1:\n-        model = torch.nn.parallel.DistributedDataParallel(\n-            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n-        )\n-\n-    # Train!\n-    logger.info(\"***** Running training *****\")\n-    logger.info(\"  Num examples = %d\", len(train_dataset))\n-    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n-    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n-    logger.info(\n-        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n-        args.train_batch_size\n-        * args.gradient_accumulation_steps\n-        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n-    )\n-    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n-    logger.info(\"  Total optimization steps = %d\", t_total)\n-\n-    global_step = 1\n-    epochs_trained = 0\n-    steps_trained_in_current_epoch = 0\n-    # Check if continuing training from a checkpoint\n-    if os.path.exists(args.model_name_or_path):\n-        try:\n-            # set global_step to global_step of last saved checkpoint from model path\n-            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n-            global_step = int(checkpoint_suffix)\n-            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n-            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n-\n-            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n-            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n-            logger.info(\"  Continuing training from global step %d\", global_step)\n-            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n-        except ValueError:\n-            logger.info(\"  Starting fine-tuning.\")\n-\n-    tr_loss, logging_loss = 0.0, 0.0\n-    model.zero_grad()\n-    train_iterator = trange(\n-        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n-    )\n-    # Added here for reproducibility\n-    set_seed(args)\n-\n-    for _ in train_iterator:\n-        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n-        for step, batch in enumerate(epoch_iterator):\n-            # Skip past any already trained steps if resuming training\n-            if steps_trained_in_current_epoch > 0:\n-                steps_trained_in_current_epoch -= 1\n-                continue\n-\n-            model.train()\n-            batch = tuple(t.to(args.device) for t in batch)\n-\n-            inputs = {\n-                \"input_ids\": batch[0],\n-                \"attention_mask\": batch[1],\n-                \"token_type_ids\": batch[2],\n-                \"start_positions\": batch[3],\n-                \"end_positions\": batch[4],\n-            }\n-\n-            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n-                del inputs[\"token_type_ids\"]\n-\n-            if args.model_type in [\"xlnet\", \"xlm\"]:\n-                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n-                if args.version_2_with_negative:\n-                    inputs.update({\"is_impossible\": batch[7]})\n-                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n-                    inputs.update(\n-                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n-                    )\n-\n-            outputs = model(**inputs)\n-            # model outputs are always tuple in transformers (see doc)\n-            loss = outputs[0]\n-\n-            if args.n_gpu > 1:\n-                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n-            if args.gradient_accumulation_steps > 1:\n-                loss = loss / args.gradient_accumulation_steps\n-\n-            if args.fp16:\n-                with amp.scale_loss(loss, optimizer) as scaled_loss:\n-                    scaled_loss.backward()\n-            else:\n-                loss.backward()\n-\n-            tr_loss += loss.item()\n-            if (step + 1) % args.gradient_accumulation_steps == 0:\n-                if args.fp16:\n-                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n-                else:\n-                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n-\n-                optimizer.step()\n-                scheduler.step()  # Update learning rate schedule\n-                model.zero_grad()\n-                global_step += 1\n-\n-                # Log metrics\n-                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n-                    # Only evaluate when single GPU otherwise metrics may not average well\n-                    if args.local_rank == -1 and args.evaluate_during_training:\n-                        results = evaluate(args, model, tokenizer)\n-                        for key, value in results.items():\n-                            tb_writer.add_scalar(f\"eval_{key}\", value, global_step)\n-                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n-                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n-                    logging_loss = tr_loss\n-\n-                # Save model checkpoint\n-                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n-                    output_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n-                    # Take care of distributed/parallel training\n-                    model_to_save = model.module if hasattr(model, \"module\") else model\n-                    model_to_save.save_pretrained(output_dir)\n-                    tokenizer.save_pretrained(output_dir)\n-\n-                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n-                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n-\n-                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n-                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n-                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n-\n-            if args.max_steps > 0 and global_step > args.max_steps:\n-                epoch_iterator.close()\n-                break\n-        if args.max_steps > 0 and global_step > args.max_steps:\n-            train_iterator.close()\n-            break\n-\n-    if args.local_rank in [-1, 0]:\n-        tb_writer.close()\n-\n-    return global_step, tr_loss / global_step\n-\n-\n-def evaluate(args, model, tokenizer, prefix=\"\"):\n-    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n-\n-    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n-        os.makedirs(args.output_dir)\n-\n-    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n-\n-    # Note that DistributedSampler samples randomly\n-    eval_sampler = SequentialSampler(dataset)\n-    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n-\n-    # multi-gpu evaluate\n-    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n-        model = torch.nn.DataParallel(model)\n-\n-    # Eval!\n-    logger.info(f\"***** Running evaluation {prefix} *****\")\n-    logger.info(\"  Num examples = %d\", len(dataset))\n-    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n-\n-    all_results = []\n-    start_time = timeit.default_timer()\n-\n-    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n-        model.eval()\n-        batch = tuple(t.to(args.device) for t in batch)\n-\n-        with torch.no_grad():\n-            inputs = {\n-                \"input_ids\": batch[0],\n-                \"attention_mask\": batch[1],\n-                \"token_type_ids\": batch[2],\n-            }\n-\n-            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n-                del inputs[\"token_type_ids\"]\n-\n-            feature_indices = batch[3]\n-\n-            # XLNet and XLM use more arguments for their predictions\n-            if args.model_type in [\"xlnet\", \"xlm\"]:\n-                inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n-                # for lang_id-sensitive xlm models\n-                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n-                    inputs.update(\n-                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n-                    )\n-            outputs = model(**inputs)\n-\n-        for i, feature_index in enumerate(feature_indices):\n-            eval_feature = features[feature_index.item()]\n-            unique_id = int(eval_feature.unique_id)\n-\n-            output = [to_list(output[i]) for output in outputs.to_tuple()]\n-\n-            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n-            # models only use two.\n-            if len(output) >= 5:\n-                start_logits = output[0]\n-                start_top_index = output[1]\n-                end_logits = output[2]\n-                end_top_index = output[3]\n-                cls_logits = output[4]\n-\n-                result = SquadResult(\n-                    unique_id,\n-                    start_logits,\n-                    end_logits,\n-                    start_top_index=start_top_index,\n-                    end_top_index=end_top_index,\n-                    cls_logits=cls_logits,\n-                )\n-\n-            else:\n-                start_logits, end_logits = output\n-                result = SquadResult(unique_id, start_logits, end_logits)\n-\n-            all_results.append(result)\n-\n-    evalTime = timeit.default_timer() - start_time\n-    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n-\n-    # Compute predictions\n-    output_prediction_file = os.path.join(args.output_dir, f\"predictions_{prefix}.json\")\n-    output_nbest_file = os.path.join(args.output_dir, f\"nbest_predictions_{prefix}.json\")\n-\n-    if args.version_2_with_negative:\n-        output_null_log_odds_file = os.path.join(args.output_dir, f\"null_odds_{prefix}.json\")\n-    else:\n-        output_null_log_odds_file = None\n-\n-    # XLNet and XLM use a more complex post-processing procedure\n-    if args.model_type in [\"xlnet\", \"xlm\"]:\n-        start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n-        end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n-\n-        predictions = compute_predictions_log_probs(\n-            examples,\n-            features,\n-            all_results,\n-            args.n_best_size,\n-            args.max_answer_length,\n-            output_prediction_file,\n-            output_nbest_file,\n-            output_null_log_odds_file,\n-            start_n_top,\n-            end_n_top,\n-            args.version_2_with_negative,\n-            tokenizer,\n-            args.verbose_logging,\n-        )\n-    else:\n-        predictions = compute_predictions_logits(\n-            examples,\n-            features,\n-            all_results,\n-            args.n_best_size,\n-            args.max_answer_length,\n-            args.do_lower_case,\n-            output_prediction_file,\n-            output_nbest_file,\n-            output_null_log_odds_file,\n-            args.verbose_logging,\n-            args.version_2_with_negative,\n-            args.null_score_diff_threshold,\n-            tokenizer,\n-        )\n-\n-    # Compute the F1 and exact scores.\n-    results = squad_evaluate(examples, predictions)\n-    return results\n-\n-\n-def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n-    if args.local_rank not in [-1, 0] and not evaluate:\n-        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-        torch.distributed.barrier()\n-\n-    # Load data features from cache or dataset file\n-    input_dir = args.data_dir if args.data_dir else \".\"\n-    cached_features_file = os.path.join(\n-        input_dir,\n-        \"cached_{}_{}_{}\".format(\n-            \"dev\" if evaluate else \"train\",\n-            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n-            str(args.max_seq_length),\n-        ),\n-    )\n-\n-    # Init features and dataset from cache if it exists\n-    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features_and_dataset = torch.load(cached_features_file, weights_only=True)\n-        features, dataset, examples = (\n-            features_and_dataset[\"features\"],\n-            features_and_dataset[\"dataset\"],\n-            features_and_dataset[\"examples\"],\n-        )\n-    else:\n-        logger.info(\"Creating features from dataset file at %s\", input_dir)\n-\n-        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n-            try:\n-                import tensorflow_datasets as tfds\n-            except ImportError:\n-                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n-\n-            if args.version_2_with_negative:\n-                logger.warning(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n-\n-            tfds_examples = tfds.load(\"squad\")\n-            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n-        else:\n-            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n-            if evaluate:\n-                examples = processor.get_dev_examples(args.data_dir, filename=args.predict_file)\n-            else:\n-                examples = processor.get_train_examples(args.data_dir, filename=args.train_file)\n-\n-        features, dataset = squad_convert_examples_to_features(\n-            examples=examples,\n-            tokenizer=tokenizer,\n-            max_seq_length=args.max_seq_length,\n-            doc_stride=args.doc_stride,\n-            max_query_length=args.max_query_length,\n-            is_training=not evaluate,\n-            return_dataset=\"pt\",\n-            threads=args.threads,\n-        )\n-\n-        if args.local_rank in [-1, 0]:\n-            logger.info(\"Saving features into cached file %s\", cached_features_file)\n-            torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n-\n-    if args.local_rank == 0 and not evaluate:\n-        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-        torch.distributed.barrier()\n-\n-    if output_examples:\n-        return dataset, examples, features\n-    return dataset\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-\n-    # Required parameters\n-    parser.add_argument(\n-        \"--model_type\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model checkpoints and predictions will be written.\",\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--data_dir\",\n-        default=None,\n-        type=str,\n-        help=\"The input data dir. Should contain the .json files for the task.\"\n-        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--train_file\",\n-        default=None,\n-        type=str,\n-        help=\"The input training file. If a data dir is specified, will look for the file there\"\n-        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--predict_file\",\n-        default=None,\n-        type=str,\n-        help=\"The input evaluation file. If a data dir is specified, will look for the file there\"\n-        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\n-        \"--cache_dir\",\n-        default=\"\",\n-        type=str,\n-        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n-    )\n-\n-    parser.add_argument(\n-        \"--version_2_with_negative\",\n-        action=\"store_true\",\n-        help=\"If true, the SQuAD examples contain some that do not have an answer.\",\n-    )\n-    parser.add_argument(\n-        \"--null_score_diff_threshold\",\n-        type=float,\n-        default=0.0,\n-        help=\"If null_score - best_non_null is greater than the threshold predict null.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=384,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n-            \"longer than this will be truncated, and sequences shorter than this will be padded.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--doc_stride\",\n-        default=128,\n-        type=int,\n-        help=\"When splitting up a long document into chunks, how much stride to take between chunks.\",\n-    )\n-    parser.add_argument(\n-        \"--max_query_length\",\n-        default=64,\n-        type=int,\n-        help=(\n-            \"The maximum number of tokens for the question. Questions longer than this will \"\n-            \"be truncated to this length.\"\n-        ),\n-    )\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n-    parser.add_argument(\n-        \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n-    )\n-    parser.add_argument(\n-        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n-    )\n-\n-    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n-    parser.add_argument(\n-        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n-    )\n-    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n-    parser.add_argument(\n-        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n-    )\n-    parser.add_argument(\n-        \"--max_steps\",\n-        default=-1,\n-        type=int,\n-        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n-    )\n-    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-    parser.add_argument(\n-        \"--n_best_size\",\n-        default=20,\n-        type=int,\n-        help=\"The total number of n-best predictions to generate in the nbest_predictions.json output file.\",\n-    )\n-    parser.add_argument(\n-        \"--max_answer_length\",\n-        default=30,\n-        type=int,\n-        help=(\n-            \"The maximum length of an answer that can be generated. This is needed because the start \"\n-            \"and end predictions are not conditioned on one another.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--verbose_logging\",\n-        action=\"store_true\",\n-        help=(\n-            \"If true, all of the warnings related to data processing will be printed. \"\n-            \"A number of warnings are expected for a normal SQuAD evaluation.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--lang_id\",\n-        default=0,\n-        type=int,\n-        help=(\n-            \"language id of input for language-specific xlm models (see\"\n-            \" tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)\"\n-        ),\n-    )\n-\n-    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n-    parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n-    parser.add_argument(\n-        \"--eval_all_checkpoints\",\n-        action=\"store_true\",\n-        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n-    )\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O1\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-\n-    parser.add_argument(\"--threads\", type=int, default=1, help=\"multiple threads for converting example to features\")\n-    args = parser.parse_args()\n-\n-    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n-        logger.warning(\n-            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n-            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n-            \"stride or increase the maximum length to ensure the features are correctly built.\"\n-        )\n-\n-    # Setup distant debugging if needed\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup CUDA, GPU & distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n-    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n-        torch.cuda.set_device(args.local_rank)\n-        device = torch.device(\"cuda\", args.local_rank)\n-        torch.distributed.init_process_group(backend=\"nccl\")\n-        args.n_gpu = 1\n-    args.device = device\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        args.local_rank,\n-        device,\n-        args.n_gpu,\n-        bool(args.local_rank != -1),\n-        args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    # Set seed\n-    set_seed(args)\n-\n-    # Load pretrained model and tokenizer\n-    if args.local_rank not in [-1, 0]:\n-        # Make sure only the first process in distributed training will download model & vocab\n-        torch.distributed.barrier()\n-\n-    args.model_type = args.model_type.lower()\n-    config = AutoConfig.from_pretrained(\n-        args.config_name if args.config_name else args.model_name_or_path,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n-        do_lower_case=args.do_lower_case,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-        use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handling\n-    )\n-    model = AutoModelForQuestionAnswering.from_pretrained(\n-        args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n-        config=config,\n-        cache_dir=args.cache_dir if args.cache_dir else None,\n-    )\n-\n-    if args.local_rank == 0:\n-        # Make sure only the first process in distributed training will download model & vocab\n-        torch.distributed.barrier()\n-\n-    model.to(args.device)\n-\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n-    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n-    # remove the need for this code, but it is still valid.\n-    if args.fp16:\n-        try:\n-            import apex\n-\n-            apex.amp.register_half_function(torch, \"einsum\")\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-\n-    # Training\n-    if args.do_train:\n-        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n-        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n-\n-    # Save the trained model and the tokenizer\n-    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n-        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n-        # They can then be reloaded using `from_pretrained()`\n-        # Take care of distributed/parallel training\n-        model_to_save = model.module if hasattr(model, \"module\") else model\n-        model_to_save.save_pretrained(args.output_dir)\n-        tokenizer.save_pretrained(args.output_dir)\n-\n-        # Good practice: save your training arguments together with the trained model\n-        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n-\n-        # Load a trained model and vocabulary that you have fine-tuned\n-        model = AutoModelForQuestionAnswering.from_pretrained(args.output_dir)  # , force_download=True)\n-\n-        # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handling\n-        # So we use use_fast=False here for now until Fast-tokenizer-compatible-examples are out\n-        tokenizer = AutoTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case, use_fast=False)\n-        model.to(args.device)\n-\n-    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n-    results = {}\n-    if args.do_eval and args.local_rank in [-1, 0]:\n-        if args.do_train:\n-            logger.info(\"Loading checkpoints saved during training for evaluation\")\n-            checkpoints = [args.output_dir]\n-            if args.eval_all_checkpoints:\n-                checkpoints = [\n-                    os.path.dirname(c)\n-                    for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n-                ]\n-\n-        else:\n-            logger.info(\"Loading checkpoint %s for evaluation\", args.model_name_or_path)\n-            checkpoints = [args.model_name_or_path]\n-\n-        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n-\n-        for checkpoint in checkpoints:\n-            # Reload the model\n-            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n-            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)\n-            model.to(args.device)\n-\n-            # Evaluate\n-            result = evaluate(args, model, tokenizer, prefix=global_step)\n-\n-            result = {k + (f\"_{global_step}\" if global_step else \"\"): v for k, v in result.items()}\n-            results.update(result)\n-\n-    logger.info(f\"Results: {results}\")\n-\n-    return results\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "9ec8d4d852cdfad2f9e72b37fb292824ab2940c7",
            "filename": "examples/legacy/question-answering/run_squad_trainer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 174,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,174 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fine-tuning the library models for question-answering.\"\"\"\n-\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoModelForQuestionAnswering,\n-    AutoTokenizer,\n-    DataCollatorWithPadding,\n-    HfArgumentParser,\n-    SquadDataset,\n-    Trainer,\n-    TrainingArguments,\n-)\n-from transformers import SquadDataTrainingArguments as DataTrainingArguments\n-from transformers.trainer_utils import is_main_process\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n-    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n-    # or just modify its tokenizer_config.json.\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_process_index,\n-        training_args.device,\n-        training_args.n_gpu,\n-        bool(training_args.parallel_mode.value == \"distributed\"),\n-        training_args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_process_index):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Prepare Question-Answering task\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handling\n-    )\n-    model = AutoModelForQuestionAnswering.from_pretrained(\n-        model_args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    # Get datasets\n-    is_language_sensitive = hasattr(model.config, \"lang2id\")\n-    train_dataset = (\n-        SquadDataset(\n-            data_args, tokenizer=tokenizer, is_language_sensitive=is_language_sensitive, cache_dir=model_args.cache_dir\n-        )\n-        if training_args.do_train\n-        else None\n-    )\n-    eval_dataset = (\n-        SquadDataset(\n-            data_args,\n-            tokenizer=tokenizer,\n-            mode=\"dev\",\n-            is_language_sensitive=is_language_sensitive,\n-            cache_dir=model_args.cache_dir,\n-        )\n-        if training_args.do_eval\n-        else None\n-    )\n-\n-    # Data collator\n-    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) if training_args.fp16 else None\n-\n-    # Initialize our Trainer\n-    trainer = Trainer(\n-        model=model,\n-        args=training_args,\n-        train_dataset=train_dataset,\n-        eval_dataset=eval_dataset,\n-        data_collator=data_collator,\n-    )\n-\n-    # Training\n-    if training_args.do_train:\n-        trainer.train(\n-            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n-        )\n-        trainer.save_model()\n-        # For convenience, we also re-save the tokenizer to the same directory,\n-        # so that you can share your model easily on huggingface.co/models =)\n-        if trainer.is_world_master():\n-            tokenizer.save_pretrained(training_args.output_dir)\n-\n-\n-def _mp_fn(index):\n-    # For xla_spawn (TPUs)\n-    main()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "9767ffbf8b30c0e797103a14cfa10f1c9f666fd5",
            "filename": "examples/legacy/run_camembert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_camembert.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,47 +0,0 @@\n-#!/usr/bin/env python\n-import torch\n-\n-from transformers import CamembertForMaskedLM, CamembertTokenizer\n-\n-\n-def fill_mask(masked_input, model, tokenizer, topk=5):\n-    # Adapted from https://github.com/pytorch/fairseq/blob/master/fairseq/models/roberta/hub_interface.py\n-    assert masked_input.count(\"<mask>\") == 1\n-    input_ids = torch.tensor(tokenizer.encode(masked_input, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n-    logits = model(input_ids)[0]  # The last hidden-state is the first element of the output tuple\n-    masked_index = (input_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()\n-    logits = logits[0, masked_index, :]\n-    prob = logits.softmax(dim=0)\n-    values, indices = prob.topk(k=topk, dim=0)\n-    topk_predicted_token_bpe = \" \".join(\n-        [tokenizer.convert_ids_to_tokens(indices[i].item()) for i in range(len(indices))]\n-    )\n-    masked_token = tokenizer.mask_token\n-    topk_filled_outputs = []\n-    for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(\" \")):\n-        predicted_token = predicted_token_bpe.replace(\"\\u2581\", \" \")\n-        if f\" {masked_token}\" in masked_input:\n-            topk_filled_outputs.append(\n-                (\n-                    masked_input.replace(f\" {masked_token}\", predicted_token),\n-                    values[index].item(),\n-                    predicted_token,\n-                )\n-            )\n-        else:\n-            topk_filled_outputs.append(\n-                (\n-                    masked_input.replace(masked_token, predicted_token),\n-                    values[index].item(),\n-                    predicted_token,\n-                )\n-            )\n-    return topk_filled_outputs\n-\n-\n-tokenizer = CamembertTokenizer.from_pretrained(\"almanach/camembert-base\")\n-model = CamembertForMaskedLM.from_pretrained(\"almanach/camembert-base\")\n-model.eval()\n-\n-masked_input = \"Le camembert est <mask> :)\"\n-print(fill_mask(masked_input, model, tokenizer, topk=3))"
        },
        {
            "sha": "7cb6caccefe11cd07c705905dd5c8a252ae61bf7",
            "filename": "examples/legacy/run_chinese_ref.py",
            "status": "removed",
            "additions": 0,
            "deletions": 147,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_chinese_ref.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_chinese_ref.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_chinese_ref.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,147 +0,0 @@\n-#!/usr/bin/env python\n-import argparse\n-import json\n-\n-from ltp import LTP\n-\n-from transformers import BertTokenizer\n-\n-\n-def _is_chinese_char(cp):\n-    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n-    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n-    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n-    #\n-    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n-    # despite its name. The modern Korean Hangul alphabet is a different block,\n-    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n-    # space-separated words, so they are not treated specially and handled\n-    # like the all of the other languages.\n-    if (\n-        (cp >= 0x4E00 and cp <= 0x9FFF)\n-        or (cp >= 0x3400 and cp <= 0x4DBF)\n-        or (cp >= 0x20000 and cp <= 0x2A6DF)\n-        or (cp >= 0x2A700 and cp <= 0x2B73F)\n-        or (cp >= 0x2B740 and cp <= 0x2B81F)\n-        or (cp >= 0x2B820 and cp <= 0x2CEAF)\n-        or (cp >= 0xF900 and cp <= 0xFAFF)\n-        or (cp >= 0x2F800 and cp <= 0x2FA1F)\n-    ):\n-        return True\n-\n-    return False\n-\n-\n-def is_chinese(word: str):\n-    # word like '180' or 'èº«é«˜' or 'ç¥ž'\n-    for char in word:\n-        char = ord(char)\n-        if not _is_chinese_char(char):\n-            return 0\n-    return 1\n-\n-\n-def get_chinese_word(tokens: list[str]):\n-    word_set = set()\n-\n-    for token in tokens:\n-        chinese_word = len(token) > 1 and is_chinese(token)\n-        if chinese_word:\n-            word_set.add(token)\n-    word_list = list(word_set)\n-    return word_list\n-\n-\n-def add_sub_symbol(bert_tokens: list[str], chinese_word_set: set()):\n-    if not chinese_word_set:\n-        return bert_tokens\n-    max_word_len = max(len(w) for w in chinese_word_set)\n-\n-    bert_word = bert_tokens\n-    start, end = 0, len(bert_word)\n-    while start < end:\n-        single_word = True\n-        if is_chinese(bert_word[start]):\n-            l = min(end - start, max_word_len)\n-            for i in range(l, 1, -1):\n-                whole_word = \"\".join(bert_word[start : start + i])\n-                if whole_word in chinese_word_set:\n-                    for j in range(start + 1, start + i):\n-                        bert_word[j] = \"##\" + bert_word[j]\n-                    start = start + i\n-                    single_word = False\n-                    break\n-        if single_word:\n-            start += 1\n-    return bert_word\n-\n-\n-def prepare_ref(lines: list[str], ltp_tokenizer: LTP, bert_tokenizer: BertTokenizer):\n-    ltp_res = []\n-\n-    for i in range(0, len(lines), 100):\n-        res = ltp_tokenizer.seg(lines[i : i + 100])[0]\n-        res = [get_chinese_word(r) for r in res]\n-        ltp_res.extend(res)\n-    assert len(ltp_res) == len(lines)\n-\n-    bert_res = []\n-    for i in range(0, len(lines), 100):\n-        res = bert_tokenizer(lines[i : i + 100], add_special_tokens=True, truncation=True, max_length=512)\n-        bert_res.extend(res[\"input_ids\"])\n-    assert len(bert_res) == len(lines)\n-\n-    ref_ids = []\n-    for input_ids, chinese_word in zip(bert_res, ltp_res):\n-        input_tokens = []\n-        for id in input_ids:\n-            token = bert_tokenizer._convert_id_to_token(id)\n-            input_tokens.append(token)\n-        input_tokens = add_sub_symbol(input_tokens, chinese_word)\n-        ref_id = []\n-        # We only save pos of chinese subwords start with ##, which mean is part of a whole word.\n-        for i, token in enumerate(input_tokens):\n-            if token[:2] == \"##\":\n-                clean_token = token[2:]\n-                # save chinese tokens' pos\n-                if len(clean_token) == 1 and _is_chinese_char(ord(clean_token)):\n-                    ref_id.append(i)\n-        ref_ids.append(ref_id)\n-\n-    assert len(ref_ids) == len(bert_res)\n-\n-    return ref_ids\n-\n-\n-def main(args):\n-    # For Chinese (Ro)Bert, the best result is from : RoBERTa-wwm-ext (https://github.com/ymcui/Chinese-BERT-wwm)\n-    # If we want to fine-tune these model, we have to use same tokenizer : LTP (https://github.com/HIT-SCIR/ltp)\n-    with open(args.file_name, encoding=\"utf-8\") as f:\n-        data = f.readlines()\n-    data = [line.strip() for line in data if len(line) > 0 and not line.isspace()]  # avoid delimiter like '\\u2029'\n-    ltp_tokenizer = LTP(args.ltp)  # faster in GPU device\n-    bert_tokenizer = BertTokenizer.from_pretrained(args.bert)\n-\n-    ref_ids = prepare_ref(data, ltp_tokenizer, bert_tokenizer)\n-\n-    with open(args.save_path, \"w\", encoding=\"utf-8\") as f:\n-        data = [json.dumps(ref) + \"\\n\" for ref in ref_ids]\n-        f.writelines(data)\n-\n-\n-if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(description=\"prepare_chinese_ref\")\n-    parser.add_argument(\n-        \"--file_name\",\n-        type=str,\n-        default=\"./resources/chinese-demo.txt\",\n-        help=\"file need process, same as training data in lm\",\n-    )\n-    parser.add_argument(\n-        \"--ltp\", type=str, default=\"./resources/ltp\", help=\"resources for LTP tokenizer, usually a path\"\n-    )\n-    parser.add_argument(\"--bert\", type=str, default=\"./resources/robert\", help=\"resources for Bert tokenizer\")\n-    parser.add_argument(\"--save_path\", type=str, default=\"./resources/ref.txt\", help=\"path to save res\")\n-\n-    args = parser.parse_args()\n-    main(args)"
        },
        {
            "sha": "db1db37d983494ff93618dbbbdddd36159b19b48",
            "filename": "examples/legacy/run_language_modeling.py",
            "status": "removed",
            "additions": 0,
            "deletions": 363,
            "changes": 363,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_language_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_language_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_language_modeling.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,363 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, CTRL, BERT, RoBERTa, XLNet).\n-GPT, GPT-2 and CTRL are fine-tuned using a causal language modeling (CLM) loss. BERT and RoBERTa are fine-tuned\n-using a masked language modeling (MLM) loss. XLNet is fine-tuned using a permutation language modeling (PLM) loss.\n-\"\"\"\n-\n-import logging\n-import math\n-import os\n-from dataclasses import dataclass, field\n-from glob import glob\n-from typing import Optional\n-\n-from torch.utils.data import ConcatDataset\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    MODEL_WITH_LM_HEAD_MAPPING,\n-    AutoConfig,\n-    AutoModelWithLMHead,\n-    AutoTokenizer,\n-    DataCollatorForLanguageModeling,\n-    DataCollatorForPermutationLanguageModeling,\n-    DataCollatorForWholeWordMask,\n-    HfArgumentParser,\n-    LineByLineTextDataset,\n-    LineByLineWithRefDataset,\n-    PreTrainedTokenizer,\n-    TextDataset,\n-    Trainer,\n-    TrainingArguments,\n-    set_seed,\n-)\n-from transformers.trainer_utils import is_main_process\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Leave None if you want to train a model from\"\n-                \" scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    train_data_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n-    )\n-    train_data_files: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The input training data files (multiple files in glob format). \"\n-                \"Very often splitting large files to smaller files can prevent tokenizer going out of memory\"\n-            )\n-        },\n-    )\n-    eval_data_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    train_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input train ref data file for whole word mask in Chinese.\"},\n-    )\n-    eval_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input eval ref data file for whole word mask in Chinese.\"},\n-    )\n-    line_by_line: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n-    )\n-\n-    mlm: bool = field(\n-        default=False, metadata={\"help\": \"Train with masked-language modeling loss instead of language modeling.\"}\n-    )\n-    whole_word_mask: bool = field(default=False, metadata={\"help\": \"Whether ot not to use whole word mask.\"})\n-    mlm_probability: float = field(\n-        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n-    )\n-    plm_probability: float = field(\n-        default=1 / 6,\n-        metadata={\n-            \"help\": (\n-                \"Ratio of length of a span of masked tokens to surrounding context length for permutation language\"\n-                \" modeling.\"\n-            )\n-        },\n-    )\n-    max_span_length: int = field(\n-        default=5, metadata={\"help\": \"Maximum length of a span of masked tokens for permutation language modeling.\"}\n-    )\n-\n-    block_size: int = field(\n-        default=-1,\n-        metadata={\n-            \"help\": (\n-                \"Optional input sequence length after tokenization. \"\n-                \"The training dataset will be truncated in block of this size for training.\"\n-                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-\n-\n-def get_dataset(\n-    args: DataTrainingArguments,\n-    tokenizer: PreTrainedTokenizer,\n-    evaluate: bool = False,\n-    cache_dir: Optional[str] = None,\n-):\n-    def _dataset(file_path, ref_path=None):\n-        if args.line_by_line:\n-            if ref_path is not None:\n-                if not args.whole_word_mask or not args.mlm:\n-                    raise ValueError(\"You need to set world whole masking and mlm to True for Chinese Whole Word Mask\")\n-                return LineByLineWithRefDataset(\n-                    tokenizer=tokenizer,\n-                    file_path=file_path,\n-                    block_size=args.block_size,\n-                    ref_path=ref_path,\n-                )\n-\n-            return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\n-        else:\n-            return TextDataset(\n-                tokenizer=tokenizer,\n-                file_path=file_path,\n-                block_size=args.block_size,\n-                overwrite_cache=args.overwrite_cache,\n-                cache_dir=cache_dir,\n-            )\n-\n-    if evaluate:\n-        return _dataset(args.eval_data_file, args.eval_ref_file)\n-    elif args.train_data_files:\n-        return ConcatDataset([_dataset(f) for f in glob(args.train_data_files)])\n-    else:\n-        return _dataset(args.train_data_file, args.train_ref_file)\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if data_args.eval_data_file is None and training_args.do_eval:\n-        raise ValueError(\n-            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n-            \"or remove the --do_eval argument.\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_process_index,\n-        training_args.device,\n-        training_args.n_gpu,\n-        bool(training_args.parallel_mode.value == \"distributed\"),\n-        training_args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_process_index):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Set seed\n-    set_seed(training_args.seed)\n-\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(model_args.config_name, cache_dir=model_args.cache_dir)\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, cache_dir=model_args.cache_dir)\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another\"\n-            \" script, save it,and load it from here, using --tokenizer_name\"\n-        )\n-\n-    if model_args.model_name_or_path:\n-        model = AutoModelWithLMHead.from_pretrained(\n-            model_args.model_name_or_path,\n-            from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n-            config=config,\n-            cache_dir=model_args.cache_dir,\n-        )\n-    else:\n-        logger.info(\"Training new model from scratch\")\n-        model = AutoModelWithLMHead.from_config(config)\n-\n-    model.resize_token_embeddings(len(tokenizer))\n-\n-    if config.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not data_args.mlm:\n-        raise ValueError(\n-            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the \"\n-            \"--mlm flag (masked language modeling).\"\n-        )\n-\n-    if data_args.block_size <= 0:\n-        data_args.block_size = tokenizer.max_len\n-        # Our input block size will be the max possible for the model\n-    else:\n-        data_args.block_size = min(data_args.block_size, tokenizer.max_len)\n-\n-    # Get datasets\n-\n-    train_dataset = (\n-        get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None\n-    )\n-    eval_dataset = (\n-        get_dataset(data_args, tokenizer=tokenizer, evaluate=True, cache_dir=model_args.cache_dir)\n-        if training_args.do_eval\n-        else None\n-    )\n-    if config.model_type == \"xlnet\":\n-        data_collator = DataCollatorForPermutationLanguageModeling(\n-            tokenizer=tokenizer,\n-            plm_probability=data_args.plm_probability,\n-            max_span_length=data_args.max_span_length,\n-        )\n-    else:\n-        if data_args.mlm and data_args.whole_word_mask:\n-            data_collator = DataCollatorForWholeWordMask(\n-                tokenizer=tokenizer, mlm_probability=data_args.mlm_probability\n-            )\n-        else:\n-            data_collator = DataCollatorForLanguageModeling(\n-                tokenizer=tokenizer, mlm=data_args.mlm, mlm_probability=data_args.mlm_probability\n-            )\n-\n-    # Initialize our Trainer\n-    trainer = Trainer(\n-        model=model,\n-        args=training_args,\n-        data_collator=data_collator,\n-        train_dataset=train_dataset,\n-        eval_dataset=eval_dataset,\n-        prediction_loss_only=True,\n-    )\n-\n-    # Training\n-    if training_args.do_train:\n-        model_path = (\n-            model_args.model_name_or_path\n-            if model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path)\n-            else None\n-        )\n-        trainer.train(model_path=model_path)\n-        trainer.save_model()\n-        # For convenience, we also re-save the tokenizer to the same directory,\n-        # so that you can share your model easily on huggingface.co/models =)\n-        if trainer.is_world_master():\n-            tokenizer.save_pretrained(training_args.output_dir)\n-\n-    # Evaluation\n-    results = {}\n-    if training_args.do_eval:\n-        logger.info(\"*** Evaluate ***\")\n-\n-        eval_output = trainer.evaluate()\n-\n-        perplexity = math.exp(eval_output[\"eval_loss\"])\n-        result = {\"perplexity\": perplexity}\n-\n-        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n-        if trainer.is_world_master():\n-            with open(output_eval_file, \"w\") as writer:\n-                logger.info(\"***** Eval results *****\")\n-                for key in sorted(result.keys()):\n-                    logger.info(\"  %s = %s\", key, str(result[key]))\n-                    writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n-\n-        results.update(result)\n-\n-    return results\n-\n-\n-def _mp_fn(index):\n-    # For xla_spawn (TPUs)\n-    main()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "62f9d3a3c083b606cce34cd35cf02442860be541",
            "filename": "examples/legacy/run_openai_gpt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 319,
            "changes": 319,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_openai_gpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_openai_gpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_openai_gpt.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,319 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\" OpenAI GPT model fine-tuning script.\n-    Adapted from https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/train.py\n-    It self adapted from https://github.com/openai/finetune-transformer-lm/blob/master/train.py\n-\n-    This script with default values fine-tunes and evaluate a pretrained OpenAI GPT on the RocStories dataset:\n-        python run_openai_gpt.py \\\n-          --model_name openai-community/openai-gpt \\\n-          --do_train \\\n-          --do_eval \\\n-          --train_dataset \"$ROC_STORIES_DIR/cloze_test_val__spring2016 - cloze_test_ALL_val.csv\" \\\n-          --eval_dataset \"$ROC_STORIES_DIR/cloze_test_test__spring2016 - cloze_test_ALL_test.csv\" \\\n-          --output_dir ../log \\\n-          --train_batch_size 16 \\\n-\"\"\"\n-\n-import argparse\n-import csv\n-import logging\n-import os\n-import random\n-\n-import numpy as np\n-import torch\n-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n-from tqdm import tqdm, trange\n-\n-from transformers import (\n-    CONFIG_NAME,\n-    WEIGHTS_NAME,\n-    OpenAIGPTDoubleHeadsModel,\n-    OpenAIGPTTokenizer,\n-    get_linear_schedule_with_warmup,\n-)\n-\n-\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n-)\n-logger = logging.getLogger(__name__)\n-\n-\n-def accuracy(out, labels):\n-    outputs = np.argmax(out, axis=1)\n-    return np.sum(outputs == labels)\n-\n-\n-def load_rocstories_dataset(dataset_path):\n-    \"\"\"Output a list of tuples(story, 1st continuation, 2nd continuation, label)\"\"\"\n-    with open(dataset_path, encoding=\"utf_8\") as f:\n-        f = csv.reader(f)\n-        output = []\n-        next(f)  # skip the first line\n-        for line in tqdm(f):\n-            output.append((\" \".join(line[1:5]), line[5], line[6], int(line[-1]) - 1))\n-    return output\n-\n-\n-def pre_process_datasets(encoded_datasets, input_len, cap_length, start_token, delimiter_token, clf_token):\n-    \"\"\"Pre-process datasets containing lists of tuples(story, 1st continuation, 2nd continuation, label)\n-\n-    To Transformer inputs of shape (n_batch, n_alternative, length) comprising for each batch, continuation:\n-    input_ids[batch, alternative, :] = [start_token] + story[:cap_length] + [delimiter_token] + cont1[:cap_length] + [clf_token]\n-    \"\"\"\n-    tensor_datasets = []\n-    for dataset in encoded_datasets:\n-        n_batch = len(dataset)\n-        input_ids = np.zeros((n_batch, 2, input_len), dtype=np.int64)\n-        mc_token_ids = np.zeros((n_batch, 2), dtype=np.int64)\n-        lm_labels = np.full((n_batch, 2, input_len), fill_value=-100, dtype=np.int64)\n-        mc_labels = np.zeros((n_batch,), dtype=np.int64)\n-        for (\n-            i,\n-            (story, cont1, cont2, mc_label),\n-        ) in enumerate(dataset):\n-            with_cont1 = [start_token] + story[:cap_length] + [delimiter_token] + cont1[:cap_length] + [clf_token]\n-            with_cont2 = [start_token] + story[:cap_length] + [delimiter_token] + cont2[:cap_length] + [clf_token]\n-            input_ids[i, 0, : len(with_cont1)] = with_cont1\n-            input_ids[i, 1, : len(with_cont2)] = with_cont2\n-            mc_token_ids[i, 0] = len(with_cont1) - 1\n-            mc_token_ids[i, 1] = len(with_cont2) - 1\n-            lm_labels[i, 0, : len(with_cont1)] = with_cont1\n-            lm_labels[i, 1, : len(with_cont2)] = with_cont2\n-            mc_labels[i] = mc_label\n-        all_inputs = (input_ids, mc_token_ids, lm_labels, mc_labels)\n-        tensor_datasets.append(tuple(torch.tensor(t) for t in all_inputs))\n-    return tensor_datasets\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--model_name\", type=str, default=\"openai-community/openai-gpt\", help=\"pretrained model name\")\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model predictions and checkpoints will be written.\",\n-    )\n-    parser.add_argument(\"--train_dataset\", type=str, default=\"\")\n-    parser.add_argument(\"--eval_dataset\", type=str, default=\"\")\n-    parser.add_argument(\"--seed\", type=int, default=42)\n-    parser.add_argument(\"--num_train_epochs\", type=int, default=3)\n-    parser.add_argument(\"--train_batch_size\", type=int, default=8)\n-    parser.add_argument(\"--eval_batch_size\", type=int, default=16)\n-    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", type=int, default=1)\n-    parser.add_argument(\n-        \"--max_steps\",\n-        default=-1,\n-        type=int,\n-        help=(\n-            \"If > 0: set total number of training                         steps to perform. Override num_train_epochs.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before                        performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\"--learning_rate\", type=float, default=6.25e-5)\n-    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-    parser.add_argument(\"--lr_schedule\", type=str, default=\"warmup_linear\")\n-    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n-    parser.add_argument(\"--lm_coef\", type=float, default=0.9)\n-    parser.add_argument(\"--n_valid\", type=int, default=374)\n-\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    args = parser.parse_args()\n-    print(args)\n-\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    random.seed(args.seed)\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    torch.cuda.manual_seed_all(args.seed)\n-\n-    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n-    n_gpu = torch.cuda.device_count()\n-    logger.info(f\"device: {device}, n_gpu {n_gpu}\")\n-\n-    if not args.do_train and not args.do_eval:\n-        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n-\n-    if not os.path.exists(args.output_dir):\n-        os.makedirs(args.output_dir)\n-\n-    # Load tokenizer and model\n-    # This loading functions also add new tokens and embeddings called `special tokens`\n-    # These new embeddings will be fine-tuned on the RocStories dataset\n-    special_tokens = [\"_start_\", \"_delimiter_\", \"_classify_\"]\n-    tokenizer = OpenAIGPTTokenizer.from_pretrained(args.model_name)\n-    tokenizer.add_tokens(special_tokens)\n-    special_tokens_ids = tokenizer.convert_tokens_to_ids(special_tokens)\n-    model = OpenAIGPTDoubleHeadsModel.from_pretrained(args.model_name)\n-    model.resize_token_embeddings(len(tokenizer))\n-    model.to(device)\n-\n-    # Load and encode the datasets\n-    def tokenize_and_encode(obj):\n-        \"\"\"Tokenize and encode a nested object\"\"\"\n-        if isinstance(obj, str):\n-            return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(obj))\n-        elif isinstance(obj, int):\n-            return obj\n-        return [tokenize_and_encode(o) for o in obj]\n-\n-    logger.info(\"Encoding dataset...\")\n-    train_dataset = load_rocstories_dataset(args.train_dataset)\n-    eval_dataset = load_rocstories_dataset(args.eval_dataset)\n-    datasets = (train_dataset, eval_dataset)\n-    encoded_datasets = tokenize_and_encode(datasets)\n-\n-    # Compute the max input length for the Transformer\n-    max_length = model.config.n_positions // 2 - 2\n-    input_length = max(\n-        len(story[:max_length]) + max(len(cont1[:max_length]), len(cont2[:max_length])) + 3\n-        for dataset in encoded_datasets\n-        for story, cont1, cont2, _ in dataset\n-    )\n-    input_length = min(input_length, model.config.n_positions)  # Max size of input for the pre-trained model\n-\n-    # Prepare inputs tensors and dataloaders\n-    tensor_datasets = pre_process_datasets(encoded_datasets, input_length, max_length, *special_tokens_ids)\n-    train_tensor_dataset, eval_tensor_dataset = tensor_datasets[0], tensor_datasets[1]\n-\n-    train_data = TensorDataset(*train_tensor_dataset)\n-    train_sampler = RandomSampler(train_data)\n-    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n-\n-    eval_data = TensorDataset(*eval_tensor_dataset)\n-    eval_sampler = SequentialSampler(eval_data)\n-    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n-\n-    # Prepare optimizer\n-    if args.do_train:\n-        if args.max_steps > 0:\n-            t_total = args.max_steps\n-            args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n-        else:\n-            t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n-\n-        param_optimizer = list(model.named_parameters())\n-        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n-        optimizer_grouped_parameters = [\n-            {\n-                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n-                \"weight_decay\": args.weight_decay,\n-            },\n-            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n-        ]\n-        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n-        scheduler = get_linear_schedule_with_warmup(\n-            optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n-        )\n-\n-    if args.do_train:\n-        nb_tr_steps, tr_loss, exp_average_loss = 0, 0, None\n-        model.train()\n-        for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n-            tr_loss = 0\n-            nb_tr_steps = 0\n-            tqdm_bar = tqdm(train_dataloader, desc=\"Training\")\n-            for step, batch in enumerate(tqdm_bar):\n-                batch = tuple(t.to(device) for t in batch)\n-                input_ids, mc_token_ids, lm_labels, mc_labels = batch\n-                losses = model(input_ids, mc_token_ids=mc_token_ids, lm_labels=lm_labels, mc_labels=mc_labels)\n-                loss = args.lm_coef * losses[0] + losses[1]\n-                loss.backward()\n-                optimizer.step()\n-                scheduler.step()\n-                optimizer.zero_grad()\n-                tr_loss += loss.item()\n-                exp_average_loss = (\n-                    loss.item() if exp_average_loss is None else 0.7 * exp_average_loss + 0.3 * loss.item()\n-                )\n-                nb_tr_steps += 1\n-                tqdm_bar.desc = f\"Training loss: {exp_average_loss:.2e} lr: {scheduler.get_lr()[0]:.2e}\"\n-\n-    # Save a trained model\n-    if args.do_train:\n-        # Save a trained model, configuration and tokenizer\n-        model_to_save = model.module if hasattr(model, \"module\") else model  # Only save the model itself\n-\n-        # If we save using the predefined names, we can load using `from_pretrained`\n-        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n-        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n-\n-        torch.save(model_to_save.state_dict(), output_model_file)\n-        model_to_save.config.to_json_file(output_config_file)\n-        tokenizer.save_vocabulary(args.output_dir)\n-\n-        # Load a trained model and vocabulary that you have fine-tuned\n-        model = OpenAIGPTDoubleHeadsModel.from_pretrained(args.output_dir)\n-        tokenizer = OpenAIGPTTokenizer.from_pretrained(args.output_dir)\n-        model.to(device)\n-\n-    if args.do_eval:\n-        model.eval()\n-        eval_loss, eval_accuracy = 0, 0\n-        nb_eval_steps, nb_eval_examples = 0, 0\n-        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n-            batch = tuple(t.to(device) for t in batch)\n-            input_ids, mc_token_ids, lm_labels, mc_labels = batch\n-            with torch.no_grad():\n-                _, mc_loss, _, mc_logits = model(\n-                    input_ids, mc_token_ids=mc_token_ids, lm_labels=lm_labels, mc_labels=mc_labels\n-                )\n-\n-            mc_logits = mc_logits.detach().cpu().numpy()\n-            mc_labels = mc_labels.to(\"cpu\").numpy()\n-            tmp_eval_accuracy = accuracy(mc_logits, mc_labels)\n-\n-            eval_loss += mc_loss.mean().item()\n-            eval_accuracy += tmp_eval_accuracy\n-\n-            nb_eval_examples += input_ids.size(0)\n-            nb_eval_steps += 1\n-\n-        eval_loss = eval_loss / nb_eval_steps\n-        eval_accuracy = eval_accuracy / nb_eval_examples\n-        train_loss = tr_loss / nb_tr_steps if args.do_train else None\n-        result = {\"eval_loss\": eval_loss, \"eval_accuracy\": eval_accuracy, \"train_loss\": train_loss}\n-\n-        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n-        with open(output_eval_file, \"w\") as writer:\n-            logger.info(\"***** Eval results *****\")\n-            for key in sorted(result.keys()):\n-                logger.info(\"  %s = %s\", key, str(result[key]))\n-                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "8c80cf8a347acb3c48dfa982aab76156bf66a45d",
            "filename": "examples/legacy/run_swag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 706,
            "changes": 706,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_swag.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,706 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"BERT finetuning runner.\n-Finetuning the library models for multiple choice on SWAG (Bert).\n-\"\"\"\n-\n-import argparse\n-import csv\n-import glob\n-import logging\n-import os\n-import random\n-\n-import numpy as np\n-import torch\n-from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n-from torch.utils.data.distributed import DistributedSampler\n-from tqdm import tqdm, trange\n-\n-import transformers\n-from transformers import (\n-    WEIGHTS_NAME,\n-    AutoConfig,\n-    AutoModelForMultipleChoice,\n-    AutoTokenizer,\n-    get_linear_schedule_with_warmup,\n-)\n-from transformers.trainer_utils import is_main_process\n-\n-\n-try:\n-    from torch.utils.tensorboard import SummaryWriter\n-except ImportError:\n-    from tensorboardX import SummaryWriter\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class SwagExample:\n-    \"\"\"A single training/test example for the SWAG dataset.\"\"\"\n-\n-    def __init__(self, swag_id, context_sentence, start_ending, ending_0, ending_1, ending_2, ending_3, label=None):\n-        self.swag_id = swag_id\n-        self.context_sentence = context_sentence\n-        self.start_ending = start_ending\n-        self.endings = [\n-            ending_0,\n-            ending_1,\n-            ending_2,\n-            ending_3,\n-        ]\n-        self.label = label\n-\n-    def __str__(self):\n-        return self.__repr__()\n-\n-    def __repr__(self):\n-        attributes = [\n-            f\"swag_id: {self.swag_id}\",\n-            f\"context_sentence: {self.context_sentence}\",\n-            f\"start_ending: {self.start_ending}\",\n-            f\"ending_0: {self.endings[0]}\",\n-            f\"ending_1: {self.endings[1]}\",\n-            f\"ending_2: {self.endings[2]}\",\n-            f\"ending_3: {self.endings[3]}\",\n-        ]\n-\n-        if self.label is not None:\n-            attributes.append(f\"label: {self.label}\")\n-\n-        return \", \".join(attributes)\n-\n-\n-class InputFeatures:\n-    def __init__(self, example_id, choices_features, label):\n-        self.example_id = example_id\n-        self.choices_features = [\n-            {\"input_ids\": input_ids, \"input_mask\": input_mask, \"segment_ids\": segment_ids}\n-            for _, input_ids, input_mask, segment_ids in choices_features\n-        ]\n-        self.label = label\n-\n-\n-def read_swag_examples(input_file, is_training=True):\n-    with open(input_file, encoding=\"utf-8\") as f:\n-        lines = list(csv.reader(f))\n-\n-    if is_training and lines[0][-1] != \"label\":\n-        raise ValueError(\"For training, the input file must contain a label column.\")\n-\n-    examples = [\n-        SwagExample(\n-            swag_id=line[2],\n-            context_sentence=line[4],\n-            start_ending=line[5],  # in the swag dataset, the\n-            # common beginning of each\n-            # choice is stored in \"sent2\".\n-            ending_0=line[7],\n-            ending_1=line[8],\n-            ending_2=line[9],\n-            ending_3=line[10],\n-            label=int(line[11]) if is_training else None,\n-        )\n-        for line in lines[1:]  # we skip the line with the column names\n-    ]\n-\n-    return examples\n-\n-\n-def convert_examples_to_features(examples, tokenizer, max_seq_length, is_training):\n-    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n-\n-    # Swag is a multiple choice task. To perform this task using Bert,\n-    # we will use the formatting proposed in \"Improving Language\n-    # Understanding by Generative Pre-Training\" and suggested by\n-    # @jacobdevlin-google in this issue\n-    # https://github.com/google-research/bert/issues/38.\n-    #\n-    # Each choice will correspond to a sample on which we run the\n-    # inference. For a given Swag example, we will create the 4\n-    # following inputs:\n-    # - [CLS] context [SEP] choice_1 [SEP]\n-    # - [CLS] context [SEP] choice_2 [SEP]\n-    # - [CLS] context [SEP] choice_3 [SEP]\n-    # - [CLS] context [SEP] choice_4 [SEP]\n-    # The model will output a single value for each input. To get the\n-    # final decision of the model, we will run a softmax over these 4\n-    # outputs.\n-    features = []\n-    for example_index, example in tqdm(enumerate(examples)):\n-        context_tokens = tokenizer.tokenize(example.context_sentence)\n-        start_ending_tokens = tokenizer.tokenize(example.start_ending)\n-\n-        choices_features = []\n-        for ending_index, ending in enumerate(example.endings):\n-            # We create a copy of the context tokens in order to be\n-            # able to shrink it according to ending_tokens\n-            context_tokens_choice = context_tokens[:]\n-            ending_tokens = start_ending_tokens + tokenizer.tokenize(ending)\n-            # Modifies `context_tokens_choice` and `ending_tokens` in\n-            # place so that the total length is less than the\n-            # specified length.  Account for [CLS], [SEP], [SEP] with\n-            # \"- 3\"\n-            _truncate_seq_pair(context_tokens_choice, ending_tokens, max_seq_length - 3)\n-\n-            tokens = [\"[CLS]\"] + context_tokens_choice + [\"[SEP]\"] + ending_tokens + [\"[SEP]\"]\n-            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(ending_tokens) + 1)\n-\n-            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n-            input_mask = [1] * len(input_ids)\n-\n-            # Zero-pad up to the sequence length.\n-            padding = [0] * (max_seq_length - len(input_ids))\n-            input_ids += padding\n-            input_mask += padding\n-            segment_ids += padding\n-\n-            assert len(input_ids) == max_seq_length\n-            assert len(input_mask) == max_seq_length\n-            assert len(segment_ids) == max_seq_length\n-\n-            choices_features.append((tokens, input_ids, input_mask, segment_ids))\n-\n-        label = example.label\n-        if example_index < 5:\n-            logger.info(\"*** Example ***\")\n-            logger.info(f\"swag_id: {example.swag_id}\")\n-            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(choices_features):\n-                logger.info(f\"choice: {choice_idx}\")\n-                logger.info(\"tokens: {}\".format(\" \".join(tokens)))\n-                logger.info(\"input_ids: {}\".format(\" \".join(map(str, input_ids))))\n-                logger.info(\"input_mask: {}\".format(\" \".join(map(str, input_mask))))\n-                logger.info(\"segment_ids: {}\".format(\" \".join(map(str, segment_ids))))\n-            if is_training:\n-                logger.info(f\"label: {label}\")\n-\n-        features.append(InputFeatures(example_id=example.swag_id, choices_features=choices_features, label=label))\n-\n-    return features\n-\n-\n-def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n-    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n-\n-    # This is a simple heuristic which will always truncate the longer sequence\n-    # one token at a time. This makes more sense than truncating an equal percent\n-    # of tokens from each, since if one sequence is very short then each token\n-    # that's truncated likely contains more information than a longer sequence.\n-    while True:\n-        total_length = len(tokens_a) + len(tokens_b)\n-        if total_length <= max_length:\n-            break\n-        if len(tokens_a) > len(tokens_b):\n-            tokens_a.pop()\n-        else:\n-            tokens_b.pop()\n-\n-\n-def accuracy(out, labels):\n-    outputs = np.argmax(out, axis=1)\n-    return np.sum(outputs == labels)\n-\n-\n-def select_field(features, field):\n-    return [[choice[field] for choice in feature.choices_features] for feature in features]\n-\n-\n-def set_seed(args):\n-    random.seed(args.seed)\n-    np.random.seed(args.seed)\n-    torch.manual_seed(args.seed)\n-    if args.n_gpu > 0:\n-        torch.cuda.manual_seed_all(args.seed)\n-\n-\n-def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n-    if args.local_rank not in [-1, 0]:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-\n-    # Load data features from cache or dataset file\n-    input_file = args.predict_file if evaluate else args.train_file\n-    cached_features_file = os.path.join(\n-        os.path.dirname(input_file),\n-        \"cached_{}_{}_{}\".format(\n-            \"dev\" if evaluate else \"train\",\n-            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n-            str(args.max_seq_length),\n-        ),\n-    )\n-    if os.path.exists(cached_features_file) and not args.overwrite_cache and not output_examples:\n-        logger.info(\"Loading features from cached file %s\", cached_features_file)\n-        features = torch.load(cached_features_file, weights_only=True)\n-    else:\n-        logger.info(\"Creating features from dataset file at %s\", input_file)\n-        examples = read_swag_examples(input_file)\n-        features = convert_examples_to_features(examples, tokenizer, args.max_seq_length, not evaluate)\n-\n-        if args.local_rank in [-1, 0]:\n-            logger.info(\"Saving features into cached file %s\", cached_features_file)\n-            torch.save(features, cached_features_file)\n-\n-    if args.local_rank == 0:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n-\n-    # Convert to Tensors and build dataset\n-    all_input_ids = torch.tensor(select_field(features, \"input_ids\"), dtype=torch.long)\n-    all_input_mask = torch.tensor(select_field(features, \"input_mask\"), dtype=torch.long)\n-    all_segment_ids = torch.tensor(select_field(features, \"segment_ids\"), dtype=torch.long)\n-    all_label = torch.tensor([f.label for f in features], dtype=torch.long)\n-\n-    if evaluate:\n-        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n-    else:\n-        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n-\n-    if output_examples:\n-        return dataset, examples, features\n-    return dataset\n-\n-\n-def train(args, train_dataset, model, tokenizer):\n-    \"\"\"Train the model\"\"\"\n-    if args.local_rank in [-1, 0]:\n-        tb_writer = SummaryWriter()\n-\n-    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n-    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n-    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n-\n-    if args.max_steps > 0:\n-        t_total = args.max_steps\n-        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n-    else:\n-        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n-\n-    # Prepare optimizer and schedule (linear warmup and decay)\n-    no_decay = [\"bias\", \"LayerNorm.weight\"]\n-    optimizer_grouped_parameters = [\n-        {\n-            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n-            \"weight_decay\": args.weight_decay,\n-        },\n-        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n-    ]\n-    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n-    scheduler = get_linear_schedule_with_warmup(\n-        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n-    )\n-    if args.fp16:\n-        try:\n-            from apex import amp\n-        except ImportError:\n-            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n-        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n-\n-    # multi-gpu training (should be after apex fp16 initialization)\n-    if args.n_gpu > 1:\n-        model = torch.nn.DataParallel(model)\n-\n-    # Distributed training (should be after apex fp16 initialization)\n-    if args.local_rank != -1:\n-        model = torch.nn.parallel.DistributedDataParallel(\n-            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n-        )\n-\n-    # Train!\n-    logger.info(\"***** Running training *****\")\n-    logger.info(\"  Num examples = %d\", len(train_dataset))\n-    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n-    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n-    logger.info(\n-        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n-        args.train_batch_size\n-        * args.gradient_accumulation_steps\n-        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n-    )\n-    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n-    logger.info(\"  Total optimization steps = %d\", t_total)\n-\n-    global_step = 0\n-    tr_loss, logging_loss = 0.0, 0.0\n-    model.zero_grad()\n-    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n-    set_seed(args)  # Added here for reproducibility\n-    for _ in train_iterator:\n-        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n-        for step, batch in enumerate(epoch_iterator):\n-            model.train()\n-            batch = tuple(t.to(args.device) for t in batch)\n-            inputs = {\n-                \"input_ids\": batch[0],\n-                \"attention_mask\": batch[1],\n-                # 'token_type_ids':  None if args.model_type == 'xlm' else batch[2],\n-                \"token_type_ids\": batch[2],\n-                \"labels\": batch[3],\n-            }\n-            # if args.model_type in ['xlnet', 'xlm']:\n-            #     inputs.update({'cls_index': batch[5],\n-            #                    'p_mask':       batch[6]})\n-            outputs = model(**inputs)\n-            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n-\n-            if args.n_gpu > 1:\n-                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n-            if args.gradient_accumulation_steps > 1:\n-                loss = loss / args.gradient_accumulation_steps\n-\n-            if args.fp16:\n-                with amp.scale_loss(loss, optimizer) as scaled_loss:\n-                    scaled_loss.backward()\n-                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n-            else:\n-                loss.backward()\n-                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n-\n-            tr_loss += loss.item()\n-            if (step + 1) % args.gradient_accumulation_steps == 0:\n-                optimizer.step()\n-                scheduler.step()  # Update learning rate schedule\n-                model.zero_grad()\n-                global_step += 1\n-\n-                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n-                    # Log metrics\n-                    if (\n-                        args.local_rank == -1 and args.evaluate_during_training\n-                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n-                        results = evaluate(args, model, tokenizer)\n-                        for key, value in results.items():\n-                            tb_writer.add_scalar(f\"eval_{key}\", value, global_step)\n-                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n-                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n-                    logging_loss = tr_loss\n-\n-                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n-                    # Save model checkpoint\n-                    output_dir = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n-                    model_to_save = (\n-                        model.module if hasattr(model, \"module\") else model\n-                    )  # Take care of distributed/parallel training\n-                    model_to_save.save_pretrained(output_dir)\n-                    tokenizer.save_vocabulary(output_dir)\n-                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n-                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n-\n-            if args.max_steps > 0 and global_step > args.max_steps:\n-                epoch_iterator.close()\n-                break\n-        if args.max_steps > 0 and global_step > args.max_steps:\n-            train_iterator.close()\n-            break\n-\n-    if args.local_rank in [-1, 0]:\n-        tb_writer.close()\n-\n-    return global_step, tr_loss / global_step\n-\n-\n-def evaluate(args, model, tokenizer, prefix=\"\"):\n-    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n-\n-    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n-        os.makedirs(args.output_dir)\n-\n-    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n-    # Note that DistributedSampler samples randomly\n-    eval_sampler = SequentialSampler(dataset) if args.local_rank == -1 else DistributedSampler(dataset)\n-    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n-\n-    # Eval!\n-    logger.info(f\"***** Running evaluation {prefix} *****\")\n-    logger.info(\"  Num examples = %d\", len(dataset))\n-    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n-\n-    eval_loss, eval_accuracy = 0, 0\n-    nb_eval_steps, nb_eval_examples = 0, 0\n-\n-    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n-        model.eval()\n-        batch = tuple(t.to(args.device) for t in batch)\n-        with torch.no_grad():\n-            inputs = {\n-                \"input_ids\": batch[0],\n-                \"attention_mask\": batch[1],\n-                # 'token_type_ids': None if args.model_type == 'xlm' else batch[2]  # XLM don't use segment_ids\n-                \"token_type_ids\": batch[2],\n-                \"labels\": batch[3],\n-            }\n-\n-            # if args.model_type in ['xlnet', 'xlm']:\n-            #     inputs.update({'cls_index': batch[4],\n-            #                    'p_mask':    batch[5]})\n-            outputs = model(**inputs)\n-            tmp_eval_loss, logits = outputs[:2]\n-            eval_loss += tmp_eval_loss.mean().item()\n-\n-        logits = logits.detach().cpu().numpy()\n-        label_ids = inputs[\"labels\"].to(\"cpu\").numpy()\n-        tmp_eval_accuracy = accuracy(logits, label_ids)\n-        eval_accuracy += tmp_eval_accuracy\n-\n-        nb_eval_steps += 1\n-        nb_eval_examples += inputs[\"input_ids\"].size(0)\n-\n-    eval_loss = eval_loss / nb_eval_steps\n-    eval_accuracy = eval_accuracy / nb_eval_examples\n-    result = {\"eval_loss\": eval_loss, \"eval_accuracy\": eval_accuracy}\n-\n-    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n-    with open(output_eval_file, \"w\") as writer:\n-        logger.info(\"***** Eval results *****\")\n-        for key in sorted(result.keys()):\n-            logger.info(\"%s = %s\", key, str(result[key]))\n-            writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n-\n-    return result\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-\n-    # Required parameters\n-    parser.add_argument(\n-        \"--train_file\", default=None, type=str, required=True, help=\"SWAG csv for training. E.g., train.csv\"\n-    )\n-    parser.add_argument(\n-        \"--predict_file\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"SWAG csv for predictions. E.g., val.csv or test.csv\",\n-    )\n-    parser.add_argument(\n-        \"--model_name_or_path\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"The output directory where the model checkpoints and predictions will be written.\",\n-    )\n-\n-    # Other parameters\n-    parser.add_argument(\n-        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name\",\n-        default=\"\",\n-        type=str,\n-        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n-    )\n-    parser.add_argument(\n-        \"--max_seq_length\",\n-        default=384,\n-        type=int,\n-        help=(\n-            \"The maximum total input sequence length after tokenization. Sequences \"\n-            \"longer than this will be truncated, and sequences shorter than this will be padded.\"\n-        ),\n-    )\n-    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n-    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n-    parser.add_argument(\n-        \"--evaluate_during_training\", action=\"store_true\", help=\"Rul evaluation during training at each logging step.\"\n-    )\n-\n-    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n-    parser.add_argument(\n-        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n-    )\n-    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n-    parser.add_argument(\n-        \"--gradient_accumulation_steps\",\n-        type=int,\n-        default=1,\n-        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n-    )\n-    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n-    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n-    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n-    parser.add_argument(\n-        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n-    )\n-    parser.add_argument(\n-        \"--max_steps\",\n-        default=-1,\n-        type=int,\n-        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n-    )\n-    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n-\n-    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n-    parser.add_argument(\"--save_steps\", type=int, default=50, help=\"Save checkpoint every X updates steps.\")\n-    parser.add_argument(\n-        \"--eval_all_checkpoints\",\n-        action=\"store_true\",\n-        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n-    )\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n-    parser.add_argument(\n-        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n-    )\n-    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n-\n-    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n-    parser.add_argument(\n-        \"--fp16\",\n-        action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n-    )\n-    parser.add_argument(\n-        \"--fp16_opt_level\",\n-        type=str,\n-        default=\"O1\",\n-        help=(\n-            \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-            \"See details at https://nvidia.github.io/apex/amp.html\"\n-        ),\n-    )\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    args = parser.parse_args()\n-\n-    # Setup distant debugging if needed\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    # Setup CUDA, GPU & distributed training\n-    if args.local_rank == -1 or args.no_cuda:\n-        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n-    else:  # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n-        torch.cuda.set_device(args.local_rank)\n-        device = torch.device(\"cuda\", args.local_rank)\n-        torch.distributed.init_process_group(backend=\"nccl\")\n-        args.n_gpu = 1\n-    args.device = device\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        args.local_rank,\n-        device,\n-        args.n_gpu,\n-        bool(args.local_rank != -1),\n-        args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-\n-    # Set seed\n-    set_seed(args)\n-\n-    # Load pretrained model and tokenizer\n-    if args.local_rank not in [-1, 0]:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n-\n-    config = AutoConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n-    )\n-    model = AutoModelForMultipleChoice.from_pretrained(\n-        args.model_name_or_path, from_tf=bool(\".ckpt\" in args.model_name_or_path), config=config\n-    )\n-\n-    if args.local_rank == 0:\n-        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n-\n-    model.to(args.device)\n-\n-    logger.info(\"Training/evaluation parameters %s\", args)\n-\n-    # Training\n-    if args.do_train:\n-        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n-        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n-        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n-\n-    # Save the trained model and the tokenizer\n-    if args.local_rank == -1 or torch.distributed.get_rank() == 0:\n-        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n-        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n-        # They can then be reloaded using `from_pretrained()`\n-        model_to_save = (\n-            model.module if hasattr(model, \"module\") else model\n-        )  # Take care of distributed/parallel training\n-        model_to_save.save_pretrained(args.output_dir)\n-        tokenizer.save_pretrained(args.output_dir)\n-\n-        # Good practice: save your training arguments together with the trained model\n-        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n-\n-        # Load a trained model and vocabulary that you have fine-tuned\n-        model = AutoModelForMultipleChoice.from_pretrained(args.output_dir)\n-        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n-        model.to(args.device)\n-\n-    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n-    results = {}\n-    if args.do_eval and args.local_rank in [-1, 0]:\n-        if args.do_train:\n-            checkpoints = [args.output_dir]\n-        else:\n-            # if do_train is False and do_eval is true, load model directly from pretrained.\n-            checkpoints = [args.model_name_or_path]\n-\n-        if args.eval_all_checkpoints:\n-            checkpoints = [\n-                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n-            ]\n-\n-        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n-\n-        for checkpoint in checkpoints:\n-            # Reload the model\n-            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n-            model = AutoModelForMultipleChoice.from_pretrained(checkpoint)\n-            tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-            model.to(args.device)\n-\n-            # Evaluate\n-            result = evaluate(args, model, tokenizer, prefix=global_step)\n-\n-            result = {k + (f\"_{global_step}\" if global_step else \"\"): v for k, v in result.items()}\n-            results.update(result)\n-\n-    logger.info(f\"Results: {results}\")\n-\n-    return results\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "1af855be139288b253e45bdab9531d76f5df410f",
            "filename": "examples/legacy/run_transfo_xl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 143,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Frun_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_transfo_xl.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,143 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Transformer XL model evaluation script.\n-Adapted from https://github.com/kimiyoung/transformer-xl.\n-In particular https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/eval.py\n-\n-This script with default values evaluates a pretrained Transformer-XL on WikiText 103\n-\"\"\"\n-\n-import argparse\n-import logging\n-import math\n-import time\n-\n-import torch\n-\n-from transformers import TransfoXLCorpus, TransfoXLLMHeadModel\n-\n-\n-logging.basicConfig(\n-    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n-)\n-logger = logging.getLogger(__name__)\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser(description=\"PyTorch Transformer Language Model\")\n-    parser.add_argument(\"--model_name\", type=str, default=\"transfo-xl/transfo-xl-wt103\", help=\"pretrained model name\")\n-    parser.add_argument(\n-        \"--split\", type=str, default=\"test\", choices=[\"all\", \"valid\", \"test\"], help=\"which split to evaluate\"\n-    )\n-    parser.add_argument(\"--batch_size\", type=int, default=10, help=\"batch size\")\n-    parser.add_argument(\"--tgt_len\", type=int, default=128, help=\"number of tokens to predict\")\n-    parser.add_argument(\"--ext_len\", type=int, default=0, help=\"length of the extended context\")\n-    parser.add_argument(\"--mem_len\", type=int, default=1600, help=\"length of the retained previous heads\")\n-    parser.add_argument(\"--clamp_len\", type=int, default=1000, help=\"max positional embedding index\")\n-    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Do not use CUDA even though CUA is available\")\n-    parser.add_argument(\"--work_dir\", type=str, required=True, help=\"path to the work_dir\")\n-    parser.add_argument(\"--no_log\", action=\"store_true\", help=\"do not log the eval result\")\n-    parser.add_argument(\"--same_length\", action=\"store_true\", help=\"set same length attention with masking\")\n-    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n-    args = parser.parse_args()\n-    assert args.ext_len >= 0, \"extended context length must be non-negative\"\n-\n-    if args.server_ip and args.server_port:\n-        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n-        import ptvsd\n-\n-        print(\"Waiting for debugger attach\")\n-        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n-        ptvsd.wait_for_attach()\n-\n-    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n-    logger.info(f\"device: {device}\")\n-\n-    # Load a pre-processed dataset\n-    # You can also build the corpus yourself using TransfoXLCorpus methods\n-    # The pre-processing involve computing word frequencies to prepare the Adaptive input and SoftMax\n-    # and tokenizing the dataset\n-    # The pre-processed corpus is a conversion (using the conversion script )\n-    corpus = TransfoXLCorpus.from_pretrained(args.model_name)\n-\n-    va_iter = corpus.get_iterator(\"valid\", args.batch_size, args.tgt_len, device=device, ext_len=args.ext_len)\n-    te_iter = corpus.get_iterator(\"test\", args.batch_size, args.tgt_len, device=device, ext_len=args.ext_len)\n-\n-    # Load a pre-trained model\n-    model = TransfoXLLMHeadModel.from_pretrained(args.model_name)\n-    model.to(device)\n-\n-    logger.info(\n-        \"Evaluating with bsz {} tgt_len {} ext_len {} mem_len {} clamp_len {}\".format(\n-            args.batch_size, args.tgt_len, args.ext_len, args.mem_len, args.clamp_len\n-        )\n-    )\n-\n-    model.reset_memory_length(args.mem_len)\n-    if args.clamp_len > 0:\n-        model.clamp_len = args.clamp_len\n-    if args.same_length:\n-        model.same_length = True\n-\n-    ###############################################################################\n-    # Evaluation code\n-    ###############################################################################\n-    def evaluate(eval_iter):\n-        # Turn on evaluation mode which disables dropout.\n-        model.eval()\n-        total_len, total_loss = 0, 0.0\n-        start_time = time.time()\n-        with torch.no_grad():\n-            mems = None\n-            for idx, (data, target, seq_len) in enumerate(eval_iter):\n-                ret = model(data, lm_labels=target, mems=mems)\n-                loss, _, mems = ret\n-                loss = loss.mean()\n-                total_loss += seq_len * loss.item()\n-                total_len += seq_len\n-            total_time = time.time() - start_time\n-        logger.info(f\"Time : {total_time:.2f}s, {1000 * total_time / (idx + 1):.2f}ms/segment\")\n-        return total_loss / total_len\n-\n-    # Run on test data.\n-    if args.split == \"all\":\n-        test_loss = evaluate(te_iter)\n-        valid_loss = evaluate(va_iter)\n-    elif args.split == \"valid\":\n-        valid_loss = evaluate(va_iter)\n-        test_loss = None\n-    elif args.split == \"test\":\n-        test_loss = evaluate(te_iter)\n-        valid_loss = None\n-\n-    def format_log(loss, split):\n-        log_str = \"| {0} loss {1:5.2f} | {0} ppl {2:9.3f} \".format(split, loss, math.exp(loss))\n-        return log_str\n-\n-    log_str = \"\"\n-    if valid_loss is not None:\n-        log_str += format_log(valid_loss, \"valid\")\n-    if test_loss is not None:\n-        log_str += format_log(test_loss, \"test\")\n-\n-    logger.info(\"=\" * 100)\n-    logger.info(log_str)\n-    logger.info(\"=\" * 100)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "831ae1f4cd0335e4b849603d7121be5413aa0bed",
            "filename": "examples/legacy/seq2seq/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 327,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2FREADME.md?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,327 +0,0 @@\n-<!---\n-Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Sequence-to-Sequence Training and Evaluation\n-\n-This directory contains examples for finetuning and evaluating transformers on summarization and translation tasks.\n-For deprecated `bertabs` instructions, see https://github.com/huggingface/transformers-research-projects/blob/main/bertabs/README.md.\n-\n-### Supported Architectures\n-\n-- `BartForConditionalGeneration`\n-- `MarianMTModel`\n-- `PegasusForConditionalGeneration`\n-- `MBartForConditionalGeneration`\n-- `FSMTForConditionalGeneration`\n-- `T5ForConditionalGeneration`\n-\n-### Download the Datasets\n-\n-#### XSUM\n-\n-```bash\n-cd examples/legacy/seq2seq\n-wget https://cdn-datasets.huggingface.co/summarization/xsum.tar.gz\n-tar -xzvf xsum.tar.gz\n-export XSUM_DIR=${PWD}/xsum\n-```\n-this should make a directory called `xsum/` with files like `test.source`.\n-To use your own data, copy that files format. Each article to be summarized is on its own line.\n-\n-#### CNN/DailyMail\n-\n-```bash\n-cd examples/legacy/seq2seq\n-wget https://cdn-datasets.huggingface.co/summarization/cnn_dm_v2.tgz\n-tar -xzvf cnn_dm_v2.tgz  # empty lines removed\n-mv cnn_cln cnn_dm\n-export CNN_DIR=${PWD}/cnn_dm\n-```\n-this should make a directory called `cnn_dm/` with 6 files.\n-\n-#### WMT16 English-Romanian Translation Data\n-\n-download with this command:\n-```bash\n-wget https://cdn-datasets.huggingface.co/translation/wmt_en_ro.tar.gz\n-tar -xzvf wmt_en_ro.tar.gz\n-export ENRO_DIR=${PWD}/wmt_en_ro\n-```\n-this should make a directory called `wmt_en_ro/` with 6 files.\n-\n-#### WMT English-German\n-\n-```bash\n-wget https://cdn-datasets.huggingface.co/translation/wmt_en_de.tgz\n-tar -xzvf wmt_en_de.tgz\n-export DATA_DIR=${PWD}/wmt_en_de\n-```\n-\n-#### FSMT datasets (wmt)\n-\n-Refer to the scripts starting with `eval_` under:\n-https://github.com/huggingface/transformers/tree/main/scripts/fsmt\n-\n-#### Pegasus (multiple datasets)\n-\n-Multiple eval datasets are available for download from:\n-https://github.com/stas00/porting/tree/master/datasets/pegasus\n-\n-\n-#### Your Data\n-\n-If you are using your own data, it must be formatted as one directory with 6 files:\n-```\n-train.source\n-train.target\n-val.source\n-val.target\n-test.source\n-test.target\n-```\n-The `.source` files are the input, the `.target` files are the desired output.\n-\n-### Tips and Tricks\n-\n-General Tips:\n-- since you need to run from `examples/legacy/seq2seq`, and likely need to modify code, the easiest workflow is fork transformers, clone your fork, and run `pip install -e .` before you get started.\n-- try `--freeze_encoder` or `--freeze_embeds` for faster training/larger batch size.  (3hr per epoch with bs=8, see the \"xsum_shared_task\" command below)\n-\n-- In addition to the pytorch-lightning .ckpt checkpoint, a transformers checkpoint will be saved.\n-Load it with `BartForConditionalGeneration.from_pretrained(f'{output_dir}/best_tfmr)`.\n-- At the moment, `--do_predict` does not work in a multi-gpu setting. You need to use `evaluate_checkpoint` or the `run_eval.py` code.\n-- This warning can be safely ignored:\n-    > \"Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-xsum and are newly initialized: ['final_logits_bias']\"\n-- Both finetuning and eval are 30% faster with `--fp16`.\n-- Read scripts before you run them!\n-\n-Summarization Tips:\n-- (summ) 1 epoch at batch size 1 for bart-large takes 24 hours and requires 13GB GPU RAM with fp16 on an NVIDIA-V100.\n-- If you want to run experiments on improving the summarization finetuning process, try the XSUM Shared Task (below). It's faster to train than CNNDM because the summaries are shorter.\n-- For CNN/DailyMail, the default `val_max_target_length` and `test_max_target_length` will truncate the ground truth labels, resulting in slightly higher rouge scores. To get accurate rouge scores, you should rerun calculate_rouge on the `{output_dir}/test_generations.txt` file saved by `trainer.test()`\n-- `--max_target_length=60 --val_max_target_length=60 --test_max_target_length=100 ` is a reasonable setting for XSUM.\n-- `wandb` can be used by specifying `--logger_name wandb`. It is useful for reproducibility. Specify the environment variable `WANDB_PROJECT='hf_xsum'` to do the XSUM shared task.\n-- If you are finetuning on your own dataset, start from `distilbart-cnn-12-6` if you want long summaries and `distilbart-xsum-12-6` if you want short summaries.\n-(It rarely makes sense to start from `bart-large` unless you are a researching finetuning methods).\n-\n-**Update 2018-07-18**\n-Datasets: `LegacySeq2SeqDataset` will be used for all tokenizers without a `prepare_seq2seq_batch` method. Otherwise, `Seq2SeqDataset` will be used.\n-Future work/help wanted: A new dataset to support multilingual tasks.\n-\n-\n-### Fine-tuning using Seq2SeqTrainer\n-To use `Seq2SeqTrainer` for fine-tuning you should use the `finetune_trainer.py` script. It subclasses `Trainer` to extend it for seq2seq training. Except the `Trainer`-related `TrainingArguments`, it shares the same argument names as that of `finetune.py` file. One notable difference is that calculating generative metrics (BLEU, ROUGE) is optional and is controlled using the `--predict_with_generate` argument.\n-\n-To see all the possible command line options, run:\n-\n-```bash\n-python finetune_trainer.py --help\n-```\n-\n-For multi-gpu training use `torch.distributed.launch`, e.g. with 2 gpus:\n-```bash\n-torchrun --nproc_per_node=2  finetune_trainer.py ...\n-```\n-\n-**At the moment, `Seq2SeqTrainer` does not support *with teacher* distillation.**\n-\n-All `Seq2SeqTrainer`-based fine-tuning scripts are included in the `builtin_trainer` directory.\n-\n-#### TPU Training\n-`Seq2SeqTrainer` supports TPU training with few caveats\n-1. As `generate` method does not work on TPU at the moment, `predict_with_generate` cannot be used. You should use `--prediction_loss_only` to only calculate loss, and do not set `--do_predict` and `--predict_with_generate`.\n-2. All sequences should be padded to be of equal length to avoid extremely slow training. (`finetune_trainer.py` does this automatically when running on TPU.)\n-\n-We provide a very simple launcher script named `xla_spawn.py` that lets you run our example scripts on multiple TPU cores without any boilerplate. Just pass a `--num_cores` flag to this script, then your regular training script with its arguments (this is similar to the `torch.distributed.launch` helper for `torch.distributed`).\n-\n-`builtin_trainer/finetune_tpu.sh` script provides minimal arguments needed for TPU training.\n-\n-The following command fine-tunes `sshleifer/student_marian_en_ro_6_3` on TPU V3-8 and should complete one epoch in ~5-6 mins.\n-\n-```bash\n-./builtin_trainer/train_distil_marian_enro_tpu.sh\n-```\n-\n-## Evaluation Commands\n-\n-To create summaries for each article in dataset, we use `run_eval.py`, here are a few commands that run eval for different tasks and models.\n-If 'translation' is in your task name, the computed metric will be BLEU. Otherwise, ROUGE will be used.\n-\n-For t5, you need to specify --task translation_{src}_to_{tgt} as follows:\n-```bash\n-export DATA_DIR=wmt_en_ro\n-./run_eval.py google-t5/t5-base \\\n-    $DATA_DIR/val.source t5_val_generations.txt \\\n-    --reference_path $DATA_DIR/val.target \\\n-    --score_path enro_bleu.json \\\n-    --task translation_en_to_ro \\\n-    --n_obs 100 \\\n-    --device cuda \\\n-    --fp16 \\\n-    --bs 32\n-```\n-\n-This command works for MBART, although the BLEU score is suspiciously low.\n-```bash\n-export DATA_DIR=wmt_en_ro\n-./run_eval.py facebook/mbart-large-en-ro $DATA_DIR/val.source mbart_val_generations.txt \\\n-    --reference_path $DATA_DIR/val.target \\\n-    --score_path enro_bleu.json \\\n-    --task translation \\\n-    --n_obs 100 \\\n-    --device cuda \\\n-    --fp16 \\\n-    --bs 32\n-```\n-\n-Summarization (xsum will be very similar):\n-```bash\n-export DATA_DIR=cnn_dm\n-./run_eval.py sshleifer/distilbart-cnn-12-6 $DATA_DIR/val.source dbart_val_generations.txt \\\n-    --reference_path $DATA_DIR/val.target \\\n-    --score_path cnn_rouge.json \\\n-    --task summarization \\\n-    --n_obs 100 \\\n-\n-th 56 \\\n-    --fp16 \\\n-    --bs 32\n-```\n-\n-### Multi-GPU Evaluation\n-here is a command to run xsum evaluation on 8 GPUs. It is more than linearly faster than run_eval.py in some cases\n-because it uses SortishSampler to minimize padding. You can also use it on 1 GPU. `data_dir` must have\n-`{type_path}.source` and `{type_path}.target`. Run `./run_distributed_eval.py --help` for all clargs.\n-\n-```bash\n-torchrun --nproc_per_node=8  run_distributed_eval.py \\\n-    --model_name sshleifer/distilbart-large-xsum-12-3  \\\n-    --save_dir xsum_generations \\\n-    --data_dir xsum \\\n-    --fp16  # you can pass generate kwargs like num_beams here, just like run_eval.py\n-```\n-\n-Contributions that implement this command for other distributed hardware setups are welcome!\n-\n-#### Single-GPU Eval: Tips and Tricks\n-\n-When using `run_eval.py`, the following features can be useful:\n-\n-* if you running the script multiple times and want to make it easier to track what arguments produced that output, use `--dump-args`. Along with the results it will also dump any custom params that were passed to the script. For example if you used: `--num_beams 8 --early_stopping true`, the output will be:\n-   ```json\n-   {'bleu': 26.887, 'n_obs': 10, 'runtime': 1, 'seconds_per_sample': 0.1, 'num_beams': 8, 'early_stopping': True}\n-   ```\n-\n-   `--info` is an additional argument available for the same purpose of tracking the conditions of the experiment. It's useful to pass things that weren't in the argument list, e.g. a language pair `--info \"lang:en-ru\"`. But also if you pass `--info` without a value it will fallback to the current date/time string, e.g. `2020-09-13 18:44:43`.\n-\n-   If using `--dump-args --info`, the output will be:\n-\n-   ```json\n-   {'bleu': 26.887, 'n_obs': 10, 'runtime': 1, 'seconds_per_sample': 0.1, 'num_beams': 8, 'early_stopping': True, 'info': '2020-09-13 18:44:43'}\n-   ```\n-\n-   If using `--dump-args --info \"pair:en-ru chkpt=best`, the output will be:\n-\n-   ```json\n-   {'bleu': 26.887, 'n_obs': 10, 'runtime': 1, 'seconds_per_sample': 0.1, 'num_beams': 8, 'early_stopping': True, 'info': 'pair=en-ru chkpt=best'}\n-   ```\n-\n-\n-* if you need to perform a parametric search in order to find the best ones that lead to the highest BLEU score, let `run_eval_search.py` to do the searching for you.\n-\n-   The script accepts the exact same arguments as `run_eval.py`, plus an additional argument `--search`. The value of `--search` is parsed, reformatted and fed to ``run_eval.py`` as additional args.\n-\n-   The format for the `--search` value is a simple string with hparams and colon separated values to try, e.g.:\n-   ```\n-    --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\n-   ```\n-   which will generate `12` `(2*3*2)` searches for a product of each hparam. For example the example that was just used will invoke `run_eval.py` repeatedly with:\n-\n-   ```\n-    --num_beams 5 --length_penalty 0.8 --early_stopping true\n-    --num_beams 5 --length_penalty 0.8 --early_stopping false\n-    [...]\n-    --num_beams 10 --length_penalty 1.2 --early_stopping false\n-   ```\n-\n-   On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\n-\n-```\n-bleu  | num_beams | length_penalty | early_stopping\n------ | --------- | -------------- | --------------\n-26.71 |         5 |            1.1 |              1\n-26.66 |         5 |            0.9 |              1\n-26.66 |         5 |            0.9 |              0\n-26.41 |         5 |            1.1 |              0\n-21.94 |         1 |            0.9 |              1\n-21.94 |         1 |            0.9 |              0\n-21.94 |         1 |            1.1 |              1\n-21.94 |         1 |            1.1 |              0\n-\n-Best score args:\n-stas/wmt19-en-ru data/en-ru/val.source data/en-ru/test_translations.txt --reference_path data/en-ru/val.target --score_path data/en-ru/test_bleu.json --bs 8 --task translation --num_beams 5 --length_penalty 1.1 --early_stopping True\n-```\n-\n-If you pass `--info \"some experiment-specific info\"` it will get printed before the results table - this is useful for scripting and multiple runs, so one can tell the different sets of results from each other.\n-\n-\n-### Contributing\n-- follow the standard contributing guidelines and code of conduct.\n-- add tests to `test_seq2seq_examples.py`\n-- To run only the seq2seq tests, you must be in the root of the repository and run:\n-```bash\n-pytest examples/seq2seq/\n-```\n-\n-### Converting pytorch-lightning checkpoints\n-pytorch lightning ``-do_predict`` often fails, after you are done training, the best way to evaluate your model is to convert it.\n-\n-This should be done for you, with a file called `{save_dir}/best_tfmr`.\n-\n-If that file doesn't exist but you have a lightning `.ckpt` file, you can run\n-```bash\n-python convert_pl_checkpoint_to_hf.py PATH_TO_CKPT  randomly_initialized_hf_model_path save_dir/best_tfmr\n-```\n-Then either `run_eval` or `run_distributed_eval` with `save_dir/best_tfmr` (see previous sections)\n-\n-\n-# Experimental Features\n-These features are harder to use and not always useful.\n-\n-###  Dynamic Batch Size for MT\n-`finetune.py` has a command line arg `--max_tokens_per_batch` that allows batches to be dynamically sized.\n-This feature can only be used:\n-- with fairseq installed\n-- on 1 GPU\n-- without sortish sampler\n-- after calling `./save_len_file.py $tok $data_dir`\n-\n-For example,\n-```bash\n-./save_len_file.py Helsinki-NLP/opus-mt-en-ro  wmt_en_ro\n-./dynamic_bs_example.sh --max_tokens_per_batch=2000 --output_dir benchmark_dynamic_bs\n-```\n-splits `wmt_en_ro/train` into 11,197 uneven length batches and can finish 1 epoch in 8 minutes on a v100.\n-\n-For comparison,\n-```bash\n-./dynamic_bs_example.sh --sortish_sampler --train_batch_size 48\n-```\n-uses 12,723 batches of length 48 and takes slightly more time 9.5 minutes.\n-\n-The feature is still experimental, because:\n-+ we can make it much more robust if we have memory mapped/preprocessed datasets.\n-+ The speedup over sortish sampler is not that large at the moment."
        },
        {
            "sha": "3cee09bb7f51087e92d778c4c9e27d76085d1b30",
            "filename": "examples/legacy/seq2seq/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2F__init__.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,5 +0,0 @@\n-import os\n-import sys\n-\n-\n-sys.path.insert(1, os.path.dirname(os.path.realpath(__file__)))"
        },
        {
            "sha": "8d568a7e4af0a8b15fea3580a1f580225cb9f639",
            "filename": "examples/legacy/seq2seq/convert_model_to_fp16.py",
            "status": "removed",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fconvert_model_to_fp16.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fconvert_model_to_fp16.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fconvert_model_to_fp16.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,36 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Union\n-\n-import fire\n-import torch\n-from tqdm import tqdm\n-\n-\n-def convert(src_path: str, map_location: str = \"cpu\", save_path: Union[str, None] = None) -> None:\n-    \"\"\"Convert a pytorch_model.bin or model.pt file to torch.float16 for faster downloads, less disk space.\"\"\"\n-    state_dict = torch.load(src_path, map_location=map_location, weights_only=True)\n-    for k, v in tqdm(state_dict.items()):\n-        if not isinstance(v, torch.Tensor):\n-            raise TypeError(\"FP16 conversion only works on paths that are saved state dicts, like pytorch_model.bin\")\n-        state_dict[k] = v.half()\n-    if save_path is None:  # overwrite src_path\n-        save_path = src_path\n-    torch.save(state_dict, save_path)\n-\n-\n-if __name__ == \"__main__\":\n-    fire.Fire(convert)"
        },
        {
            "sha": "2cff6f59920ab701c464fbb7372fc59e3c7519a3",
            "filename": "examples/legacy/seq2seq/download_wmt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fdownload_wmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fdownload_wmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fdownload_wmt.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,67 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from pathlib import Path\n-\n-import fire\n-from tqdm import tqdm\n-\n-\n-def download_wmt_dataset(src_lang=\"ro\", tgt_lang=\"en\", dataset=\"wmt16\", save_dir=None) -> None:\n-    \"\"\"Download a dataset using the datasets package and save it to the format expected by finetune.py\n-    Format of save_dir: train.source, train.target, val.source, val.target, test.source, test.target.\n-\n-    Args:\n-        src_lang: <str> source language\n-        tgt_lang: <str> target language\n-        dataset: <str> wmt16, wmt17, etc. wmt16 is a good start as it's small. To get the full list run `import datasets; print([d.id for d in datasets.list_datasets() if \"wmt\" in d.id])`\n-        save_dir: <str>, where to save the datasets, defaults to f'{dataset}-{src_lang}-{tgt_lang}'\n-\n-    Usage:\n-        >>> download_wmt_dataset('ro', 'en', dataset='wmt16') # saves to wmt16-ro-en\n-    \"\"\"\n-    try:\n-        import datasets\n-    except (ModuleNotFoundError, ImportError):\n-        raise ImportError(\"run pip install datasets\")\n-    pair = f\"{src_lang}-{tgt_lang}\"\n-    print(f\"Converting {dataset}-{pair}\")\n-    ds = datasets.load_dataset(dataset, pair)\n-    if save_dir is None:\n-        save_dir = f\"{dataset}-{pair}\"\n-    save_dir = Path(save_dir)\n-    save_dir.mkdir(exist_ok=True)\n-\n-    for split in ds:\n-        print(f\"Splitting {split} with {ds[split].num_rows} records\")\n-\n-        # to save to val.source, val.target like summary datasets\n-        fn = \"val\" if split == \"validation\" else split\n-        src_path = save_dir.joinpath(f\"{fn}.source\")\n-        tgt_path = save_dir.joinpath(f\"{fn}.target\")\n-        src_fp = src_path.open(\"w+\")\n-        tgt_fp = tgt_path.open(\"w+\")\n-\n-        # reader is the bottleneck so writing one record at a time doesn't slow things down\n-        for x in tqdm(ds[split]):\n-            ex = x[\"translation\"]\n-            src_fp.write(ex[src_lang] + \"\\n\")\n-            tgt_fp.write(ex[tgt_lang] + \"\\n\")\n-\n-    print(f\"Saved {dataset} dataset to {save_dir}\")\n-\n-\n-if __name__ == \"__main__\":\n-    fire.Fire(download_wmt_dataset)"
        },
        {
            "sha": "60023df7bad6aedba86987650c3d730aeb1ac229",
            "filename": "examples/legacy/seq2seq/finetune.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ffinetune.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ffinetune.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,24 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# the proper usage is documented in the README, you need to specify data_dir, output_dir and model_name_or_path\n-# run ./finetune.sh --help to see all the possible options\n-python finetune_trainer.py \\\n-    --learning_rate=3e-5 \\\n-    --fp16 \\\n-    --do_train --do_eval --do_predict \\\n-    --eval_strategy steps \\\n-    --predict_with_generate \\\n-    --n_val 1000 \\\n-    \"$@\""
        },
        {
            "sha": "d165322b643471a7ccdfc189d4171deec9e8104d",
            "filename": "examples/legacy/seq2seq/finetune_tpu.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ffinetune_tpu.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ffinetune_tpu.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_tpu.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,26 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-export TPU_NUM_CORES=8\n-\n-# the proper usage is documented in the README, you need to specify data_dir, output_dir and model_name_or_path\n-# run ./finetune_tpu.sh --help to see all the possible options\n-# To specify the number of cores to use, use the TPU_NUM_DEVICES environment variable\n-python xla_spawn.py finetune_trainer.py \\\n-    --learning_rate=3e-5 \\\n-    --do_train --do_eval \\\n-    --eval_strategy steps \\\n-    --prediction_loss_only \\\n-    --n_val 1000 \\\n-    \"$@\""
        },
        {
            "sha": "c88e765d0bd7a9d9698e55b4b2b3694c1b30e7f2",
            "filename": "examples/legacy/seq2seq/finetune_trainer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 370,
            "changes": 370,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,370 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-from seq2seq_trainer import Seq2SeqTrainer\n-from seq2seq_training_args import Seq2SeqTrainingArguments\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoModelForSeq2SeqLM,\n-    AutoTokenizer,\n-    HfArgumentParser,\n-    MBartTokenizer,\n-    MBartTokenizerFast,\n-    set_seed,\n-)\n-from transformers.trainer_utils import is_main_process\n-from transformers.training_args import ParallelMode\n-from utils import (\n-    Seq2SeqDataCollator,\n-    Seq2SeqDataset,\n-    assert_all_frozen,\n-    build_compute_metrics_fn,\n-    freeze_embeds,\n-    freeze_params,\n-    lmap,\n-    save_json,\n-    use_task_specific_params,\n-    write_txt_file,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    freeze_encoder: bool = field(default=False, metadata={\"help\": \"Whether tp freeze the encoder.\"})\n-    freeze_embeds: bool = field(default=False, metadata={\"help\": \"Whether  to freeze the embeddings.\"})\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    data_dir: str = field(\n-        metadata={\"help\": \"The input data dir. Should contain the .tsv files (or other data files) for the task.\"}\n-    )\n-    task: Optional[str] = field(\n-        default=\"summarization\",\n-        metadata={\"help\": \"Task name, summarization (or summarization_{dataset} for pegasus) or translation\"},\n-    )\n-    max_source_length: Optional[int] = field(\n-        default=1024,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_target_length: Optional[int] = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    val_max_target_length: Optional[int] = field(\n-        default=142,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded. \"\n-                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n-                \"during ``evaluate`` and ``predict``.\"\n-            )\n-        },\n-    )\n-    test_max_target_length: Optional[int] = field(\n-        default=142,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for test target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    n_train: Optional[int] = field(default=-1, metadata={\"help\": \"# training examples. -1 means use all.\"})\n-    n_val: Optional[int] = field(default=-1, metadata={\"help\": \"# validation examples. -1 means use all.\"})\n-    n_test: Optional[int] = field(default=-1, metadata={\"help\": \"# test examples. -1 means use all.\"})\n-    src_lang: Optional[str] = field(default=None, metadata={\"help\": \"Source language id for translation.\"})\n-    tgt_lang: Optional[str] = field(default=None, metadata={\"help\": \"Target language id for translation.\"})\n-    eval_beams: Optional[int] = field(default=None, metadata={\"help\": \"# num_beams to use for evaluation.\"})\n-    ignore_pad_token_for_loss: bool = field(\n-        default=True,\n-        metadata={\"help\": \"If only pad tokens should be ignored. This assumes that `config.pad_token_id` is defined.\"},\n-    )\n-\n-\n-def handle_metrics(split, metrics, output_dir):\n-    \"\"\"\n-    Log and save metrics\n-\n-    Args:\n-    - split: one of train, val, test\n-    - metrics: metrics dict\n-    - output_dir: where to save the metrics\n-    \"\"\"\n-\n-    logger.info(f\"***** {split} metrics *****\")\n-    for key in sorted(metrics.keys()):\n-        logger.info(f\"  {key} = {metrics[key]}\")\n-    save_json(metrics, os.path.join(output_dir, f\"{split}_results.json\"))\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n-\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_process_index,\n-        training_args.device,\n-        training_args.n_gpu,\n-        bool(training_args.parallel_mode == ParallelMode.DISTRIBUTED),\n-        training_args.fp16,\n-    )\n-    transformers.utils.logging.enable_default_handler()\n-    transformers.utils.logging.enable_explicit_format()\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_process_index):\n-        transformers.utils.logging.set_verbosity_info()\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Set seed\n-    set_seed(training_args.seed)\n-\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    extra_model_params = (\"encoder_layerdrop\", \"decoder_layerdrop\", \"dropout\", \"attention_dropout\")\n-    for p in extra_model_params:\n-        if getattr(training_args, p, None):\n-            assert hasattr(config, p), f\"({config.__class__.__name__}) doesn't have a `{p}` attribute\"\n-            setattr(config, p, getattr(training_args, p))\n-\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-    )\n-    model = AutoModelForSeq2SeqLM.from_pretrained(\n-        model_args.model_name_or_path,\n-        from_tf=\".ckpt\" in model_args.model_name_or_path,\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    # use task specific params\n-    use_task_specific_params(model, data_args.task)\n-\n-    # set num_beams for evaluation\n-    if data_args.eval_beams is None:\n-        data_args.eval_beams = model.config.num_beams\n-\n-    # set decoder_start_token_id for MBart\n-    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n-        assert data_args.tgt_lang is not None and data_args.src_lang is not None, (\n-            \"mBart requires --tgt_lang and --src_lang\"\n-        )\n-        if isinstance(tokenizer, MBartTokenizer):\n-            model.config.decoder_start_token_id = tokenizer.lang_code_to_id[data_args.tgt_lang]\n-        else:\n-            model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(data_args.tgt_lang)\n-\n-    if model_args.freeze_embeds:\n-        freeze_embeds(model)\n-    if model_args.freeze_encoder:\n-        freeze_params(model.get_encoder())\n-        assert_all_frozen(model.get_encoder())\n-\n-    dataset_class = Seq2SeqDataset\n-\n-    # Get datasets\n-    train_dataset = (\n-        dataset_class(\n-            tokenizer,\n-            type_path=\"train\",\n-            data_dir=data_args.data_dir,\n-            n_obs=data_args.n_train,\n-            max_target_length=data_args.max_target_length,\n-            max_source_length=data_args.max_source_length,\n-            prefix=model.config.prefix or \"\",\n-        )\n-        if training_args.do_train\n-        else None\n-    )\n-    eval_dataset = (\n-        dataset_class(\n-            tokenizer,\n-            type_path=\"val\",\n-            data_dir=data_args.data_dir,\n-            n_obs=data_args.n_val,\n-            max_target_length=data_args.val_max_target_length,\n-            max_source_length=data_args.max_source_length,\n-            prefix=model.config.prefix or \"\",\n-        )\n-        if training_args.do_eval\n-        else None\n-    )\n-    test_dataset = (\n-        dataset_class(\n-            tokenizer,\n-            type_path=\"test\",\n-            data_dir=data_args.data_dir,\n-            n_obs=data_args.n_test,\n-            max_target_length=data_args.test_max_target_length,\n-            max_source_length=data_args.max_source_length,\n-            prefix=model.config.prefix or \"\",\n-        )\n-        if training_args.do_predict\n-        else None\n-    )\n-\n-    # Initialize our Trainer\n-    compute_metrics_fn = (\n-        build_compute_metrics_fn(data_args.task, tokenizer) if training_args.predict_with_generate else None\n-    )\n-    trainer = Seq2SeqTrainer(\n-        model=model,\n-        args=training_args,\n-        data_args=data_args,\n-        train_dataset=train_dataset,\n-        eval_dataset=eval_dataset,\n-        data_collator=Seq2SeqDataCollator(tokenizer, data_args, model.config.decoder_start_token_id),\n-        compute_metrics=compute_metrics_fn,\n-        processing_class=tokenizer,\n-    )\n-\n-    all_metrics = {}\n-    # Training\n-    if training_args.do_train:\n-        logger.info(\"*** Train ***\")\n-\n-        train_result = trainer.train(\n-            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n-        )\n-        metrics = train_result.metrics\n-        metrics[\"train_n_objs\"] = data_args.n_train\n-\n-        trainer.save_model()  # this also saves the tokenizer\n-\n-        if trainer.is_world_process_zero():\n-            handle_metrics(\"train\", metrics, training_args.output_dir)\n-            all_metrics.update(metrics)\n-\n-            # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n-            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n-\n-            # For convenience, we also re-save the tokenizer to the same directory,\n-            # so that you can share your model easily on huggingface.co/models =)\n-            tokenizer.save_pretrained(training_args.output_dir)\n-\n-    # Evaluation\n-    if training_args.do_eval:\n-        logger.info(\"*** Evaluate ***\")\n-\n-        metrics = trainer.evaluate(metric_key_prefix=\"val\")\n-        metrics[\"val_n_objs\"] = data_args.n_val\n-        metrics[\"val_loss\"] = round(metrics[\"val_loss\"], 4)\n-\n-        if trainer.is_world_process_zero():\n-            handle_metrics(\"val\", metrics, training_args.output_dir)\n-            all_metrics.update(metrics)\n-\n-    if training_args.do_predict:\n-        logger.info(\"*** Predict ***\")\n-\n-        test_output = trainer.predict(test_dataset=test_dataset, metric_key_prefix=\"test\")\n-        metrics = test_output.metrics\n-        metrics[\"test_n_objs\"] = data_args.n_test\n-\n-        if trainer.is_world_process_zero():\n-            metrics[\"test_loss\"] = round(metrics[\"test_loss\"], 4)\n-            handle_metrics(\"test\", metrics, training_args.output_dir)\n-            all_metrics.update(metrics)\n-\n-            if training_args.predict_with_generate:\n-                test_preds = tokenizer.batch_decode(\n-                    test_output.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n-                )\n-                test_preds = lmap(str.strip, test_preds)\n-                write_txt_file(test_preds, os.path.join(training_args.output_dir, \"test_generations.txt\"))\n-\n-    if trainer.is_world_process_zero():\n-        save_json(all_metrics, os.path.join(training_args.output_dir, \"all_results.json\"))\n-\n-    return all_metrics\n-\n-\n-def _mp_fn(index):\n-    # For xla_spawn (TPUs)\n-    main()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "e6095cecc8e99f231b80a3779b594cc29fd0ddda",
            "filename": "examples/legacy/seq2seq/minify_dataset.py",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fminify_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fminify_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fminify_dataset.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,34 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from pathlib import Path\n-\n-import fire\n-\n-\n-def minify(src_dir: str, dest_dir: str, n: int):\n-    \"\"\"Write first n lines of each file f in src_dir to dest_dir/f\"\"\"\n-    src_dir = Path(src_dir)\n-    dest_dir = Path(dest_dir)\n-    dest_dir.mkdir(exist_ok=True)\n-    for path in src_dir.iterdir():\n-        new = [x.rstrip() for x in list(path.open().readlines())][:n]\n-        dest_path = dest_dir.joinpath(path.name)\n-        print(dest_path)\n-        dest_path.open(\"w\").write(\"\\n\".join(new))\n-\n-\n-if __name__ == \"__main__\":\n-    fire.Fire(minify)"
        },
        {
            "sha": "6cc15e02552be105a28ac93d177b09a7f9bc92a2",
            "filename": "examples/legacy/seq2seq/old_test_calculate_rouge.py",
            "status": "removed",
            "additions": 0,
            "deletions": 109,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_calculate_rouge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_calculate_rouge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_calculate_rouge.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,109 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from collections import defaultdict\n-from pathlib import Path\n-\n-import pandas as pd\n-from rouge_cli import calculate_rouge_path\n-\n-from utils import calculate_rouge\n-\n-\n-PRED = [\n-    'Prosecutor: \"No videos were used in the crash investigation\" German papers say they saw a cell phone video of the'\n-    ' final seconds on board Flight 9525. The Germanwings co-pilot says he had a \"previous episode of severe'\n-    \" depression\\\" German airline confirms it knew of Andreas Lubitz's depression years before he took control.\",\n-    \"The Palestinian Authority officially becomes the 123rd member of the International Criminal Court. The formal\"\n-    \" accession was marked with a ceremony at The Hague, in the Netherlands. The Palestinians signed the ICC's\"\n-    \" founding Rome Statute in January. Israel and the United States opposed the Palestinians' efforts to join the\"\n-    \" body.\",\n-    \"Amnesty International releases its annual report on the death penalty. The report catalogs the use of\"\n-    \" state-sanctioned killing as a punitive measure across the globe. At least 607 people were executed around the\"\n-    \" world in 2014, compared to 778 in 2013. The U.S. remains one of the worst offenders for imposing capital\"\n-    \" punishment.\",\n-]\n-\n-TGT = [\n-    'Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports .'\n-    ' Journalists at Bild and Paris Match are \"very confident\" the video clip is real, an editor says . Andreas Lubitz'\n-    \" had informed his Lufthansa training school of an episode of severe depression, airline says .\",\n-    \"Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\"\n-    \" Israel and the United States opposed the move, which could open the door to war crimes investigations against\"\n-    \" Israelis .\",\n-    \"Amnesty's annual death penalty report catalogs encouraging signs, but setbacks in numbers of those sentenced to\"\n-    \" death . Organization claims that governments around the world are using the threat of terrorism to advance\"\n-    \" executions . The number of executions worldwide has gone down by almost 22% compared with 2013, but death\"\n-    \" sentences up by 28% .\",\n-]\n-\n-\n-def test_disaggregated_scores_are_determinstic():\n-    no_aggregation = calculate_rouge(PRED, TGT, bootstrap_aggregation=False, rouge_keys=[\"rouge2\", \"rougeL\"])\n-    assert isinstance(no_aggregation, defaultdict)\n-    no_aggregation_just_r2 = calculate_rouge(PRED, TGT, bootstrap_aggregation=False, rouge_keys=[\"rouge2\"])\n-    assert (\n-        pd.DataFrame(no_aggregation[\"rouge2\"]).fmeasure.mean()\n-        == pd.DataFrame(no_aggregation_just_r2[\"rouge2\"]).fmeasure.mean()\n-    )\n-\n-\n-def test_newline_cnn_improvement():\n-    k = \"rougeLsum\"\n-    score = calculate_rouge(PRED, TGT, newline_sep=True, rouge_keys=[k])[k]\n-    score_no_sep = calculate_rouge(PRED, TGT, newline_sep=False, rouge_keys=[k])[k]\n-    assert score > score_no_sep\n-\n-\n-def test_newline_irrelevant_for_other_metrics():\n-    k = [\"rouge1\", \"rouge2\", \"rougeL\"]\n-    score_sep = calculate_rouge(PRED, TGT, newline_sep=True, rouge_keys=k)\n-    score_no_sep = calculate_rouge(PRED, TGT, newline_sep=False, rouge_keys=k)\n-    assert score_sep == score_no_sep\n-\n-\n-def test_single_sent_scores_dont_depend_on_newline_sep():\n-    pred = [\n-        \"Her older sister, Margot Frank, died in 1945, a month earlier than previously thought.\",\n-        'Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports .',\n-    ]\n-    tgt = [\n-        \"Margot Frank, died in 1945, a month earlier than previously thought.\",\n-        'Prosecutor: \"No videos were used in the crash investigation\" German papers say they saw a cell phone video of'\n-        \" the final seconds on board Flight 9525.\",\n-    ]\n-    assert calculate_rouge(pred, tgt, newline_sep=True) == calculate_rouge(pred, tgt, newline_sep=False)\n-\n-\n-def test_pegasus_newline():\n-    pred = [\n-        \"\"\"\" \"a person who has such a video needs to immediately give it to the investigators,\" prosecutor says .<n> \"it is a very disturbing scene,\" editor-in-chief of bild online tells \"erin burnett: outfront\" \"\"\"\n-    ]\n-    tgt = [\n-        \"\"\" Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports . Journalists at Bild and Paris Match are \"very confident\" the video clip is real, an editor says . Andreas Lubitz had informed his Lufthansa training school of an episode of severe depression, airline says .\"\"\"\n-    ]\n-\n-    prev_score = calculate_rouge(pred, tgt, rouge_keys=[\"rougeLsum\"], newline_sep=False)[\"rougeLsum\"]\n-    new_score = calculate_rouge(pred, tgt, rouge_keys=[\"rougeLsum\"])[\"rougeLsum\"]\n-    assert new_score > prev_score\n-\n-\n-def test_rouge_cli():\n-    data_dir = Path(\"examples/seq2seq/test_data/wmt_en_ro\")\n-    metrics = calculate_rouge_path(data_dir.joinpath(\"test.source\"), data_dir.joinpath(\"test.target\"))\n-    assert isinstance(metrics, dict)\n-    metrics_default_dict = calculate_rouge_path(\n-        data_dir.joinpath(\"test.source\"), data_dir.joinpath(\"test.target\"), bootstrap_aggregation=False\n-    )\n-    assert isinstance(metrics_default_dict, defaultdict)"
        },
        {
            "sha": "4c14d2644f0a0e7dbc971a7f72edd739c7fb902e",
            "filename": "examples/legacy/seq2seq/old_test_datasets.py",
            "status": "removed",
            "additions": 0,
            "deletions": 247,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_datasets.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_datasets.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_datasets.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,247 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import os\n-from pathlib import Path\n-\n-import numpy as np\n-import pytest\n-from pack_dataset import pack_data_dir\n-from parameterized import parameterized\n-from save_len_file import save_len_file\n-from torch.utils.data import DataLoader\n-\n-from transformers import AutoTokenizer\n-from transformers.models.mbart.modeling_mbart import shift_tokens_right\n-from transformers.testing_utils import TestCasePlus, slow\n-from utils import FAIRSEQ_AVAILABLE, DistributedSortishSampler, LegacySeq2SeqDataset, Seq2SeqDataset\n-\n-\n-BERT_BASE_CASED = \"google-bert/bert-base-cased\"\n-PEGASUS_XSUM = \"google/pegasus-xsum\"\n-ARTICLES = [\" Sam ate lunch today.\", \"Sams lunch ingredients.\"]\n-SUMMARIES = [\"A very interesting story about what I ate for lunch.\", \"Avocado, celery, turkey, coffee\"]\n-T5_TINY = \"patrickvonplaten/t5-tiny-random\"\n-BART_TINY = \"sshleifer/bart-tiny-random\"\n-MBART_TINY = \"sshleifer/tiny-mbart\"\n-MARIAN_TINY = \"sshleifer/tiny-marian-en-de\"\n-\n-\n-def _dump_articles(path: Path, articles: list):\n-    content = \"\\n\".join(articles)\n-    Path(path).open(\"w\").writelines(content)\n-\n-\n-def make_test_data_dir(tmp_dir):\n-    for split in [\"train\", \"val\", \"test\"]:\n-        _dump_articles(os.path.join(tmp_dir, f\"{split}.source\"), ARTICLES)\n-        _dump_articles(os.path.join(tmp_dir, f\"{split}.target\"), SUMMARIES)\n-    return tmp_dir\n-\n-\n-class TestAll(TestCasePlus):\n-    @parameterized.expand(\n-        [\n-            MBART_TINY,\n-            MARIAN_TINY,\n-            T5_TINY,\n-            BART_TINY,\n-            PEGASUS_XSUM,\n-        ],\n-    )\n-    @slow\n-    def test_seq2seq_dataset_truncation(self, tok_name):\n-        tokenizer = AutoTokenizer.from_pretrained(tok_name)\n-        tmp_dir = make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir())\n-        max_len_source = max(len(tokenizer.encode(a)) for a in ARTICLES)\n-        max_len_target = max(len(tokenizer.encode(a)) for a in SUMMARIES)\n-        max_src_len = 4\n-        max_tgt_len = 8\n-        assert max_len_target > max_src_len  # Will be truncated\n-        assert max_len_source > max_src_len  # Will be truncated\n-        src_lang, tgt_lang = \"ro_RO\", \"de_DE\"  # ignored for all but mbart, but never causes error.\n-        train_dataset = Seq2SeqDataset(\n-            tokenizer,\n-            data_dir=tmp_dir,\n-            type_path=\"train\",\n-            max_source_length=max_src_len,\n-            max_target_length=max_tgt_len,  # ignored\n-            src_lang=src_lang,\n-            tgt_lang=tgt_lang,\n-        )\n-        dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=train_dataset.collate_fn)\n-        for batch in dataloader:\n-            assert isinstance(batch, dict)\n-            assert batch[\"attention_mask\"].shape == batch[\"input_ids\"].shape\n-            # show that articles were trimmed.\n-            assert batch[\"input_ids\"].shape[1] == max_src_len\n-            # show that targets are the same len\n-            assert batch[\"labels\"].shape[1] == max_tgt_len\n-            if tok_name != MBART_TINY:\n-                continue\n-            # check language codes in correct place\n-            batch[\"decoder_input_ids\"] = shift_tokens_right(batch[\"labels\"], tokenizer.pad_token_id)\n-            assert batch[\"decoder_input_ids\"][0, 0].item() == tokenizer.lang_code_to_id[tgt_lang]\n-            assert batch[\"decoder_input_ids\"][0, -1].item() == tokenizer.eos_token_id\n-            assert batch[\"input_ids\"][0, -2].item() == tokenizer.eos_token_id\n-            assert batch[\"input_ids\"][0, -1].item() == tokenizer.lang_code_to_id[src_lang]\n-\n-            break  # No need to test every batch\n-\n-    @parameterized.expand([BART_TINY, BERT_BASE_CASED])\n-    def test_legacy_dataset_truncation(self, tok):\n-        tokenizer = AutoTokenizer.from_pretrained(tok)\n-        tmp_dir = make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir())\n-        max_len_source = max(len(tokenizer.encode(a)) for a in ARTICLES)\n-        max_len_target = max(len(tokenizer.encode(a)) for a in SUMMARIES)\n-        trunc_target = 4\n-        train_dataset = LegacySeq2SeqDataset(\n-            tokenizer,\n-            data_dir=tmp_dir,\n-            type_path=\"train\",\n-            max_source_length=20,\n-            max_target_length=trunc_target,\n-        )\n-        dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=train_dataset.collate_fn)\n-        for batch in dataloader:\n-            assert batch[\"attention_mask\"].shape == batch[\"input_ids\"].shape\n-            # show that articles were trimmed.\n-            assert batch[\"input_ids\"].shape[1] == max_len_source\n-            assert 20 >= batch[\"input_ids\"].shape[1]  # trimmed significantly\n-            # show that targets were truncated\n-            assert batch[\"labels\"].shape[1] == trunc_target  # Truncated\n-            assert max_len_target > trunc_target  # Truncated\n-            break  # No need to test every batch\n-\n-    def test_pack_dataset(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n-\n-        tmp_dir = Path(make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()))\n-        orig_examples = tmp_dir.joinpath(\"train.source\").open().readlines()\n-        save_dir = Path(make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()))\n-        pack_data_dir(tokenizer, tmp_dir, 128, save_dir)\n-        orig_paths = {x.name for x in tmp_dir.iterdir()}\n-        new_paths = {x.name for x in save_dir.iterdir()}\n-        packed_examples = save_dir.joinpath(\"train.source\").open().readlines()\n-        # orig: [' Sam ate lunch today.\\n', 'Sams lunch ingredients.']\n-        # desired_packed: [' Sam ate lunch today.\\n Sams lunch ingredients.']\n-        assert len(packed_examples) < len(orig_examples)\n-        assert len(packed_examples) == 1\n-        assert len(packed_examples[0]) == sum(len(x) for x in orig_examples)\n-        assert orig_paths == new_paths\n-\n-    @pytest.mark.skipif(not FAIRSEQ_AVAILABLE, reason=\"This test requires fairseq\")\n-    def test_dynamic_batch_size(self):\n-        if not FAIRSEQ_AVAILABLE:\n-            return\n-        ds, max_tokens, tokenizer = self._get_dataset(max_len=64)\n-        required_batch_size_multiple = 64\n-        batch_sampler = ds.make_dynamic_sampler(max_tokens, required_batch_size_multiple=required_batch_size_multiple)\n-        batch_sizes = [len(x) for x in batch_sampler]\n-        assert len(set(batch_sizes)) > 1  # it's not dynamic batch size if every batch is the same length\n-        assert sum(batch_sizes) == len(ds)  # no dropped or added examples\n-        data_loader = DataLoader(ds, batch_sampler=batch_sampler, collate_fn=ds.collate_fn, num_workers=2)\n-        failures = []\n-        num_src_per_batch = []\n-        for batch in data_loader:\n-            src_shape = batch[\"input_ids\"].shape\n-            bs = src_shape[0]\n-            assert bs % required_batch_size_multiple == 0 or bs < required_batch_size_multiple\n-            num_src_tokens = np.product(batch[\"input_ids\"].shape)\n-            num_src_per_batch.append(num_src_tokens)\n-            if num_src_tokens > (max_tokens * 1.1):\n-                failures.append(num_src_tokens)\n-        assert num_src_per_batch[0] == max(num_src_per_batch)\n-        if failures:\n-            raise AssertionError(f\"too many tokens in {len(failures)} batches\")\n-\n-    def test_sortish_sampler_reduces_padding(self):\n-        ds, _, tokenizer = self._get_dataset(max_len=512)\n-        bs = 2\n-        sortish_sampler = ds.make_sortish_sampler(bs, shuffle=False)\n-\n-        naive_dl = DataLoader(ds, batch_size=bs, collate_fn=ds.collate_fn, num_workers=2)\n-        sortish_dl = DataLoader(ds, batch_size=bs, collate_fn=ds.collate_fn, num_workers=2, sampler=sortish_sampler)\n-\n-        pad = tokenizer.pad_token_id\n-\n-        def count_pad_tokens(data_loader, k=\"input_ids\"):\n-            return [batch[k].eq(pad).sum().item() for batch in data_loader]\n-\n-        assert sum(count_pad_tokens(sortish_dl, k=\"labels\")) < sum(count_pad_tokens(naive_dl, k=\"labels\"))\n-        assert sum(count_pad_tokens(sortish_dl)) < sum(count_pad_tokens(naive_dl))\n-        assert len(sortish_dl) == len(naive_dl)\n-\n-    def _get_dataset(self, n_obs=1000, max_len=128):\n-        if os.getenv(\"USE_REAL_DATA\", None):\n-            data_dir = \"examples/seq2seq/wmt_en_ro\"\n-            max_tokens = max_len * 2 * 64\n-            if not Path(data_dir).joinpath(\"train.len\").exists():\n-                save_len_file(MARIAN_TINY, data_dir)\n-        else:\n-            data_dir = \"examples/seq2seq/test_data/wmt_en_ro\"\n-            max_tokens = max_len * 4\n-            save_len_file(MARIAN_TINY, data_dir)\n-\n-        tokenizer = AutoTokenizer.from_pretrained(MARIAN_TINY)\n-        ds = Seq2SeqDataset(\n-            tokenizer,\n-            data_dir=data_dir,\n-            type_path=\"train\",\n-            max_source_length=max_len,\n-            max_target_length=max_len,\n-            n_obs=n_obs,\n-        )\n-        return ds, max_tokens, tokenizer\n-\n-    def test_distributed_sortish_sampler_splits_indices_between_procs(self):\n-        ds, max_tokens, tokenizer = self._get_dataset()\n-        ids1 = set(DistributedSortishSampler(ds, 256, num_replicas=2, rank=0, add_extra_examples=False))\n-        ids2 = set(DistributedSortishSampler(ds, 256, num_replicas=2, rank=1, add_extra_examples=False))\n-        assert ids1.intersection(ids2) == set()\n-\n-    @parameterized.expand(\n-        [\n-            MBART_TINY,\n-            MARIAN_TINY,\n-            T5_TINY,\n-            BART_TINY,\n-            PEGASUS_XSUM,\n-        ],\n-    )\n-    def test_dataset_kwargs(self, tok_name):\n-        tokenizer = AutoTokenizer.from_pretrained(tok_name, use_fast=False)\n-        if tok_name == MBART_TINY:\n-            train_dataset = Seq2SeqDataset(\n-                tokenizer,\n-                data_dir=make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()),\n-                type_path=\"train\",\n-                max_source_length=4,\n-                max_target_length=8,\n-                src_lang=\"EN\",\n-                tgt_lang=\"FR\",\n-            )\n-            kwargs = train_dataset.dataset_kwargs\n-            assert \"src_lang\" in kwargs and \"tgt_lang\" in kwargs\n-        else:\n-            train_dataset = Seq2SeqDataset(\n-                tokenizer,\n-                data_dir=make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()),\n-                type_path=\"train\",\n-                max_source_length=4,\n-                max_target_length=8,\n-            )\n-            kwargs = train_dataset.dataset_kwargs\n-            assert \"add_prefix_space\" not in kwargs if tok_name != BART_TINY else \"add_prefix_space\" in kwargs\n-            assert len(kwargs) == 1 if tok_name == BART_TINY else len(kwargs) == 0"
        },
        {
            "sha": "93bd9839920d3b225b3a4806e416d4199387433d",
            "filename": "examples/legacy/seq2seq/old_test_fsmt_bleu_score.py",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_fsmt_bleu_score.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_fsmt_bleu_score.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_fsmt_bleu_score.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,70 +0,0 @@\n-# Copyright 2020 Huggingface\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import json\n-import unittest\n-\n-from parameterized import parameterized\n-\n-from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n-from transformers.testing_utils import get_tests_dir, require_torch, slow, torch_device\n-from utils import calculate_bleu\n-\n-\n-filename = get_tests_dir() + \"/test_data/fsmt/fsmt_val_data.json\"\n-with open(filename, encoding=\"utf-8\") as f:\n-    bleu_data = json.load(f)\n-\n-\n-@require_torch\n-class ModelEvalTester(unittest.TestCase):\n-    def get_tokenizer(self, mname):\n-        return FSMTTokenizer.from_pretrained(mname)\n-\n-    def get_model(self, mname):\n-        model = FSMTForConditionalGeneration.from_pretrained(mname).to(torch_device)\n-        if torch_device == \"cuda\":\n-            model.half()\n-        return model\n-\n-    @parameterized.expand(\n-        [\n-            [\"en-ru\", 26.0],\n-            [\"ru-en\", 22.0],\n-            [\"en-de\", 22.0],\n-            [\"de-en\", 29.0],\n-        ]\n-    )\n-    @slow\n-    def test_bleu_scores(self, pair, min_bleu_score):\n-        # note: this test is not testing the best performance since it only evals a small batch\n-        # but it should be enough to detect a regression in the output quality\n-        mname = f\"facebook/wmt19-{pair}\"\n-        tokenizer = self.get_tokenizer(mname)\n-        model = self.get_model(mname)\n-\n-        src_sentences = bleu_data[pair][\"src\"]\n-        tgt_sentences = bleu_data[pair][\"tgt\"]\n-\n-        batch = tokenizer(src_sentences, return_tensors=\"pt\", truncation=True, padding=\"longest\").to(torch_device)\n-        outputs = model.generate(\n-            input_ids=batch.input_ids,\n-            num_beams=8,\n-        )\n-        decoded_sentences = tokenizer.batch_decode(\n-            outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n-        )\n-        scores = calculate_bleu(decoded_sentences, tgt_sentences)\n-        print(scores)\n-        self.assertGreaterEqual(scores[\"bleu\"], min_bleu_score)"
        },
        {
            "sha": "864b97c7466a36a27eec3bea2e9aa28e9695f21f",
            "filename": "examples/legacy/seq2seq/old_test_seq2seq_examples.py",
            "status": "removed",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_seq2seq_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_seq2seq_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_seq2seq_examples.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,132 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-import os\n-import sys\n-from pathlib import Path\n-from unittest.mock import patch\n-\n-from parameterized import parameterized\n-from run_eval import run_generate\n-from run_eval_search import run_search\n-\n-from transformers.testing_utils import CaptureStdout, TestCasePlus, slow\n-from utils import ROUGE_KEYS\n-\n-\n-logging.basicConfig(level=logging.DEBUG)\n-logger = logging.getLogger()\n-\n-\n-def _dump_articles(path: Path, articles: list):\n-    content = \"\\n\".join(articles)\n-    Path(path).open(\"w\").writelines(content)\n-\n-\n-T5_TINY = \"patrickvonplaten/t5-tiny-random\"\n-BART_TINY = \"sshleifer/bart-tiny-random\"\n-MBART_TINY = \"sshleifer/tiny-mbart\"\n-\n-stream_handler = logging.StreamHandler(sys.stdout)\n-logger.addHandler(stream_handler)\n-logging.disable(logging.CRITICAL)  # remove noisy download output from tracebacks\n-\n-\n-class TestTheRest(TestCasePlus):\n-    def run_eval_tester(self, model):\n-        input_file_name = Path(self.get_auto_remove_tmp_dir()) / \"utest_input.source\"\n-        output_file_name = input_file_name.parent / \"utest_output.txt\"\n-        assert not output_file_name.exists()\n-        articles = [\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County.\"]\n-        _dump_articles(input_file_name, articles)\n-\n-        score_path = str(Path(self.get_auto_remove_tmp_dir()) / \"scores.json\")\n-        task = \"translation_en_to_de\" if model == T5_TINY else \"summarization\"\n-        testargs = f\"\"\"\n-            run_eval_search.py\n-            {model}\n-            {input_file_name}\n-            {output_file_name}\n-            --score_path {score_path}\n-            --task {task}\n-            --num_beams 2\n-            --length_penalty 2.0\n-            \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_generate()\n-            assert Path(output_file_name).exists()\n-            # os.remove(Path(output_file_name))\n-\n-    # test one model to quickly (no-@slow) catch simple problems and do an\n-    # extensive testing of functionality with multiple models as @slow separately\n-    def test_run_eval(self):\n-        self.run_eval_tester(T5_TINY)\n-\n-    # any extra models should go into the list here - can be slow\n-    @parameterized.expand([BART_TINY, MBART_TINY])\n-    @slow\n-    def test_run_eval_slow(self, model):\n-        self.run_eval_tester(model)\n-\n-    # testing with 2 models to validate: 1. translation (t5) 2. summarization (mbart)\n-    @parameterized.expand([T5_TINY, MBART_TINY])\n-    @slow\n-    def test_run_eval_search(self, model):\n-        input_file_name = Path(self.get_auto_remove_tmp_dir()) / \"utest_input.source\"\n-        output_file_name = input_file_name.parent / \"utest_output.txt\"\n-        assert not output_file_name.exists()\n-\n-        text = {\n-            \"en\": [\"Machine learning is great, isn't it?\", \"I like to eat bananas\", \"Tomorrow is another great day!\"],\n-            \"de\": [\n-                \"Maschinelles Lernen ist groÃŸartig, oder?\",\n-                \"Ich esse gerne Bananen\",\n-                \"Morgen ist wieder ein toller Tag!\",\n-            ],\n-        }\n-\n-        tmp_dir = Path(self.get_auto_remove_tmp_dir())\n-        score_path = str(tmp_dir / \"scores.json\")\n-        reference_path = str(tmp_dir / \"val.target\")\n-        _dump_articles(input_file_name, text[\"en\"])\n-        _dump_articles(reference_path, text[\"de\"])\n-        task = \"translation_en_to_de\" if model == T5_TINY else \"summarization\"\n-        testargs = f\"\"\"\n-            run_eval_search.py\n-            {model}\n-            {str(input_file_name)}\n-            {str(output_file_name)}\n-            --score_path {score_path}\n-            --reference_path {reference_path}\n-            --task {task}\n-            \"\"\".split()\n-        testargs.extend([\"--search\", \"num_beams=1:2 length_penalty=0.9:1.0\"])\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            with CaptureStdout() as cs:\n-                run_search()\n-            expected_strings = [\" num_beams | length_penalty\", model, \"Best score args\"]\n-            un_expected_strings = [\"Info\"]\n-            if \"translation\" in task:\n-                expected_strings.append(\"bleu\")\n-            else:\n-                expected_strings.extend(ROUGE_KEYS)\n-            for w in expected_strings:\n-                assert w in cs.out\n-            for w in un_expected_strings:\n-                assert w not in cs.out\n-            assert Path(output_file_name).exists()\n-            os.remove(Path(output_file_name))"
        },
        {
            "sha": "6625f061b5660793a5a054acd4eab518622bf5f8",
            "filename": "examples/legacy/seq2seq/old_test_seq2seq_examples_multi_gpu.py",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_seq2seq_examples_multi_gpu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_seq2seq_examples_multi_gpu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_seq2seq_examples_multi_gpu.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,55 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# as due to their complexity multi-gpu tests could impact other tests, and to aid debug we have those in a separate module.\n-\n-import os\n-import sys\n-\n-from transformers.testing_utils import TestCasePlus, execute_subprocess_async, get_gpu_count, require_torch_gpu, slow\n-\n-from .utils import load_json\n-\n-\n-class TestSummarizationDistillerMultiGPU(TestCasePlus):\n-    @classmethod\n-    def setUpClass(cls):\n-        return cls\n-\n-    @slow\n-    @require_torch_gpu\n-    def test_distributed_eval(self):\n-        output_dir = self.get_auto_remove_tmp_dir()\n-        args = f\"\"\"\n-            --model_name Helsinki-NLP/opus-mt-en-ro\n-            --save_dir {output_dir}\n-            --data_dir {self.test_file_dir_str}/test_data/wmt_en_ro\n-            --num_beams 2\n-            --task translation\n-        \"\"\".split()\n-\n-        # we want this test to run even if there is only one GPU, but if there are more we use them all\n-        n_gpu = get_gpu_count()\n-        distributed_args = f\"\"\"\n-            -m torch.distributed.launch\n-            --nproc_per_node={n_gpu}\n-            {self.test_file_dir}/run_distributed_eval.py\n-        \"\"\".split()\n-        cmd = [sys.executable] + distributed_args + args\n-        execute_subprocess_async(cmd, env=self.get_env())\n-\n-        metrics_save_path = os.path.join(output_dir, \"test_bleu.json\")\n-        metrics = load_json(metrics_save_path)\n-        # print(metrics)\n-        self.assertGreaterEqual(metrics[\"bleu\"], 25)"
        },
        {
            "sha": "b9733daf85e186b72eeb24d2b07347f10cea2586",
            "filename": "examples/legacy/seq2seq/old_test_tatoeba_conversion.py",
            "status": "removed",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_tatoeba_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fold_test_tatoeba_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_tatoeba_conversion.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,38 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import os\n-import tempfile\n-import unittest\n-\n-from transformers.models.marian.convert_marian_tatoeba_to_pytorch import DEFAULT_REPO, TatoebaConverter\n-from transformers.testing_utils import slow\n-from transformers.utils import cached_property\n-\n-\n-@unittest.skipUnless(os.path.exists(DEFAULT_REPO), \"Tatoeba directory does not exist.\")\n-class TatoebaConversionTester(unittest.TestCase):\n-    @cached_property\n-    def resolver(self):\n-        tmp_dir = tempfile.mkdtemp()\n-        return TatoebaConverter(save_dir=tmp_dir)\n-\n-    @slow\n-    def test_resolver(self):\n-        self.resolver.convert_models([\"heb-eng\"])\n-\n-    @slow\n-    def test_model_card(self):\n-        content, mmeta = self.resolver.write_model_card(\"opus-mt-he-en\", dry_run=True)\n-        assert mmeta[\"long_pair\"] == \"heb-eng\""
        },
        {
            "sha": "d85e31a7c2c3dccea914be0615d88df4aadcd239",
            "filename": "examples/legacy/seq2seq/pack_dataset.py",
            "status": "removed",
            "additions": 0,
            "deletions": 87,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,87 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fill examples with bitext up to max_tokens without breaking up examples.\n-[['I went', 'yo fui'],\n-['to the store', 'a la tienda']\n-]\n-=> ['I went to the store', 'yo fui a la tienda']\n-\"\"\"\n-\n-import argparse\n-import shutil\n-from pathlib import Path\n-\n-from tqdm import tqdm\n-\n-from transformers import AutoTokenizer\n-\n-\n-def pack_examples(tok, src_examples, tgt_examples, max_tokens=1024):\n-    finished_src, finished_tgt = [], []\n-\n-    sorted_examples = list(zip(src_examples, tgt_examples))\n-    new_src, new_tgt = sorted_examples[0]\n-\n-    def is_too_big(strang):\n-        return tok(strang, return_tensors=\"pt\").input_ids.shape[1] > max_tokens\n-\n-    for src, tgt in tqdm(sorted_examples[1:]):\n-        cand_src = new_src + \" \" + src\n-        cand_tgt = new_tgt + \" \" + tgt\n-        if is_too_big(cand_src) or is_too_big(cand_tgt):  # can't fit, finalize example\n-            finished_src.append(new_src)\n-            finished_tgt.append(new_tgt)\n-            new_src, new_tgt = src, tgt\n-        else:  # can fit, keep adding\n-            new_src, new_tgt = cand_src, cand_tgt\n-\n-    # cleanup\n-    if new_src:\n-        assert new_tgt\n-        finished_src.append(new_src)\n-        finished_tgt.append(new_tgt)\n-    return finished_src, finished_tgt\n-\n-\n-def pack_data_dir(tok, data_dir: Path, max_tokens, save_path):\n-    save_path = Path(save_path)\n-    save_path.mkdir(exist_ok=True)\n-    for split in [\"train\"]:\n-        src_path, tgt_path = data_dir / f\"{split}.source\", data_dir / f\"{split}.target\"\n-        src_docs = [x.rstrip() for x in Path(src_path).open()]\n-        tgt_docs = [x.rstrip() for x in Path(tgt_path).open()]\n-        packed_src, packed_tgt = pack_examples(tok, src_docs, tgt_docs, max_tokens)\n-        print(f\"packed {split} split from {len(src_docs)} examples -> {len(packed_src)}.\")\n-        Path(save_path / f\"{split}.source\").open(\"w\").write(\"\\n\".join(packed_src))\n-        Path(save_path / f\"{split}.target\").open(\"w\").write(\"\\n\".join(packed_tgt))\n-    for split in [\"val\", \"test\"]:\n-        src_path, tgt_path = data_dir / f\"{split}.source\", data_dir / f\"{split}.target\"\n-        shutil.copyfile(src_path, save_path / f\"{split}.source\")\n-        shutil.copyfile(tgt_path, save_path / f\"{split}.target\")\n-\n-\n-def packer_cli():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--tok_name\", type=str, help=\"like facebook/bart-large-cnn,google-t5/t5-base, etc.\")\n-    parser.add_argument(\"--max_seq_len\", type=int, default=128)\n-    parser.add_argument(\"--data_dir\", type=str)\n-    parser.add_argument(\"--save_path\", type=str)\n-    args = parser.parse_args()\n-    tokenizer = AutoTokenizer.from_pretrained(args.tok_name)\n-    return pack_data_dir(tokenizer, Path(args.data_dir), args.max_seq_len, args.save_path)\n-\n-\n-if __name__ == \"__main__\":\n-    packer_cli()"
        },
        {
            "sha": "434f647adea299b8dea378171fa687d38d78ead6",
            "filename": "examples/legacy/seq2seq/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frequirements.txt?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,20 +0,0 @@\n-tensorboard\n-scikit-learn\n-seqeval\n-psutil\n-sacrebleu\n-rouge-score\n-tensorflow_datasets\n-matplotlib\n-git-python==1.0.3\n-faiss-cpu\n-streamlit\n-elasticsearch\n-nltk\n-pandas\n-datasets >= 1.1.3\n-fire\n-pytest<8.0.1\n-conllu\n-sentencepiece != 0.1.92\n-protobuf"
        },
        {
            "sha": "938f0d1d7227f5687ec45f35f8dcff659172dfe2",
            "filename": "examples/legacy/seq2seq/romanian_postprocessing.md",
            "status": "removed",
            "additions": 0,
            "deletions": 65,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fromanian_postprocessing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fromanian_postprocessing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fromanian_postprocessing.md?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,65 +0,0 @@\n-### Motivation\n-Without processing, english-> romanian mbart-large-en-ro gets BLEU score 26.8 on the WMT data.\n-With post processing, it can score 37..\n-Here is the postprocessing code, stolen from @mjpost in this [issue](https://github.com/pytorch/fairseq/issues/1758)\n-\n-\n-\n-### Instructions\n-Note: You need to have your test_generations.txt before you start this process.\n-(1) Setup `mosesdecoder` and `wmt16-scripts`\n-```bash\n-cd $HOME\n-git clone git@github.com:moses-smt/mosesdecoder.git\n-cd mosesdecoder  \n-git clone git@github.com:rsennrich/wmt16-scripts.git\n-```\n-\n-(2) define a function for post processing.\n- It removes diacritics and does other things I don't understand \n-```bash\n-ro_post_process () {\n-  sys=$1\n-  ref=$2\n-  export MOSES_PATH=$HOME/mosesdecoder\n-  REPLACE_UNICODE_PUNCT=$MOSES_PATH/scripts/tokenizer/replace-unicode-punctuation.perl\n-  NORM_PUNC=$MOSES_PATH/scripts/tokenizer/normalize-punctuation.perl\n-  REM_NON_PRINT_CHAR=$MOSES_PATH/scripts/tokenizer/remove-non-printing-char.perl\n-  REMOVE_DIACRITICS=$MOSES_PATH/wmt16-scripts/preprocess/remove-diacritics.py\n-  NORMALIZE_ROMANIAN=$MOSES_PATH/wmt16-scripts/preprocess/normalise-romanian.py\n-  TOKENIZER=$MOSES_PATH/scripts/tokenizer/tokenizer.perl\n-\n-\n-\n-  lang=ro\n-  for file in $sys $ref; do\n-    cat $file \\\n-    | $REPLACE_UNICODE_PUNCT \\\n-    | $NORM_PUNC -l $lang \\\n-    | $REM_NON_PRINT_CHAR \\\n-    | $NORMALIZE_ROMANIAN \\\n-    | $REMOVE_DIACRITICS \\\n-    | $TOKENIZER -no-escape -l $lang \\\n-    > $(basename $file).tok\n-  done\n-  # compute BLEU\n-  cat $(basename $sys).tok | sacrebleu -tok none -s none -b $(basename $ref).tok\n-}\n-```\n-\n-(3) Call the function on test_generations.txt and test.target\n-For example,\n-```bash\n-ro_post_process enro_finetune/test_generations.txt wmt_en_ro/test.target\n-```\n-This will split out a new blue score and write a new fine called `test_generations.tok` with post-processed outputs.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-```"
        },
        {
            "sha": "d46544b96153e4f93a6d9a69664dc03e98fd79ec",
            "filename": "examples/legacy/seq2seq/rouge_cli.py",
            "status": "removed",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frouge_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frouge_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frouge_cli.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,31 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import fire\n-\n-from utils import calculate_rouge, save_json\n-\n-\n-def calculate_rouge_path(pred_path, tgt_path, save_path=None, **kwargs):\n-    \"\"\"Kwargs will be passed to calculate_rouge\"\"\"\n-    pred_lns = [x.strip() for x in open(pred_path)]\n-    tgt_lns = [x.strip() for x in open(tgt_path)][: len(pred_lns)]\n-    metrics = calculate_rouge(pred_lns, tgt_lns, **kwargs)\n-    if save_path is not None:\n-        save_json(metrics, save_path, indent=None)\n-    return metrics  # these print nicely\n-\n-\n-if __name__ == \"__main__\":\n-    fire.Fire(calculate_rouge_path)"
        },
        {
            "sha": "53fab8af9970cf51fadd782f77d19da837ee38a2",
            "filename": "examples/legacy/seq2seq/run_distributed_eval.py",
            "status": "removed",
            "additions": 0,
            "deletions": 261,
            "changes": 261,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,261 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import argparse\n-import shutil\n-import time\n-from json import JSONDecodeError\n-from logging import getLogger\n-from pathlib import Path\n-from typing import Optional\n-\n-import torch\n-from torch.utils.data import DataLoader\n-from tqdm import tqdm\n-\n-from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n-from utils import (\n-    Seq2SeqDataset,\n-    calculate_bleu,\n-    calculate_rouge,\n-    chunks,\n-    lmap,\n-    load_json,\n-    parse_numeric_n_bool_cl_kwargs,\n-    save_json,\n-    use_task_specific_params,\n-    write_txt_file,\n-)\n-\n-\n-logger = getLogger(__name__)\n-\n-\n-def eval_data_dir(\n-    data_dir,\n-    save_dir: str,\n-    model_name: str,\n-    bs: int = 8,\n-    max_source_length: int = 1024,\n-    type_path=\"val\",\n-    n_obs=None,\n-    fp16=False,\n-    task=\"summarization\",\n-    local_rank=None,\n-    num_return_sequences=1,\n-    dataset_kwargs: Optional[dict] = None,\n-    prefix=\"\",\n-    **generate_kwargs,\n-) -> dict:\n-    \"\"\"Run evaluation on part of the data for one gpu and save to {save_dir}/rank_{rank}_output.json\"\"\"\n-    model_name = str(model_name)\n-    assert local_rank is not None\n-    torch.distributed.init_process_group(backend=\"nccl\", rank=local_rank)\n-\n-    save_dir = Path(save_dir)\n-    save_path = save_dir.joinpath(f\"rank_{local_rank}_output.json\")\n-    torch.cuda.set_device(local_rank)\n-    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n-    if fp16:\n-        model = model.half()\n-    # determine if we need to increase num_beams\n-    use_task_specific_params(model, task)  # update config with task specific params\n-    num_beams = generate_kwargs.pop(\"num_beams\", model.config.num_beams)  # AttributeError risk?\n-    if num_return_sequences > num_beams:\n-        num_beams = num_return_sequences\n-\n-    tokenizer = AutoTokenizer.from_pretrained(model_name)\n-    logger.info(f\"Inferred tokenizer type: {tokenizer.__class__}\")  # if this is wrong, check config.model_type.\n-\n-    if max_source_length is None:\n-        max_source_length = tokenizer.model_max_length\n-    if prefix is None:\n-        prefix = prefix or getattr(model.config, \"prefix\", \"\") or \"\"\n-    ds = Seq2SeqDataset(\n-        tokenizer,\n-        data_dir,\n-        max_source_length,\n-        max_target_length=1024,\n-        type_path=type_path,\n-        n_obs=n_obs,\n-        prefix=prefix,\n-        **dataset_kwargs,\n-    )\n-    # I set shuffle=True for a more accurate progress bar.\n-    # If all the longest samples are first, the prog bar estimate is too high at the beginning.\n-    sampler = ds.make_sortish_sampler(bs, distributed=True, add_extra_examples=False, shuffle=True)\n-    data_loader = DataLoader(ds, sampler=sampler, batch_size=bs, collate_fn=ds.collate_fn)\n-    results = []\n-    for batch in tqdm(data_loader):\n-        summaries = model.generate(\n-            input_ids=batch[\"input_ids\"].to(model.device),\n-            attention_mask=batch[\"attention_mask\"].to(model.device),\n-            num_return_sequences=num_return_sequences,\n-            num_beams=num_beams,\n-            **generate_kwargs,\n-        )\n-        preds = tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-        ids = batch[\"ids\"]\n-        if num_return_sequences > 1:\n-            preds = chunks(preds, num_return_sequences)  # batch size chunks, each of size num_return_seq\n-        for i, pred in enumerate(preds):\n-            results.append({\"pred\": pred, \"id\": ids[i].item()})\n-    save_json(results, save_path)\n-    return results, sampler.num_replicas\n-\n-\n-def run_generate():\n-    parser = argparse.ArgumentParser(\n-        epilog=\"Unspecified args like --num_beams=2 --decoder_start_token_id=4 are passed to model.generate\"\n-    )\n-    parser.add_argument(\"--data_dir\", type=str, help=\"like cnn_dm/test.source\")\n-    parser.add_argument(\n-        \"--model_name\",\n-        type=str,\n-        help=\"like facebook/bart-large-cnn,google-t5/t5-base, etc.\",\n-        default=\"sshleifer/distilbart-xsum-12-3\",\n-    )\n-    parser.add_argument(\"--save_dir\", type=str, help=\"where to save\", default=\"tmp_gen\")\n-    parser.add_argument(\"--max_source_length\", type=int, default=None)\n-    parser.add_argument(\n-        \"--type_path\", type=str, default=\"test\", help=\"which subset to evaluate typically train/val/test\"\n-    )\n-    parser.add_argument(\"--task\", type=str, default=\"summarization\", help=\"used for task_specific_params + metrics\")\n-    parser.add_argument(\"--bs\", type=int, default=8, required=False, help=\"batch size\")\n-    parser.add_argument(\n-        \"--local_rank\", type=int, default=-1, required=False, help=\"should be passed by distributed.launch\"\n-    )\n-\n-    parser.add_argument(\n-        \"--n_obs\", type=int, default=None, required=False, help=\"How many observations. Defaults to all.\"\n-    )\n-    parser.add_argument(\n-        \"--num_return_sequences\", type=int, default=1, required=False, help=\"How many sequences to return\"\n-    )\n-    parser.add_argument(\n-        \"--sync_timeout\",\n-        type=int,\n-        default=600,\n-        required=False,\n-        help=\"How long should master process wait for other processes to finish.\",\n-    )\n-    parser.add_argument(\"--src_lang\", type=str, default=None, required=False)\n-    parser.add_argument(\"--tgt_lang\", type=str, default=None, required=False)\n-    parser.add_argument(\n-        \"--prefix\", type=str, required=False, default=None, help=\"will be added to the beginning of src examples\"\n-    )\n-    parser.add_argument(\"--fp16\", action=\"store_true\")\n-    parser.add_argument(\"--debug\", action=\"store_true\")\n-    start_time = time.time()\n-    args, rest = parser.parse_known_args()\n-    generate_kwargs = parse_numeric_n_bool_cl_kwargs(rest)\n-    if generate_kwargs and args.local_rank <= 0:\n-        print(f\"parsed the following generate kwargs: {generate_kwargs}\")\n-    json_save_dir = Path(args.save_dir + \"_tmp\")\n-    Path(json_save_dir).mkdir(exist_ok=True)  # this handles locking.\n-    intermediate_files = list(json_save_dir.glob(\"rank_*.json\"))\n-    if intermediate_files:\n-        raise ValueError(f\"Found files at {json_save_dir} please move or remove them.\")\n-        # In theory, a node could finish and save before another node hits this. If this happens, we can address later.\n-    dataset_kwargs = {}\n-    if args.src_lang is not None:\n-        dataset_kwargs[\"src_lang\"] = args.src_lang\n-    if args.tgt_lang is not None:\n-        dataset_kwargs[\"tgt_lang\"] = args.tgt_lang\n-\n-    Path(args.save_dir).mkdir(exist_ok=True)\n-    results, num_replicas = eval_data_dir(\n-        args.data_dir,\n-        json_save_dir,\n-        args.model_name,\n-        type_path=args.type_path,\n-        bs=args.bs,\n-        fp16=args.fp16,\n-        task=args.task,\n-        local_rank=args.local_rank,\n-        n_obs=args.n_obs,\n-        max_source_length=args.max_source_length,\n-        num_return_sequences=args.num_return_sequences,\n-        prefix=args.prefix,\n-        dataset_kwargs=dataset_kwargs,\n-        **generate_kwargs,\n-    )\n-\n-    if args.local_rank <= 0:\n-        save_dir = Path(args.save_dir)\n-        save_dir.mkdir(exist_ok=True)\n-        partial_results = gather_results_from_each_node(num_replicas, json_save_dir, args.sync_timeout)\n-        preds = combine_partial_results(partial_results)\n-        if args.num_return_sequences > 1:\n-            save_path = save_dir.joinpath(\"pseudolabel_results.json\")\n-            print(f\"Saving aggregated results at {save_path}, intermediate in {json_save_dir}/\")\n-            save_json(preds, save_path)\n-            return\n-        tgt_file = Path(args.data_dir).joinpath(args.type_path + \".target\")\n-        with open(tgt_file) as f:\n-            labels = [x.rstrip() for x in f][: len(preds)]\n-\n-        # Calculate metrics, save metrics,  and save _generations.txt\n-        calc_bleu = \"translation\" in args.task\n-        score_fn = calculate_bleu if calc_bleu else calculate_rouge\n-        metric_name = \"bleu\" if calc_bleu else \"rouge\"\n-        metrics: dict = score_fn(preds, labels)\n-        metrics[\"n_obs\"] = len(preds)\n-        runtime = time.time() - start_time\n-        metrics[\"seconds_per_sample\"] = round(runtime / metrics[\"n_obs\"], 4)\n-        metrics[\"n_gpus\"] = num_replicas\n-        # TODO(@stas00): add whatever metadata to metrics\n-        metrics_save_path = save_dir.joinpath(f\"{args.type_path}_{metric_name}.json\")\n-        save_json(metrics, metrics_save_path, indent=None)\n-        print(metrics)\n-        write_txt_file(preds, save_dir.joinpath(f\"{args.type_path}_generations.txt\"))\n-        if args.debug:\n-            write_txt_file(labels, save_dir.joinpath(f\"{args.type_path}.target\"))\n-        else:\n-            shutil.rmtree(json_save_dir)\n-\n-\n-def combine_partial_results(partial_results) -> list:\n-    \"\"\"Concatenate partial results into one file, then sort it by id.\"\"\"\n-    records = []\n-    for partial_result in partial_results:\n-        records.extend(partial_result)\n-    records = sorted(records, key=lambda x: x[\"id\"])\n-    preds = [x[\"pred\"] for x in records]\n-    return preds\n-\n-\n-def gather_results_from_each_node(num_replicas, save_dir, timeout) -> list[dict[str, list]]:\n-    # WAIT FOR lots of .json files\n-    start_wait = time.time()\n-    logger.info(\"waiting for all nodes to finish\")\n-    json_data = None\n-    while (time.time() - start_wait) < timeout:\n-        json_files = list(save_dir.glob(\"rank_*.json\"))\n-        if len(json_files) < num_replicas:\n-            continue\n-        try:\n-            # make sure all json files are fully saved\n-            json_data = lmap(load_json, json_files)\n-            return json_data\n-        except JSONDecodeError:\n-            continue\n-    raise TimeoutError(\"Rank 0 gave up on waiting for other processes\")\n-    # Unreachable\n-\n-\n-if __name__ == \"__main__\":\n-    # Usage for MT:\n-    run_generate()"
        },
        {
            "sha": "180143046bb6f30ca6584c991469fdd132c36dd9",
            "filename": "examples/legacy/seq2seq/run_eval.py",
            "status": "removed",
            "additions": 0,
            "deletions": 184,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frun_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frun_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_eval.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,184 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import argparse\n-import datetime\n-import json\n-import time\n-import warnings\n-from logging import getLogger\n-from pathlib import Path\n-\n-import torch\n-from tqdm import tqdm\n-\n-from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n-from utils import calculate_bleu, calculate_rouge, chunks, parse_numeric_n_bool_cl_kwargs, use_task_specific_params\n-\n-\n-logger = getLogger(__name__)\n-\n-\n-DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-\n-\n-def generate_summaries_or_translations(\n-    examples: list[str],\n-    out_file: str,\n-    model_name: str,\n-    batch_size: int = 8,\n-    device: str = DEFAULT_DEVICE,\n-    fp16=False,\n-    task=\"summarization\",\n-    prefix=None,\n-    **generate_kwargs,\n-) -> dict:\n-    \"\"\"Save model.generate results to <out_file>, and return how long it took.\"\"\"\n-    fout = Path(out_file).open(\"w\", encoding=\"utf-8\")\n-    model_name = str(model_name)\n-    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n-    if fp16:\n-        model = model.half()\n-\n-    tokenizer = AutoTokenizer.from_pretrained(model_name)\n-    logger.info(f\"Inferred tokenizer type: {tokenizer.__class__}\")  # if this is wrong, check config.model_type.\n-\n-    start_time = time.time()\n-    # update config with task specific params\n-    use_task_specific_params(model, task)\n-    if prefix is None:\n-        prefix = prefix or getattr(model.config, \"prefix\", \"\") or \"\"\n-    for examples_chunk in tqdm(list(chunks(examples, batch_size))):\n-        examples_chunk = [prefix + text for text in examples_chunk]\n-        batch = tokenizer(examples_chunk, return_tensors=\"pt\", truncation=True, padding=\"longest\").to(device)\n-        summaries = model.generate(\n-            input_ids=batch.input_ids,\n-            attention_mask=batch.attention_mask,\n-            **generate_kwargs,\n-        )\n-        dec = tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-        for hypothesis in dec:\n-            fout.write(hypothesis + \"\\n\")\n-            fout.flush()\n-    fout.close()\n-    runtime = int(time.time() - start_time)  # seconds\n-    n_obs = len(examples)\n-    return {\"n_obs\": n_obs, \"runtime\": runtime, \"seconds_per_sample\": round(runtime / n_obs, 4)}\n-\n-\n-def datetime_now():\n-    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n-\n-\n-def run_generate(verbose=True):\n-    \"\"\"\n-\n-    Takes input text, generates output, and then using reference calculates the BLEU scores.\n-\n-    The results are saved to a file and returned to the caller, and printed out unless ``verbose=False`` is passed.\n-\n-    Args:\n-        verbose (:obj:`bool`, `optional`, defaults to :obj:`True`): print results to stdout\n-\n-    Returns:\n-        a tuple: ``(scores, params}``\n-        - ``scores``: a dict of scores data ``{'bleu': 39.6501, 'n_obs': 2000, 'runtime': 186, 'seconds_per_sample': 0.093}``\n-        - ``params``: a dict of custom params, e.g. ``{'num_beams': 5, 'length_penalty': 0.8}``\n-    \"\"\"\n-\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"model_name\", type=str, help=\"like facebook/bart-large-cnn,google-t5/t5-base, etc.\")\n-    parser.add_argument(\"input_path\", type=str, help=\"like cnn_dm/test.source\")\n-    parser.add_argument(\"save_path\", type=str, help=\"where to save summaries\")\n-    parser.add_argument(\"--reference_path\", type=str, required=False, help=\"like cnn_dm/test.target\")\n-    parser.add_argument(\"--score_path\", type=str, required=False, default=\"metrics.json\", help=\"where to save metrics\")\n-    parser.add_argument(\"--device\", type=str, required=False, default=DEFAULT_DEVICE, help=\"cuda, cuda:1, cpu etc.\")\n-    parser.add_argument(\n-        \"--prefix\", type=str, required=False, default=None, help=\"will be added to the beginning of src examples\"\n-    )\n-    parser.add_argument(\"--task\", type=str, default=\"summarization\", help=\"used for task_specific_params + metrics\")\n-    parser.add_argument(\"--bs\", type=int, default=8, required=False, help=\"batch size\")\n-    parser.add_argument(\n-        \"--n_obs\", type=int, default=-1, required=False, help=\"How many observations. Defaults to all.\"\n-    )\n-    parser.add_argument(\"--fp16\", action=\"store_true\")\n-    parser.add_argument(\"--dump-args\", action=\"store_true\", help=\"print the custom hparams with the results\")\n-    parser.add_argument(\n-        \"--info\",\n-        nargs=\"?\",\n-        type=str,\n-        const=datetime_now(),\n-        help=(\n-            \"use in conjunction w/ --dump-args to print with the results whatever other info you'd like, e.g.\"\n-            \" lang=en-ru. If no value is passed, the current datetime string will be used.\"\n-        ),\n-    )\n-    # Unspecified args like --num_beams=2 --decoder_start_token_id=4 are passed to model.generate\n-    args, rest = parser.parse_known_args()\n-    parsed_args = parse_numeric_n_bool_cl_kwargs(rest)\n-    if parsed_args and verbose:\n-        print(f\"parsed the following generate kwargs: {parsed_args}\")\n-    examples = [\" \" + x.rstrip() if \"t5\" in args.model_name else x.rstrip() for x in open(args.input_path)]\n-    if args.n_obs > 0:\n-        examples = examples[: args.n_obs]\n-    Path(args.save_path).parent.mkdir(exist_ok=True)\n-\n-    if args.reference_path is None and Path(args.score_path).exists():\n-        warnings.warn(f\"score_path {args.score_path} will be overwritten unless you type ctrl-c.\")\n-\n-    if args.device == \"cpu\" and args.fp16:\n-        # this mix leads to RuntimeError: \"threshold_cpu\" not implemented for 'Half'\n-        raise ValueError(\"Can't mix --fp16 and --device cpu\")\n-\n-    runtime_metrics = generate_summaries_or_translations(\n-        examples,\n-        args.save_path,\n-        args.model_name,\n-        batch_size=args.bs,\n-        device=args.device,\n-        fp16=args.fp16,\n-        task=args.task,\n-        prefix=args.prefix,\n-        **parsed_args,\n-    )\n-\n-    if args.reference_path is None:\n-        return {}\n-\n-    # Compute scores\n-    score_fn = calculate_bleu if \"translation\" in args.task else calculate_rouge\n-    output_lns = [x.rstrip() for x in open(args.save_path)]\n-    reference_lns = [x.rstrip() for x in open(args.reference_path)][: len(output_lns)]\n-    scores: dict = score_fn(output_lns, reference_lns)\n-    scores.update(runtime_metrics)\n-\n-    if args.dump_args:\n-        scores.update(parsed_args)\n-    if args.info:\n-        scores[\"info\"] = args.info\n-\n-    if verbose:\n-        print(scores)\n-\n-    if args.score_path is not None:\n-        json.dump(scores, open(args.score_path, \"w\"))\n-\n-    return scores\n-\n-\n-if __name__ == \"__main__\":\n-    # Usage for MT:\n-    # python run_eval.py MODEL_NAME $DATA_DIR/test.source $save_dir/test_translations.txt --reference_path $DATA_DIR/test.target --score_path $save_dir/test_bleu.json  --task translation $@\n-    run_generate(verbose=True)"
        },
        {
            "sha": "e911eca57048ca5764177a7910e008950d6bc695",
            "filename": "examples/legacy/seq2seq/run_eval_search.py",
            "status": "removed",
            "additions": 0,
            "deletions": 158,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frun_eval_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Frun_eval_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_eval_search.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,158 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import argparse\n-import itertools\n-import operator\n-import sys\n-from collections import OrderedDict\n-\n-from run_eval import datetime_now, run_generate\n-\n-from utils import ROUGE_KEYS\n-\n-\n-# A table of supported tasks and the list of scores in the order of importance to be sorted by.\n-# To add a new task, simply list the score names that `run_eval.run_generate()` returns\n-task_score_names = {\n-    \"translation\": [\"bleu\"],\n-    \"summarization\": ROUGE_KEYS,\n-}\n-\n-\n-def parse_search_arg(search):\n-    groups = search.split()\n-    entries = dict(g.split(\"=\") for g in groups)\n-    entry_names = list(entries.keys())\n-    sets = [[f\"--{k} {v}\" for v in vs.split(\":\")] for k, vs in entries.items()]\n-    matrix = [list(x) for x in itertools.product(*sets)]\n-    return matrix, entry_names\n-\n-\n-def run_search():\n-    \"\"\"\n-     Run parametric search over the desired hparam space with help of ``run_eval.py``.\n-\n-     All the arguments except ``--search`` are passed to ``run_eval.py`` as is. The values inside of \"--search\" are parsed, reformatted and fed to ``run_eval.py`` as additional args.\n-\n-    The format for the ``--search`` value is a simple string with hparams and colon separated values to try, e.g.:\n-    ```\n-     --search \"num_beams=5:10 length_penalty=0.8:1.0:1.2 early_stopping=true:false\"\n-    ```\n-    which will generate ``12`` ``(2*3*2)`` searches for a product of each hparam. For example the example that was just used will invoke ``run_eval.py`` repeatedly with:\n-\n-    ```\n-     --num_beams 5 --length_penalty 0.8 --early_stopping true\n-     --num_beams 5 --length_penalty 0.8 --early_stopping false\n-     [...]\n-     --num_beams 10 --length_penalty 1.2 --early_stopping false\n-    ```\n-\n-    On completion, this function prints a markdown table of the results sorted by the best BLEU score and the winning arguments.\n-\n-\n-    \"\"\"\n-    prog = sys.argv[0]\n-\n-    parser = argparse.ArgumentParser(\n-        usage=(\n-            \"\\n\\nImportant: this script accepts all arguments `run_eval.py` accepts and then a few extra, therefore\"\n-            \" refer to `run_eval.py -h` for the complete list.\"\n-        )\n-    )\n-    parser.add_argument(\n-        \"--search\",\n-        type=str,\n-        required=False,\n-        help='param space to search, e.g. \"num_beams=5:10 length_penalty=0.8:1.0:1.2\"',\n-    )\n-    parser.add_argument(\n-        \"--bs\", type=int, default=8, required=False, help=\"initial batch size (may get reduced if it's too big)\"\n-    )\n-    parser.add_argument(\"--task\", type=str, help=\"used for task_specific_params + metrics\")\n-    parser.add_argument(\n-        \"--info\",\n-        nargs=\"?\",\n-        type=str,\n-        const=datetime_now(),\n-        help=(\n-            \"add custom notes to be printed before the results table. If no value is passed, the current datetime\"\n-            \" string will be used.\"\n-        ),\n-    )\n-    args, args_main = parser.parse_known_args()\n-    # we share some of the args\n-    args_main.extend([\"--task\", args.task])\n-    args_normal = [prog] + args_main\n-\n-    # to support variations like translation_en_to_de\"\n-    task = \"translation\" if \"translation\" in args.task else \"summarization\"\n-\n-    matrix, col_names = parse_search_arg(args.search)\n-    col_names[0:0] = task_score_names[task]  # score cols first\n-    col_widths = {col: len(str(col)) for col in col_names}\n-    results = []\n-    for r in matrix:\n-        hparams = dict(x.replace(\"--\", \"\").split() for x in r)\n-        args_exp = \" \".join(r).split()\n-        args_exp.extend([\"--bs\", str(args.bs)])  # in case we need to reduce its size due to CUDA OOM\n-        sys.argv = args_normal + args_exp\n-\n-        # XXX: need to trap CUDA OOM and lower args.bs if that happens and retry\n-\n-        scores = run_generate(verbose=False)\n-        # make sure scores are first in the table\n-        result = OrderedDict()\n-        for score in task_score_names[task]:\n-            result[score] = scores[score]\n-        result.update(hparams)\n-        results.append(result)\n-\n-        # find widest entries\n-        for k, v in result.items():\n-            l = len(str(v))\n-            if l > col_widths[k]:\n-                col_widths[k] = l\n-\n-    results_sorted = sorted(results, key=operator.itemgetter(*task_score_names[task]), reverse=True)\n-    print(\" | \".join([f\"{col:{col_widths[col]}}\" for col in col_names]))\n-    print(\" | \".join([f\"{'-' * col_widths[col]}\" for col in col_names]))\n-    for row in results_sorted:\n-        print(\" | \".join([f\"{row[col]:{col_widths[col]}}\" for col in col_names]))\n-\n-    best = results_sorted[0]\n-    for score in task_score_names[task]:\n-        del best[score]\n-    best_args = [f\"--{k} {v}\" for k, v in best.items()]\n-    dyn_args = [\"--bs\", str(args.bs)]\n-    if args.info:\n-        print(f\"\\nInfo: {args.info}\")\n-    print(\"\\nBest score args:\")\n-    print(\" \".join(args_main + best_args + dyn_args))\n-\n-    return results_sorted\n-\n-\n-if __name__ == \"__main__\":\n-    # Usage:\n-    # [normal-run_eval_search.py cmd plus] \\\n-    # --search=\"num_beams=1:5:10 length_penalty=0.8:1:1.2 early_stopping=true:false\"\n-    #\n-    # Example:\n-    # PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py $MODEL_NAME \\\n-    # $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target \\\n-    # --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation \\\n-    # --search=\"num_beams=1:5:10 length_penalty=0.8:1:1.2 early_stopping=true:false\"\n-    run_search()"
        },
        {
            "sha": "9e73b59e7e5a2b0a480779db987464f8b8320cee",
            "filename": "examples/legacy/seq2seq/save_len_file.py",
            "status": "removed",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fsave_len_file.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fsave_len_file.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fsave_len_file.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,56 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import fire\n-from torch.utils.data import DataLoader\n-from tqdm import tqdm\n-\n-from transformers import AutoTokenizer\n-from utils import Seq2SeqDataset, pickle_save\n-\n-\n-def save_len_file(\n-    tokenizer_name, data_dir, max_source_length=1024, max_target_length=1024, consider_target=False, **kwargs\n-):\n-    \"\"\"Save max(src_len, tgt_len) for each example to allow dynamic batching.\"\"\"\n-    tok = AutoTokenizer.from_pretrained(tokenizer_name)\n-    train_ds = Seq2SeqDataset(tok, data_dir, max_source_length, max_target_length, type_path=\"train\", **kwargs)\n-    pad = tok.pad_token_id\n-\n-    def get_lens(ds):\n-        dl = tqdm(\n-            DataLoader(ds, batch_size=512, num_workers=8, shuffle=False, collate_fn=ds.collate_fn),\n-            desc=str(ds.len_file),\n-        )\n-        max_lens = []\n-        for batch in dl:\n-            src_lens = batch[\"input_ids\"].ne(pad).sum(1).tolist()\n-            tgt_lens = batch[\"labels\"].ne(pad).sum(1).tolist()\n-            if consider_target:\n-                for src, tgt in zip(src_lens, tgt_lens):\n-                    max_lens.append(max(src, tgt))\n-            else:\n-                max_lens.extend(src_lens)\n-        return max_lens\n-\n-    train_lens = get_lens(train_ds)\n-    val_ds = Seq2SeqDataset(tok, data_dir, max_source_length, max_target_length, type_path=\"val\", **kwargs)\n-    val_lens = get_lens(val_ds)\n-    pickle_save(train_lens, train_ds.len_file)\n-    pickle_save(val_lens, val_ds.len_file)\n-\n-\n-if __name__ == \"__main__\":\n-    fire.Fire(save_len_file)"
        },
        {
            "sha": "1b7b17fde8d6b0e7f2eed7420c0570012558b1ed",
            "filename": "examples/legacy/seq2seq/save_randomly_initialized_model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fsave_randomly_initialized_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fsave_randomly_initialized_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fsave_randomly_initialized_model.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,39 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import fire\n-\n-from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n-\n-\n-def save_randomly_initialized_version(config_name: str, save_dir: str, **config_kwargs):\n-    \"\"\"Save a randomly initialized version of a model using a pretrained config.\n-    Args:\n-        config_name: which config to use\n-        save_dir: where to save the resulting model and tokenizer\n-        config_kwargs: Passed to AutoConfig\n-\n-    Usage::\n-        save_randomly_initialized_version(\"facebook/bart-large-cnn\", \"distilbart_random_cnn_6_3\", encoder_layers=6, decoder_layers=3, num_beams=3)\n-    \"\"\"\n-    cfg = AutoConfig.from_pretrained(config_name, **config_kwargs)\n-    model = AutoModelForSeq2SeqLM.from_config(cfg)\n-    model.save_pretrained(save_dir)\n-    AutoTokenizer.from_pretrained(config_name).save_pretrained(save_dir)\n-    return model\n-\n-\n-if __name__ == \"__main__\":\n-    fire.Fire(save_randomly_initialized_version)"
        },
        {
            "sha": "54a07967efa31c31ee1219d1a25808df0108388a",
            "filename": "examples/legacy/seq2seq/sentence_splitter.py",
            "status": "removed",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fsentence_splitter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fsentence_splitter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fsentence_splitter.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,35 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import re\n-\n-from filelock import FileLock\n-\n-\n-try:\n-    import nltk\n-\n-    NLTK_AVAILABLE = True\n-except (ImportError, ModuleNotFoundError):\n-    NLTK_AVAILABLE = False\n-\n-if NLTK_AVAILABLE:\n-    with FileLock(\".lock\") as lock:\n-        nltk.download(\"punkt\", quiet=True)\n-\n-\n-def add_newline_to_end_of_each_sentence(x: str) -> str:\n-    \"\"\"This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS.\"\"\"\n-    re.sub(\"<n>\", \"\", x)  # remove pegasus newline char\n-    assert NLTK_AVAILABLE, \"nltk must be installed to separate newlines between sentences. (pip install nltk)\"\n-    return \"\\n\".join(nltk.sent_tokenize(x))"
        },
        {
            "sha": "054ebd63c30adf44d6eb6c02b4accb918631dd66",
            "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 248,
            "changes": 248,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,248 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Any, Optional, Union\n-\n-import torch\n-from torch import nn\n-from torch.utils.data import DistributedSampler, RandomSampler\n-\n-from transformers import PreTrainedModel, Trainer, logging\n-from transformers.models.fsmt.configuration_fsmt import FSMTConfig\n-from transformers.optimization import (\n-    Adafactor,\n-    get_constant_schedule,\n-    get_constant_schedule_with_warmup,\n-    get_cosine_schedule_with_warmup,\n-    get_cosine_with_hard_restarts_schedule_with_warmup,\n-    get_linear_schedule_with_warmup,\n-    get_polynomial_decay_schedule_with_warmup,\n-)\n-from transformers.trainer_pt_utils import get_tpu_sampler\n-from transformers.training_args import ParallelMode\n-from transformers.utils import is_torch_xla_available\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-arg_to_scheduler = {\n-    \"linear\": get_linear_schedule_with_warmup,\n-    \"cosine\": get_cosine_schedule_with_warmup,\n-    \"cosine_w_restarts\": get_cosine_with_hard_restarts_schedule_with_warmup,\n-    \"polynomial\": get_polynomial_decay_schedule_with_warmup,\n-    \"constant\": get_constant_schedule,\n-    \"constant_w_warmup\": get_constant_schedule_with_warmup,\n-}\n-\n-\n-class Seq2SeqTrainer(Trainer):\n-    def __init__(self, config=None, data_args=None, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        if config is None:\n-            assert isinstance(self.model, PreTrainedModel), (\n-                \"If no `config` is passed the model to be trained has to be of type `PreTrainedModel`, but is\"\n-                f\" {self.model.__class__}\"\n-            )\n-            self.config = self.model.config\n-        else:\n-            self.config = config\n-\n-        self.data_args = data_args\n-        self.vocab_size = self.config.tgt_vocab_size if isinstance(self.config, FSMTConfig) else self.config.vocab_size\n-\n-        if self.args.label_smoothing != 0 or (self.data_args is not None and self.data_args.ignore_pad_token_for_loss):\n-            assert self.config.pad_token_id is not None, (\n-                \"Make sure that `config.pad_token_id` is correctly defined when ignoring `pad_token` for loss\"\n-                \" calculation or doing label smoothing.\"\n-            )\n-\n-        if self.config.pad_token_id is None and self.config.eos_token_id is not None:\n-            logger.warning(\n-                f\"The `config.pad_token_id` is `None`. Using `config.eos_token_id` = {self.config.eos_token_id} for\"\n-                \" padding..\"\n-            )\n-\n-        if self.args.label_smoothing == 0:\n-            self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)\n-        else:\n-            # dynamically import label_smoothed_nll_loss\n-            from utils import label_smoothed_nll_loss\n-\n-            self.loss_fn = label_smoothed_nll_loss\n-\n-    def create_optimizer_and_scheduler(self, num_training_steps: int):\n-        \"\"\"\n-        Setup the optimizer and the learning rate scheduler.\n-\n-        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n-        Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n-        \"\"\"\n-        if self.optimizer is None:\n-            no_decay = [\"bias\", \"LayerNorm.weight\"]\n-            optimizer_grouped_parameters = [\n-                {\n-                    \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n-                    \"weight_decay\": self.args.weight_decay,\n-                },\n-                {\n-                    \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n-                    \"weight_decay\": 0.0,\n-                },\n-            ]\n-            if self.args.adafactor:\n-                optimizer_cls = Adafactor\n-                optimizer_kwargs = {\"scale_parameter\": False, \"relative_step\": False}\n-            else:\n-                optimizer_cls = torch.optim.AdamW\n-                optimizer_kwargs = {\n-                    \"betas\": (self.args.adam_beta1, self.args.adam_beta2),\n-                    \"eps\": self.args.adam_epsilon,\n-                }\n-            optimizer_kwargs[\"lr\"] = self.args.learning_rate\n-            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n-\n-        if self.lr_scheduler is None:\n-            self.lr_scheduler = self._get_lr_scheduler(num_training_steps)\n-        else:  # ignoring --lr_scheduler\n-            logger.warning(\"scheduler is passed to `Seq2SeqTrainer`, `--lr_scheduler` arg is ignored.\")\n-\n-    def _get_lr_scheduler(self, num_training_steps):\n-        schedule_func = arg_to_scheduler[self.args.lr_scheduler]\n-        if self.args.lr_scheduler == \"constant\":\n-            scheduler = schedule_func(self.optimizer)\n-        elif self.args.lr_scheduler == \"constant_w_warmup\":\n-            scheduler = schedule_func(self.optimizer, num_warmup_steps=self.args.warmup_steps)\n-        else:\n-            scheduler = schedule_func(\n-                self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps\n-            )\n-        return scheduler\n-\n-    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n-        if isinstance(self.train_dataset, torch.utils.data.IterableDataset):\n-            return None\n-        elif is_torch_xla_available():\n-            return get_tpu_sampler(self.train_dataset)\n-        else:\n-            if self.args.sortish_sampler:\n-                self.train_dataset.make_sortish_sampler(\n-                    self.args.per_device_train_batch_size,\n-                    distributed=(self.args.parallel_mode == ParallelMode.DISTRIBUTED),\n-                )\n-\n-            return (\n-                RandomSampler(self.train_dataset)\n-                if self.args.local_process_index == -1\n-                else DistributedSampler(self.train_dataset)\n-            )\n-\n-    def _compute_loss(self, model, inputs, labels):\n-        if self.args.label_smoothing == 0:\n-            if self.data_args is not None and self.data_args.ignore_pad_token_for_loss:\n-                # force training to ignore pad token\n-                logits = model(**inputs, use_cache=False)[0]\n-                loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n-            else:\n-                # compute usual loss via models\n-                loss, logits = model(**inputs, labels=labels, use_cache=False)[:2]\n-        else:\n-            # compute label smoothed loss\n-            logits = model(**inputs, use_cache=False)[0]\n-            lprobs = torch.nn.functional.log_softmax(logits, dim=-1)\n-            loss, _ = self.loss_fn(lprobs, labels, self.args.label_smoothing, ignore_index=self.config.pad_token_id)\n-        return loss, logits\n-\n-    def compute_loss(self, model, inputs):\n-        labels = inputs.pop(\"labels\")\n-        loss, _ = self._compute_loss(model, inputs, labels)\n-        return loss\n-\n-    def prediction_step(\n-        self,\n-        model: nn.Module,\n-        inputs: dict[str, Union[torch.Tensor, Any]],\n-        prediction_loss_only: bool,\n-        ignore_keys: Optional[list[str]] = None,\n-    ) -> tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n-        \"\"\"\n-        Perform an evaluation step on :obj:`model` using obj:`inputs`.\n-\n-        Subclass and override to inject custom behavior.\n-\n-        Args:\n-            model (:obj:`nn.Module`):\n-                The model to evaluate.\n-            inputs (:obj:`dict[str, Union[torch.Tensor, Any]]`):\n-                The inputs and targets of the model.\n-\n-                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n-                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n-            prediction_loss_only (:obj:`bool`):\n-                Whether or not to return the loss only.\n-\n-        Return:\n-            tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n-            A tuple with the loss, logits and labels (each being optional).\n-        \"\"\"\n-        inputs = self._prepare_inputs(inputs)\n-\n-        gen_kwargs = {\n-            \"max_length\": self.data_args.val_max_target_length\n-            if self.data_args is not None\n-            else self.config.max_length,\n-            \"num_beams\": self.data_args.eval_beams if self.data_args is not None else self.config.num_beams,\n-        }\n-\n-        if self.args.predict_with_generate and not self.args.prediction_loss_only:\n-            generated_tokens = self.model.generate(\n-                inputs[\"input_ids\"],\n-                attention_mask=inputs[\"attention_mask\"],\n-                **gen_kwargs,\n-            )\n-            # in case the batch is shorter than max length, the output should be padded\n-            if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n-                generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n-\n-        labels = inputs.pop(\"labels\")\n-        with torch.no_grad():\n-            # compute loss on predict data\n-            loss, logits = self._compute_loss(model, inputs, labels)\n-\n-        loss = loss.mean().detach()\n-        if self.args.prediction_loss_only:\n-            return (loss, None, None)\n-\n-        logits = generated_tokens if self.args.predict_with_generate else logits\n-\n-        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n-            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n-\n-        return (loss, logits, labels)\n-\n-    def _pad_tensors_to_max_len(self, tensor, max_length):\n-        # If PAD token is not defined at least EOS token has to be defined\n-        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n-\n-        if pad_token_id is None:\n-            raise ValueError(\n-                \"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be\"\n-                f\" padded to `max_length`={max_length}\"\n-            )\n-\n-        padded_tensor = pad_token_id * torch.ones(\n-            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n-        )\n-        padded_tensor[:, : tensor.shape[-1]] = tensor\n-        return padded_tensor"
        },
        {
            "sha": "9da1c69262a8c0c3907597ecffd435dc78929919",
            "filename": "examples/legacy/seq2seq/seq2seq_training_args.py",
            "status": "removed",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fseq2seq_training_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fseq2seq_training_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fseq2seq_training_args.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,60 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-from seq2seq_trainer import arg_to_scheduler\n-\n-from transformers import TrainingArguments\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class Seq2SeqTrainingArguments(TrainingArguments):\n-    \"\"\"\n-    Parameters:\n-        label_smoothing (:obj:`float`, `optional`, defaults to 0):\n-            The label smoothing epsilon to apply (if not zero).\n-        sortish_sampler (:obj:`bool`, `optional`, defaults to :obj:`False`):\n-            Whether to SortishSampler or not. It sorts the inputs according to lengths in-order to minimizing the padding size.\n-        predict_with_generate (:obj:`bool`, `optional`, defaults to :obj:`False`):\n-            Whether to use generate to calculate generative metrics (ROUGE, BLEU).\n-    \"\"\"\n-\n-    label_smoothing: Optional[float] = field(\n-        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (if not zero).\"}\n-    )\n-    sortish_sampler: bool = field(default=False, metadata={\"help\": \"Whether to SortishSampler or not.\"})\n-    predict_with_generate: bool = field(\n-        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n-    )\n-    adafactor: bool = field(default=False, metadata={\"help\": \"whether to use adafactor\"})\n-    encoder_layerdrop: Optional[float] = field(\n-        default=None, metadata={\"help\": \"Encoder layer dropout probability. Goes into model.config.\"}\n-    )\n-    decoder_layerdrop: Optional[float] = field(\n-        default=None, metadata={\"help\": \"Decoder layer dropout probability. Goes into model.config.\"}\n-    )\n-    dropout: Optional[float] = field(default=None, metadata={\"help\": \"Dropout probability. Goes into model.config.\"})\n-    attention_dropout: Optional[float] = field(\n-        default=None, metadata={\"help\": \"Attention dropout probability. Goes into model.config.\"}\n-    )\n-    lr_scheduler: Optional[str] = field(\n-        default=\"linear\",\n-        metadata={\"help\": f\"Which lr scheduler to use. Selected in {sorted(arg_to_scheduler.keys())}\"},\n-    )"
        },
        {
            "sha": "8f518822b4510ce36609a379fea5456075d2d245",
            "filename": "examples/legacy/seq2seq/test_data/fsmt/build-eval-data.py",
            "status": "removed",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Fbuild-eval-data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Fbuild-eval-data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Fbuild-eval-data.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,32 +0,0 @@\n-#!/usr/bin/env python\n-\n-import json\n-import subprocess\n-\n-\n-pairs = [\n-    [\"en\", \"ru\"],\n-    [\"ru\", \"en\"],\n-    [\"en\", \"de\"],\n-    [\"de\", \"en\"],\n-]\n-\n-n_objs = 8\n-\n-\n-def get_all_data(pairs, n_objs):\n-    text = {}\n-    for src, tgt in pairs:\n-        pair = f\"{src}-{tgt}\"\n-        cmd = f\"sacrebleu -t wmt19 -l {pair} --echo src\".split()\n-        src_lines = subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").splitlines()\n-        cmd = f\"sacrebleu -t wmt19 -l {pair} --echo ref\".split()\n-        tgt_lines = subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").splitlines()\n-        text[pair] = {\"src\": src_lines[:n_objs], \"tgt\": tgt_lines[:n_objs]}\n-    return text\n-\n-\n-text = get_all_data(pairs, n_objs)\n-filename = \"./fsmt_val_data.json\"\n-with open(filename, \"w\", encoding=\"utf-8\") as f:\n-    bleu_data = json.dump(text, f, indent=2, ensure_ascii=False)"
        },
        {
            "sha": "f38b305733314aaa134ecdb016b7f3bbea81a6d0",
            "filename": "examples/legacy/seq2seq/test_data/fsmt/fsmt_val_data.json",
            "status": "removed",
            "additions": 0,
            "deletions": 90,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Ffsmt_val_data.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Ffsmt_val_data.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Ffsmt%2Ffsmt_val_data.json?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,90 +0,0 @@\n-{\n-  \"en-ru\": {\n-    \"src\": [\n-      \"Welsh AMs worried about 'looking like muppets'\",\n-      \"There is consternation among some AMs at a suggestion their title should change to MWPs (Member of the Welsh Parliament).\",\n-      \"It has arisen because of plans to change the name of the assembly to the Welsh Parliament.\",\n-      \"AMs across the political spectrum are worried it could invite ridicule.\",\n-      \"One Labour AM said his group was concerned \\\"it rhymes with Twp and Pwp.\\\"\",\n-      \"For readers outside of Wales: In Welsh twp means daft and pwp means poo.\",\n-      \"A Plaid AM said the group as a whole was \\\"not happy\\\" and has suggested alternatives.\",\n-      \"A Welsh Conservative said his group was \\\"open minded\\\" about the name change, but noted it was a short verbal hop from MWP to Muppet.\"\n-    ],\n-    \"tgt\": [\n-      \"Ð§Ð»ÐµÐ½Ñ‹ ÐÐ°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð°ÑÑÐ°Ð¼Ð±Ð»ÐµÐ¸ Ð£ÑÐ»ÑŒÑÐ° Ð¾Ð±ÐµÑÐ¿Ð¾ÐºÐ¾ÐµÐ½Ñ‹, Ñ‡Ñ‚Ð¾ \\\"Ð²Ñ‹Ð³Ð»ÑÐ´ÑÑ‚ ÐºÐ°Ðº ÐºÑƒÐºÐ»Ñ‹\\\"\",\n-      \"ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ‡Ð»ÐµÐ½Ñ‹ ÐÐ°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð°ÑÑÐ°Ð¼Ð±Ð»ÐµÐ¸ Ð£ÑÐ»ÑŒÑÐ° Ð² ÑƒÐ¶Ð°ÑÐµ Ð¾Ñ‚ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð¾ Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾ Ð¸Ñ… Ð½Ð°Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð¸Ð·Ð¼ÐµÐ½Ð¸Ñ‚ÑŒÑÑ Ð½Ð° MPW (Ñ‡Ð»ÐµÐ½Ñ‹ ÐŸÐ°Ñ€Ð»Ð°Ð¼ÐµÐ½Ñ‚Ð° Ð£ÑÐ»ÑŒÑÐ°).\",\n-      \"Ð­Ñ‚Ð¾Ñ‚ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð±Ñ‹Ð» Ð¿Ð¾Ð´Ð½ÑÑ‚ Ð² ÑÐ²ÑÐ·Ð¸ Ñ Ð¿Ð»Ð°Ð½Ð°Ð¼Ð¸ Ð¿Ð¾ Ð¿ÐµÑ€ÐµÐ¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð°ÑÑÐ°Ð¼Ð±Ð»ÐµÐ¸ Ð² ÐŸÐ°Ñ€Ð»Ð°Ð¼ÐµÐ½Ñ‚ Ð£ÑÐ»ÑŒÑÐ°.\",\n-      \"Ð§Ð»ÐµÐ½Ñ‹ ÐÐ°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð°ÑÑÐ°Ð¼Ð±Ð»ÐµÐ¸ Ð£ÑÐ»ÑŒÑÐ° Ð²ÑÐµÐ³Ð¾ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¿ÐµÐºÑ‚Ñ€Ð° Ð¾Ð±ÐµÑÐ¿Ð¾ÐºÐ¾ÐµÐ½Ñ‹, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¿Ð¾Ñ€Ð¾Ð´Ð¸Ñ‚ÑŒ Ð½Ð°ÑÐ¼ÐµÑˆÐºÐ¸.\",\n-      \"ÐžÐ´Ð¸Ð½ Ð¸Ð· Ð»ÐµÐ¹Ð±Ð¾Ñ€Ð¸ÑÑ‚ÑÐºÐ¸Ñ… Ñ‡Ð»ÐµÐ½Ð¾Ð² ÐÐ°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð°ÑÑÐ°Ð¼Ð±Ð»ÐµÐ¸ Ð£ÑÐ»ÑŒÑÐ° ÑÐºÐ°Ð·Ð°Ð», Ñ‡Ñ‚Ð¾ ÐµÐ³Ð¾ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ Ð¾Ð±ÐµÑÐ¿Ð¾ÐºÐ¾ÐµÐ½Ð° Ñ‚ÐµÐ¼, Ñ‡Ñ‚Ð¾ \\\"ÑÑ‚Ð¾ Ñ€Ð¸Ñ„Ð¼ÑƒÐµÑ‚ÑÑ Ñ Twp Ð¸ Pwp\\\".\",\n-      \"Ð”Ð»Ñ Ñ‡Ð¸Ñ‚Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð·Ð° Ð¿Ñ€ÐµÐ´Ð»Ð°Ð¼Ð¸ Ð£ÑÐ»ÑŒÑÐ°: Ð¿Ð¾-Ð²Ð°Ð»Ð»Ð¸Ð¹ÑÐºÐ¸ twp Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚ \\\"Ð³Ð»ÑƒÐ¿Ñ‹Ð¹\\\", Ð° pwp Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚ \\\"ÐºÐ°ÐºÐ°ÑˆÐºÐ°\\\".\",\n-      \"Ð§Ð»ÐµÐ½ ÐÐ°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð°ÑÑÐ°Ð¼Ð±Ð»ÐµÐ¸ Ð¾Ñ‚ ÐŸÐ»Ð°Ð¹Ð´ ÑÐºÐ°Ð·Ð°Ð», Ñ‡Ñ‚Ð¾ ÑÑ‚Ð° Ð¿Ð°Ñ€Ñ‚Ð¸Ñ Ð² Ñ†ÐµÐ»Ð¾Ð¼ \\\"Ð½Ðµ ÑÑ‡Ð°ÑÑ‚Ð»Ð¸Ð²Ð°\\\" Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸Ð» Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ñ‹.\",\n-      \"ÐŸÑ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒ ÐšÐ¾Ð½ÑÐµÑ€Ð²Ð°Ñ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¿Ð°Ñ€Ñ‚Ð¸Ð¸ Ð£ÑÐ»ÑŒÑÐ° ÑÐºÐ°Ð·Ð°Ð», Ñ‡Ñ‚Ð¾ ÐµÐ³Ð¾ Ð¿Ð°Ñ€Ñ‚Ð¸Ñ \\\"Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð°\\\" Ðº Ð¿ÐµÑ€ÐµÐ¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð¸ÑŽ, Ð½Ð¾ Ð¾Ñ‚Ð¼ÐµÑ‚Ð¸Ð», Ñ‡Ñ‚Ð¾ Ð¼ÐµÐ¶Ð´Ñƒ WMP Ð¸ Muppet Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ°Ñ Ñ€Ð°Ð·Ð½Ð¸Ñ†Ð° Ð² Ð¿Ñ€Ð¾Ð¸Ð·Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¸.\"\n-    ]\n-  },\n-  \"ru-en\": {\n-    \"src\": [\n-      \"ÐÐ°Ð·Ð²Ð°Ð½Ð¾ Ñ‡Ð¸ÑÐ»Ð¾ Ð³Ð¾Ñ‚Ð¾Ð²ÑÑ‰Ð¸Ñ…ÑÑ Ðº Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐµ Ð² Ð”Ð¾Ð½Ð±Ð°ÑÑ Ð½Ð¾Ð²Ð¾Ð±Ñ€Ð°Ð½Ñ†ÐµÐ² Ð¸Ð· Ð£ÐºÑ€Ð°Ð¸Ð½Ñ‹\",\n-      \"ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÐµÐ»ÑŒ ÐÐ°Ñ€Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¸Ð»Ð¸Ñ†Ð¸Ð¸ ÑÐ°Ð¼Ð¾Ð¿Ñ€Ð¾Ð²Ð¾Ð·Ð³Ð»Ð°ÑˆÐµÐ½Ð½Ð¾Ð¹ Ð›ÑƒÐ³Ð°Ð½ÑÐºÐ¾Ð¹ ÐÐ°Ñ€Ð¾Ð´Ð½Ð¾Ð¹ Ð ÐµÑÐ¿ÑƒÐ±Ð»Ð¸ÐºÐ¸ (Ð›ÐÐ ) ÐÐ½Ð´Ñ€ÐµÐ¹ ÐœÐ°Ñ€Ð¾Ñ‡ÐºÐ¾ Ð·Ð°ÑÐ²Ð¸Ð», Ñ‡Ñ‚Ð¾ Ð·Ð¸Ð¼Ð¾Ð¹ 2018-2019 Ð³Ð¾Ð´Ð° Ð£ÐºÑ€Ð°Ð¸Ð½Ð° Ð½Ð°Ð¿Ñ€Ð°Ð²Ð¸Ñ‚ Ð² Ð”Ð¾Ð½Ð±Ð°ÑÑ Ð½Ðµ Ð¼ÐµÐ½ÐµÐµ 3 Ñ‚Ñ‹Ñ. Ð½Ð¾Ð²Ð¾Ð±Ñ€Ð°Ð½Ñ†ÐµÐ².\",\n-      \"ÐŸÐ¾ ÐµÐ³Ð¾ ÑÐ»Ð¾Ð²Ð°Ð¼, Ñ‚Ð°ÐºÐ¸Ð¼ Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð¼ ÐšÐ¸ÐµÐ² Ð¿Ð»Ð°Ð½Ð¸Ñ€ÑƒÐµÑ‚ \\\"Ñ…Ð¾Ñ‚ÑŒ ÐºÐ°Ðº-Ñ‚Ð¾ Ð´Ð¾ÑƒÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑ‚Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð´Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ñ\\\".\",\n-      \"\\\"ÐÐµÐ¶ÐµÐ»Ð°Ð½Ð¸Ðµ Ð³Ñ€Ð°Ð¶Ð´Ð°Ð½ Ð£ÐºÑ€Ð°Ð¸Ð½Ñ‹ Ð¿Ñ€Ð¾Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒ ÑÐ»ÑƒÐ¶Ð±Ñƒ Ð² Ñ€ÑÐ´Ð°Ñ… Ð’Ð¡ Ð£ÐºÑ€Ð°Ð¸Ð½Ñ‹, Ð¼Ð°ÑÑÐ¾Ð²Ñ‹Ðµ ÑƒÐ²Ð¾Ð»ÑŒÐ½ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸Ð²ÐµÐ»Ð¸ Ðº Ð½Ð¸Ð·ÐºÐ¾Ð¹ ÑƒÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑ‚Ð¾Ð²Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾Ð´Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ð¹\\\", - Ñ€Ð°ÑÑÐºÐ°Ð·Ð°Ð» ÐœÐ°Ñ€Ð¾Ñ‡ÐºÐ¾, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ñ†Ð¸Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ \\\"Ð Ð˜Ð ÐÐ¾Ð²Ð¾ÑÑ‚Ð¸\\\".\",\n-      \"ÐžÐ½ Ñ‚Ð°ÐºÐ¶Ðµ Ð½Ðµ Ð¸ÑÐºÐ»ÑŽÑ‡Ð¸Ð», Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ†Ð¸Ñ„Ñ€Ñ‹ Ð¿Ñ€Ð¸Ð·Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð°Ñ€Ð¼Ð¸ÑŽ ÑƒÐºÑ€Ð°Ð¸Ð½Ñ†ÐµÐ² Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ñ‹ Ð² ÑÐ»ÑƒÑ‡Ð°Ðµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸.\",\n-      \"Ð’ 2014-2017 Ð³Ð¾Ð´Ð°Ñ… ÐšÐ¸ÐµÐ² Ð½Ð°Ñ‡Ð°Ð» Ñ‚Ð°Ðº Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÐ¼ÑƒÑŽ Ð°Ð½Ñ‚Ð¸Ñ‚ÐµÑ€Ñ€Ð¾Ñ€Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑŽ (ÐÐ¢Ðž), ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¿Ð¾Ð·Ð¶Ðµ ÑÐ¼ÐµÐ½Ð¸Ð»Ð¸ Ð½Ð° Ð¾Ð¿ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ñ‹Ñ… ÑÐ¸Ð» (ÐžÐžÐ¡).\",\n-      \"ÐŸÑ€ÐµÐ´Ð¿Ð¾Ð»Ð°Ð³Ð°Ð»Ð¾ÑÑŒ, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð° Ð¼ÐµÑ€Ð° Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÑ‚ Ðº ÑƒÑÐ¸Ð»ÐµÐ½Ð¸ÑŽ Ñ€Ð¾Ð»Ð¸ ÑƒÐºÑ€Ð°Ð¸Ð½ÑÐºÐ¸Ñ… ÑÐ¸Ð»Ð¾Ð²Ð¸ÐºÐ¾Ð² Ð² ÑƒÑ€ÐµÐ³ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸.\",\n-      \"Ð’ ÐºÐ¾Ð½Ñ†Ðµ Ð°Ð²Ð³ÑƒÑÑ‚Ð° 2018 Ð³Ð¾Ð´Ð° ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ñ Ð² Ð”Ð¾Ð½Ð±Ð°ÑÑÐµ Ð¾Ð±Ð¾ÑÑ‚Ñ€Ð¸Ð»Ð°ÑÑŒ Ð¸Ð·-Ð·Ð° ÑƒÐ±Ð¸Ð¹ÑÑ‚Ð²Ð° Ð³Ð»Ð°Ð²Ñ‹ Ð”ÐÐ  ÐÐ»ÐµÐºÑÐ°Ð½Ð´Ñ€Ð° Ð—Ð°Ñ…Ð°Ñ€Ñ‡ÐµÐ½ÐºÐ¾.\"\n-    ],\n-    \"tgt\": [\n-      \"The number of new Ukrainian recruits ready to go to Donbass has become public\",\n-      \"Official representative of the peoplesâ€™ militia of the self-proclaimed Lugansk Peopleâ€™s Republic Andrey Marochko claimed that Ukrainian will send at least 3 thousand new recruits to Donbass in winter 2018-2019.\",\n-      \"This is how Kyiv tries â€œat least somehow to staff the units,â€ he said.\",\n-      \"â€œThe unwillingness of Ukrainian citizens to serve in the Ukraineâ€™s military forces, mass resignments lead to low understaffing,â€ said Marochko cited by RIA Novosti.\",\n-      \"Also, he doesnâ€™t exclude that the real numbers of conscripts in the Ukrainian army can be raised is necessary.\",\n-      \"In 2014-2017, Kyiv started so-called antiterrorist operation, that ws later changed to the united forces operation.\",\n-      \"This measure was supposed to strengthen the role of the Ukrainian military in settling the situation.\",\n-      \"In the late August 2018, the situation in Donbass escalated as the DNR head Aleksandr Zakharchenko was killed.\"\n-    ]\n-  },\n-  \"en-de\": {\n-    \"src\": [\n-      \"Welsh AMs worried about 'looking like muppets'\",\n-      \"There is consternation among some AMs at a suggestion their title should change to MWPs (Member of the Welsh Parliament).\",\n-      \"It has arisen because of plans to change the name of the assembly to the Welsh Parliament.\",\n-      \"AMs across the political spectrum are worried it could invite ridicule.\",\n-      \"One Labour AM said his group was concerned \\\"it rhymes with Twp and Pwp.\\\"\",\n-      \"For readers outside of Wales: In Welsh twp means daft and pwp means poo.\",\n-      \"A Plaid AM said the group as a whole was \\\"not happy\\\" and has suggested alternatives.\",\n-      \"A Welsh Conservative said his group was \\\"open minded\\\" about the name change, but noted it was a short verbal hop from MWP to Muppet.\"\n-    ],\n-    \"tgt\": [\n-      \"Walisische Ageordnete sorgen sich \\\"wie DÃ¶del auszusehen\\\"\",\n-      \"Es herrscht BestÃ¼rzung unter einigen Mitgliedern der Versammlung Ã¼ber einen Vorschlag, der ihren Titel zu MWPs (Mitglied der walisischen Parlament) Ã¤ndern soll.\",\n-      \"Der Grund dafÃ¼r waren PlÃ¤ne, den Namen der Nationalversammlung in Walisisches Parlament zu Ã¤ndern.\",\n-      \"Mitglieder aller Parteien der Nationalversammlung haben Bedenken, dass sie sich dadurch Spott aussetzen kÃ¶nnten.\",\n-      \"Ein Labour-Abgeordneter sagte, dass seine Gruppe \\\"sich mit Twp und Pwp reimt\\\".\",\n-      \"Hinweis fÃ¼r den Leser: â€žtwpâ€œ im Walisischen bedeutet â€žbescheuertâ€œ und â€žpwpâ€œ bedeutet â€žKackeâ€œ.\",\n-      \"Ein Versammlungsmitglied von Plaid Cymru sagte, die Gruppe als Ganzes sei \\\"nicht glÃ¼cklich\\\" und hat Alternativen vorgeschlagen.\",\n-      \"Ein walisischer Konservativer sagte, seine Gruppe wÃ¤re â€žoffenâ€œ fÃ¼r eine NamensÃ¤nderung, wies aber darauf hin, dass es von â€žMWPâ€œ (Mitglied des Walisischen Parlaments) nur ein kurzer verbaler Sprung zu â€žMuppetâ€œ ist.\"\n-    ]\n-  },\n-  \"de-en\": {\n-    \"src\": [\n-      \"SchÃ¶ne MÃ¼nchnerin 2018: SchÃ¶ne MÃ¼nchnerin 2018 in Hvar: Neun Dates\",\n-      \"Von az, aktualisiert am 04.05.2018 um 11:11\",\n-      \"Ja, sie will...\",\n-      \"\\\"SchÃ¶ne MÃ¼nchnerin\\\" 2018 werden!\",\n-      \"Am Nachmittag wartet erneut eine Ãœberraschung auf unsere Kandidatinnen: sie werden das romantische Candlelight-Shooting vor der MY SOLARIS nicht alleine bestreiten, sondern an der Seite von Male-Model Fabian!\",\n-      \"Hvar - Flirten, kokettieren, verfÃ¼hren - keine einfachen Aufgaben fÃ¼r unsere MÃ¤dchen.\",\n-      \"Insbesondere dann, wenn in Deutschland ein Freund wartet.\",\n-      \"Dennoch liefern die neun \\\"SchÃ¶ne MÃ¼nchnerin\\\"-Kandidatinnen beim Shooting mit People-Fotograf Tuan ab und trotzen Wind, Gischt und Regen wie echte Profis.\"\n-    ],\n-    \"tgt\": [\n-      \"The Beauty of Munich 2018: the Beauty of Munich 2018 in Hvar: Nine dates\",\n-      \"From A-Z, updated on 04/05/2018 at 11:11\",\n-      \"Yes, she wants to...\",\n-      \"to become \\\"The Beauty of Munich\\\" in 2018!\",\n-      \"In the afternoon there is another surprise waiting for our contestants: they will be competing for the romantic candlelight photo shoot at MY SOLARIS not alone, but together with a male-model Fabian!\",\n-      \"Hvar with its flirting, coquetting, and seduction is not an easy task for our girls.\",\n-      \"Especially when there is a boyfriend waiting in Germany.\",\n-      \"Despite dealing with wind, sprays and rain, the nine contestants of \\\"The Beauty of Munich\\\" behaved like real professionals at the photo shoot with People-photographer Tuan.\"\n-    ]\n-  }\n-}\n\\ No newline at end of file"
        },
        {
            "sha": "3eea3d95b8e1548803217cb4c69cc44358b1e9fb",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/test.source",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftest.source",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftest.source",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftest.source?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,20 +0,0 @@\n-UN Chief Says There Is No Military Solution in Syria Secretary-General Ban Ki-moon says his response to Russia's stepped up military support for Syria is that \"there is no military solution\" to the nearly five-year conflict and more weapons will only worsen the violence and misery for millions of people. The U.N. chief again urged all parties, including the divided U.N. Security Council, to unite and support inclusive negotiations to find a political solution. Ban told a news conference Wednesday that he plans to meet with foreign ministers of the five permanent council nations - the U.S., Russia, China, Britain and France - on the sidelines of the General Assembly's ministerial session later this month to discuss Syria.\n-He expressed regret that divisions in the council and among the Syrian people and regional powers \"made this situation unsolvable.\" Ban urged the five permanent members to show the solidarity and unity they did in achieving an Iran nuclear deal in addressing the Syria crisis. 8 Poll Numbers That Show Donald Trump Is For Real Some have tried to label him a flip-flopper. Others have dismissed him as a joke. And some are holding out for an implosion. But no matter how some Republicans are trying to drag Donald Trump down from atop the polls, it hasn't worked (yet).\n-Ten of the last 11 national polls have shown Donald Trump's lead at double digits, and some are starting to ask seriously what it means for the real estate mogul's nomination chances. Of course, it's still early in the election cycle. None of this is to say that Trump is likely to win the Republican nomination. Pundits point out that at this time in 2011, Rick Perry's lead was giving way to a rising Herman Cain, neither of whom won even one state in the nomination process. And there are many reasons he would struggle in a general election. But outside groups like Jeb Bush's Super PAC and the economic conservative group Club for Growth are recognizing Trump's staying power and beginning to unload their dollars to topple him.\n-Here are some recent poll numbers that suggest that the real estate mogul isn't just a passing phase: Trump's favorability ratings have turned 180 degrees. Right before Donald Trump announced his candidacy in mid-June, a Monmouth University poll showed only two in 10 Republicans had a positive view of the real estate mogul. By mid-July, it was 40 percent. In early August, it was 52 percent. Now, six in 10 Republicans have a favorable view of Donald Trump. Roughly three in 10 say they have a negative view. And these numbers hold up in early states. A Quinnipiac poll in Iowa last week found that 60 percent of Republicans there had a favorable view of Trump.\n-Two-thirds of GOP voters would be happy with Trump as the nominee. In a CNN/ORC poll last week, 67 percent of Republicans said they would be either \"enthusiastic\" or \"satisfied\" if Trump were the nominee. Only two in 10 say they would be \"upset\" if he were the nominee. Only Ben Carson generates roughly the same level of enthusiasm as Trump (43 percent say they would be \"enthusiastic\" vs. 40 percent who say the same of Trump). The next closest in enthusiasm? Marco Rubio with only 21 percent.\n-On the flip side, 47 percent of Republican voters say they would be \"dissatisfied\" or \"upset\" if establishment favorite Jeb Bush becomes the nominee. A majority of Republicans don't see Trump's temperament as a problem. While Donald Trump has been widely criticized for his bombast and insults, 52 percent of leaned Republican voters nationwide think that the real estate mogul has the right temperament to be president, according to Monday's ABC News/Washington Post poll. The same number holds in the first-in-the-nation caucus state of Iowa, where the same 52 percent of Republicans think he has the personality to be commander in chief, according to Quinnipiac last week.\n-Still, 44 percent think he doesn't have the personality to serve effectively, and almost six in 10 independents say his temperament does not belong in the White House, according to ABC/Post. Republican voters are getting used to the idea. When they put on their pundit hats, Republican voters think Trump is for real. When asked who is most likely to win the GOP nomination, four in 10 said Trump was the best bet, according to a CNN/ORC poll out last week. That's a change from when four in 10 placed their money on Jeb Bush in late July. Full disclosure: GOP voters haven't had the clearest crystal ball in the past.\n-At this time last cycle, four in 10 Republicans picked Rick Perry to win the nomination, vs. only 28 percent for eventual nominee Mitt Romney. Still, it shows that a plurality of GOP voters see Trump's campaign as plausible. Even if Republicans rallied around another candidate, Trump still beats almost everyone. Some pundits point out that the splintered field is likely contributing to Trump's lead, while anti-Trump support is be spread diffusely among more than a dozen other candidates. But a Monmouth University poll in early September shows that, in a hypothetical head-to-head matchup between Trump and most other Republican candidates, Trump almost always garners majority support.\n-He leads Carly Fiorina by 13 points, Marco Rubio by 14 points, Walker by 15 points, Jeb Bush by 19 points, and, finally, Rand Paul, John Kasich and Chris Christie by 33 points each. He's in a dead heat with Ted Cruz. The only candidate who beats him? Ben Carson would lead the businessman by a wide 19 points in a hypothetical head-to-head. A bare majority of Donald Trump's supporters say they've made up their minds. A new CBS/NYT poll out on Tuesday shows that just more than half of voters who support Trump say they have locked in their votes. Obviously, a lot can happen to change that, and no one can really say they would never change their mind.\n-46 percent said they are leaving the door open to switching candidates. Still, Trump's strongest competition at the moment is from fellow outsider neurosurgeon Ben Carson, but voters who say they have made up their minds are twice as likely to go for Trump. Six in 10 Republicans say they agree with Trump on immigration. Even since Donald Trump called immigrants from Mexico \"rapists\" in his campaign announcement speech two months ago, immigration has been front and center in the 2016 conversation. Some are worried that Trump's bombast will drive crucial Hispanic voters away from the Republican Party and damage rebranding efforts.\n-But according to Monday's new ABC/Post poll, six in 10 Republicans say they agree with Trump on immigration issues. So as long as immigration remains in the spotlight, it seems Donald Trump will remain too. Frustration with government is climbing to new highs. Donald Trump and Ben Carson now account for roughly half of the support from Republican voters, largely due to their outsider status. Six in 10 Republicans in Monday's new ABC/Post poll say they want a political outsider over someone with government experience. And they are angry at Washington, too.\n-A Des Moines Register/Bloomberg poll in Iowa from two weeks ago shows that three in four Iowa Republicans are frustrated with Republicans in Congress, with 54 percent \"unsatisfied\" and 21 percent \"mad as hell.\" Jeremy Corbyn to make debut at Prime Minister's Questions Since his election, Mr Corbyn's debut at PMQs has been keenly awaited New Labour leader Jeremy Corbyn is to make his debut at Prime Minister's Questions later, taking on David Cameron for the first time.\n-Mr Corbyn will rise to ask the first of his six allotted questions shortly after midday, with his performance likely to be closely scrutinised by the media and Labour MPs. He has called for \"less theatre and more facts\" at the weekly showpiece. He has also said he could skip some sessions, leaving them to colleagues. The encounter will be the first parliamentary test of Mr Corbyn's leadership, coming after his appointment of a shadow cabinet and his speech to the TUC annual congress on Tuesday.\n-Meanwhile, the Labour leader's decision to stand in silence during the singing of the national anthem at a service on Tuesday to mark the 75th anniversary of the Battle of Britain has attracted criticism from a number of Tory MPs and is the focus of several front page stories in the newspapers. Mr Corbyn's decision not to sing the national anthem has attracted attention A spokesman for Mr Corbyn said he had \"stood in respectful silence\" and did recognise the \"heroism of the Royal Air Force in the Battle of Britain.\"\n-But a member of Mr Corbyn's shadow cabinet, Owen Smith, told BBC Two's Newsnight programme he would have advised the Labour leader to sing the national anthem \"irrespective\" of his belief that the monarchy should be abolished. Nearly a dozen shadow ministers have refused to serve in Mr Corbyn's top team, citing differences over the economy, defence and foreign affairs, while less than a sixth of the parliamentary party originally backed him as leader. BBC political correspondent Robin Brant says policy differences are also \"stacking up\" within Labour following Mr Corbyn's appointment over its position on the European Union and the government's cap on benefits.\n-Mr Corbyn told the TUC conference Labour was putting forward amendments to remove the whole idea of a cap altogether. Hours later Mr Smith, the shadow work and pensions secretary, said the party was \"very clear\" that it was only opposing government plans to reduce the level of cap from Â£26,000 to Â£23,000. Mr Corbyn will be the fifth Labour leader that David Cameron has faced across the despatch box over the past decade since he became Tory leader. The Labour leader, who has promised a different approach to politics, says he has \"crowd sourced\" ideas for questions to ask Mr Cameron and has been given more than 30,000 suggestions.\n-The Islington North MP has said PMQs is too confrontational and that he will refrain from both \"repartee\" and trading barbs, instead vowing to focus on serious issues such as poverty, inequality and the challenges facing young people. Mr Corbyn has said that Angela Eagle, the shadow business secretary, will deputise for him at PMQs when he does not attend - for instance when Mr Cameron is travelling abroad. He has also floated the idea of allowing other colleagues to take the floor on occasion, saying he had approached the Commons Speaker John Bercow to discuss the issue.\n-When he became leader in 2005, Mr Cameron said he wanted to move away from the \"Punch and Judy\" style of politics often associated with PMQs but admitted some years later that he had failed. Since it was first televised in 1990, PMQs has been seen as a key barometer of a leader's judgement, their command of the Commons and their standing among their fellow MPs although critics have argued it has become a caricature and is in need of far-reaching reforms. 'Shot in Joburg': Homeless youth trained as photographers Downtown Johannesburg is a tough place to be homeless.\n-But one group of former street children have found a way to learn a skill and make a living. \"I was shot in Joburg\" is a non-profit studio that teaches homeless youngsters how to take photographs of their neighbourhood and make a profit from it. BBC News went to meet one of the project's first graduates. JD Sports boss says higher wages could hurt expansion JD Sports Executive Chairman Peter Cowgill says a higher minimum wage for UK workers could mean \"more spending power in the pockets of potential consumers.\" But that spending power is unlikely to outweigh the higher labour costs at his firm, he says.\n-The costs could hit JD Sports' expansion plans, he added, which could mean fewer extra jobs. Thanasi Kokkinakis backed by Tennis Australia president Steve Healy Thanasi Kokkinakis deserves kudos rather than criticism for his behaviour. Thanasi Kokkinakis has been the collateral damage in the recent storm around his friend Nick Kyrgios and deserves kudos rather than criticism for his own behaviour, according to Tennis Australia president Steve Healy.\n\\ No newline at end of file"
        },
        {
            "sha": "8c88fd05326fcfe503ef7eba77921c182758a290",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/test.target",
            "status": "removed",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftest.target",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftest.target",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftest.target?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,20 +0,0 @@\n-È˜eful ONU declarÄƒ cÄƒ nu existÄƒ soluÈ›ii militare Ã®n Siria Secretarul General Ban Ki-moon afirmÄƒ cÄƒ rÄƒspunsul sÄƒu la suportul militar al Rusiei pentru Siria este cÄƒ â€žnu existÄƒ o soluÈ›ie militarÄƒâ€ la conflictul care dureazÄƒ de aproape cinci ani iar mai multe arme nu ar face decÃ¢t sÄƒ agraveze violenÈ›a È™i suferinÈ›a a milioane de oameni. È˜eful ONU a solicitat din nou tuturor pÄƒrÈ›ilor, inclusiv Consiliului de securitate ONU divizat sÄƒ se unifice È™i sÄƒ susÈ›inÄƒ negocierile pentru a gÄƒsi o soluÈ›ie politicÄƒ. Ban a declarat miercuri Ã®n cadrul unei conferinÈ›e cÄƒ intenÈ›ioneazÄƒ sÄƒ se Ã®ntÃ¢lneascÄƒ luna aceasta cu miniÈ™trii de externe din cinci È›Äƒri permanent prezente Ã®n consiliu - SUA, Rusia, China, Anglia È™i FranÈ›a - pe marginea sesiunii ministeriale a AdunÄƒrii Generale pentru a discuta despre Siria.\n-Ban È™i-a exprimat regretul cÄƒ divizÄƒrile Ã®n consiliu È™i Ã®ntre poporul sirian È™i puterile regionale â€žau fÄƒcut aceastÄƒ situaÈ›ie de nerezolvatâ€. Ban le-a cerut celor cinci membri permanenÈ›i sÄƒ dea dovadÄƒ de solidaritatea È™i unitatea arÄƒtate atunci cÃ¢nd au reuÈ™it sÄƒ Ã®ncheie un acord referitor la armele nucleare ale Iranului, abordÃ¢nd astfel criza din Siria. 8 cifre din sondaje care aratÄƒ cÄƒ Donald Trump are È™anse reale Unii au Ã®ncercat sÄƒ Ã®l eticheteze ca politician â€žflip-flopâ€. AlÈ›ii l-au numit o glumÄƒ. Iar alÈ›ii aÈ™teaptÄƒ implozia. ÃŽnsÄƒ indiferent de modul Ã®n care unii republicani Ã®ncearcÄƒ sÄƒ Ã®l dÄƒrÃ¢me pe Donald Trump din vÃ¢rful sondajelor, nu a funcÈ›ionat (Ã®ncÄƒ).\n-Zece din ultimele 11 sondaje naÈ›ionale au arÄƒtat cÄƒ Donald Trump conduce cu un procent din douÄƒ cifre iar unele voci Ã®ncep sÄƒ se Ã®ntrebe serios ce Ã®nseamnÄƒ acest lucru pentru È™ansele de numire ale mogulului imobiliar. Desigur, este Ã®ncÄƒ prematur. Nimic din toate acestea nu spune cÄƒ Trump va cÃ¢È™tiga cursa pentru nominalizarea republicanilor. Pundits aratÄƒ cÄƒ, Ã®n aceeaÈ™i perioadÄƒ a anului 2011, avansul lui Rick Perry Ã®i fÄƒcea loc lui Herman Cain Ã®n sondaje, dar niciunul dintre ei nu a cÃ¢È™tigat Ã®n vreun stat Ã®n cursa de nominalizare. Iar motivele pentru care s-ar lupta din greu la alegerile generale sunt numeroase. ÃŽnsÄƒ grupurile din exterior precum Super PAC al lui Jeb Bush È™i grupul conservator economic Club for Growth admit puterea lui Trump È™i Ã®ncep sÄƒ Ã®l susÈ›inÄƒ cu bani.\n-ÃŽn continuare vÄƒ prezentÄƒm cÃ¢teva cifre din sondaje recente care sugereazÄƒ cÄƒ mogulul imobiliar nu este doar ceva trecÄƒtor: Cifrele care indicÄƒ susÈ›inerea faÈ›Äƒ de Trump s-au Ã®ntors la 180 grade. Chiar Ã®nainte ca Donald Trump sÄƒ Ã®È™i anunÈ›e candidatura, la mijlocul lui iunie, un sondaj realizat de Universitatea din Monmouth arÄƒta cÄƒ doar doi din 10 republicani aveau o pÄƒrere pozitivÄƒ despre mogulul imobiliar. PÃ¢nÄƒ la mijlocul lui iulie, procentul a urcat la 40%. La Ã®nceputul lui august, era 52%. ÃŽn prezent, È™ase din 10 republicani au o pÄƒrere favorabilÄƒ despre Donald Trump. Aproximativ trei din 10 declarÄƒ cÄƒ au o pÄƒrere negativÄƒ. Aceste cifre se menÈ›in. Un sondaj realizat sÄƒptÄƒmÃ¢na trecutÄƒ de Quinnipiac Ã®n Iowa a concluzionat cÄƒ 60% dintre republicanii din regiune au o pÄƒrere favorabilÄƒ despre Trump.\n-DouÄƒ treimi dintre alegÄƒtorii GOP ar fi fericiÈ›i dacÄƒ Trump ar cÃ¢È™tiga cursa pentru nominalizare. ÃŽntr-un sondaj realizat sÄƒptÄƒmÃ¢na trecutÄƒ de CNN/ORC, 67% dintre republicani au declarat cÄƒ ar fi â€žentuziasmaÈ›iâ€ sau â€žmulÈ›umiÈ›iâ€ dacÄƒ Trump ar cÃ¢È™tiga cursa pentru nominalizare. Doar doi din 10 declarÄƒ cÄƒ ar fi â€žsupÄƒraÈ›iâ€ dacÄƒ Trump ar cÃ¢È™tiga cursa pentru nominalizare. Doar Ben Carson genereazÄƒ aproximativ acelaÈ™i nivel de entuziasm ca Trump (43% declarÄƒ cÄƒ ar fi â€žentuziasmaÈ›iâ€ faÈ›Äƒ de 40% care declarÄƒ acelaÈ™i lucru despre Trump). Cel mai aproape Ã®n ceea ce priveÈ™te entuziasmul? Marco Rubio, cu doar 21%.\n-De partea cealaltÄƒ, 47% dintre alegÄƒtorii republicani afirmÄƒ cÄƒ ar fi â€žnemulÈ›umiÈ›iâ€ sau â€žsupÄƒraÈ›iâ€ dacÄƒ favoritul Jeb Bush cÃ¢È™tigÄƒ cursa pentru nominalizare. Majoritatea republicanilor nu considerÄƒ temperamentul lui Trump o problemÄƒ. DeÈ™i Donald Trump a fost puternic criticat pentru insultele aduse È™i stilul sÄƒu bombastic, 52% dintre alegÄƒtorii republicani la nivel naÈ›ional considerÄƒ cÄƒ mogulul imobiliar are temperamentul potrivit pentru a fi preÈ™edinte, conform sondajului realizat luni de ABC News/Washington Post. RegÄƒsim aceleaÈ™i cifre Ã®n statul Iowa, unde tot 52% dintre republicani cred cÄƒ Trump are personalitatea potrivitÄƒ pentru a fi conducÄƒtor, conform sondajului realizat sÄƒptÄƒmÃ¢na trecutÄƒ de Quinnipiac.\n-TotuÈ™i, 44% sunt de pÄƒrere cÄƒ nu are personalitatea necesarÄƒ pentru a acÈ›iona eficient È™i aproape È™ase din 10 independenÈ›i afirmÄƒ cÄƒ temperamentul sÄƒu nu are ce cÄƒuta la Casa AlbÄƒ, conform ABC/Post. AlegÄƒtorii republicani se obiÈ™nuiesc cu ideea. Atunci cÃ¢nd iau atitudinea de intelectuali, alegÄƒtorii republicani considerÄƒ cÄƒ Trump este autentic. Conform unui sondaj realizat sÄƒptÄƒmÃ¢na trecutÄƒ de CNN/ORC, la Ã®ntrebarea cine are cele mai multe È™anse sÄƒ cÃ¢È™tige cursa pentru nominalizare GOP, patru din 10 au declarat cÄƒ Trump. SituaÈ›ia s-a schimbat faÈ›Äƒ de finalul lui iulie, cÃ¢nd patru din 10 ar fi pariat pe Jeb Bush. Informare completÄƒ: Ã®n trecut, alegÄƒtorii GOP nu au citit foarte bine viitorul.\n-ÃŽn aceeaÈ™i perioadÄƒ a ultimelor alegeri, patru din 10 republicani l-au ales pe Rick Perry Ã®n cursa pentru nominalizare, faÈ›Äƒ de doar 28% pentru Mitt Romney. ÃŽnsÄƒ, aceste cifre aratÄƒ cÄƒ majoritatea alegÄƒtorilor GOP considerÄƒ plauzibilÄƒ campania lui Trump. Chiar dacÄƒ republicanii sau repliat spre un alt candidat. Trump Ã®ncÄƒ se aflÄƒ Ã®n fruntea tuturor. Unele voci spun cÄƒ situaÈ›ia divizatÄƒ va contribui probabil la victoria lui Trump, Ã®n timp ce susÈ›inerea contra lui Trump se va Ã®mpÄƒrÈ›i la mai mult de doisprezece candidaÈ›i. ÃŽnsÄƒ un sondaj derulat la Ã®nceputul lui septembrie de Universitatea din Monmouth aratÄƒ cÄƒ, Ã®n situaÈ›ia ipoteticÄƒ a unei colaborÄƒri Ã®ntre Trump È™i majoritatea celorlalÈ›i candidaÈ›i republicani, aproape Ã®ntotdeauna Trump va beneficia de susÈ›inerea majoritarÄƒ.\n-Trump se aflÄƒ la distanÈ›Äƒ de 13 puncte de Carly Fiorina, la 14 puncte de Marco Rubio, la 15 puncte de Walker, la 19 puncte de Jeb Bush È™i, Ã®n cele din urmÄƒ, la cÃ¢te 33 de puncte faÈ›Äƒ de Rand Paul, John Kasich È™i Chris Christie. Este aproape la egalitate cu Ted Cruz. Singurul candidat care Ã®l Ã®nvinge? Ben Carson l-ar Ã®nvinge pe omul de afaceri cu 19 puncte Ã®ntr-o confruntare ipoteticÄƒ de unu la unu. Majoritatea susÈ›inÄƒtorilor lui Donald Trump declarÄƒ cÄƒ s-au decis. Un nou sondaj realizat marÈ›i de CBS/NYT aratÄƒ cÄƒ peste jumÄƒtate dintre alegÄƒtorii care Ã®l susÈ›in pe Trump declarÄƒ cÄƒ nu Ã®È™i schimbÄƒ opÈ›iunea de vot. Evident, se pot Ã®ntÃ¢mpla multe Ã®n acest sens È™i nimeni nu poate spune cÄƒ aceÈ™tia nu se vor rÄƒzgÃ¢ndi niciodatÄƒ.\n-46% afirmÄƒ cÄƒ lasÄƒ portiÈ›a deschisÄƒ posibilitÄƒÈ›ii de a-È™i schimba opÈ›iunea. Cu toate acestea, cel mai important adversar al lui Trump este Ã®n prezent neurochirurgul Ben Carson, Ã®nsÄƒ este de douÄƒ ori mai probabil ca alegÄƒtorii care declarÄƒ cÄƒ s-au decis sÄƒ voteze cu Trump. È˜ase din 10 republicani afirmÄƒ cÄƒ sunt de acord cu Trump Ã®n problema imigrÄƒrii. De cÃ¢nd Donald Trump i-a numit pe imigranÈ›ii din Mexic â€žviolatoriâ€ Ã®n discursul de deschidere a campaniei sale, Ã®n urmÄƒ cu douÄƒ luni, imigrarea a fost subiectul central Ã®n campania pentru 2016. Unii sunt Ã®ngrijoraÈ›i cÄƒ stilul bombastic al lui Trump va duce la o scindare Ã®ntre alegÄƒtorii hispanici importanÈ›i È™i Partidul Republican È™i va prejudicia eforturile de rebranding.\n-ÃŽnsÄƒ, conform sondajului realizat luni de ABC/Post, È™ase din 10 republicani afirmÄƒ cÄƒ sunt de acord cu Trump Ã®n problema imigrÄƒrii. AÈ™a cÄƒ, se pare cÄƒ atÃ¢ta timp cÃ¢t problema imigrÄƒrii rÄƒmÃ¢ne Ã®n lumina reflectoarelor, la fel va rÄƒmÃ¢ne È™i Doland Trump. Frustrarea faÈ›Äƒ de autoritÄƒÈ›i atinge noi culmi. Donald Trump È™i Ben Carson sunt acum susÈ›inuÈ›i de aproape jumÄƒtate dintre alegÄƒtorii republicani, Ã®n mare parte datoritÄƒ statutului lor de outsideri. Conform sondajului realizat luni de ABC/Post, È™ase din 10 republicani afirmÄƒ cÄƒ preferÄƒ un outsider politic Ã®n detrimentul cuiva cu experienÈ›Äƒ Ã®n guvernare. Oamenii sunt de asemenea supÄƒraÈ›i pe autoritÄƒÈ›ile de la Washington.\n-Un sondaj derulat Ã®n urmÄƒ cu douÄƒ sÄƒptÄƒmÃ¢ni Ã®n Iowa de cÄƒtre Des Moines Register/Bloomberg aratÄƒ cÄƒ trei din patru republicani din Iowa sunt frustraÈ›i de prestaÈ›ia republicanilor din COngres, 54% declarÃ¢ndu-se â€žnemulÈ›umiÈ›iâ€ iar 21% â€žnervoÈ™i la culmeâ€. Jeremy Corbyn Ã®È™i face debutul la Prime Minister's Questions ÃŽncÄƒ de la alegerea sa, debutul domnului Corbyn la PMQs a fost Ã®ndelung aÈ™teptat Noul lider al Partidului Laburist, Jeremy Corbyn, Ã®È™i va face mai tÃ¢rziu debutul la Prime Minister's Questions, confruntÃ¢ndu-se pentru prima datÄƒ cu David Cameron.\n-Dl Corbyn va adresa primele dintre cele È™ase Ã®ntrebÄƒri la care are dreptul la scurt timp dupÄƒ prÃ¢nz; prestaÈ›ia sa va fi probabil analizatÄƒ Ã®ndeaproape de mass-media È™i parlamentarii laburiÈ™ti. ÃŽn cadrul apariÈ›iilor sÄƒptÄƒmÃ¢nale, el a cerut â€žmai puÈ›in teatru È™i mai multe fapteâ€. A declarat de asemenea cÄƒ poate renunÈ›a la cÃ¢teva participÄƒri È™i cÄƒ le cedeazÄƒ colegilor sÄƒi. Confruntarea va fi primul test parlamentar al Dl Corbyn Ã®n poziÈ›ie de lider, venind dupÄƒ ce a numit un â€žcabinet fantomÄƒâ€ È™i dupÄƒ discursul pe care l-a È›inut marÈ›i la congresul anual TUC.\n-ÃŽntre timp, decizia liderului Partidului laburist de a pÄƒstra tÄƒcerea la rostirea imnului naÈ›ional Ã®n cadrul unei slujbe È›inute marÈ›i cu ocazia aniversÄƒrii a 75 de ani de la BÄƒtÄƒlia Angliei a atras critici din partea unor parlamentari conservatori È™i a È›inut prima paginÄƒ a ziarelor. Decizia domnului Corbyn de a nu cÃ¢nta imnul naÈ›ional a atras atenÈ›ia Un purtÄƒtor de cuvÃ¢nt al Dl Corbyn a declarat cÄƒ acesta â€ža pÄƒstrat tÄƒcerea Ã®n mod respectuosâ€ È™i a recunoscut â€žeroismul ForÈ›elor aeriene britanice Ã®n BÄƒtÄƒlia Angliei.â€\n-ÃŽnsÄƒ un membru al cabinetului fantomÄƒ al Dl Corbyn, Owen Smith, a declarat pentru emisiunea Two's Newsnight transmisÄƒ de BBC cÄƒ i-ar fi recomandat liderului laburist sÄƒ cÃ¢nte imnul naÈ›ional â€žindiferentâ€ de credinÈ›a sa cÄƒ monarhia ar trebui abolitÄƒ. ÃŽn jur de doisprezece miniÈ™tri din cabinetul fantomÄƒ au refuzat sÄƒ facÄƒ parte din echipa de frunte a Dl Corbyn, argumentÃ¢nd prin diferenÈ›e de opinie legate de economie, apÄƒrare È™i externe, Ã®n timp ce mai puÈ›in de o È™esime din partidul parlamentar l-a susÈ›inut ca lider. Corespondentul politic al BBC, Robin Brant, declarÄƒ cÄƒ diferenÈ›ele de politicÄƒ â€žse cumuleazÄƒâ€ Ã®n Partidul Laburist dupÄƒ numirea domnului Corbyn referitor la poziÈ›ia sa faÈ›Äƒ de Uniunea EuropeanÄƒ È™i limita de beneficii.\n-Dl Corbyn a declarat la conferinÈ›a TUC cÄƒ Partidul Laburist va aduce modificÄƒri prin care se va elimina integral ideea limitÄƒrii. CÃ¢teva ore mai tÃ¢rziu, Dl Smith, Ministrul Muncii È™i Pensiilor, a declarat cÄƒ partidul â€žeste foarte clarâ€ Ã®n opoziÈ›ia exclusivÄƒ faÈ›Äƒ de planurile guvernului de a reduce nivelul â€žcapâ€ de la 26.000 lire la 23.000 lire. Dl Corbyn va fi al cincilea lider laburist cu care se confruntÄƒ David Cameron la tribunÄƒ Ã®n ultimul deceniu, de cÃ¢nd a preluat conducerea Partidului Conservator. Liderul laburist, care a promis o abordare diferitÄƒ a politicii, spune cÄƒ are idei â€ždin surse externeâ€ pentru Ã®ntrebÄƒri pe care sÄƒ i le adreseze Domnului Cameron È™i cÄƒ a primit peste 30.000 de sugestii.\n-Parlamentarul Islington North a afirmat cÄƒ PMQs implicÄƒ un nivel de confruntare prea Ã®nalt È™i cÄƒ se va abÈ›ine de la replici È™i atacuri, angajÃ¢ndu-se sÄƒ se concentreze Ã®n schimb pe probleme serioase precum sÄƒrÄƒcia, inegalitatea È™i provocÄƒrile cu care se confruntÄƒ tinerii. Dl Corbyn a declarat cÄƒ Angela Eagle, Ministrul de finanÈ›e, Ã®i va È›ine locul la PMQs atunci cÃ¢nd el nu poate participa - de exemplu atunci cÃ¢nd Dl Cameron se deplaseazÄƒ Ã®n strÄƒinÄƒtate. A exprimat de asemenea ideea cÄƒ va permite altor colegi sÄƒ ia cuvÃ¢ntul ocazional, spunÃ¢nd cÄƒ l-a abordat pe PreÈ™edintele Camerei DeputaÈ›ilor, John Bercow, pentru a discuta acest aspect.\n-ÃŽn 2005, cÃ¢nd a preluat conducerea, Dl Cameron a declarat cÄƒ doreÈ™te sÄƒ renunÈ›e la stilul politic â€žPunch and Judyâ€ asociat adesea cu PMQs Ã®nsÄƒ a recunoscut cÃ¢È›iva ani mai tÃ¢rziu cÄƒ nu a reuÈ™it Ã®n demersul sÄƒu. De la prima transmisie, Ã®n 1990, PMQs a fost consideratÄƒ un barometru cheie al raÈ›ionamentului unui lider, al modului Ã®n care acesta conduce Camera DeputaÈ›ilor È™i a poziÈ›iei sale Ã®n rÃ¢ndul colegilor parlamentari, deÈ™i criticii afirmÄƒ a ca devenit o caricaturÄƒ È™i cÄƒ are nevoie de o reformare profundÄƒ. â€žCadru Ã®n Joburgâ€: Tineri fÄƒrÄƒ adÄƒpost beneficiazÄƒ de cursuri de fotografie Este dificil sÄƒ fii un om fÄƒrÄƒ adÄƒpost Ã®n Johannesburg.\n-ÃŽnsÄƒ un grup de oameni care au trÄƒit pe strÄƒzi Ã®n copilÄƒrie au gÄƒsit un mod de a Ã®nvÄƒÈ›a o meserie È™i de a-È™i cÃ¢È™tiga traiul. â€žI was shot Ã®n Joburgâ€ este un studio non-profit care Ã®i Ã®nvaÈ›Äƒ pe tinerii fÄƒrÄƒ adÄƒpost sÄƒ facÄƒ fotografii ale zonelor Ã®n care trÄƒiesc È™i sÄƒ cÃ¢È™tige bani din asta. BBC News s-a Ã®ntÃ¢lnit cu unul dintre primii absolvenÈ›i ai proiectului. È˜eful JD Sports spune cÄƒ salariile mai mari ar putea dÄƒuna extinderii PreÈ™edintele JD Sports, Peter Cowgill, declarÄƒ cÄƒ o creÈ™tere a salariului minim Ã®n Marea Britanie ar putea Ã®nsemna â€žo putere de cumpÄƒrare mai mare Ã®n buzunarele potenÈ›ialilor consumatori.â€ Este Ã®nsÄƒ puÈ›in probabil ca respectiva putere de cumpÄƒrare sÄƒ depÄƒÈ™eascÄƒ costurile mai mari pentru forÈ›a de muncÄƒ Ã®n cadrul firmei, afirmÄƒ el.\n-Costurile ar putea avea impact asupra planurilor de extindere ale JD Sports, a adÄƒugat el, ceea ce ar putea Ã®nsemna mai puÈ›ine locuri de muncÄƒ noi. Thanasi Kokkinakis susÈ›inut de preÈ™edintele Tennis Australia, Steve Healy Thanasi Kokkinakis ar merita sÄƒ fie lÄƒudat È™i nu criticat pentru comportamentul sÄƒu. Thanasi Kokkinakis a fost victimÄƒ colateralÄƒ Ã®n â€žfurtunaâ€ creatÄƒ Ã®n jurul prietenului sÄƒu, Nick Kyrgios, iar comportamentul sÄƒu meritÄƒ mai degrabÄƒ cuvinte de laudÄƒ È™i nu criticÄƒ, Ã®n opinia preÈ™edintelui Tennis Australia, Steve Healy.\n\\ No newline at end of file"
        },
        {
            "sha": "33ce003c8ae3139914a389a714812a2ab13aece4",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/train.len",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.len",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.len",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.len?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc"
        },
        {
            "sha": "d77722d4a57002e81b832fc94a326fc4acebb0d8",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/train.source",
            "status": "removed",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.source",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.source",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.source?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,11 +0,0 @@\n-Corrections to votes and voting intentions: see Minutes Assignment conferred on a Member: see Minutes Membership of committees and delegations: see Minutes Decisions concerning certain documents: see Minutes Forwarding of texts adopted during the sitting: see Minutes Dates for next sittings: see Minutes\n-Membership of Parliament: see Minutes Approval of Minutes of previous sitting: see Minutes Membership of Parliament: see Minutes Verification of credentials: see Minutes Documents received: see Minutes Written statements and oral questions (tabling): see Minutes Petitions: see Minutes Texts of agreements forwarded by the Council: see Minutes Action taken on Parliament's resolutions: see Minutes Agenda for next sitting: see Minutes Closure of sitting (The sitting was closed at 7.45 p.m.)\n-Election of Vice-Presidents of the European Parliament (deadline for submitting nominations): see Minutes (The sitting was suspended at 12.40 p.m. and resumed at 3.00 p.m.) Election of Quaestors of the European Parliament (deadline for submitting nominations): see Minutes (The sitting was suspended at 3.25 p.m. and resumed at 6.00 p.m.) Agenda for next sitting: see Minutes Closure of sitting (The sitting was closed at 6.15 p.m.) Opening of the sitting (The sitting was opened at 9.35 a.m.) Documents received: see Minutes Approval of Minutes of previous sitting: see Minutes Membership of Parliament: see Minutes\n-Membership of committees (deadline for tabling amendments): see Minutes (The sitting was suspended at 7 p.m. and resumed at 9 p.m.) Agenda for next sitting: see Minutes Closure of sitting (The sitting was suspended at 23.25 p.m.) Documents received: see Minutes Communication of Council common positions: see Minutes (The sitting was suspended at 11.35 a.m. and resumed for voting time at noon) Approval of Minutes of previous sitting: see Minutes Committee of Inquiry into the crisis of the Equitable Life Assurance Society (extension of mandate): see Minutes\n-Announcement by the President: see Minutes 1. Membership of committees (vote) 2. Amendment of the ACP-EC Partnership Agreement (vote) 4. Certification of train drivers operating locomotives and trains on the railway system in the Community (vote) 6. Law applicable to non-contractual obligations (\"ROME II\") (vote) 8. Seventh and eighth annual reports on arms exports (vote) Corrections to votes and voting intentions: see Minutes Membership of committees and delegations: see Minutes Request for waiver of parliamentary immunity: see Minutes Decisions concerning certain documents: see Minutes\n-Written statements for entry\n-Written statements for entry in the register (Rule 116): see Minutes Forwarding of texts adopted during the sitting: see Minutes Dates for next sittings: see Minutes Adjournment of the session I declare the session of the European Parliament adjourned. (The sitting was closed at 1 p.m.) Approval of Minutes of previous sitting: see Minutes Membership of Parliament: see Minutes Request for the defence of parliamentary immunity: see Minutes Appointments to committees (proposal by the Conference of Presidents): see Minutes Documents received: see Minutes Texts of agreements forwarded by the Council: see Minutes\n-Action taken on Parliament's resolutions: see Minutes Oral questions and written statements (tabling): see Minutes Written statements (Rule 116): see Minutes Agenda: see Minutes 1. Appointments to parliamentary committees (vote): see Minutes Voting time Agenda for next sitting: see Minutes Closure of sitting (The sitting was closed at 12 midnight) Opening of the sitting (The sitting was opened at 09.05) Documents received: see Minutes Approval of Minutes of previous sitting: see Minutes 1. Protection of passengers against displaced luggage (vote) 2.\n-Approval of motor vehicles with regard to the forward field of vision of the driver (vote) 3. EC-Korea Agreement on scientific and technological cooperation (vote) 4. Mainstreaming sustainability in development cooperation policies (vote) 5. Draft Amending Budget No 1/2007 (vote) 7. EC-Gabon Fisheries Partnership (vote) 10. Limitation periods in cross-border disputes involving personal injuries and fatal accidents (vote) 12. Strategy for a strengthened partnership with the Pacific Islands (vote) 13. The European private company statute (vote) That concludes the vote.\n-Corrections to votes and voting intentions: see Minutes Assignment conferred on a Member: see Minutes Membership of committees and delegations: see Minutes Decisions concerning certain documents: see Minutes Forwarding of texts adopted during the sitting: see Minutes Dates for next sittings: see Minutes\n-Written statements for entry"
        },
        {
            "sha": "f18d80d3d47d6cae112d7f705effdb26beeb1efe",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/train.target",
            "status": "removed",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.target",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.target",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Ftrain.target?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,11 +0,0 @@\n-CorectÄƒrile voturilor ÅŸi intenÅ£iile de vot: a se vedea procesul-verbal Misiune Ã®ncredinÅ£atÄƒ unui deputat: consultaÅ£i procesul-verbal ComponenÅ£a comisiilor ÅŸi a delegaÅ£iilor: a se vedea procesul-verbal Decizii privind anumite documente: a se vedea procesul-verbal Transmiterea textelor adoptate Ã®n cursul prezentei ÅŸedinÅ£e: a se vedea procesul-verbal Calendarul urmÄƒtoarelor ÅŸedinÅ£e: a se vedea procesul-verbal\n-ComponenÅ£a Parlamentului: a se vedea procesul-verbal Aprobarea procesului-verbal al ÅŸedinÅ£ei precedente: a se vedea procesul-verbal ComponenÅ£a Parlamentului: a se vedea procesul-verbal Verificarea prerogativelor: a se vedea procesul-verbal Depunere de documente: a se vedea procesul-verbal DeclaraÅ£ii scrise ÅŸi Ã®ntrebÄƒri orale (depunere): consultaÅ£i procesul-verbal PetiÅ£ii: a se vedea procesul-verbal Transmiterea de cÄƒtre Consiliu a textelor acordurilor: a se vedea procesul-verbal Cursul dat rezoluÅ£iilor Parlamentului: a se vedea procesul-verbal Ordinea de zi a urmÄƒtoarei ÅŸedinÅ£e: a se vedea procesul-verbal Ridicarea ÅŸedinÅ£ei (Se levanta la sesiÃ³n a las 19.45 horas)\n-Alegerea vicepreÅŸedinÅ£ilor Parlamentului European (termenul de depunere a candidaturilor): consultaÅ£i procesul-verbal (Die Sitzung wird um 12.40 Uhr unterbrochen und um 15.00 Uhr wiederaufgenommen). Alegerea chestorilor Parlamentului European (termenul de depunere a candidaturilor): consultaÅ£i procesul-verbal (Die Sitzung wird um 15.25 Uhr unterbrochen und um 18.00 Uhr wiederaufgenommen). Ordinea de zi a urmÄƒtoarei ÅŸedinÅ£e: a se vedea procesul-verbal Ridicarea ÅŸedinÅ£ei (Die Sitzung wird um 18.15 Uhr geschlossen.) Deschiderea ÅŸedinÅ£ei (Die Sitzung wird um 9.35 Uhr erÃ¶ffnet.) Depunerea documentelor: a se vedea procesul-verbal Aprobarea procesului-verbal al ÅŸedinÅ£ei precedente: a se vedea procesul-verbal ComponenÅ£a Parlamentului: a se vedea procesul-verbal\n-ComponenÅ£a comisiilor (termenul de depunere a amendamentelor): consultaÅ£i procesul-verbal (La seduta, sospesa alle 19.00, Ã¨ ripresa alle 21.00) Ordinea de zi a urmÄƒtoarei ÅŸedinÅ£e: a se vedea procesul-verbal Ridicarea ÅŸedinÅ£ei (Die Sitzung wird um 23.25 Uhr geschlossen.) Depunerea documentelor: a se vedea procesul-verbal Comunicarea poziÅ£iilor comune ale Parlamentului: a se vedea procesul-verbal (La sÃ©ance, suspendue Ã  11h35 dans l'attente de l'Heure des votes, est reprise Ã  midi) Aprobarea procesului-verbal al ÅŸedinÅ£ei precedente: a se vedea procesul-verbal Comisia de anchetÄƒ privind criza societÄƒÅ£ii de asigurÄƒri \"Equitable Lifeâ€ (prelungirea mandatului): consultaÅ£i procesul-verbal\n-Comunicarea PreÅŸedintelui: consultaÅ£i procesul-verbal 1. ComponenÅ£a comisiilor (vot) 2. Modificarea Acordului de parteneriat ACP-CE (\"Acordul de la Cotonouâ€) (vot) 4. Certificarea mecanicilor de locomotivÄƒ care conduc locomotive ÅŸi trenuri Ã®n sistemul feroviar comunitar (vot) 6. Legea aplicabilÄƒ obligaÅ£iilor necontractuale (\"Roma IIâ€) (vot) 8. Al ÅŸaptelea ÅŸi al optulea raport anual privind exportul de armament (vot) CorectÄƒrile voturilor ÅŸi intenÅ£iile de vot: a se vedea procesul-verbal ComponenÅ£a comisiilor ÅŸi a delegaÅ£iilor: a se vedea procesul-verbal Cerere de ridicare a imunitÄƒÅ£ii parlamentare: consultaÅ£i procesul-verbal Decizii privind anumite documente: a se vedea procesul-verbal\n-DeclaraÅ£ii scrise Ã®nscrise\n-DeclaraÅ£ii scrise Ã®nscrise Ã®n registru (articolul 116 din Regulamentul de procedurÄƒ): a se vedea procesul-verbal Transmiterea textelor adoptate Ã®n cursul prezentei ÅŸedinÅ£e: a se vedea procesul-verbal Calendarul urmÄƒtoarelor ÅŸedinÅ£e: a se vedea procesul-verbal ÃŽntreruperea sesiunii Dichiaro interrotta la sessione del Parlamento europeo. (La seduta Ã¨ tolta alle 13.00) Aprobarea procesului-verbal al ÅŸedinÅ£ei precedente: a se vedea procesul-verbal ComponenÅ£a Parlamentului: a se vedea procesul-verbal Cerere de apÄƒrare a imunitÄƒÅ£ii parlamentare: consultaÅ£i procesul-verbal Numiri Ã®n comisii (propunerea ConferinÅ£ei preÅŸedinÅ£ilor): consultaÅ£i procesul-verbal Depunerea documentelor: a se vedea procesul-verbal Transmiterea de cÄƒtre Consiliu a textelor acordurilor: a se vedea procesul-verbal\n-ContinuÄƒri ale rezoluÅ£iilor Parlamentului: consultaÅ£i procesul-verbal DeclaraÅ£ii scrise ÅŸi Ã®ntrebÄƒri orale (depunere): consultaÅ£i procesul-verbal DeclaraÅ£ii scrise (articolul 116 din Regulamentul de procedurÄƒ) Ordinea de zi: a se vedea procesul-verbal 1. Numiri Ã®n comisiile parlamentare (vot): consultaÅ£i procesul-verbal Timpul afectat votului Ordinea de zi a urmÄƒtoarei ÅŸedinÅ£e: a se vedea procesul-verbal Ridicarea ÅŸedinÅ£ei (La seduta Ã¨ tolta alle 24.00) Deschiderea ÅŸedinÅ£ei (The sitting was opened at 09.05) Depunerea documentelor: a se vedea procesul-verbal Aprobarea procesului-verbal al ÅŸedinÅ£ei precedente: a se vedea procesul-verbal 1. ProtecÅ£ia pasagerilor Ã®mpotriva deplasÄƒrii bagajelor (vot) 2.\n-Omologarea vehiculelor cu motor cu privire la cÃ¢mpul de vizibilitate Ã®nainte al conducÄƒtorului auto (vot) 3. Acordul CE-Coreea de cooperare ÅŸtiinÅ£ificÄƒ ÅŸi tehnologicÄƒ (vot) 4. Integrarea durabilitÄƒÅ£ii Ã®n politicile de cooperare pentru dezvoltare (vot) 5. Proiect de buget rectificativ nr.1/2007 (vot) 7. Acordul de parteneriat Ã®n domeniul pescuitului Ã®ntre Comunitatea EuropeanÄƒ ÅŸi Republica GabonezÄƒ (vot) 10. Termenele de prescripÅ£ie aplicabile Ã®n cadrul litigiilor transfrontaliere cu privire la vÄƒtÄƒmÄƒrile corporale ÅŸi accidentele mortale (vot) 12. RelaÅ£iile UE cu insulele din Pacific: Strategie pentru un parteneriat consolidat (vot) 13. Statutul societÄƒÅ£ii private europene (vot) Damit ist die Abstimmungsstunde beendet.\n-CorectÄƒrile voturilor ÅŸi intenÅ£iile de vot: a se vedea procesul-verbal Misiune Ã®ncredinÅ£atÄƒ unui deputat: consultaÅ£i procesul-verbal ComponenÅ£a comisiilor ÅŸi a delegaÅ£iilor: a se vedea procesul-verbal Decizii privind anumite documente: a se vedea procesul-verbal Transmiterea textelor adoptate Ã®n cursul prezentei ÅŸedinÅ£e: a se vedea procesul-verbal Calendarul urmÄƒtoarelor ÅŸedinÅ£e: a se vedea procesul-verbal\n-DeclaraÅ£ii scrise Ã®nscrise"
        },
        {
            "sha": "897314a960b28d927b597805693e63f9de71d903",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/val.len",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.len",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.len",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.len?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc"
        },
        {
            "sha": "c895d0ae247e2bc529ae4f94be6079cd36f50fa2",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/val.source",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.source",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.source",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.source?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,16 +0,0 @@\n-Brazil's Former Presidential Chief-of-Staff to Stand Trial A federal judge on Tuesday accepted the charges filed against Brazil's former presidential chief of staff for his alleged involvement in a massive corruption scheme at state-owned oil company Petrobras. The federal prosecutor's office said Jose Dirceu will face trial on the corruption, racketeering and money laundering charges filed earlier this month. Fourteen other people will also be tried, including Joao Vaccari Neto, the former treasurer of Brazil's governing Workers' Party and Renato de Souza Duque, Petrobras' former head of corporate services.\n-Dirceu is the most senior member of the ruling Workers' Party to be taken into custody in connection with the scheme. Dirceu served as former President Luiz Inacio Lula da Silva's chief of staff between 2003 and 2005. He was arrested early August in his home, where he already was under house arrest serving an 11-year sentence for his involvement in a cash-for-votes scheme in Congress more than 10 years ago. Prosecutors have said that Dirceu masterminded the kickback scheme at Petrobras, accepted bribes while in office and continued to receive payments from contractors after he was jailed in late 2013 for the vote-buying scandal.\n-According to prosecutors, the scheme at Petrobras involved roughly $2 billion in bribes and other illegal funds. Some of that money was allegedly funneled back to campaign coffers of the ruling party and its allies. It also allegedly included the payment of bribes to Petrobras executives in return for inflated contracts. 'Miraculous' recovery for Peshawar massacre schoolboy A teenager paralysed after being shot four times in Pakistan's deadliest terror attack has made a \"miraculous\" recovery following treatment in the UK. Muhammad Ibrahim Khan, 13, had been told by doctors in Pakistan that he would never walk again.\n-At least 140 people, mostly children, were killed when gunmen stormed Peshawar's Army Public School last December. Muhammad, who arrived in London last month for surgery, is being discharged from hospital later. Exactly nine months ago, on an ordinary Tuesday morning, Muhammad sat in his first aid class listening to his teachers intently. At the same time seven gunmen disguised in security uniforms were entering the Army Public School. They were strapped with explosives and had one simple mission in mind: Kill every man, woman and child they came across. \"I can't forget what happened that day,\" Muhammad says with a severe stare.\n-We were sitting in the auditorium, we were asking questions... and then we heard heavy gunfire outside. The terrorists moved inside and they started killing - our teacher was burned alive. Muhammad described pulling four other pupils out of the auditorium as the carnage unfolded. He said he then heard his friend, Hamza calling to him. He said, 'oh brother save me'. I held his hand. That's when I was shot in the back, and he was shot in the head. Most of the people killed in the attack were pupils Hamza died in Muhammad's arms. Muhammad recalled blacking out after that, and the next thing he knew he was in a hospital bed, paralysed from the waist down.\n-Doctors in Peshawar in northern Pakistan, and then Rawalpindi, close to the capital, told his family there was no treatment, and he would never walk again. \"Seeing him I felt like my soul had left my body,\" says Muhammad's father, Sher Khan Those nine months were the hardest in my life. But Mr Khan and his wife, Sherbano, refused to believe that their cricket-mad son would never be able to use his legs again. They campaigned, and appealed for help on Pakistani TV, gaining the support of high profile people such as cricketer turned politician Imran Khan.\n-Finally, they were able to raise the funds to bring Muhammad to the UK and provide him with treatment at London's private Harley Street Clinic. Consultant neurosurgeon Irfan Malik described Muhammad as \"terrified\" when he first arrived at the hospital. \"He'd spent the last [few] months lying on a bed, unable to move side to side,\" says Mr Malik. He was weak, he had a pressure sore on his back. He wasn't in great shape. A vertebra at the base of Muhammad's spine was destroyed Muhammad was shot in his shoulder, his hip, and his back during the attack, damaging his lower spine - leading to paralysis.\n-But during six hours of surgery, Mr Malik and his team were able to reattach nerve endings and reconstruct the damaged part of the spine. Even Mr Malik was surprised at what happened next. Exactly one week after the surgery Muhammad stood up and started taking steps and walking. We were not expecting to get that sort of excellent result. That was miraculous,\" he says. Less than two weeks after his operation, Muhammad is ready to leave hospital and start the long road to recovery. Muhammad has defied the odds and started to walk again He says he wants to build his strength and continue his education in the UK. But he says he is determined to return to Pakistan, join the army and help fight terrorism.\n-\"I feel like I have a second chance at life,\" he says as he shows off pictures he's drawn of guns scribbled out next to school books and pens Muhammad grows physically stronger every day but the psychological trauma he continues to endure is unimaginable. \"My anger is not diminishing\" he says. In my school little kids were killed. What was their crime? His mother, wiping a tear from her eye, caressed his head and said: \"I can see my son walking again.\" He'll be able to get on with his normal life. 'Super Voice' 4G service from Three offers better signal Three is making use of a lower frequency 4G spectrum that can travel more widely\n-Mobile phone provider Three has launched a UK service it says will improve reception inside buildings and in rural black spots. Its 4G Super Voice enables customers to make calls and send texts using a lower frequency spectrum. Other networks are looking into introducing the technology, known as Voice Over Long-Term Evolution (VoLTE). It currently works on only the Samsung Galaxy S5, but recent iPhone handsets will be added in the coming months. Three said up to 5.5 million customers would have access to the service by 2017.\n-Chief technology officer Bryn Jones said: \"By the end of the year, one million of our customers will have access to better indoor coverage and be able to use their phones in more places than ever before.\" Stars prepare for panto season Pantomime season is big business for theatres up and down the UK, with many getting ready for this year's season now. Some of the biggest names in showbusiness now take part in the yuletide theatre. Matthew Kelly and Hayley Mills will be appearing in Cinderella - one as an ugly sister, the other as fairy godmother. They reveal their panto secrets to BBC Breakfast. Steven Wilson: 'If I don't do anything, I feel this creeping guilt'\n-Steven Wilson was recently the big winner at the Progressive Music Awards Steven Wilson is often dubbed the hardest working musician in the world of progressive rock. The multi-talented musician won three prizes at this month's Progressive Music Awards in London, including album of the year for Hand. The Guardian's five-star review called it \"a smart, soulful and immersive work of art.\" Since the 1980s, Wilson has been the driving force in a number of musical projects, the best known of which is the rock band Porcupine Tree. Now, ahead of two sell-out shows at the Royal Albert Hall, Wilson is releasing a vinyl-only double LP, Transience, to showcase the \"more accessible\" side of his solo output.\n-He tells the BBC about his love of vinyl, his busy schedule and explains how comic actor Matt Berry came to be his support act. What does vinyl mean to you? I grew up at the very tail end of the vinyl era, and at the time, I remember, we couldn't wait for CD to come along because vinyl was so frustrating. You would buy the record, take it home, and it would have a scratch, and you would have to take it back again. I love CDs, and for some kinds of music - classical for example - it is better than vinyl. But the problem with the CD and digital downloads is that there's nothing you can really cherish or treasure. Owning vinyl is like having a beautiful painting hanging in your living room.\n-It's something you can hold, pore over the lyrics and immerse yourself in the art work. I thought it was just a nostalgic thing, but it can't be if kids too young to remember vinyl are enjoying that kind of experience. Do you have a piece of vinyl that you treasure? The truth is I got rid of 100% of my vinyl in the 90s. All the vinyl I have is re-bought. I started off from the perspective that I wanted to recreate the collection I had when I was 15, but it's gone beyond that. The first record which I persuaded my parents to buy for me was Electric Light Orchestra's Out of the Blue.\n-If I still had my original copy, it would have sentimental value, but, alas, it's in a charity shop somewhere. Steven Wilson hopes the album will be a doorway for potential new fans Why release your new compilation Transience on vinyl? It was originally conceived as an idea for Record Store Day, but we missed the boat on that. My record company had suggested I put together some of my shorter, more accessible songs. I got a bit obsessed by the idea to make something like \"an introduction to Steven Wilson,\" and I was committed to it being a vinyl-only release. Anyone who buys the vinyl does also get a high-resolution download.\n-Do you have a concern that the album won't show your work in a true light?\n\\ No newline at end of file"
        },
        {
            "sha": "178d85d71902c8104f7446c3b9b8880553b49ed0",
            "filename": "examples/legacy/seq2seq/test_data/wmt_en_ro/val.target",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.target",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.target",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftest_data%2Fwmt_en_ro%2Fval.target?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,16 +0,0 @@\n-Fostul È™ef al cabinetului prezidenÈ›ial brazilian este adus Ã®n faÈ›a instanÈ›ei MarÈ›i, un judecÄƒtor federal a acceptat acuzaÈ›iile aduse Ã®mpotriva fostului È™ef al cabinetului prezidenÈ›ial brazilian pentru presupusa implicare a acestuia Ã®ntr-o schemÄƒ masivÄƒ de corupÈ›ie privind compania petrolierÄƒ de stat Petrobras. Biroul procurorului federal a declarat cÄƒ Jose Dirceu va fi trimis Ã®n judecatÄƒ pentru acuzaÈ›iile de corupÈ›ie, Ã®nÈ™elÄƒtorie È™i spÄƒlare de bani aduse Ã®n aceastÄƒ lunÄƒ. Alte paisprezece persoane vor fi judecate, printre acestea numÄƒrÃ¢ndu-se Joao Vaccari Neto, fostul trezorier al Partidului Muncitorilor, aflat la putere Ã®n Brazilia, È™i Renato de Souza Duque, fostul preÈ™edinte al serviciilor pentru Ã®ntreprinderi ale Petrobras.\n-Dirceu este cel mai vechi membru al Partidului Muncitorilor aflat la guvernare luat Ã®n custodie pentru legÄƒturile cu aceastÄƒ schemÄƒ. Dirceu a servit ca È™ef de cabinet al fostului preÈ™edinte Luiz Inacio Lula da Silva Ã®ntre 2003 È™i 2005. A fost arestat la Ã®nceputul lui august de acasÄƒ, unde deja se afla sub arest la domiciliu, cu o pedeapsÄƒ de 11 ani pentru implicarea Ã®ntr-o schemÄƒ de cumpÄƒrare a voturilor Ã®n Congres cu peste 10 ani Ã®n urmÄƒ. Procurorii au declarat cÄƒ Dirceu a dezvoltat schema de luare de mitÄƒ de la Petrobras, a acceptat mitÄƒ Ã®n timp ce se afla Ã®n funcÈ›ie È™i a continuat sÄƒ primeascÄƒ plÄƒÈ›i de la antreprenori dupÄƒ ce a fost Ã®nchis la sfÃ¢rÈ™itul lui 2013 pentru scandalul voturilor cumpÄƒrate.\n-Conform procurorilor, schema de la Petrobras a implicat aproximativ 2 miliarde de dolari sub formÄƒ de mitÄƒ È™i alte fonduri ilegale. O parte din acei bani s-ar fi Ã®ntors Ã®n fondul de campanie al partidului aflat la guvernare È™i al aliaÈ›ilor acestora. De asemenea, ar fi inclus mitÄƒ cÄƒtre directorii Petrobras Ã®n schimbul unor contracte umflate. Recuperarea â€žmiraculoasÄƒâ€ a unui elev supravieÈ›uitor al masacrului de la Peshawar Un adolescent paralizat dupÄƒ ce fusese Ã®mpuÈ™cat de patru ori Ã®n cel mai cumplit atac terorist din Pakistan a reuÈ™it o recuperare â€žmiraculoasÄƒâ€ dupÄƒ ce a urmat un tratament Ã®n Regatul Unit. Lui Mohamed Ibrahim Khan, Ã®n vÃ¢rstÄƒ de 13 ani, doctorii din Pakistan Ã®i spuseserÄƒ cÄƒ nu va mai putea sÄƒ meargÄƒ niciodatÄƒ.\n-Cel puÈ›in 140 de persoane, majoritatea copii, au fost ucise cÃ¢nd bÄƒrbaÈ›i Ã®narmaÈ›i au atacat È™coala publicÄƒ a armatei din Peshawar Ã®n luna decembrie a anului trecut. Mohamed, care a sosit la Londra luna trecutÄƒ pentru operaÈ›ie, va fi externat mai tÃ¢rziu din spital. Exact cu nouÄƒ luni Ã®n urmÄƒ, Ã®ntr-o dimineaÈ›Äƒ obiÈ™nuitÄƒ de marÈ›i, Mohamed stÄƒtea la ora de primul ajutor È™i Ã®È™i asculta atent profesorii. Chiar atunci, È™apte bÄƒrbaÈ›i Ã®narmaÈ›i deghizaÈ›i Ã®n uniformele agenÈ›ilor de pazÄƒ intrau Ã®n È™coala publicÄƒ a armatei. Purtau centuri cu explozivi È™i aveau de Ã®ndeplinit o misiune simplÄƒ: sÄƒ Ã®i ucidÄƒ pe toÈ›i bÄƒrbaÈ›ii, femeile È™i copiii care le ieÈ™eau Ã®n cale. â€žNu pot uita ce s-a Ã®ntÃ¢mplat Ã®n acea ziâ€, spune Mohamed cu o privire asprÄƒ.\n-StÄƒteam Ã®n amfiteatru, puneam Ã®ntrebÄƒri... apoi am auzit focuri de armÄƒ afarÄƒ. TeroriÈ™tii au intrat Ã®nÄƒuntru È™i au Ã®nceput sÄƒ ucidÄƒ. Profesorul nostru a fost ars de viu. Mohamed descrie cum a scos patru elevi din amfiteatru Ã®n timp ce se desfÄƒÈ™ura carnagiul. Apoi spune cÄƒ È™i-a auzit prietenul, pe Hamza, strigÃ¢ndu-l. Spunea â€žoh, frate, salveazÄƒ-mÄƒâ€. L-am È›inut de mÃ¢nÄƒ. Atunci eu am fost Ã®mpuÈ™cat Ã®n spate, iar el Ã®n cap. Cei mai mulÈ›i dintre cei uciÈ™i Ã®n atac erau elevi Hamza a murit Ã®n braÈ›ele lui Mohamed. Mohamed Ã®È™i aminteÈ™te cÄƒ imediat dupÄƒ asta a leÈ™inat È™i cÄƒ urmÄƒtorul lucru pe care l-a È™tiut a fost cÄƒ se afla pe un pat de spital, paralizat de la brÃ¢u Ã®n jos.\n-Doctorii din Peshawar din nordul Pakistanului, apoi cei din Rawalpindi, aproape de capitalÄƒ, i-au spus familiei sale cÄƒ nu exista tratament È™i cÄƒ nu va mai putea merge niciodatÄƒ. â€žCÃ¢nd l-am vÄƒzut, am simÈ›it cum Ã®mi iese sufletulâ€, spune Sher Khan, tatÄƒl lui Mohamed. Acele nouÄƒ luni au fost cele mai grele din viaÈ›a mea. ÃŽnsÄƒ Khan È™i soÈ›ia lui, Sherbano, au refuzat sÄƒ creadÄƒ cÄƒ fiul lor atÃ¢t de pasionat de crichet nu-È™i va mai putea folosi vreodatÄƒ picioarele. Au fÄƒcut o campanie È™i au cerut ajutor de la televiziunea pakistanezÄƒ, atrÄƒgÃ¢nd sprijinul unor oameni faimoÈ™i precum Imran Khan, jucÄƒtor de crichet devenit politician.\n-ÃŽntr-un final, au reuÈ™it sÄƒ strÃ¢ngÄƒ fonduri pentru a-l duce pe Mohamed Ã®n Regatul Unit È™i a-i oferi tratament la clinica privatÄƒ Harley Street din Londra. Neurochirurgul consultant Irfan Malik l-a descris pe Mohamed drept â€žÃ®nspÄƒimÃ¢ntatâ€ cÃ¢nd acesta a ajuns la spital. â€žÃŽÈ™i petrecuse ultimele [cÃ¢teva] luni zÄƒcÃ¢nd Ã®n pat, fÄƒrÄƒ sÄƒ se poatÄƒ miÈ™ca de pe o parte pe alta, spune Malik. Era slÄƒbit, se pusese multÄƒ presiune pe spatele lui. Nu era Ã®ntr-o formÄƒ prea bunÄƒ. O vertebrÄƒ de la baza coloanei vertebrale a lui Mohamed fusese distrusÄƒ Mohamed fusese Ã®mpuÈ™cat Ã®n umÄƒr, Ã®n È™old È™i Ã®n spate Ã®n timpul atacului, iar coloana vertebralÄƒ inferioarÄƒ Ã®i fusese distrusÄƒ, ducÃ¢nd la paralizie.\n-ÃŽnsÄƒ, Ã®n timpul unei operaÈ›ii care a durat È™ase ore, Malik È™i echipa lui au reuÈ™it sÄƒ lege din nou terminaÈ›iile nervoase È™i sÄƒ reconstruiascÄƒ partea distrusÄƒ a coloanei. Chiar È™i Malik a fost surprins de ceea ce s-a Ã®ntÃ¢mplat Ã®n continuare. Exact la o sÄƒptÄƒmÃ¢nÄƒ dupÄƒ operaÈ›ie, Mohamed s-a ridicat È™i a Ã®nceput sÄƒ facÄƒ paÈ™i È™i sÄƒ meargÄƒ. Nu ne aÈ™teptam la un rezultat atÃ¢t de bun. A fost un miracolâ€, spune acesta. ÃŽn mai puÈ›in de douÄƒ sÄƒptÄƒmÃ¢ni de la operaÈ›ie, Mohamed este gata sÄƒ pÄƒrÄƒseascÄƒ spitalul È™i sÄƒ Ã®nceapÄƒ procesul lung de recuperare. Mohamed a sfidat soarta È™i a Ã®nceput sÄƒ meargÄƒ din nou Vrea sÄƒ devinÄƒ puternic È™i sÄƒ Ã®È™i continue studiile Ã®n Regatul Unit. ÃŽnsÄƒ este hotÄƒrÃ¢t sÄƒ revinÄƒ Ã®n Pakistan, sÄƒ se Ã®nroleze Ã®n armatÄƒ È™i sÄƒ lupte Ã®mpotriva terorismului.\n-â€žSimt cÄƒ am Ã®ncÄƒ o È™ansÄƒ la viaÈ›Äƒâ€ spune el, arÄƒtÃ¢nd imaginile cu arme desenate de el lÃ¢ngÄƒ manuale È™colare È™i stilouri Fizic, Mohamed devine tot mai puternic Ã®n fiecare zi, Ã®nsÄƒ trauma psihologicÄƒ prin care trece È™i acum este de neimaginat. â€žFuria mea nu a scÄƒzutâ€, mÄƒrturiseÈ™te el. ÃŽn È™coala mea au fost uciÈ™i copii mici. Ce crimÄƒ au comis ei? Mama lui Ã®È™i È™terge o lacrimÄƒ, Ã®l mÃ¢ngÃ¢ie pe creÈ™tet È™i spune: â€žÃŽmi vÄƒd fiul mergÃ¢nd din nouâ€. Va putea sÄƒ-È™i continue firesc viaÈ›a. Serviciul 4G â€žSuper Voiceâ€ de la Three oferÄƒ semnal mai bun Three foloseÈ™te un spectru 4G cu o frecvenÈ›Äƒ mai joasÄƒ, care poate acoperi o zonÄƒ mai extinsÄƒ\n-Furnizorul de telefonie mobilÄƒ Three a lansat Ã®n Regatul Unit un serviciu despre care spune cÄƒ va Ã®mbunÄƒtÄƒÈ›i recepÈ›ia Ã®n interiorul clÄƒdirilor È™i Ã®n zonele rurale fÄƒrÄƒ semnal. Serviciul 4G Super Voice le permite clienÈ›ilor sÄƒ efectueze apeluri È™i sÄƒ trimitÄƒ mesaje text folosind un spectru cu o frecvenÈ›Äƒ mai joasÄƒ. È˜i alte reÈ›ele intenÈ›ioneazÄƒ sÄƒ introducÄƒ aceeaÈ™i tehnologie, cunoscutÄƒ ca â€žVoice Over Long-Term Evolution (VoLTE)â€. Aceasta funcÈ›ioneazÄƒ momentan doar cu Samsung Galaxy S5, Ã®nsÄƒ telefoanele iPhone recente vor beneficia de ea Ã®n lunile urmÄƒtoare. Three menÈ›ioneazÄƒ cÄƒ pÃ¢nÄƒ la 5,5 milioane de clienÈ›i vor avea acces la serviciu pÃ¢nÄƒ Ã®n 2017.\n-Responsabilul È™ef pentru tehnologie, Bryn Jones a declarat: â€žPÃ¢nÄƒ la sfÃ¢rÈ™itul anului, un milion dintre clienÈ›ii noÈ™tri vor avea acces la o acoperire mai bunÄƒ Ã®n interior È™i Ã®È™i vor putea folosi telefoanele Ã®n mai multe locuri ca pÃ¢nÄƒ acumâ€. Vedetele se pregÄƒtesc pentru stagiunea de pantomimÄƒ Stagiunea de pantomimÄƒ este foarte importantÄƒ pentru teatrele din tot Regatul Unit, multe dintre ele pregÄƒtindu-se acum pentru stagiunea din acest an. Acum, la teatrul de CrÄƒciun participÄƒ unele dintre numele cele mai mari din showbusiness. Matthew Kelly È™i Hayley Mills vor apÄƒrea Ã®n CenuÈ™Äƒreasa - primul Ã®n rolul uneia dintre surorile rele, iar a doua Ã®n rolul zÃ¢nei. AceÈ™tia dezvÄƒluie secretele pantomimei lor la BBC Breakfast. Steven Wilson: â€žDacÄƒ nu fac nimic, mÄƒ simt vinovatâ€\n-Steven Wilson a fost desemnat recent drept marele cÃ¢È™tigÄƒtor al Progressive Music Awards Steven Wilson a fost numit de multe ori drept cel mai muncitor muzician din lumea rockului progresiv. Talentatul muzician a cÃ¢È™tigat trei premii la Progressive Music Awards, care a avut loc luna aceasta la Londra, printre care È™i premiul pentru cel mai bun album al anului pentru Hand. ÃŽn recenzia sa de cinci stele, The Guardian a numit albumul â€žo operÄƒ de artÄƒ inteligentÄƒ, expresivÄƒ È™i captivantÄƒâ€. ÃŽncÄƒ din anii 1980, Wilson este motorul mai multor proiecte muzicale, cel mai cunoscut dintre acestea fiind trupa de rock Porcupine Tree. Acum, Ã®nainte de douÄƒ spectacole cu casa Ã®nchisÄƒ la Royal Albert Hall, Wilson lanseazÄƒ un dublu LP doar Ã®n format vinil, Transience, pentru a arÄƒta latura â€žmai accesibilÄƒâ€ a activitÄƒÈ›ii sale solo.\n-A povestit pentru BBC despre dragostea lui pentru viniluri È™i despre programul sÄƒu Ã®ncÄƒrcat È™i a explicat cum a ajuns actorul de comedie Matt Berry sÄƒ Ã®i deschidÄƒ spectacolele. Ce Ã®nseamnÄƒ vinil pentru tine? Am crescut chiar Ã®n perioada de sfÃ¢rÈ™it a erei vinilurilor È™i Ã®mi amintesc cÄƒ atunci abia aÈ™teptam apariÈ›ia CD-ului, cÄƒci vinilul era atÃ¢t de enervant. CumpÄƒrai un disc, mergeai cu el acasÄƒ, avea o zgÃ¢rieturÄƒ È™i trebuia sÄƒ Ã®l aduci Ã®napoi. Iubesc CD-urile, iar pentru anumite tipuri de muzicÄƒ, de exemplu cea clasicÄƒ, sunt mai bune decÃ¢t vinilurile. ÃŽnsÄƒ problema cu CD-urile È™i cu descÄƒrcÄƒrile digitale este aceea cÄƒ nu mai existÄƒ nimic pe care sÄƒ Ã®l preÈ›uieÈ™ti cu adevÄƒrat. SÄƒ ai un vinil e ca È™i cum ai avea un tablou frumos agÄƒÈ›at Ã®n sufragerie.\n-E ceva ce poÈ›i È›ine Ã®n mÃ¢nÄƒ, Ã®n timp ce te laÈ™i absorbit de versuri È™i copleÈ™it de actul artistic. Am crezut cÄƒ e doar o chestie nostalgicÄƒ, Ã®nsÄƒ nu are cum sÄƒ fie aÈ™a dacÄƒ unor puÈ™ti prea tineri sÄƒ-È™i aminteascÄƒ de viniluri le place acest gen de experienÈ›Äƒ. Ai vreun vinil la care È›ii Ã®n mod special? Recunosc cÄƒ am scÄƒpat de toate vinilurile Ã®n anii '90. Toate vinilurile pe care le am sunt cumpÄƒrate din nou. Am pornit de la ideea de a reface colecÈ›ia pe care o aveam la 15 ani, Ã®nsÄƒ am trecut de limita aceea. Primul disc pe care mi-am convins pÄƒrinÈ›ii sÄƒ mi-l cumpere a fost Out of the Blue de la Electric Light Orchestra.\n-DacÄƒ aÈ™ mai fi avut Ã®ncÄƒ exemplarul iniÈ›ial, acesta ar fi avut valoare sentimentalÄƒ, Ã®nsÄƒ, din pÄƒcate, se aflÄƒ pe undeva printr-un magazin de caritate. Steven Wilson sperÄƒ cÄƒ albumul va fi o poartÄƒ cÄƒtre posibili fani noi De ce È›i-ai lansat noua compilaÈ›ie Transience pe vinil? Aceasta a fost conceputÄƒ iniÈ›ial ca idee pentru Ziua magazinelor de discuri, Ã®nsÄƒ am ratat ocazia. Casa mea de discuri sugerase sÄƒ adun cÃ¢teva dintre melodiile mele mai scurte È™i mai accesibile. Am ajuns sÄƒ fiu uÈ™or obsedat de ideea de a face ceva gen â€žintroducere Ã®n muzica lui Steven Wilsonâ€ È™i am È›inut neapÄƒrat ca proiectul sÄƒ fie lansat doar pe vinil. Cine cumpÄƒrÄƒ vinilul primeÈ™te, de asemenea, È™i o variantÄƒ descÄƒrcatÄƒ la rezoluÈ›ie Ã®naltÄƒ.\n-EÈ™ti Ã®ngrijorat cÄƒ albumul nu va arÄƒta muzica ta Ã®n adevÄƒrata ei luminÄƒ?\n\\ No newline at end of file"
        },
        {
            "sha": "d0e1075bb0790cbe66c8139dcc53be61746512c2",
            "filename": "examples/legacy/seq2seq/train_distil_marian_enro.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,38 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-export WANDB_PROJECT=distil-marian\n-export BS=64\n-export GAS=1\n-export m=sshleifer/student_marian_en_ro_6_3\n-export MAX_LEN=128\n-python finetune_trainer.py \\\n-    --tokenizer_name $m --model_name_or_path $m \\\n-    --data_dir $ENRO_DIR \\\n-    --output_dir marian_en_ro_6_3 \\\n-    --learning_rate=3e-4 \\\n-    --warmup_steps 500 --sortish_sampler \\\n-    --fp16 \\\n-    --gradient_accumulation_steps=$GAS \\\n-    --per_device_train_batch_size=$BS --per_device_eval_batch_size=$BS \\\n-    --freeze_encoder --freeze_embeds \\\n-    --num_train_epochs=6 \\\n-    --save_steps 3000 --eval_steps 3000 \\\n-    --max_source_length $MAX_LEN --max_target_length $MAX_LEN \\\n-    --val_max_target_length $MAX_TGT_LEN --test_max_target_length $MAX_TGT_LEN \\\n-    --do_train --do_eval --do_predict \\\n-    --eval_strategy steps \\\n-    --predict_with_generate --logging_first_step \\\n-    --task translation --label_smoothing_factor 0.1 \\\n-    \"$@\""
        },
        {
            "sha": "af589f0c54f8c759a744f2091c451d44a029ce42",
            "filename": "examples/legacy/seq2seq/train_distil_marian_enro_tpu.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distil_marian_enro_tpu.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,37 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-export WANDB_PROJECT=distil-marian\n-export BS=64\n-export m=sshleifer/student_marian_en_ro_6_3\n-export MAX_LEN=128\n-\n-python xla_spawn.py finetune_trainer.py \\\n-    --tokenizer_name $m --model_name_or_path $m \\\n-    --data_dir $ENRO_DIR \\\n-    --output_dir marian_en_ro_6_3 \\\n-    --learning_rate=3e-4 \\\n-    --warmup_steps 500 \\\n-    --per_device_train_batch_size=$BS --per_device_eval_batch_size=$BS \\\n-    --freeze_encoder --freeze_embeds \\\n-    --num_train_epochs=6 \\\n-    --save_steps 500 --eval_steps 500 \\\n-    --logging_first_step --logging_steps 200 \\\n-    --max_source_length $MAX_LEN --max_target_length $MAX_LEN \\\n-    --val_max_target_length $MAX_TGT_LEN --test_max_target_length $MAX_TGT_LEN \\\n-    --do_train --do_eval \\\n-    --eval_strategy steps \\\n-    --prediction_loss_only \\\n-    --task translation --label_smoothing_factor 0.1 \\\n-    \"$@\""
        },
        {
            "sha": "a490019588ce89bf4c92c15615b86819bffeb487",
            "filename": "examples/legacy/seq2seq/train_distilbart_cnn.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_distilbart_cnn.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_distilbart_cnn.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_distilbart_cnn.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,39 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-export WANDB_PROJECT=distilbart-trainer\n-export BS=32\n-export m=sshleifer/student_cnn_12_6\n-export tok=facebook/bart-large\n-export MAX_TGT_LEN=142\n-\n-python finetune_trainer.py \\\n-    --model_name_or_path $m --tokenizer_name $tok \\ \n-    --data_dir cnn_dm \\\n-    --output_dir distilbart-cnn-12-6 \\\n-    --learning_rate=3e-5 \\\n-    --warmup_steps 500 --sortish_sampler \\\n-    --fp16 \\\n-    --n_val 500 \\\n-    --gradient_accumulation_steps=1 \\\n-    --per_device_train_batch_size=$BS --per_device_eval_batch_size=$BS \\\n-    --freeze_encoder --freeze_embeds \\\n-    --num_train_epochs=2 \\\n-    --save_steps 3000 --eval_steps 3000 \\\n-    --logging_first_step \\\n-    --max_target_length 56 --val_max_target_length $MAX_TGT_LEN --test_max_target_length $MAX_TGT_LEN\\\n-    --do_train --do_eval --do_predict \\\n-    --eval_strategy steps \\\n-    --predict_with_generate --sortish_sampler \\\n-    \"$@\""
        },
        {
            "sha": "fb31790a2c192cc09a8c6f5a26417a467cd05885",
            "filename": "examples/legacy/seq2seq/train_mbart_cc25_enro.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_mbart_cc25_enro.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Ftrain_mbart_cc25_enro.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ftrain_mbart_cc25_enro.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,35 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-python finetune_trainer.py \\\n-    --model_name_or_path=facebook/mbart-large-cc25 \\\n-    --data_dir $ENRO_DIR \\\n-    --output_dir mbart_cc25_enro \\\n-    --learning_rate=3e-5 \\\n-    --warmup_steps 500 \\ \n-    --fp16 \\\n-    --label_smoothing 0.1 \\\n-    --adam_eps 1e-06 \\\n-    --src_lang en_XX --tgt_lang ro_RO \\\n-    --freeze_embeds \\\n-    --per_device_train_batch_size=4 --per_device_eval_batch_size=4 \\\n-    --max_source_length 128 --max_target_length 128 --val_max_target_length 128 --test_max_target_length 128\\\n-    --sortish_sampler \\\n-    --num_train_epochs 6 \\\n-    --save_steps 25000 --eval_steps 25000 --logging_steps 1000 \\\n-    --do_train --do_eval --do_predict \\\n-    --eval_strategy steps \\\n-    --predict_with_generate --logging_first_step \\\n-    --task translation \\\n-    \"$@\""
        },
        {
            "sha": "d296c846fbb7024beef72204bb32e3bb0dbe329c",
            "filename": "examples/legacy/seq2seq/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 640,
            "changes": 640,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Futils.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,640 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import itertools\n-import json\n-import linecache\n-import math\n-import os\n-import pickle\n-import socket\n-from collections.abc import Iterable\n-from logging import getLogger\n-from pathlib import Path\n-from typing import Callable, Union\n-\n-import git\n-import numpy as np\n-import torch\n-import torch.distributed as dist\n-from rouge_score import rouge_scorer, scoring\n-from sacrebleu import corpus_bleu\n-from sentence_splitter import add_newline_to_end_of_each_sentence\n-from torch import nn\n-from torch.utils.data import Dataset, Sampler\n-\n-from transformers import BartTokenizer, EvalPrediction, PreTrainedTokenizer, T5Tokenizer\n-from transformers.models.bart.modeling_bart import shift_tokens_right\n-from transformers.utils import cached_property\n-\n-\n-try:\n-    from fairseq.data.data_utils import batch_by_size\n-\n-    FAIRSEQ_AVAILABLE = True\n-except (ImportError, ModuleNotFoundError):\n-    FAIRSEQ_AVAILABLE = False\n-\n-\n-def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n-    \"\"\"From fairseq\"\"\"\n-    if target.dim() == lprobs.dim() - 1:\n-        target = target.unsqueeze(-1)\n-    nll_loss = -lprobs.gather(dim=-1, index=target)\n-    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n-    if ignore_index is not None:\n-        pad_mask = target.eq(ignore_index)\n-        nll_loss.masked_fill_(pad_mask, 0.0)\n-        smooth_loss.masked_fill_(pad_mask, 0.0)\n-    else:\n-        nll_loss = nll_loss.squeeze(-1)\n-        smooth_loss = smooth_loss.squeeze(-1)\n-\n-    nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n-    smooth_loss = smooth_loss.sum()\n-    eps_i = epsilon / lprobs.size(-1)\n-    loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n-    return loss, nll_loss\n-\n-\n-def lmap(f: Callable, x: Iterable) -> list:\n-    \"\"\"list(map(f, x))\"\"\"\n-    return list(map(f, x))\n-\n-\n-def calculate_bleu(output_lns, refs_lns, **kwargs) -> dict:\n-    \"\"\"Uses sacrebleu's corpus_bleu implementation.\"\"\"\n-    return {\"bleu\": round(corpus_bleu(output_lns, [refs_lns], **kwargs).score, 4)}\n-\n-\n-def build_compute_metrics_fn(task_name: str, tokenizer: PreTrainedTokenizer) -> Callable[[EvalPrediction], dict]:\n-    def non_pad_len(tokens: np.ndarray) -> int:\n-        return np.count_nonzero(tokens != tokenizer.pad_token_id)\n-\n-    def decode_pred(pred: EvalPrediction) -> tuple[list[str], list[str]]:\n-        pred_ids = pred.predictions\n-        label_ids = pred.label_ids\n-        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n-        label_ids[label_ids == -100] = tokenizer.pad_token_id\n-        label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n-        pred_str = lmap(str.strip, pred_str)\n-        label_str = lmap(str.strip, label_str)\n-        return pred_str, label_str\n-\n-    def summarization_metrics(pred: EvalPrediction) -> dict:\n-        pred_str, label_str = decode_pred(pred)\n-        rouge: dict = calculate_rouge(pred_str, label_str)\n-        summ_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)\n-        rouge.update({\"gen_len\": summ_len})\n-        return rouge\n-\n-    def translation_metrics(pred: EvalPrediction) -> dict:\n-        pred_str, label_str = decode_pred(pred)\n-        bleu: dict = calculate_bleu(pred_str, label_str)\n-        gen_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)\n-        bleu.update({\"gen_len\": gen_len})\n-        return bleu\n-\n-    compute_metrics_fn = summarization_metrics if \"summarization\" in task_name else translation_metrics\n-    return compute_metrics_fn\n-\n-\n-def trim_batch(\n-    input_ids,\n-    pad_token_id,\n-    attention_mask=None,\n-):\n-    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n-    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n-    if attention_mask is None:\n-        return input_ids[:, keep_column_mask]\n-    else:\n-        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n-\n-\n-class AbstractSeq2SeqDataset(Dataset):\n-    def __init__(\n-        self,\n-        tokenizer,\n-        data_dir,\n-        max_source_length,\n-        max_target_length,\n-        type_path=\"train\",\n-        n_obs=None,\n-        prefix=\"\",\n-        **dataset_kwargs,\n-    ):\n-        super().__init__()\n-        self.src_file = Path(data_dir).joinpath(type_path + \".source\")\n-        self.tgt_file = Path(data_dir).joinpath(type_path + \".target\")\n-        self.len_file = Path(data_dir).joinpath(type_path + \".len\")\n-        if os.path.exists(self.len_file):\n-            self.src_lens = pickle_load(self.len_file)\n-            self.used_char_len = False\n-        else:\n-            self.src_lens = self.get_char_lens(self.src_file)\n-            self.used_char_len = True\n-        self.max_source_length = max_source_length\n-        self.max_target_length = max_target_length\n-        assert min(self.src_lens) > 0, f\"found empty line in {self.src_file}\"\n-        self.tokenizer = tokenizer\n-        self.prefix = prefix if prefix is not None else \"\"\n-\n-        if n_obs is not None:\n-            self.src_lens = self.src_lens[:n_obs]\n-        self.pad_token_id = self.tokenizer.pad_token_id\n-        self.dataset_kwargs = dataset_kwargs\n-        dataset_kwargs.update({\"add_prefix_space\": True} if isinstance(self.tokenizer, BartTokenizer) else {})\n-\n-    def __len__(self):\n-        return len(self.src_lens)\n-\n-    @staticmethod\n-    def get_char_lens(data_file):\n-        return [len(x) for x in Path(data_file).open()]\n-\n-    @cached_property\n-    def tgt_lens(self):\n-        \"\"\"Length in characters of target documents\"\"\"\n-        return self.get_char_lens(self.tgt_file)\n-\n-    def make_sortish_sampler(self, batch_size, distributed=False, shuffle=True, **kwargs):\n-        if distributed:\n-            return DistributedSortishSampler(self, batch_size, shuffle=shuffle, **kwargs)\n-        else:\n-            return SortishSampler(self.src_lens, batch_size, shuffle=shuffle)\n-\n-    def make_dynamic_sampler(self, max_tokens_per_batch=1024, **kwargs):\n-        assert FAIRSEQ_AVAILABLE, \"Dynamic batch size requires `pip install fairseq`\"\n-        assert not self.used_char_len, \"You must call  python make_len_file.py before calling make_dynamic_sampler\"\n-        sorted_indices = list(self.make_sortish_sampler(1024, shuffle=False))\n-\n-        def num_tokens_in_example(i):\n-            return min(self.src_lens[i], self.max_target_length)\n-\n-        # call fairseq cython function\n-        batch_sampler: list[list[int]] = batch_by_size(\n-            sorted_indices,\n-            num_tokens_fn=num_tokens_in_example,\n-            max_tokens=max_tokens_per_batch,\n-            required_batch_size_multiple=64,\n-        )\n-        shuffled_batches = [batch_sampler[i] for i in np.random.permutation(range(len(batch_sampler)))]\n-        # move the largest batch to the front to OOM quickly (uses an approximation for padding)\n-        approximate_toks_per_batch = [max(self.src_lens[i] for i in batch) * len(batch) for batch in shuffled_batches]\n-        largest_batch_idx = np.argmax(approximate_toks_per_batch)\n-        shuffled_batches[0], shuffled_batches[largest_batch_idx] = (\n-            shuffled_batches[largest_batch_idx],\n-            shuffled_batches[0],\n-        )\n-        return shuffled_batches\n-\n-    def __getitem__(self, item):\n-        raise NotImplementedError(\"You must implement this\")\n-\n-    def collate_fn(self, batch):\n-        raise NotImplementedError(\"You must implement this\")\n-\n-\n-class LegacySeq2SeqDataset(AbstractSeq2SeqDataset):\n-    def __getitem__(self, index) -> dict[str, torch.Tensor]:\n-        \"\"\"Call tokenizer on src and tgt_lines\"\"\"\n-        index = index + 1  # linecache starts at 1\n-        source_line = self.prefix + linecache.getline(str(self.src_file), index).rstrip(\"\\n\")\n-        tgt_line = linecache.getline(str(self.tgt_file), index).rstrip(\"\\n\")\n-        assert source_line, f\"empty source line for index {index}\"\n-        assert tgt_line, f\"empty tgt line for index {index}\"\n-        source_inputs = self.encode_line(self.tokenizer, source_line, self.max_source_length)\n-        target_inputs = self.encode_line(self.tokenizer, tgt_line, self.max_target_length)\n-\n-        source_ids = source_inputs[\"input_ids\"].squeeze()\n-        target_ids = target_inputs[\"input_ids\"].squeeze()\n-        src_mask = source_inputs[\"attention_mask\"].squeeze()\n-        return {\n-            \"input_ids\": source_ids,\n-            \"attention_mask\": src_mask,\n-            \"labels\": target_ids,\n-        }\n-\n-    def encode_line(self, tokenizer, line, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n-        \"\"\"Only used by LegacyDataset\"\"\"\n-        return tokenizer(\n-            [line],\n-            max_length=max_length,\n-            padding=\"max_length\" if pad_to_max_length else None,\n-            truncation=True,\n-            return_tensors=return_tensors,\n-            **self.dataset_kwargs,\n-        )\n-\n-    def collate_fn(self, batch) -> dict[str, torch.Tensor]:\n-        input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n-        masks = torch.stack([x[\"attention_mask\"] for x in batch])\n-        target_ids = torch.stack([x[\"labels\"] for x in batch])\n-        pad_token_id = self.pad_token_id\n-        y = trim_batch(target_ids, pad_token_id)\n-        source_ids, source_mask = trim_batch(input_ids, pad_token_id, attention_mask=masks)\n-        batch = {\n-            \"input_ids\": source_ids,\n-            \"attention_mask\": source_mask,\n-            \"labels\": y,\n-        }\n-        return batch\n-\n-\n-class Seq2SeqDataset(AbstractSeq2SeqDataset):\n-    \"\"\"A dataset that calls prepare_seq2seq_batch.\"\"\"\n-\n-    def __getitem__(self, index) -> dict[str, str]:\n-        index = index + 1  # linecache starts at 1\n-        source_line = self.prefix + linecache.getline(str(self.src_file), index).rstrip(\"\\n\")\n-        tgt_line = linecache.getline(str(self.tgt_file), index).rstrip(\"\\n\")\n-        assert source_line, f\"empty source line for index {index}\"\n-        assert tgt_line, f\"empty tgt line for index {index}\"\n-        return {\"tgt_texts\": tgt_line, \"src_texts\": source_line, \"id\": index - 1}\n-\n-    def collate_fn(self, batch) -> dict[str, torch.Tensor]:\n-        \"\"\"Call prepare_seq2seq_batch.\"\"\"\n-        batch_encoding: dict[str, torch.Tensor] = self.tokenizer.prepare_seq2seq_batch(\n-            [x[\"src_texts\"] for x in batch],\n-            tgt_texts=[x[\"tgt_texts\"] for x in batch],\n-            max_length=self.max_source_length,\n-            max_target_length=self.max_target_length,\n-            return_tensors=\"pt\",\n-            **self.dataset_kwargs,\n-        ).data\n-        batch_encoding[\"ids\"] = torch.tensor([x[\"id\"] for x in batch])\n-        return batch_encoding\n-\n-\n-class Seq2SeqDataCollator:\n-    def __init__(self, tokenizer, data_args, decoder_start_token_id):\n-        self.tokenizer = tokenizer\n-        self.pad_token_id = tokenizer.pad_token_id\n-        self.decoder_start_token_id = decoder_start_token_id\n-        assert self.pad_token_id is not None, (\n-            f\"pad_token_id is not defined for ({self.tokenizer.__class__.__name__}), it must be defined.\"\n-        )\n-        self.data_args = data_args\n-        self.dataset_kwargs = {\"add_prefix_space\": True} if isinstance(tokenizer, BartTokenizer) else {}\n-        if data_args.src_lang is not None:\n-            self.dataset_kwargs[\"src_lang\"] = data_args.src_lang\n-        if data_args.tgt_lang is not None:\n-            self.dataset_kwargs[\"tgt_lang\"] = data_args.tgt_lang\n-\n-    def __call__(self, batch) -> dict[str, torch.Tensor]:\n-        if hasattr(self.tokenizer, \"prepare_seq2seq_batch\"):\n-            batch = self._encode(batch)\n-            input_ids, attention_mask, labels = (\n-                batch[\"input_ids\"],\n-                batch[\"attention_mask\"],\n-                batch[\"labels\"],\n-            )\n-        else:\n-            input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n-            attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n-            labels = torch.stack([x[\"labels\"] for x in batch])\n-\n-            labels = trim_batch(labels, self.pad_token_id)\n-            input_ids, attention_mask = trim_batch(input_ids, self.pad_token_id, attention_mask=attention_mask)\n-\n-        if isinstance(self.tokenizer, T5Tokenizer):\n-            decoder_input_ids = self._shift_right_t5(labels)\n-        else:\n-            decoder_input_ids = shift_tokens_right(labels, self.pad_token_id, self.decoder_start_token_id)\n-\n-        batch = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"labels\": labels,\n-        }\n-        return batch\n-\n-    def _shift_right_t5(self, input_ids):\n-        # shift inputs to the right\n-        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n-        shifted_input_ids[..., 0] = self.pad_token_id\n-        return shifted_input_ids\n-\n-    def _encode(self, batch) -> dict[str, torch.Tensor]:\n-        batch_encoding = self.tokenizer.prepare_seq2seq_batch(\n-            [x[\"src_texts\"] for x in batch],\n-            tgt_texts=[x[\"tgt_texts\"] for x in batch],\n-            max_length=self.data_args.max_source_length,\n-            max_target_length=self.data_args.max_target_length,\n-            padding=\"longest\",\n-            return_tensors=\"pt\",\n-            **self.dataset_kwargs,\n-        )\n-        return batch_encoding.data\n-\n-\n-class SortishSampler(Sampler):\n-    \"Go through the text data by order of src length with a bit of randomness. From fastai repo.\"\n-\n-    def __init__(self, data, batch_size, shuffle=True):\n-        self.data, self.bs, self.shuffle = data, batch_size, shuffle\n-\n-    def __len__(self) -> int:\n-        return len(self.data)\n-\n-    def __iter__(self):\n-        return iter(sortish_sampler_indices(self.data, self.bs, shuffle=self.shuffle))\n-\n-\n-def sortish_sampler_indices(data: list, bs: int, shuffle=True) -> np.array:\n-    \"Go through the text data by order of src length with a bit of randomness. From fastai repo.\"\n-    if not shuffle:\n-        return np.argsort(np.array(data) * -1)\n-\n-    def key_fn(i):\n-        return data[i]\n-\n-    idxs = np.random.permutation(len(data))\n-    sz = bs * 50\n-    ck_idx = [idxs[i : i + sz] for i in range(0, len(idxs), sz)]\n-    sort_idx = np.concatenate([sorted(s, key=key_fn, reverse=True) for s in ck_idx])\n-    sz = bs\n-    ck_idx = [sort_idx[i : i + sz] for i in range(0, len(sort_idx), sz)]\n-    max_ck = np.argmax([key_fn(ck[0]) for ck in ck_idx])  # find the chunk with the largest key,\n-    ck_idx[0], ck_idx[max_ck] = ck_idx[max_ck], ck_idx[0]  # then make sure it goes first.\n-    sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([], dtype=int)\n-    sort_idx = np.concatenate((ck_idx[0], sort_idx))\n-    return sort_idx\n-\n-\n-class DistributedSortishSampler(Sampler):\n-    \"\"\"Copied from torch DistributedSampler\"\"\"\n-\n-    def __init__(self, dataset, batch_size, num_replicas=None, rank=None, add_extra_examples=True, shuffle=True):\n-        if num_replicas is None:\n-            if not dist.is_available():\n-                raise RuntimeError(\"Requires distributed package to be available\")\n-            num_replicas = dist.get_world_size()\n-        if rank is None:\n-            if not dist.is_available():\n-                raise RuntimeError(\"Requires distributed package to be available\")\n-            rank = dist.get_rank()\n-        self.dataset = dataset\n-        self.num_replicas = num_replicas\n-        self.rank = rank\n-        self.epoch = 0\n-        if add_extra_examples:\n-            self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n-            self.total_size = self.num_samples * self.num_replicas\n-        else:\n-            self.total_size = len(dataset)\n-            self.num_samples = len(self.available_indices)\n-        self.batch_size = batch_size\n-        self.add_extra_examples = add_extra_examples\n-        self.shuffle = shuffle\n-\n-    def __iter__(self) -> Iterable:\n-        g = torch.Generator()\n-        g.manual_seed(self.epoch)\n-\n-        sortish_data = [self.dataset.src_lens[i] for i in self.available_indices]\n-        sortish_indices = sortish_sampler_indices(sortish_data, self.batch_size, shuffle=self.shuffle)\n-        indices = [self.available_indices[i] for i in sortish_indices]\n-        assert len(indices) == self.num_samples\n-        return iter(indices)\n-\n-    @cached_property\n-    def available_indices(self) -> np.array:\n-        indices = list(range(len(self.dataset)))\n-        # add extra samples to make it evenly divisible\n-        indices += indices[: (self.total_size - len(indices))]\n-        assert len(indices) == self.total_size\n-        # subsample\n-        available_indices = indices[self.rank : self.total_size : self.num_replicas]\n-        return available_indices\n-\n-    def __len__(self):\n-        return self.num_samples\n-\n-    def set_epoch(self, epoch):\n-        self.epoch = epoch\n-\n-\n-logger = getLogger(__name__)\n-\n-\n-def use_task_specific_params(model, task):\n-    \"\"\"Update config with summarization specific params.\"\"\"\n-    task_specific_params = model.config.task_specific_params\n-\n-    if task_specific_params is not None:\n-        pars = task_specific_params.get(task, {})\n-        logger.info(f\"setting model.config to task specific params for {task}:\\n {pars}\")\n-        logger.info(\"note: command line args may override some of these\")\n-        model.config.update(pars)\n-\n-\n-def pickle_load(path):\n-    \"\"\"pickle.load(path)\"\"\"\n-    with open(path, \"rb\") as f:\n-        return pickle.load(f)\n-\n-\n-def pickle_save(obj, path):\n-    \"\"\"pickle.dump(obj, path)\"\"\"\n-    with open(path, \"wb\") as f:\n-        return pickle.dump(obj, f)\n-\n-\n-def flatten_list(summary_ids: list[list]):\n-    return list(itertools.chain.from_iterable(summary_ids))\n-\n-\n-def save_git_info(folder_path: str) -> None:\n-    \"\"\"Save git information to output_dir/git_log.json\"\"\"\n-    repo_infos = get_git_info()\n-    save_json(repo_infos, os.path.join(folder_path, \"git_log.json\"))\n-\n-\n-def save_json(content, path, indent=4, **json_dump_kwargs):\n-    with open(path, \"w\") as f:\n-        json.dump(content, f, indent=indent, sort_keys=True, **json_dump_kwargs)\n-\n-\n-def load_json(path):\n-    with open(path) as f:\n-        return json.load(f)\n-\n-\n-def get_git_info():\n-    try:\n-        repo = git.Repo(search_parent_directories=True)\n-        repo_infos = {\n-            \"repo_id\": str(repo),\n-            \"repo_sha\": str(repo.head.object.hexsha),\n-            \"repo_branch\": str(repo.active_branch),\n-            \"hostname\": str(socket.gethostname()),\n-        }\n-        return repo_infos\n-    except TypeError:\n-        return {\n-            \"repo_id\": None,\n-            \"repo_sha\": None,\n-            \"repo_branch\": None,\n-            \"hostname\": None,\n-        }\n-\n-\n-ROUGE_KEYS = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n-\n-\n-def extract_rouge_mid_statistics(dct):\n-    new_dict = {}\n-    for k1, v1 in dct.items():\n-        mid = v1.mid\n-        new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in [\"precision\", \"recall\", \"fmeasure\"]}\n-    return new_dict\n-\n-\n-def calculate_rouge(\n-    pred_lns: list[str],\n-    tgt_lns: list[str],\n-    use_stemmer=True,\n-    rouge_keys=ROUGE_KEYS,\n-    return_precision_and_recall=False,\n-    bootstrap_aggregation=True,\n-    newline_sep=True,\n-) -> dict:\n-    \"\"\"Calculate rouge using rouge_scorer package.\n-\n-    Args:\n-        pred_lns: list of summaries generated by model\n-        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)\n-        use_stemmer:  Bool indicating whether Porter stemmer should be used to\n-        strip word suffixes to improve matching.\n-        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum\n-        return_precision_and_recall: (False) whether to also return precision and recall.\n-        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False\n-            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``\n-        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL\n-        on multi sentence summaries (CNN/DM dataset).\n-\n-    Returns:\n-         dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys\n-\n-    \"\"\"\n-    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)\n-    aggregator = scoring.BootstrapAggregator()\n-    for pred, tgt in zip(tgt_lns, pred_lns):\n-        # rougeLsum expects \"\\n\" separated sentences within a summary\n-        if newline_sep:\n-            pred = add_newline_to_end_of_each_sentence(pred)\n-            tgt = add_newline_to_end_of_each_sentence(tgt)\n-        scores = scorer.score(pred, tgt)\n-        aggregator.add_scores(scores)\n-\n-    if bootstrap_aggregation:\n-        result = aggregator.aggregate()\n-        if return_precision_and_recall:\n-            return extract_rouge_mid_statistics(result)  # here we return dict\n-        else:\n-            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n-\n-    else:\n-        return aggregator._scores  # here we return defaultdict(list)\n-\n-\n-# Utilities for freezing parameters and checking whether they are frozen\n-\n-\n-def freeze_params(model: nn.Module):\n-    \"\"\"Set requires_grad=False for each of model.parameters()\"\"\"\n-    for par in model.parameters():\n-        par.requires_grad = False\n-\n-\n-def freeze_embeds(model):\n-    \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n-    model_type = model.config.model_type\n-\n-    if model_type in [\"t5\", \"mt5\"]:\n-        freeze_params(model.shared)\n-        for d in [model.encoder, model.decoder]:\n-            freeze_params(d.embed_tokens)\n-    elif model_type == \"fsmt\":\n-        for d in [model.model.encoder, model.model.decoder]:\n-            freeze_params(d.embed_positions)\n-            freeze_params(d.embed_tokens)\n-    else:\n-        freeze_params(model.model.shared)\n-        for d in [model.model.encoder, model.model.decoder]:\n-            freeze_params(d.embed_positions)\n-            freeze_params(d.embed_tokens)\n-\n-\n-def grad_status(model: nn.Module) -> Iterable:\n-    return (par.requires_grad for par in model.parameters())\n-\n-\n-def any_requires_grad(model: nn.Module) -> bool:\n-    return any(grad_status(model))\n-\n-\n-def assert_all_frozen(model):\n-    model_grads: list[bool] = list(grad_status(model))\n-    n_require_grad = sum(lmap(int, model_grads))\n-    npars = len(model_grads)\n-    assert not any(model_grads), f\"{n_require_grad / npars:.1%} of {npars} weights require grad\"\n-\n-\n-def assert_not_all_frozen(model):\n-    model_grads: list[bool] = list(grad_status(model))\n-    npars = len(model_grads)\n-    assert any(model_grads), f\"none of {npars} weights require grad\"\n-\n-\n-def parse_numeric_n_bool_cl_kwargs(unparsed_args: list[str]) -> dict[str, Union[int, float, bool]]:\n-    \"\"\"\n-    Parse an argv list of unspecified command line args to a dict.\n-    Assumes all values are either numeric or boolean in the form of true/false.\n-    \"\"\"\n-    result = {}\n-    assert len(unparsed_args) % 2 == 0, f\"got odd number of unparsed args: {unparsed_args}\"\n-    num_pairs = len(unparsed_args) // 2\n-    for pair_num in range(num_pairs):\n-        i = 2 * pair_num\n-        assert unparsed_args[i].startswith(\"--\")\n-        if unparsed_args[i + 1].lower() == \"true\":\n-            value = True\n-        elif unparsed_args[i + 1].lower() == \"false\":\n-            value = False\n-        else:\n-            try:\n-                value = int(unparsed_args[i + 1])\n-            except ValueError:\n-                value = float(unparsed_args[i + 1])  # this can raise another informative ValueError\n-\n-        result[unparsed_args[i][2:]] = value\n-    return result\n-\n-\n-def write_txt_file(ordered_tgt, path):\n-    f = Path(path).open(\"w\")\n-    for ln in ordered_tgt:\n-        f.write(ln + \"\\n\")\n-        f.flush()\n-\n-\n-def chunks(lst, n):\n-    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n-    for i in range(0, len(lst), n):\n-        yield lst[i : i + n]"
        },
        {
            "sha": "6005dda93c08bbcb1c9bd94456b84cc9cbdcffa7",
            "filename": "examples/legacy/seq2seq/xla_spawn.py",
            "status": "removed",
            "additions": 0,
            "deletions": 82,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fxla_spawn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Fseq2seq%2Fxla_spawn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fxla_spawn.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,82 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-A simple launcher script for TPU training\n-\n-Inspired by https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py\n-\n-::\n-    >>> python xla_spawn.py --num_cores=NUM_CORES_YOU_HAVE\n-               YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n-               arguments of your training script)\n-\n-\"\"\"\n-\n-import importlib\n-import sys\n-from argparse import REMAINDER, ArgumentParser\n-from pathlib import Path\n-\n-import torch_xla.distributed.xla_multiprocessing as xmp\n-\n-\n-def parse_args():\n-    \"\"\"\n-    Helper function parsing the command line options\n-    @retval ArgumentParser\n-    \"\"\"\n-    parser = ArgumentParser(\n-        description=(\n-            \"PyTorch TPU distributed training launch helper utility that will spawn up multiple distributed processes\"\n-        )\n-    )\n-\n-    # Optional arguments for the launch helper\n-    parser.add_argument(\"--num_cores\", type=int, default=1, help=\"Number of TPU cores to use (1 or 8).\")\n-\n-    # positional\n-    parser.add_argument(\n-        \"training_script\",\n-        type=str,\n-        help=(\n-            \"The full path to the single TPU training \"\n-            \"program/script to be launched in parallel, \"\n-            \"followed by all the arguments for the \"\n-            \"training script\"\n-        ),\n-    )\n-\n-    # rest from the training program\n-    parser.add_argument(\"training_script_args\", nargs=REMAINDER)\n-\n-    return parser.parse_args()\n-\n-\n-def main():\n-    args = parse_args()\n-\n-    # Import training_script as a module.\n-    script_fpath = Path(args.training_script)\n-    sys.path.append(str(script_fpath.parent.resolve()))\n-    mod_name = script_fpath.stem\n-    mod = importlib.import_module(mod_name)\n-\n-    # Patch sys.argv\n-    sys.argv = [args.training_script] + args.training_script_args\n-\n-    xmp.spawn(mod._mp_fn, args=())\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "fbf17f84d2d7eed9ad889e77c8b965f9d5115abd",
            "filename": "examples/legacy/token-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 294,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2FREADME.md?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,294 +0,0 @@\n-## Token classification\n-\n-Based on the scripts [`run_ner.py`](https://github.com/huggingface/transformers/blob/main/examples/legacy/token-classification/run_ner.py).\n-\n-The following examples are covered in this section:\n-\n-* NER on the GermEval 2014 (German NER) dataset\n-* Emerging and Rare Entities task: WNUTâ€™17 (English NER) dataset\n-\n-Details and results for the fine-tuning provided by @stefan-it.\n-\n-### GermEval 2014 (German NER) dataset\n-\n-#### Data (Download and pre-processing steps)\n-\n-Data can be obtained from the [GermEval 2014](https://sites.google.com/site/germeval2014ner/data) shared task page.\n-\n-Here are the commands for downloading and pre-processing train, dev and test datasets. The original data format has four (tab-separated) columns, in a pre-processing step only the two relevant columns (token and outer span NER annotation) are extracted:\n-\n-```bash\n-curl -L 'https://drive.google.com/uc?export=download&id=1Jjhbal535VVz2ap4v4r_rN1UEHTdLK5P' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > train.txt.tmp\n-curl -L 'https://drive.google.com/uc?export=download&id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > dev.txt.tmp\n-curl -L 'https://drive.google.com/uc?export=download&id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > test.txt.tmp\n-```\n-\n-The GermEval 2014 dataset contains some strange \"control character\" tokens like `'\\x96', '\\u200e', '\\x95', '\\xad' or '\\x80'`.\n-One problem with these tokens is, that `BertTokenizer` returns an empty token for them, resulting in misaligned `InputExample`s.\n-The `preprocess.py` script located in the `scripts` folder a) filters these tokens and b) splits longer sentences into smaller ones (once the max. subtoken length is reached).\n-\n-Let's define some variables that we need for further pre-processing steps and training the model:\n-\n-```bash\n-export MAX_LENGTH=128\n-export BERT_MODEL=google-bert/bert-base-multilingual-cased\n-```\n-\n-Run the pre-processing script on training, dev and test datasets:\n-\n-```bash\n-python3 scripts/preprocess.py train.txt.tmp $BERT_MODEL $MAX_LENGTH > train.txt\n-python3 scripts/preprocess.py dev.txt.tmp $BERT_MODEL $MAX_LENGTH > dev.txt\n-python3 scripts/preprocess.py test.txt.tmp $BERT_MODEL $MAX_LENGTH > test.txt\n-```\n-\n-The GermEval 2014 dataset has much more labels than CoNLL-2002/2003 datasets, so an own set of labels must be used:\n-\n-```bash\n-cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n-```\n-\n-#### Prepare the run\n-\n-Additional environment variables must be set:\n-\n-```bash\n-export OUTPUT_DIR=germeval-model\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SAVE_STEPS=750\n-export SEED=1\n-```\n-\n-#### Run the Pytorch version\n-\n-To start training, just run:\n-\n-```bash\n-python3 run_ner.py --data_dir ./ \\\n---labels ./labels.txt \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---per_device_train_batch_size $BATCH_SIZE \\\n---save_steps $SAVE_STEPS \\\n---seed $SEED \\\n---do_train \\\n---do_eval \\\n---do_predict\n-```\n-\n-If your GPU supports half-precision training, just add the `--fp16` flag. After training, the model will be both evaluated on development and test datasets.\n-\n-#### JSON-based configuration file\n-\n-Instead of passing all parameters via commandline arguments, the `run_ner.py` script also supports reading parameters from a json-based configuration file:\n-\n-```json\n-{\n-    \"data_dir\": \".\",\n-    \"labels\": \"./labels.txt\",\n-    \"model_name_or_path\": \"google-bert/bert-base-multilingual-cased\",\n-    \"output_dir\": \"germeval-model\",\n-    \"max_seq_length\": 128,\n-    \"num_train_epochs\": 3,\n-    \"per_device_train_batch_size\": 32,\n-    \"save_steps\": 750,\n-    \"seed\": 1,\n-    \"do_train\": true,\n-    \"do_eval\": true,\n-    \"do_predict\": true\n-}\n-```\n-\n-It must be saved with a `.json` extension and can be used by running `python3 run_ner.py config.json`.\n-\n-#### Evaluation\n-\n-Evaluation on development dataset outputs the following for our example:\n-\n-```bash\n-10/04/2019 00:42:06 - INFO - __main__ -   ***** Eval results  *****\n-10/04/2019 00:42:06 - INFO - __main__ -     f1 = 0.8623348017621146\n-10/04/2019 00:42:06 - INFO - __main__ -     loss = 0.07183869666975543\n-10/04/2019 00:42:06 - INFO - __main__ -     precision = 0.8467916366258111\n-10/04/2019 00:42:06 - INFO - __main__ -     recall = 0.8784592370979806\n-```\n-\n-On the test dataset the following results could be achieved:\n-\n-```bash\n-10/04/2019 00:42:42 - INFO - __main__ -   ***** Eval results  *****\n-10/04/2019 00:42:42 - INFO - __main__ -     f1 = 0.8614389652384803\n-10/04/2019 00:42:42 - INFO - __main__ -     loss = 0.07064602487454782\n-10/04/2019 00:42:42 - INFO - __main__ -     precision = 0.8604651162790697\n-10/04/2019 00:42:42 - INFO - __main__ -     recall = 0.8624150210424085\n-```\n-\n-#### Run the Tensorflow 2 version\n-\n-To start training, just run:\n-\n-```bash\n-python3 run_tf_ner.py --data_dir ./ \\\n---labels ./labels.txt \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---per_device_train_batch_size $BATCH_SIZE \\\n---save_steps $SAVE_STEPS \\\n---seed $SEED \\\n---do_train \\\n---do_eval \\\n---do_predict\n-```\n-\n-Such as the Pytorch version, if your GPU supports half-precision training, just add the `--fp16` flag. After training, the model will be both evaluated on development and test datasets.\n-\n-#### Evaluation\n-\n-Evaluation on development dataset outputs the following for our example:\n-```bash\n-           precision    recall  f1-score   support\n-\n- LOCderiv     0.7619    0.6154    0.6809        52\n-  PERpart     0.8724    0.8997    0.8858      4057\n-  OTHpart     0.9360    0.9466    0.9413       711\n-  ORGpart     0.7015    0.6989    0.7002       269\n-  LOCpart     0.7668    0.8488    0.8057       496\n-      LOC     0.8745    0.9191    0.8963       235\n- ORGderiv     0.7723    0.8571    0.8125        91\n- OTHderiv     0.4800    0.6667    0.5581        18\n-      OTH     0.5789    0.6875    0.6286        16\n- PERderiv     0.5385    0.3889    0.4516        18\n-      PER     0.5000    0.5000    0.5000         2\n-      ORG     0.0000    0.0000    0.0000         3\n-\n-micro avg     0.8574    0.8862    0.8715      5968\n-macro avg     0.8575    0.8862    0.8713      5968\n-```\n-\n-On the test dataset the following results could be achieved:\n-```bash\n-           precision    recall  f1-score   support\n-\n-  PERpart     0.8847    0.8944    0.8896      9397\n-  OTHpart     0.9376    0.9353    0.9365      1639\n-  ORGpart     0.7307    0.7044    0.7173       697\n-      LOC     0.9133    0.9394    0.9262       561\n-  LOCpart     0.8058    0.8157    0.8107      1150\n-      ORG     0.0000    0.0000    0.0000         8\n- OTHderiv     0.5882    0.4762    0.5263        42\n- PERderiv     0.6571    0.5227    0.5823        44\n-      OTH     0.4906    0.6667    0.5652        39\n- ORGderiv     0.7016    0.7791    0.7383       172\n- LOCderiv     0.8256    0.6514    0.7282       109\n-      PER     0.0000    0.0000    0.0000        11\n-\n-micro avg     0.8722    0.8774    0.8748     13869\n-macro avg     0.8712    0.8774    0.8740     13869\n-```\n-\n-### Emerging and Rare Entities task: WNUTâ€™17 (English NER) dataset\n-\n-Description of the WNUTâ€™17 task from the [shared task website](http://noisy-text.github.io/2017/index.html):\n-\n-> The WNUTâ€™17 shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions.\n-> Named entities form the basis of many modern approaches to other tasks (like event clustering and summarization), but recall on\n-> them is a real problem in noisy text - even among annotators. This drop tends to be due to novel entities and surface forms.\n-\n-Six labels are available in the dataset. An overview can be found on this [page](http://noisy-text.github.io/2017/files/).\n-\n-#### Data (Download and pre-processing steps)\n-\n-The dataset can be downloaded from the [official GitHub](https://github.com/leondz/emerging_entities_17) repository.\n-\n-The following commands show how to prepare the dataset for fine-tuning:\n-\n-```bash\n-mkdir -p data_wnut_17\n-\n-curl -L 'https://github.com/leondz/emerging_entities_17/raw/master/wnut17train.conll'  | tr '\\t' ' ' > data_wnut_17/train.txt.tmp\n-curl -L 'https://github.com/leondz/emerging_entities_17/raw/master/emerging.dev.conll' | tr '\\t' ' ' > data_wnut_17/dev.txt.tmp\n-curl -L 'https://raw.githubusercontent.com/leondz/emerging_entities_17/master/emerging.test.annotated' | tr '\\t' ' ' > data_wnut_17/test.txt.tmp\n-```\n-\n-Let's define some variables that we need for further pre-processing steps:\n-\n-```bash\n-export MAX_LENGTH=128\n-export BERT_MODEL=google-bert/bert-large-cased\n-```\n-\n-Here we use the English BERT large model for fine-tuning.\n-The `preprocess.py` scripts splits longer sentences into smaller ones (once the max. subtoken length is reached):\n-\n-```bash\n-python3 scripts/preprocess.py data_wnut_17/train.txt.tmp $BERT_MODEL $MAX_LENGTH > data_wnut_17/train.txt\n-python3 scripts/preprocess.py data_wnut_17/dev.txt.tmp $BERT_MODEL $MAX_LENGTH > data_wnut_17/dev.txt\n-python3 scripts/preprocess.py data_wnut_17/test.txt.tmp $BERT_MODEL $MAX_LENGTH > data_wnut_17/test.txt\n-```\n-\n-In the last pre-processing step, the `labels.txt` file needs to be generated. This file contains all available labels:\n-\n-```bash\n-cat data_wnut_17/train.txt data_wnut_17/dev.txt data_wnut_17/test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > data_wnut_17/labels.txt\n-```\n-\n-#### Run the Pytorch version\n-\n-Fine-tuning with the PyTorch version can be started using the `run_ner.py` script. In this example we use a JSON-based configuration file.\n-\n-This configuration file looks like:\n-\n-```json\n-{\n-    \"data_dir\": \"./data_wnut_17\",\n-    \"labels\": \"./data_wnut_17/labels.txt\",\n-    \"model_name_or_path\": \"google-bert/bert-large-cased\",\n-    \"output_dir\": \"wnut-17-model-1\",\n-    \"max_seq_length\": 128,\n-    \"num_train_epochs\": 3,\n-    \"per_device_train_batch_size\": 32,\n-    \"save_steps\": 425,\n-    \"seed\": 1,\n-    \"do_train\": true,\n-    \"do_eval\": true,\n-    \"do_predict\": true,\n-    \"fp16\": false\n-}\n-```\n-\n-If your GPU supports half-precision training, please set `fp16` to `true`.\n-\n-Save this JSON-based configuration under `wnut_17.json`. The fine-tuning can be started with `python3 run_ner_old.py wnut_17.json`.\n-\n-#### Evaluation\n-\n-Evaluation on development dataset outputs the following:\n-\n-```bash\n-05/29/2020 23:33:44 - INFO - __main__ -   ***** Eval results *****\n-05/29/2020 23:33:44 - INFO - __main__ -     eval_loss = 0.26505235286212275\n-05/29/2020 23:33:44 - INFO - __main__ -     eval_precision = 0.7008264462809918\n-05/29/2020 23:33:44 - INFO - __main__ -     eval_recall = 0.507177033492823\n-05/29/2020 23:33:44 - INFO - __main__ -     eval_f1 = 0.5884802220680084\n-05/29/2020 23:33:44 - INFO - __main__ -     epoch = 3.0\n-```\n-\n-On the test dataset the following results could be achieved:\n-\n-```bash\n-05/29/2020 23:33:44 - INFO - transformers.trainer -   ***** Running Prediction *****\n-05/29/2020 23:34:02 - INFO - __main__ -     eval_loss = 0.30948806500973547\n-05/29/2020 23:34:02 - INFO - __main__ -     eval_precision = 0.5840108401084011\n-05/29/2020 23:34:02 - INFO - __main__ -     eval_recall = 0.3994439295644115\n-05/29/2020 23:34:02 - INFO - __main__ -     eval_f1 = 0.47440836543753434\n-```\n-\n-WNUTâ€™17 is a very difficult task. Current state-of-the-art results on this dataset can be found [here](https://nlpprogress.com/english/named_entity_recognition.html)."
        },
        {
            "sha": "b5f1e5f83bc7ffa20756edbfff35b8282caef828",
            "filename": "examples/legacy/token-classification/run.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,36 +0,0 @@\n-## The relevant files are currently on a shared Google\n-## drive at https://drive.google.com/drive/folders/1kC0I2UGl2ltrluI9NqDjaQJGw5iliw_J\n-## Monitor for changes and eventually migrate to use the `datasets` library\n-curl -L 'https://drive.google.com/uc?export=download&id=1Jjhbal535VVz2ap4v4r_rN1UEHTdLK5P' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > train.txt.tmp\n-curl -L 'https://drive.google.com/uc?export=download&id=1ZfRcQThdtAR5PPRjIDtrVP7BtXSCUBbm' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > dev.txt.tmp\n-curl -L 'https://drive.google.com/uc?export=download&id=1u9mb7kNJHWQCWyweMDRMuTFoOHOfeBTH' \\\n-| grep -v \"^#\" | cut -f 2,3 | tr '\\t' ' ' > test.txt.tmp\n-\n-export MAX_LENGTH=128\n-export BERT_MODEL=bert-base-multilingual-cased\n-python3 scripts/preprocess.py train.txt.tmp $BERT_MODEL $MAX_LENGTH > train.txt\n-python3 scripts/preprocess.py dev.txt.tmp $BERT_MODEL $MAX_LENGTH > dev.txt\n-python3 scripts/preprocess.py test.txt.tmp $BERT_MODEL $MAX_LENGTH > test.txt\n-cat train.txt dev.txt test.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n-export OUTPUT_DIR=germeval-model\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SAVE_STEPS=750\n-export SEED=1\n-\n-python3 run_ner.py \\\n---task_type NER \\\n---data_dir . \\\n---labels ./labels.txt \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---per_gpu_train_batch_size $BATCH_SIZE \\\n---save_steps $SAVE_STEPS \\\n---seed $SEED \\\n---do_train \\\n---do_eval \\\n---do_predict"
        },
        {
            "sha": "13341555b699a45f3c2aed59672d950291f54dd4",
            "filename": "examples/legacy/token-classification/run_chunk.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun_chunk.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun_chunk.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun_chunk.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,37 +0,0 @@\n-if ! [ -f ./dev.txt ]; then\n-  echo \"Downloading CONLL2003 dev dataset....\"\n-  curl -L -o ./dev.txt 'https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/valid.txt'\n-fi\n-\n-if ! [ -f ./test.txt ]; then\n-  echo \"Downloading CONLL2003 test dataset....\"\n-  curl -L -o ./test.txt 'https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/test.txt'\n-fi\n-\n-if ! [ -f ./train.txt ]; then\n-  echo \"Downloading CONLL2003 train dataset....\"\n-  curl -L -o ./train.txt 'https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt'\n-fi\n-\n-export MAX_LENGTH=200\n-export BERT_MODEL=bert-base-uncased\n-export OUTPUT_DIR=chunker-model\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SAVE_STEPS=750\n-export SEED=1\n-\n-python3 run_ner.py \\\n---task_type Chunk \\\n---data_dir . \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---per_gpu_train_batch_size $BATCH_SIZE \\\n---save_steps $SAVE_STEPS \\\n---seed $SEED \\\n---do_train \\\n---do_eval \\\n---do_predict\n-"
        },
        {
            "sha": "c73b4f99d9e251ce4cae7fc5a25f46b619561d25",
            "filename": "examples/legacy/token-classification/run_ner.py",
            "status": "removed",
            "additions": 0,
            "deletions": 313,
            "changes": 313,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,313 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fine-tuning the library models for named entity recognition on CoNLL-2003.\"\"\"\n-\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from importlib import import_module\n-from typing import Optional\n-\n-import numpy as np\n-from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\n-from torch import nn\n-from utils_ner import Split, TokenClassificationDataset, TokenClassificationTask\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoModelForTokenClassification,\n-    AutoTokenizer,\n-    DataCollatorWithPadding,\n-    EvalPrediction,\n-    HfArgumentParser,\n-    Trainer,\n-    TrainingArguments,\n-    set_seed,\n-)\n-from transformers.trainer_utils import is_main_process\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    task_type: Optional[str] = field(\n-        default=\"NER\", metadata={\"help\": \"Task type to fine tune in training (e.g. NER, POS, etc)\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n-    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n-    # or just modify its tokenizer_config.json.\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    data_dir: str = field(\n-        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n-    )\n-    labels: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n-    )\n-    max_seq_length: int = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    module = import_module(\"tasks\")\n-    try:\n-        token_classification_task_clazz = getattr(module, model_args.task_type)\n-        token_classification_task: TokenClassificationTask = token_classification_task_clazz()\n-    except AttributeError:\n-        raise ValueError(\n-            f\"Task {model_args.task_type} needs to be defined as a TokenClassificationTask subclass in {module}. \"\n-            f\"Available tasks classes are: {TokenClassificationTask.__subclasses__()}\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n-    )\n-    logger.warning(\n-        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_process_index,\n-        training_args.device,\n-        training_args.n_gpu,\n-        bool(training_args.parallel_mode.value == \"distributed\"),\n-        training_args.fp16,\n-    )\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_process_index):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Set seed\n-    set_seed(training_args.seed)\n-\n-    # Prepare CONLL-2003 task\n-    labels = token_classification_task.get_labels(data_args.labels)\n-    label_map: dict[int, str] = dict(enumerate(labels))\n-    num_labels = len(labels)\n-\n-    # Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        num_labels=num_labels,\n-        id2label=label_map,\n-        label2id={label: i for i, label in enumerate(labels)},\n-        cache_dir=model_args.cache_dir,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=model_args.use_fast,\n-    )\n-    model = AutoModelForTokenClassification.from_pretrained(\n-        model_args.model_name_or_path,\n-        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-    )\n-\n-    # Get datasets\n-    train_dataset = (\n-        TokenClassificationDataset(\n-            token_classification_task=token_classification_task,\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            labels=labels,\n-            model_type=config.model_type,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-            mode=Split.train,\n-        )\n-        if training_args.do_train\n-        else None\n-    )\n-    eval_dataset = (\n-        TokenClassificationDataset(\n-            token_classification_task=token_classification_task,\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            labels=labels,\n-            model_type=config.model_type,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-            mode=Split.dev,\n-        )\n-        if training_args.do_eval\n-        else None\n-    )\n-\n-    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> tuple[list[int], list[int]]:\n-        preds = np.argmax(predictions, axis=2)\n-\n-        batch_size, seq_len = preds.shape\n-\n-        out_label_list = [[] for _ in range(batch_size)]\n-        preds_list = [[] for _ in range(batch_size)]\n-\n-        for i in range(batch_size):\n-            for j in range(seq_len):\n-                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n-                    out_label_list[i].append(label_map[label_ids[i][j]])\n-                    preds_list[i].append(label_map[preds[i][j]])\n-\n-        return preds_list, out_label_list\n-\n-    def compute_metrics(p: EvalPrediction) -> dict:\n-        preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n-        return {\n-            \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n-            \"precision\": precision_score(out_label_list, preds_list),\n-            \"recall\": recall_score(out_label_list, preds_list),\n-            \"f1\": f1_score(out_label_list, preds_list),\n-        }\n-\n-    # Data collator\n-    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8) if training_args.fp16 else None\n-\n-    # Initialize our Trainer\n-    trainer = Trainer(\n-        model=model,\n-        args=training_args,\n-        train_dataset=train_dataset,\n-        eval_dataset=eval_dataset,\n-        compute_metrics=compute_metrics,\n-        data_collator=data_collator,\n-    )\n-\n-    # Training\n-    if training_args.do_train:\n-        trainer.train(\n-            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n-        )\n-        trainer.save_model()\n-        # For convenience, we also re-save the tokenizer to the same directory,\n-        # so that you can share your model easily on huggingface.co/models =)\n-        if trainer.is_world_process_zero():\n-            tokenizer.save_pretrained(training_args.output_dir)\n-\n-    # Evaluation\n-    results = {}\n-    if training_args.do_eval:\n-        logger.info(\"*** Evaluate ***\")\n-\n-        result = trainer.evaluate()\n-\n-        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n-        if trainer.is_world_process_zero():\n-            with open(output_eval_file, \"w\") as writer:\n-                logger.info(\"***** Eval results *****\")\n-                for key, value in result.items():\n-                    logger.info(\"  %s = %s\", key, value)\n-                    writer.write(\"{} = {}\\n\".format(key, value))\n-\n-            results.update(result)\n-\n-    # Predict\n-    if training_args.do_predict:\n-        test_dataset = TokenClassificationDataset(\n-            token_classification_task=token_classification_task,\n-            data_dir=data_args.data_dir,\n-            tokenizer=tokenizer,\n-            labels=labels,\n-            model_type=config.model_type,\n-            max_seq_length=data_args.max_seq_length,\n-            overwrite_cache=data_args.overwrite_cache,\n-            mode=Split.test,\n-        )\n-\n-        predictions, label_ids, metrics = trainer.predict(test_dataset)\n-        preds_list, _ = align_predictions(predictions, label_ids)\n-\n-        output_test_results_file = os.path.join(training_args.output_dir, \"test_results.txt\")\n-        if trainer.is_world_process_zero():\n-            with open(output_test_results_file, \"w\") as writer:\n-                for key, value in metrics.items():\n-                    logger.info(\"  %s = %s\", key, value)\n-                    writer.write(\"{} = {}\\n\".format(key, value))\n-\n-        # Save predictions\n-        output_test_predictions_file = os.path.join(training_args.output_dir, \"test_predictions.txt\")\n-        if trainer.is_world_process_zero():\n-            with open(output_test_predictions_file, \"w\") as writer:\n-                with open(os.path.join(data_args.data_dir, \"test.txt\")) as f:\n-                    token_classification_task.write_predictions_to_file(writer, f, preds_list)\n-\n-    return results\n-\n-\n-def _mp_fn(index):\n-    # For xla_spawn (TPUs)\n-    main()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "7d76ed8a2a8a94bc2cd258c42b78bcdb9ba3243b",
            "filename": "examples/legacy/token-classification/run_pos.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun_pos.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Frun_pos.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun_pos.sh?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,37 +0,0 @@\n-if ! [ -f ./dev.txt ]; then\n-  echo \"Download dev dataset....\"\n-  curl -L -o ./dev.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-dev.conllu'\n-fi\n-\n-if ! [ -f ./test.txt ]; then\n-  echo \"Download test dataset....\"\n-  curl -L -o ./test.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-test.conllu'\n-fi\n-\n-if ! [ -f ./train.txt ]; then\n-  echo \"Download train dataset....\"\n-  curl -L -o ./train.txt 'https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-train.conllu'\n-fi\n-\n-export MAX_LENGTH=200\n-export BERT_MODEL=bert-base-uncased\n-export OUTPUT_DIR=postagger-model\n-export BATCH_SIZE=32\n-export NUM_EPOCHS=3\n-export SAVE_STEPS=750\n-export SEED=1\n-\n-python3 run_ner.py \\\n---task_type POS \\\n---data_dir . \\\n---model_name_or_path $BERT_MODEL \\\n---output_dir $OUTPUT_DIR \\\n---max_seq_length  $MAX_LENGTH \\\n---num_train_epochs $NUM_EPOCHS \\\n---per_gpu_train_batch_size $BATCH_SIZE \\\n---save_steps $SAVE_STEPS \\\n---seed $SEED \\\n---do_train \\\n---do_eval \\\n---do_predict\n-"
        },
        {
            "sha": "8a4cef710db149b3d508026777df8923467f650f",
            "filename": "examples/legacy/token-classification/scripts/preprocess.py",
            "status": "removed",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,41 +0,0 @@\n-import sys\n-\n-from transformers import AutoTokenizer\n-\n-\n-dataset = sys.argv[1]\n-model_name_or_path = sys.argv[2]\n-max_len = int(sys.argv[3])\n-\n-subword_len_counter = 0\n-\n-tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n-max_len -= tokenizer.num_special_tokens_to_add()\n-\n-with open(dataset) as f_p:\n-    for line in f_p:\n-        line = line.rstrip()\n-\n-        if not line:\n-            print(line)\n-            subword_len_counter = 0\n-            continue\n-\n-        token = line.split()[0]\n-\n-        current_subwords_len = len(tokenizer.tokenize(token))\n-\n-        # Token contains strange control characters like \\x96 or \\x95\n-        # Just filter out the complete line\n-        if current_subwords_len == 0:\n-            continue\n-\n-        if (subword_len_counter + current_subwords_len) > max_len:\n-            print()\n-            print(line)\n-            subword_len_counter = current_subwords_len\n-            continue\n-\n-        subword_len_counter += current_subwords_len\n-\n-        print(line)"
        },
        {
            "sha": "7e406fa7757ad6f43a6ab026a0182ce24c13e325",
            "filename": "examples/legacy/token-classification/tasks.py",
            "status": "removed",
            "additions": 0,
            "deletions": 162,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Ftasks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Ftasks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Ftasks.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,162 +0,0 @@\n-import logging\n-import os\n-from typing import TextIO, Union\n-\n-from conllu import parse_incr\n-from utils_ner import InputExample, Split, TokenClassificationTask\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class NER(TokenClassificationTask):\n-    def __init__(self, label_idx=-1):\n-        # in NER datasets, the last column is usually reserved for NER label\n-        self.label_idx = label_idx\n-\n-    def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> list[InputExample]:\n-        if isinstance(mode, Split):\n-            mode = mode.value\n-        file_path = os.path.join(data_dir, f\"{mode}.txt\")\n-        guid_index = 1\n-        examples = []\n-        with open(file_path, encoding=\"utf-8\") as f:\n-            words = []\n-            labels = []\n-            for line in f:\n-                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n-                    if words:\n-                        examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n-                        guid_index += 1\n-                        words = []\n-                        labels = []\n-                else:\n-                    splits = line.split(\" \")\n-                    words.append(splits[0])\n-                    if len(splits) > 1:\n-                        labels.append(splits[self.label_idx].replace(\"\\n\", \"\"))\n-                    else:\n-                        # Examples could have no label for mode = \"test\"\n-                        labels.append(\"O\")\n-            if words:\n-                examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n-        return examples\n-\n-    def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, preds_list: list):\n-        example_id = 0\n-        for line in test_input_reader:\n-            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n-                writer.write(line)\n-                if not preds_list[example_id]:\n-                    example_id += 1\n-            elif preds_list[example_id]:\n-                output_line = line.split()[0] + \" \" + preds_list[example_id].pop(0) + \"\\n\"\n-                writer.write(output_line)\n-            else:\n-                logger.warning(\"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0])\n-\n-    def get_labels(self, path: str) -> list[str]:\n-        if path:\n-            with open(path) as f:\n-                labels = f.read().splitlines()\n-            if \"O\" not in labels:\n-                labels = [\"O\"] + labels\n-            return labels\n-        else:\n-            return [\"O\", \"B-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n-\n-\n-class Chunk(NER):\n-    def __init__(self):\n-        # in CONLL2003 dataset chunk column is second-to-last\n-        super().__init__(label_idx=-2)\n-\n-    def get_labels(self, path: str) -> list[str]:\n-        if path:\n-            with open(path) as f:\n-                labels = f.read().splitlines()\n-            if \"O\" not in labels:\n-                labels = [\"O\"] + labels\n-            return labels\n-        else:\n-            return [\n-                \"O\",\n-                \"B-ADVP\",\n-                \"B-INTJ\",\n-                \"B-LST\",\n-                \"B-PRT\",\n-                \"B-NP\",\n-                \"B-SBAR\",\n-                \"B-VP\",\n-                \"B-ADJP\",\n-                \"B-CONJP\",\n-                \"B-PP\",\n-                \"I-ADVP\",\n-                \"I-INTJ\",\n-                \"I-LST\",\n-                \"I-PRT\",\n-                \"I-NP\",\n-                \"I-SBAR\",\n-                \"I-VP\",\n-                \"I-ADJP\",\n-                \"I-CONJP\",\n-                \"I-PP\",\n-            ]\n-\n-\n-class POS(TokenClassificationTask):\n-    def read_examples_from_file(self, data_dir, mode: Union[Split, str]) -> list[InputExample]:\n-        if isinstance(mode, Split):\n-            mode = mode.value\n-        file_path = os.path.join(data_dir, f\"{mode}.txt\")\n-        guid_index = 1\n-        examples = []\n-\n-        with open(file_path, encoding=\"utf-8\") as f:\n-            for sentence in parse_incr(f):\n-                words = []\n-                labels = []\n-                for token in sentence:\n-                    words.append(token[\"form\"])\n-                    labels.append(token[\"upos\"])\n-                assert len(words) == len(labels)\n-                if words:\n-                    examples.append(InputExample(guid=f\"{mode}-{guid_index}\", words=words, labels=labels))\n-                    guid_index += 1\n-        return examples\n-\n-    def write_predictions_to_file(self, writer: TextIO, test_input_reader: TextIO, preds_list: list):\n-        example_id = 0\n-        for sentence in parse_incr(test_input_reader):\n-            s_p = preds_list[example_id]\n-            out = \"\"\n-            for token in sentence:\n-                out += f\"{token['form']} ({token['upos']}|{s_p.pop(0)}) \"\n-            out += \"\\n\"\n-            writer.write(out)\n-            example_id += 1\n-\n-    def get_labels(self, path: str) -> list[str]:\n-        if path:\n-            with open(path) as f:\n-                return f.read().splitlines()\n-        else:\n-            return [\n-                \"ADJ\",\n-                \"ADP\",\n-                \"ADV\",\n-                \"AUX\",\n-                \"CCONJ\",\n-                \"DET\",\n-                \"INTJ\",\n-                \"NOUN\",\n-                \"NUM\",\n-                \"PART\",\n-                \"PRON\",\n-                \"PROPN\",\n-                \"PUNCT\",\n-                \"SCONJ\",\n-                \"SYM\",\n-                \"VERB\",\n-                \"X\",\n-            ]"
        },
        {
            "sha": "b45a4fab40de1cc12e3e81aa10fabc73d966c55d",
            "filename": "examples/legacy/token-classification/utils_ner.py",
            "status": "removed",
            "additions": 0,
            "deletions": 268,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Futils_ner.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,268 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n-# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Named entity recognition fine-tuning: utilities to work with CoNLL-2003 task.\"\"\"\n-\n-import logging\n-import os\n-from dataclasses import dataclass\n-from enum import Enum\n-from typing import Optional, Union\n-\n-from filelock import FileLock\n-\n-from transformers import PreTrainedTokenizer, is_torch_available\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@dataclass\n-class InputExample:\n-    \"\"\"\n-    A single training/test example for token classification.\n-\n-    Args:\n-        guid: Unique id for the example.\n-        words: list. The words of the sequence.\n-        labels: (Optional) list. The labels for each word of the sequence. This should be\n-        specified for train and dev examples, but not for test examples.\n-    \"\"\"\n-\n-    guid: str\n-    words: list[str]\n-    labels: Optional[list[str]]\n-\n-\n-@dataclass\n-class InputFeatures:\n-    \"\"\"\n-    A single set of features of data.\n-    Property names are the same names as the corresponding inputs to a model.\n-    \"\"\"\n-\n-    input_ids: list[int]\n-    attention_mask: list[int]\n-    token_type_ids: Optional[list[int]] = None\n-    label_ids: Optional[list[int]] = None\n-\n-\n-class Split(Enum):\n-    train = \"train\"\n-    dev = \"dev\"\n-    test = \"test\"\n-\n-\n-class TokenClassificationTask:\n-    @staticmethod\n-    def read_examples_from_file(data_dir, mode: Union[Split, str]) -> list[InputExample]:\n-        raise NotImplementedError\n-\n-    @staticmethod\n-    def get_labels(path: str) -> list[str]:\n-        raise NotImplementedError\n-\n-    @staticmethod\n-    def convert_examples_to_features(\n-        examples: list[InputExample],\n-        label_list: list[str],\n-        max_seq_length: int,\n-        tokenizer: PreTrainedTokenizer,\n-        cls_token_at_end=False,\n-        cls_token=\"[CLS]\",\n-        cls_token_segment_id=1,\n-        sep_token=\"[SEP]\",\n-        sep_token_extra=False,\n-        pad_on_left=False,\n-        pad_token=0,\n-        pad_token_segment_id=0,\n-        pad_token_label_id=-100,\n-        sequence_a_segment_id=0,\n-        mask_padding_with_zero=True,\n-    ) -> list[InputFeatures]:\n-        \"\"\"Loads a data file into a list of `InputFeatures`\n-        `cls_token_at_end` define the location of the CLS token:\n-            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n-            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n-        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n-        \"\"\"\n-        # TODO clean up all this to leverage built-in features of tokenizers\n-\n-        label_map = {label: i for i, label in enumerate(label_list)}\n-\n-        features = []\n-        for ex_index, example in enumerate(examples):\n-            if ex_index % 10_000 == 0:\n-                logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n-\n-            tokens = []\n-            label_ids = []\n-            for word, label in zip(example.words, example.labels):\n-                word_tokens = tokenizer.tokenize(word)\n-\n-                # google-bert/bert-base-multilingual-cased sometimes output \"nothing ([]) when calling tokenize with just a space.\n-                if len(word_tokens) > 0:\n-                    tokens.extend(word_tokens)\n-                    # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n-                    label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n-\n-            # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n-            special_tokens_count = tokenizer.num_special_tokens_to_add()\n-            if len(tokens) > max_seq_length - special_tokens_count:\n-                tokens = tokens[: (max_seq_length - special_tokens_count)]\n-                label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n-\n-            # The convention in BERT is:\n-            # (a) For sequence pairs:\n-            #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n-            #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n-            # (b) For single sequences:\n-            #  tokens:   [CLS] the dog is hairy . [SEP]\n-            #  type_ids:   0   0   0   0  0     0   0\n-            #\n-            # Where \"type_ids\" are used to indicate whether this is the first\n-            # sequence or the second sequence. The embedding vectors for `type=0` and\n-            # `type=1` were learned during pre-training and are added to the wordpiece\n-            # embedding vector (and position vector). This is not *strictly* necessary\n-            # since the [SEP] token unambiguously separates the sequences, but it makes\n-            # it easier for the model to learn the concept of sequences.\n-            #\n-            # For classification tasks, the first vector (corresponding to [CLS]) is\n-            # used as the \"sentence vector\". Note that this only makes sense because\n-            # the entire model is fine-tuned.\n-            tokens += [sep_token]\n-            label_ids += [pad_token_label_id]\n-            if sep_token_extra:\n-                # roberta uses an extra separator b/w pairs of sentences\n-                tokens += [sep_token]\n-                label_ids += [pad_token_label_id]\n-            segment_ids = [sequence_a_segment_id] * len(tokens)\n-\n-            if cls_token_at_end:\n-                tokens += [cls_token]\n-                label_ids += [pad_token_label_id]\n-                segment_ids += [cls_token_segment_id]\n-            else:\n-                tokens = [cls_token] + tokens\n-                label_ids = [pad_token_label_id] + label_ids\n-                segment_ids = [cls_token_segment_id] + segment_ids\n-\n-            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n-\n-            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n-            # tokens are attended to.\n-            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n-\n-            # Zero-pad up to the sequence length.\n-            padding_length = max_seq_length - len(input_ids)\n-            if pad_on_left:\n-                input_ids = ([pad_token] * padding_length) + input_ids\n-                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n-                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n-                label_ids = ([pad_token_label_id] * padding_length) + label_ids\n-            else:\n-                input_ids += [pad_token] * padding_length\n-                input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n-                segment_ids += [pad_token_segment_id] * padding_length\n-                label_ids += [pad_token_label_id] * padding_length\n-\n-            assert len(input_ids) == max_seq_length\n-            assert len(input_mask) == max_seq_length\n-            assert len(segment_ids) == max_seq_length\n-            assert len(label_ids) == max_seq_length\n-\n-            if ex_index < 5:\n-                logger.info(\"*** Example ***\")\n-                logger.info(\"guid: %s\", example.guid)\n-                logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n-                logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n-                logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n-                logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n-                logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n-\n-            if \"token_type_ids\" not in tokenizer.model_input_names:\n-                segment_ids = None\n-\n-            features.append(\n-                InputFeatures(\n-                    input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids, label_ids=label_ids\n-                )\n-            )\n-        return features\n-\n-\n-if is_torch_available():\n-    import torch\n-    from torch import nn\n-    from torch.utils.data import Dataset\n-\n-    class TokenClassificationDataset(Dataset):\n-        features: list[InputFeatures]\n-        pad_token_label_id: int = nn.CrossEntropyLoss().ignore_index\n-        # Use cross entropy ignore_index as padding label id so that only\n-        # real label ids contribute to the loss later.\n-\n-        def __init__(\n-            self,\n-            token_classification_task: TokenClassificationTask,\n-            data_dir: str,\n-            tokenizer: PreTrainedTokenizer,\n-            labels: list[str],\n-            model_type: str,\n-            max_seq_length: Optional[int] = None,\n-            overwrite_cache=False,\n-            mode: Split = Split.train,\n-        ):\n-            # Load data features from cache or dataset file\n-            cached_features_file = os.path.join(\n-                data_dir,\n-                f\"cached_{mode.value}_{tokenizer.__class__.__name__}_{str(max_seq_length)}\",\n-            )\n-\n-            # Make sure only the first process in distributed training processes the dataset,\n-            # and the others will use the cache.\n-            lock_path = cached_features_file + \".lock\"\n-            with FileLock(lock_path):\n-                if os.path.exists(cached_features_file) and not overwrite_cache:\n-                    logger.info(f\"Loading features from cached file {cached_features_file}\")\n-                    self.features = torch.load(cached_features_file, weights_only=True)\n-                else:\n-                    logger.info(f\"Creating features from dataset file at {data_dir}\")\n-                    examples = token_classification_task.read_examples_from_file(data_dir, mode)\n-                    # TODO clean up all this to leverage built-in features of tokenizers\n-                    self.features = token_classification_task.convert_examples_to_features(\n-                        examples,\n-                        labels,\n-                        max_seq_length,\n-                        tokenizer,\n-                        cls_token_at_end=bool(model_type == \"xlnet\"),\n-                        # xlnet has a cls token at the end\n-                        cls_token=tokenizer.cls_token,\n-                        cls_token_segment_id=2 if model_type == \"xlnet\" else 0,\n-                        sep_token=tokenizer.sep_token,\n-                        sep_token_extra=False,\n-                        # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n-                        pad_on_left=bool(tokenizer.padding_side == \"left\"),\n-                        pad_token=tokenizer.pad_token_id,\n-                        pad_token_segment_id=tokenizer.pad_token_type_id,\n-                        pad_token_label_id=self.pad_token_label_id,\n-                    )\n-                    logger.info(f\"Saving features into cached file {cached_features_file}\")\n-                    torch.save(self.features, cached_features_file)\n-\n-        def __len__(self):\n-            return len(self.features)\n-\n-        def __getitem__(self, i) -> InputFeatures:\n-            return self.features[i]"
        },
        {
            "sha": "f5f096d5127897fcd35a2a7e071813a3527841a2",
            "filename": "examples/pytorch/language-modeling/README.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2FREADME.md?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -24,8 +24,6 @@ objectives in our [model summary](https://huggingface.co/transformers/model_summ\n \n There are two sets of scripts provided. The first set leverages the Trainer API. The second set with `no_trainer` in the suffix uses a custom training loop and leverages the ðŸ¤— Accelerate library . Both sets use the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n \n-**Note:** The old script `run_language_modeling.py` is still available [here](https://github.com/huggingface/transformers/blob/main/examples/legacy/run_language_modeling.py).\n-\n The following examples, will run on datasets hosted on our [hub](https://huggingface.co/datasets) or with your own\n text files for training and validation. We give examples of both below.\n "
        },
        {
            "sha": "ae0e0b67c8748bbfc848d22e2ec73ecef7ff95d9",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -372,13 +372,8 @@\n     _import_structure[\"data.datasets\"] = [\n         \"GlueDataset\",\n         \"GlueDataTrainingArguments\",\n-        \"LineByLineTextDataset\",\n-        \"LineByLineWithRefDataset\",\n-        \"LineByLineWithSOPTextDataset\",\n         \"SquadDataset\",\n         \"SquadDataTrainingArguments\",\n-        \"TextDataset\",\n-        \"TextDatasetForNextSentencePrediction\",\n     ]\n     _import_structure[\"generation\"].extend(\n         [\n@@ -524,13 +519,8 @@\n     from .data.data_collator import default_data_collator as default_data_collator\n     from .data.datasets import GlueDataset as GlueDataset\n     from .data.datasets import GlueDataTrainingArguments as GlueDataTrainingArguments\n-    from .data.datasets import LineByLineTextDataset as LineByLineTextDataset\n-    from .data.datasets import LineByLineWithRefDataset as LineByLineWithRefDataset\n-    from .data.datasets import LineByLineWithSOPTextDataset as LineByLineWithSOPTextDataset\n     from .data.datasets import SquadDataset as SquadDataset\n     from .data.datasets import SquadDataTrainingArguments as SquadDataTrainingArguments\n-    from .data.datasets import TextDataset as TextDataset\n-    from .data.datasets import TextDatasetForNextSentencePrediction as TextDatasetForNextSentencePrediction\n     from .feature_extraction_sequence_utils import SequenceFeatureExtractor as SequenceFeatureExtractor\n \n     # Feature Extractor"
        },
        {
            "sha": "83d14268a6aca724467c5e967e5849954eda3798",
            "filename": "src/transformers/data/datasets/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/src%2Ftransformers%2Fdata%2Fdatasets%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/src%2Ftransformers%2Fdata%2Fdatasets%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2F__init__.py?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -13,11 +13,4 @@\n # limitations under the License.\n \n from .glue import GlueDataset, GlueDataTrainingArguments\n-from .language_modeling import (\n-    LineByLineTextDataset,\n-    LineByLineWithRefDataset,\n-    LineByLineWithSOPTextDataset,\n-    TextDataset,\n-    TextDatasetForNextSentencePrediction,\n-)\n from .squad import SquadDataset, SquadDataTrainingArguments"
        },
        {
            "sha": "85d7e5360df36a3cac9a639398fff420b8189211",
            "filename": "src/transformers/data/datasets/language_modeling.py",
            "status": "removed",
            "additions": 0,
            "deletions": 514,
            "changes": 514,
            "blob_url": "https://github.com/huggingface/transformers/blob/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5689dd6b8ed0257c6c43c0937d0d224d05774ecc/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Flanguage_modeling.py?ref=5689dd6b8ed0257c6c43c0937d0d224d05774ecc",
            "patch": "@@ -1,514 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import json\n-import os\n-import pickle\n-import random\n-import time\n-import warnings\n-from typing import Optional\n-\n-import torch\n-from filelock import FileLock\n-from torch.utils.data import Dataset\n-\n-from ...tokenization_utils import PreTrainedTokenizer\n-from ...utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-DEPRECATION_WARNING = (\n-    \"This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets \"\n-    \"library. You can have a look at this example script for pointers: {0}\"\n-)\n-\n-\n-class TextDataset(Dataset):\n-    def __init__(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        file_path: str,\n-        block_size: int,\n-        overwrite_cache=False,\n-        cache_dir: Optional[str] = None,\n-    ):\n-        warnings.warn(\n-            DEPRECATION_WARNING.format(\n-                \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\"\n-            ),\n-            FutureWarning,\n-        )\n-        if os.path.isfile(file_path) is False:\n-            raise ValueError(f\"Input file path {file_path} not found\")\n-\n-        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n-\n-        directory, filename = os.path.split(file_path)\n-        cached_features_file = os.path.join(\n-            cache_dir if cache_dir is not None else directory,\n-            f\"cached_lm_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n-        )\n-\n-        # Make sure only the first process in distributed training processes the dataset,\n-        # and the others will use the cache.\n-        lock_path = cached_features_file + \".lock\"\n-        with FileLock(lock_path):\n-            if os.path.exists(cached_features_file) and not overwrite_cache:\n-                start = time.time()\n-                with open(cached_features_file, \"rb\") as handle:\n-                    self.examples = pickle.load(handle)\n-                logger.info(\n-                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n-                )\n-\n-            else:\n-                logger.info(f\"Creating features from dataset file at {directory}\")\n-\n-                self.examples = []\n-                with open(file_path, encoding=\"utf-8\") as f:\n-                    text = f.read()\n-\n-                tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n-\n-                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n-                    self.examples.append(\n-                        tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n-                    )\n-                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n-                # If your dataset is small, first you should look for a bigger one :-) and second you\n-                # can change this behavior by adding (model specific) padding.\n-\n-                start = time.time()\n-                with open(cached_features_file, \"wb\") as handle:\n-                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n-                logger.info(\n-                    f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n-                )\n-\n-    def __len__(self):\n-        return len(self.examples)\n-\n-    def __getitem__(self, i) -> torch.Tensor:\n-        return torch.tensor(self.examples[i], dtype=torch.long)\n-\n-\n-class LineByLineTextDataset(Dataset):\n-    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n-        warnings.warn(\n-            DEPRECATION_WARNING.format(\n-                \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\"\n-            ),\n-            FutureWarning,\n-        )\n-        if os.path.isfile(file_path) is False:\n-            raise ValueError(f\"Input file path {file_path} not found\")\n-        # Here, we do not cache the features, operating under the assumption\n-        # that we will soon use fast multithreaded tokenizers from the\n-        # `tokenizers` repo everywhere =)\n-        logger.info(f\"Creating features from dataset file at {file_path}\")\n-\n-        with open(file_path, encoding=\"utf-8\") as f:\n-            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n-\n-        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n-        self.examples = batch_encoding[\"input_ids\"]\n-        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n-\n-    def __len__(self):\n-        return len(self.examples)\n-\n-    def __getitem__(self, i) -> dict[str, torch.tensor]:\n-        return self.examples[i]\n-\n-\n-class LineByLineWithRefDataset(Dataset):\n-    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int, ref_path: str):\n-        warnings.warn(\n-            DEPRECATION_WARNING.format(\n-                \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm_wwm.py\"\n-            ),\n-            FutureWarning,\n-        )\n-        if os.path.isfile(file_path) is False:\n-            raise ValueError(f\"Input file path {file_path} not found\")\n-        if os.path.isfile(ref_path) is False:\n-            raise ValueError(f\"Ref file path {file_path} not found\")\n-        # Here, we do not cache the features, operating under the assumption\n-        # that we will soon use fast multithreaded tokenizers from the\n-        # `tokenizers` repo everywhere =)\n-        logger.info(f\"Creating features from dataset file at {file_path}\")\n-        logger.info(f\"Use ref segment results at {ref_path}\")\n-        with open(file_path, encoding=\"utf-8\") as f:\n-            data = f.readlines()  # use this method to avoid delimiter '\\u2029' to split a line\n-        data = [line.strip() for line in data if len(line) > 0 and not line.isspace()]\n-        # Get ref inf from file\n-        with open(ref_path, encoding=\"utf-8\") as f:\n-            ref = [json.loads(line) for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n-        if len(data) != len(ref):\n-            raise ValueError(\n-                f\"Length of Input file should be equal to Ref file. But the length of {file_path} is {len(data)} \"\n-                f\"while length of {ref_path} is {len(ref)}\"\n-            )\n-\n-        batch_encoding = tokenizer(data, add_special_tokens=True, truncation=True, max_length=block_size)\n-        self.examples = batch_encoding[\"input_ids\"]\n-        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n-\n-        n = len(self.examples)\n-        for i in range(n):\n-            self.examples[i][\"chinese_ref\"] = torch.tensor(ref[i], dtype=torch.long)\n-\n-    def __len__(self):\n-        return len(self.examples)\n-\n-    def __getitem__(self, i) -> dict[str, torch.tensor]:\n-        return self.examples[i]\n-\n-\n-class LineByLineWithSOPTextDataset(Dataset):\n-    \"\"\"\n-    Dataset for sentence order prediction task, prepare sentence pairs for SOP task\n-    \"\"\"\n-\n-    def __init__(self, tokenizer: PreTrainedTokenizer, file_dir: str, block_size: int):\n-        warnings.warn(\n-            DEPRECATION_WARNING.format(\n-                \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\"\n-            ),\n-            FutureWarning,\n-        )\n-        if os.path.isdir(file_dir) is False:\n-            raise ValueError(f\"{file_dir} is not a directory\")\n-        logger.info(f\"Creating features from dataset file folder at {file_dir}\")\n-        self.examples = []\n-        # TODO: randomness could apply a random seed, ex. rng = random.Random(random_seed)\n-        # file path looks like ./dataset/wiki_1, ./dataset/wiki_2\n-        for file_name in os.listdir(file_dir):\n-            file_path = os.path.join(file_dir, file_name)\n-            if os.path.isfile(file_path) is False:\n-                raise ValueError(f\"{file_path} is not a file\")\n-            article_open = False\n-            with open(file_path, encoding=\"utf-8\") as f:\n-                original_lines = f.readlines()\n-                article_lines = []\n-                for line in original_lines:\n-                    if \"<doc id=\" in line:\n-                        article_open = True\n-                    elif \"</doc>\" in line:\n-                        article_open = False\n-                        document = [\n-                            tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line))\n-                            for line in article_lines[1:]\n-                            if (len(line) > 0 and not line.isspace())\n-                        ]\n-\n-                        examples = self.create_examples_from_document(document, block_size, tokenizer)\n-                        self.examples.extend(examples)\n-                        article_lines = []\n-                    else:\n-                        if article_open:\n-                            article_lines.append(line)\n-\n-        logger.info(\"Dataset parse finished.\")\n-\n-    def create_examples_from_document(self, document, block_size, tokenizer, short_seq_prob=0.1):\n-        \"\"\"Creates examples for a single document.\"\"\"\n-\n-        # Account for special tokens\n-        max_num_tokens = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n-\n-        # We *usually* want to fill up the entire sequence since we are padding\n-        # to `block_size` anyways, so short sequences are generally wasted\n-        # computation. However, we *sometimes*\n-        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n-        # sequences to minimize the mismatch between pretraining and fine-tuning.\n-        # The `target_seq_length` is just a rough target however, whereas\n-        # `block_size` is a hard limit.\n-        target_seq_length = max_num_tokens\n-        if random.random() < short_seq_prob:\n-            target_seq_length = random.randint(2, max_num_tokens)\n-\n-        # We DON'T just concatenate all of the tokens from a document into a long\n-        # sequence and choose an arbitrary split point because this would make the\n-        # next sentence prediction task too easy. Instead, we split the input into\n-        # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n-        # input.\n-        examples = []\n-        current_chunk = []  # a buffer stored current working segments\n-        current_length = 0\n-        i = 0\n-        while i < len(document):\n-            segment = document[i]  # get a segment\n-            if not segment:\n-                i += 1\n-                continue\n-            current_chunk.append(segment)  # add a segment to current chunk\n-            current_length += len(segment)  # overall token length\n-            # if current length goes to the target length or reaches the end of file, start building token a and b\n-            if i == len(document) - 1 or current_length >= target_seq_length:\n-                if current_chunk:\n-                    # `a_end` is how many segments from `current_chunk` go into the `A` (first) sentence.\n-                    a_end = 1\n-                    # if current chunk has more than 2 sentences, pick part of it `A` (first) sentence\n-                    if len(current_chunk) >= 2:\n-                        a_end = random.randint(1, len(current_chunk) - 1)\n-                    # token a\n-                    tokens_a = []\n-                    for j in range(a_end):\n-                        tokens_a.extend(current_chunk[j])\n-\n-                    # token b\n-                    tokens_b = []\n-                    for j in range(a_end, len(current_chunk)):\n-                        tokens_b.extend(current_chunk[j])\n-\n-                    if len(tokens_a) == 0 or len(tokens_b) == 0:\n-                        continue\n-\n-                    # switch tokens_a and tokens_b randomly\n-                    if random.random() < 0.5:\n-                        is_next = False\n-                        tokens_a, tokens_b = tokens_b, tokens_a\n-                    else:\n-                        is_next = True\n-\n-                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n-                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n-                        while True:\n-                            total_length = len(tokens_a) + len(tokens_b)\n-                            if total_length <= max_num_tokens:\n-                                break\n-                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n-                            if not (len(trunc_tokens) >= 1):\n-                                raise ValueError(\"Sequence length to be truncated must be no less than one\")\n-                            # We want to sometimes truncate from the front and sometimes from the\n-                            # back to add more randomness and avoid biases.\n-                            if random.random() < 0.5:\n-                                del trunc_tokens[0]\n-                            else:\n-                                trunc_tokens.pop()\n-\n-                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n-                    if not (len(tokens_a) >= 1):\n-                        raise ValueError(f\"Length of sequence a is {len(tokens_a)} which must be no less than 1\")\n-                    if not (len(tokens_b) >= 1):\n-                        raise ValueError(f\"Length of sequence b is {len(tokens_b)} which must be no less than 1\")\n-\n-                    # add special tokens\n-                    input_ids = tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n-                    # add token type ids, 0 for sentence a, 1 for sentence b\n-                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n-\n-                    example = {\n-                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n-                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n-                        \"sentence_order_label\": torch.tensor(0 if is_next else 1, dtype=torch.long),\n-                    }\n-                    examples.append(example)\n-                current_chunk = []  # clear current chunk\n-                current_length = 0  # reset current text length\n-            i += 1  # go to next line\n-        return examples\n-\n-    def __len__(self):\n-        return len(self.examples)\n-\n-    def __getitem__(self, i) -> dict[str, torch.tensor]:\n-        return self.examples[i]\n-\n-\n-class TextDatasetForNextSentencePrediction(Dataset):\n-    def __init__(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        file_path: str,\n-        block_size: int,\n-        overwrite_cache=False,\n-        short_seq_probability=0.1,\n-        nsp_probability=0.5,\n-    ):\n-        warnings.warn(\n-            DEPRECATION_WARNING.format(\n-                \"https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\"\n-            ),\n-            FutureWarning,\n-        )\n-        if not os.path.isfile(file_path):\n-            raise ValueError(f\"Input file path {file_path} not found\")\n-\n-        self.short_seq_probability = short_seq_probability\n-        self.nsp_probability = nsp_probability\n-\n-        directory, filename = os.path.split(file_path)\n-        cached_features_file = os.path.join(\n-            directory,\n-            f\"cached_nsp_{tokenizer.__class__.__name__}_{block_size}_{filename}\",\n-        )\n-\n-        self.tokenizer = tokenizer\n-\n-        # Make sure only the first process in distributed training processes the dataset,\n-        # and the others will use the cache.\n-        lock_path = cached_features_file + \".lock\"\n-\n-        # Input file format:\n-        # (1) One sentence per line. These should ideally be actual sentences, not\n-        # entire paragraphs or arbitrary spans of text. (Because we use the\n-        # sentence boundaries for the \"next sentence prediction\" task).\n-        # (2) Blank lines between documents. Document boundaries are needed so\n-        # that the \"next sentence prediction\" task doesn't span between documents.\n-        #\n-        # Example:\n-        # I am very happy.\n-        # Here is the second sentence.\n-        #\n-        # A new document.\n-\n-        with FileLock(lock_path):\n-            if os.path.exists(cached_features_file) and not overwrite_cache:\n-                start = time.time()\n-                with open(cached_features_file, \"rb\") as handle:\n-                    self.examples = pickle.load(handle)\n-                logger.info(\n-                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n-                )\n-            else:\n-                logger.info(f\"Creating features from dataset file at {directory}\")\n-\n-                self.documents = [[]]\n-                with open(file_path, encoding=\"utf-8\") as f:\n-                    while True:\n-                        line = f.readline()\n-                        if not line:\n-                            break\n-                        line = line.strip()\n-\n-                        # Empty lines are used as document delimiters\n-                        if not line and len(self.documents[-1]) != 0:\n-                            self.documents.append([])\n-                        tokens = tokenizer.tokenize(line)\n-                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n-                        if tokens:\n-                            self.documents[-1].append(tokens)\n-\n-                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n-                self.examples = []\n-                for doc_index, document in enumerate(self.documents):\n-                    self.create_examples_from_document(document, doc_index, block_size)\n-\n-                start = time.time()\n-                with open(cached_features_file, \"wb\") as handle:\n-                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n-                logger.info(\n-                    f\"Saving features into cached file {cached_features_file} [took {time.time() - start:.3f} s]\"\n-                )\n-\n-    def create_examples_from_document(self, document: list[list[int]], doc_index: int, block_size: int):\n-        \"\"\"Creates examples for a single document.\"\"\"\n-\n-        max_num_tokens = block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n-\n-        # We *usually* want to fill up the entire sequence since we are padding\n-        # to `block_size` anyways, so short sequences are generally wasted\n-        # computation. However, we *sometimes*\n-        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n-        # sequences to minimize the mismatch between pretraining and fine-tuning.\n-        # The `target_seq_length` is just a rough target however, whereas\n-        # `block_size` is a hard limit.\n-        target_seq_length = max_num_tokens\n-        if random.random() < self.short_seq_probability:\n-            target_seq_length = random.randint(2, max_num_tokens)\n-\n-        current_chunk = []  # a buffer stored current working segments\n-        current_length = 0\n-        i = 0\n-\n-        while i < len(document):\n-            segment = document[i]\n-            current_chunk.append(segment)\n-            current_length += len(segment)\n-            if i == len(document) - 1 or current_length >= target_seq_length:\n-                if current_chunk:\n-                    # `a_end` is how many segments from `current_chunk` go into the `A`\n-                    # (first) sentence.\n-                    a_end = 1\n-                    if len(current_chunk) >= 2:\n-                        a_end = random.randint(1, len(current_chunk) - 1)\n-\n-                    tokens_a = []\n-                    for j in range(a_end):\n-                        tokens_a.extend(current_chunk[j])\n-\n-                    tokens_b = []\n-\n-                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n-                        is_random_next = True\n-                        target_b_length = target_seq_length - len(tokens_a)\n-\n-                        # This should rarely go for more than one iteration for large\n-                        # corpora. However, just to be careful, we try to make sure that\n-                        # the random document is not the same as the document\n-                        # we're processing.\n-                        for _ in range(10):\n-                            random_document_index = random.randint(0, len(self.documents) - 1)\n-                            if random_document_index != doc_index:\n-                                break\n-\n-                        random_document = self.documents[random_document_index]\n-                        random_start = random.randint(0, len(random_document) - 1)\n-                        for j in range(random_start, len(random_document)):\n-                            tokens_b.extend(random_document[j])\n-                            if len(tokens_b) >= target_b_length:\n-                                break\n-                        # We didn't actually use these segments so we \"put them back\" so\n-                        # they don't go to waste.\n-                        num_unused_segments = len(current_chunk) - a_end\n-                        i -= num_unused_segments\n-                    # Actual next\n-                    else:\n-                        is_random_next = False\n-                        for j in range(a_end, len(current_chunk)):\n-                            tokens_b.extend(current_chunk[j])\n-\n-                    if not (len(tokens_a) >= 1):\n-                        raise ValueError(f\"Length of sequence a is {len(tokens_a)} which must be no less than 1\")\n-                    if not (len(tokens_b) >= 1):\n-                        raise ValueError(f\"Length of sequence b is {len(tokens_b)} which must be no less than 1\")\n-\n-                    # add special tokens\n-                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n-                    # add token type ids, 0 for sentence a, 1 for sentence b\n-                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n-\n-                    example = {\n-                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n-                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n-                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n-                    }\n-\n-                    self.examples.append(example)\n-\n-                current_chunk = []\n-                current_length = 0\n-\n-            i += 1\n-\n-    def __len__(self):\n-        return len(self.examples)\n-\n-    def __getitem__(self, i):\n-        return self.examples[i]"
        },
        {
            "sha": "a315700a9c010a00a46a66bc71be18958501dae3",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -93,27 +93,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class LineByLineTextDataset(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class LineByLineWithRefDataset(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class LineByLineWithSOPTextDataset(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class SquadDataset(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -128,20 +107,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class TextDataset(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n-class TextDatasetForNextSentencePrediction(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class AlternatingCodebooksLogitsProcessor(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "2905d9c48ed97fede53e7a660974da13d9c4e2ce",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 23,
            "deletions": 27,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -143,7 +143,6 @@\n         GlueDataTrainingArguments,\n         GPT2Config,\n         GPT2LMHeadModel,\n-        LineByLineTextDataset,\n         LlamaConfig,\n         LlamaForCausalLM,\n         PreTrainedModel,\n@@ -164,6 +163,25 @@\n PATH_SAMPLE_TEXT = f\"{get_tests_dir()}/fixtures/sample_text.txt\"\n \n \n+def get_dataset(file_path, tokenizer, max_len):\n+    dataset = datasets.load_dataset(\"text\", data_files=file_path)\n+\n+    # Filter out empty lines\n+    dataset = dataset.filter(lambda example: len(example[\"text\"].strip()) > 0)\n+\n+    # Define tokenization function\n+    def tokenize_function(examples):\n+        tokenized = tokenizer(examples[\"text\"], add_special_tokens=True, truncation=True, max_length=max_len)\n+        # Add labels as a copy of input_ids\n+        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n+        return tokenized\n+\n+    # Apply tokenization and remove original text column\n+    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n+\n+    return tokenized_dataset[\"train\"]\n+\n+\n class StoreLossCallback(TrainerCallback):\n     \"\"\"\n     Simple callback to store the loss.\n@@ -1529,13 +1547,7 @@ def test_multiple_peft_adapters(self):\n         tiny_model = get_peft_model(tiny_model, peft_config, \"adapter1\")\n         tiny_model.add_adapter(\"adapter2\", peft_config)\n \n-        train_dataset = LineByLineTextDataset(\n-            tokenizer=tokenizer,\n-            file_path=PATH_SAMPLE_TEXT,\n-            block_size=tokenizer.max_len_single_sentence,\n-        )\n-        for example in train_dataset.examples:\n-            example[\"labels\"] = example[\"input_ids\"]\n+        train_dataset = get_dataset(PATH_SAMPLE_TEXT, tokenizer, tokenizer.max_len_single_sentence)\n \n         tokenizer.pad_token = tokenizer.eos_token\n \n@@ -3755,13 +3767,7 @@ def test_trainer_eval_multiple(self):\n         MODEL_ID = \"openai-community/gpt2\"\n         tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n         model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n-        dataset = LineByLineTextDataset(\n-            tokenizer=tokenizer,\n-            file_path=PATH_SAMPLE_TEXT,\n-            block_size=tokenizer.max_len_single_sentence,\n-        )\n-        for example in dataset.examples:\n-            example[\"labels\"] = example[\"input_ids\"]\n+        dataset = get_dataset(PATH_SAMPLE_TEXT, tokenizer, tokenizer.max_len_single_sentence)\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             training_args = TrainingArguments(\n                 output_dir=tmp_dir,\n@@ -3785,11 +3791,7 @@ def test_trainer_eval_multiple(self):\n     def test_trainer_eval_lm(self):\n         MODEL_ID = \"distilbert/distilroberta-base\"\n         tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n-        dataset = LineByLineTextDataset(\n-            tokenizer=tokenizer,\n-            file_path=PATH_SAMPLE_TEXT,\n-            block_size=tokenizer.max_len_single_sentence,\n-        )\n+        dataset = get_dataset(PATH_SAMPLE_TEXT, tokenizer, tokenizer.max_len_single_sentence)\n         self.assertEqual(len(dataset), 31)\n \n     def test_training_iterable_dataset(self):\n@@ -4976,13 +4978,7 @@ def test_trainer_works_without_model_config(self):\n         model = BasicTextGenerationModel(vocab_size=tokenizer.vocab_size, hidden_size=32)\n         # Note that this class does not have a config attribute\n \n-        train_dataset = LineByLineTextDataset(\n-            tokenizer=tokenizer,\n-            file_path=PATH_SAMPLE_TEXT,\n-            block_size=tokenizer.max_len_single_sentence,\n-        )\n-        for example in train_dataset.examples:\n-            example[\"labels\"] = example[\"input_ids\"]\n+        train_dataset = get_dataset(PATH_SAMPLE_TEXT, tokenizer, tokenizer.max_len_single_sentence)\n \n         with tempfile.TemporaryDirectory() as tmpdir:\n             training_args = TrainingArguments("
        },
        {
            "sha": "e01f72d74fc9bd603a5d8095b105245cce6fb298",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -948,9 +948,6 @@ def find_all_documented_objects() -> list[str]:\n     \"DataCollatorForSOP\",\n     \"GlueDataset\",\n     \"GlueDataTrainingArguments\",\n-    \"LineByLineTextDataset\",\n-    \"LineByLineWithRefDataset\",\n-    \"LineByLineWithSOPTextDataset\",\n     \"NerPipeline\",\n     \"OwlViTFeatureExtractor\",\n     \"PretrainedBartModel\",\n@@ -962,8 +959,6 @@ def find_all_documented_objects() -> list[str]:\n     \"SquadFeatures\",\n     \"SquadV1Processor\",\n     \"SquadV2Processor\",\n-    \"TextDataset\",\n-    \"TextDatasetForNextSentencePrediction\",\n     \"Wav2Vec2ForMaskedLM\",\n     \"Wav2Vec2Tokenizer\",\n     \"glue_compute_metrics\","
        },
        {
            "sha": "fe8f4b898f9aa734cf8a7d133c49f1df23a6020b",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a0ae4bb81caf000e655cd326712377f777a7a74/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a0ae4bb81caf000e655cd326712377f777a7a74/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=1a0ae4bb81caf000e655cd326712377f777a7a74",
            "patch": "@@ -322,7 +322,6 @@ src/transformers/convert_slow_tokenizer.py\n src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py\n src/transformers/data/data_collator.py\n src/transformers/data/datasets/glue.py\n-src/transformers/data/datasets/language_modeling.py\n src/transformers/data/datasets/squad.py\n src/transformers/data/metrics/squad_metrics.py\n src/transformers/data/processors/glue.py"
        }
    ],
    "stats": {
        "total": 10472,
        "additions": 23,
        "deletions": 10449
    }
}