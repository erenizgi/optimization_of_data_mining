{
    "author": "TKONIY",
    "message": "Fix flash_attention.py: wrong argument passing for attn_implementation (#41347)\n\n* Fix flash_attention.py: wrong argument passing for attn_implementation\n\nThe name of the attn type argument for `_flash_attention_forward()` should be `implementation`, instead of `attn_implementation` which currently uses in the function call. This would result in wrong type specification.\n\n* modify the kwargs inside _flash_attention_forward\n\n* fix the doc\n\n* fix typo\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "ae60c77689d8e9f4cd765e88047064fa41458ca7",
    "files": [
        {
            "sha": "1b64c657333baac463b4fedf47c20765b396a9ff",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae60c77689d8e9f4cd765e88047064fa41458ca7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae60c77689d8e9f4cd765e88047064fa41458ca7/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=ae60c77689d8e9f4cd765e88047064fa41458ca7",
            "patch": "@@ -545,7 +545,7 @@ def _flash_attention_forward(\n     max_length_q: Optional[int] = None,\n     max_length_k: Optional[int] = None,\n     target_dtype: Optional[torch.dtype] = None,\n-    implementation: Optional[str] = None,\n+    attn_implementation: Optional[str] = None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -564,11 +564,11 @@ def _flash_attention_forward(\n         attention_mask (`torch.Tensor`, *optional*):\n             The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n             position of padding tokens and 1 for the position of non-padding tokens.\n-        implementation (`str`, *optional*):\n+        attn_implementation (`str`, *optional*):\n             The attention implementation to use. If None, will default to the one based on the environment.\n     \"\"\"\n     (flash_fn, flash_varlen_fn, pad_fn, unpad_fn), process_flash_kwargs_fn = lazy_import_flash_attention(\n-        implementation\n+        attn_implementation\n     )\n \n     # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}