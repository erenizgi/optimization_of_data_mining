{
    "author": "gante",
    "message": "Cache: don't show warning in forward passes when `past_key_values` is None (#33541)",
    "sha": "80b774eb2906c34265c54b9534758ffc0d619cb7",
    "files": [
        {
            "sha": "05ab9eafa7234949593be1248155a19838b9d86f",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 29,
            "deletions": 4,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -120,7 +120,7 @@ To enable quantization of the key-value cache, one needs to indicate `cache_impl\n Quantization related arguments should be passed to the `generation_config` either as a `dict` or an instance of a [`~QuantizedCacheConfig`] class.\n One has to indicate which quantization backend to use in the [`~QuantizedCacheConfig`], the default is `quanto`.\n \n-It is recommended to set `axis-key/axis-value` parameters in the cache config to `0` if you're using the `quanto` backend and to `1` if you're using the `HQQ` backend. For other config values, please use the defaults unless you're running out of memory. In that case, you may consider decreasing the residual length. \n+It is recommended to set `axis-key/axis-value` parameters in the cache config to `0` if you're using the `quanto` backend and to `1` if you're using the `HQQ` backend. For other config values, please use the defaults unless you're running out of memory. In that case, you may consider decreasing the residual length.\n \n <Tip warning={true}>\n \n@@ -308,7 +308,7 @@ Unlike other cache classes, this one can't be used directly by indicating a `cac\n \n ### Encoder-Decoder Cache\n \n-The [`~EncoderDecoderCache`] is a wrapper designed to handle the caching needs of encoder-decoder models. This cache type is specifically built to manage both self-attention and cross-attention caches, ensuring storage and retrieval of past key/values required for these complex models. Cool thing about Encoder-Decoder Cache is that you can set different cache types for the encoder and for the decoder, depending on your use case. Currently this cache is only supported in [Whisper](./model_doc/whisper) models but we will be adding more models soon. \n+The [`~EncoderDecoderCache`] is a wrapper designed to handle the caching needs of encoder-decoder models. This cache type is specifically built to manage both self-attention and cross-attention caches, ensuring storage and retrieval of past key/values required for these complex models. Cool thing about Encoder-Decoder Cache is that you can set different cache types for the encoder and for the decoder, depending on your use case. Currently this cache is only supported in [Whisper](./model_doc/whisper) models but we will be adding more models soon.\n \n In terms of usage, there is nothing special to be done and calling `generate()` or `forward()` will handle everything for you.\n \n@@ -379,7 +379,7 @@ Sometimes you would want to first fill-in cache object with key/values for certa\n >>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n >>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n->>> # Init StaticCache with big enough max-length (1024 tokens for the below example) \n+>>> # Init StaticCache with big enough max-length (1024 tokens for the below example)\n >>> # You can also init a DynamicCache, if that suits you better\n >>> prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n \n@@ -394,10 +394,35 @@ Sometimes you would want to first fill-in cache object with key/values for certa\n >>> for prompt in prompts:\n ...     new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n ...     past_key_values = copy.deepcopy(prompt_cache)\n-...     outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20) \n+...     outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n ...     response = tokenizer.batch_decode(outputs)[0]\n ...     responses.append(response)\n \n >>> print(responses)\n ['<s> You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTitle: The Ultimate Guide to Travelling: Tips, Tricks, and', '<s> You are a helpful assistant. What is the capital of France?\\n\\nYes, the capital of France is Paris.</s>']\n ```\n+\n+\n+## Legacy cache format\n+\n+Prior to the introduction of the `Cache` object, the cache of LLMs used to be a tuple of tuples of tensors. The legacy\n+format has a dynamic size, growing as we generate text -- very similar to `DynamicCache`. If your project depend on\n+this legacy format, you can seamlessly convert it to a `DynamicCache` and back.\n+\n+```python\n+>>> import torch\n+>>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+>>> inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+\n+>>> # `return_dict_in_generate=True` is required to return the cache. `return_legacy_cache` forces the returned cache\n+>>> # to be of the legacy type\n+>>> generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n+\n+>>> # We can convert a legacy cache to a DynamicCache -- and the other way around. This is helpful if you have custom\n+>>> # logic to manipulate a cache in a specific format.\n+>>> cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n+>>> legacy_format_cache = cache.to_legacy_cache()\n+```"
        },
        {
            "sha": "b5b221b6b37f83ff37aa05c4de4c1a8f9f877aa1",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 15,
            "deletions": 11,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -687,14 +687,18 @@ def forward(\n             inputs_embeds = self.word_embeddings(input_ids)\n \n         # kept for BC (non `Cache` `past_key_values` inputs)\n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"Using `past_key_values` as a tuple is deprecated and will be removed in v4.45. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         batch_size, seq_length, _ = inputs_embeds.shape\n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -765,9 +769,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "be57838975c0b042afdf9164511a8839af533e1b",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -526,14 +526,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        use_legacy_cache = False\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if not self.training:\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]\n@@ -608,9 +612,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "cb1b3f885798c89cbafadb50ca57613f4ac6e59e",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -910,16 +910,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "7263713c08400747ebe07d49050c3ada727e1548",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1059,16 +1059,19 @@ def forward(\n \n         inputs_embeds = nn.functional.dropout(inputs_embeds, p=self.emb_pdrop, training=self.training)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "9a37fe22e1779e74681180feabb9fbd4ae85fa22",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1031,17 +1031,21 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n \n-        # Compute alibi tensor: check build_alibi_tensor documentation\n-        use_legacy_cache = False\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if not self.training:\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n \n+        # Compute alibi tensor: check build_alibi_tensor documentation\n         alibi = None\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         batch_size, seq_length, _ = inputs_embeds.shape\n@@ -1126,9 +1130,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "36f2d1c594abaa5618b98e2204159da1afa993f1",
            "filename": "src/transformers/models/gemma/diff_gemma.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgemma%2Fdiff_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgemma%2Fdiff_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fdiff_gemma.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -476,12 +476,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False  # noqa: F841\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True  # noqa: F841\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "dd4c899d13d465d2dd6ccd710ce8c15646834eab",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 13,
            "deletions": 15,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -828,12 +828,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        return_legacy_cache = False  # noqa: F841\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True  # noqa: F841\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -856,15 +863,6 @@ def forward(\n         # See https://github.com/huggingface/transformers/pull/29402\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None"
        },
        {
            "sha": "7471eec811a065e0cce518d8b13a3dd1612543e7",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -417,14 +417,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -463,9 +468,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "28309f7738ebe20a28e85a83195a6d4deff3ebae",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -741,14 +741,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if not self.training:\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]\n@@ -822,9 +826,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "274c571fa8939cc565e70acd53e6de61a10e690f",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -943,14 +943,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n-        use_legacy_cache = False\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if not self.training:\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]\n@@ -1021,9 +1025,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_attentions] if v is not None)"
        },
        {
            "sha": "048e108a8ec2d7e9d9e66c910c9bd10fba5c9368",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -663,14 +663,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n-        use_legacy_cache = False\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if not self.training:\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]\n@@ -725,9 +729,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_attentions] if v is not None)"
        },
        {
            "sha": "84f6d985f764742d3979795cc1e56d9b47763fee",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -813,14 +813,18 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        use_legacy_cache = False\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if not self.training:\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n \n         seq_length = inputs_embeds.shape[1]\n@@ -917,9 +921,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "f62de411a4fa5f02aad0763e1edb65ba0e11736b",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -834,14 +834,19 @@ def forward(\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "1289bda2d0fd3b6ac9d2707ea725e433b7c316c2",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1239,15 +1239,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n         if use_cache and not isinstance(past_key_values, Cache):\n-            if not self.training:\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n                 logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.45. \"\n-                    \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n                 )\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length, _ = inputs_embeds.shape\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "41be300095e710841f70e41a17a8c3797a516103",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1345,11 +1345,19 @@ def forward(\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n         past_seen_tokens = 0\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if use_cache:\n-            if not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n-                return_legacy_cache = True\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n                 past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n             past_seen_tokens = past_key_values.get_seq_length()\n \n         if inputs_embeds is not None and input_ids is None and past_seen_tokens == 0:"
        },
        {
            "sha": "7c4394d0e1a168637bf424447c2cf2e0515b8e13",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1033,12 +1033,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "0bc44f314b5e8634d2bca4666280be187636c2cc",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -944,16 +944,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "80992734046ad626dd96525d2df86f1751c6ad30",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -762,14 +762,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "fcc0d66e19c4a48f1df1f40bed089139a14eddd1",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1018,14 +1018,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -1095,9 +1100,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "03fa524532a02e8653e941e2e68323118db5c76e",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -866,16 +866,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "2cbde7dc86316965da0dc03671642beca8d33af8",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1007,14 +1007,19 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        # kept for BC (non `Cache` `past_key_values` inputs)\n         return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n+        if use_cache and not isinstance(past_key_values, Cache):\n             return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n-            )\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "a6fd2284afb691eee4a1db2730902b7356e4b640",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -685,14 +685,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -761,9 +766,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)"
        },
        {
            "sha": "4d0a076b5f9a33fd144f69ab0423c9d3ca4187a5",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -976,14 +976,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -1053,9 +1058,10 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast("
        },
        {
            "sha": "e0ca84be1848435b71b889885ce1d0f1cb5e1488",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1003,14 +1003,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -1074,9 +1079,10 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast("
        },
        {
            "sha": "aafecb95b6aafe3355cfb1a33957962449157ce4",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -915,14 +915,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -991,9 +996,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)"
        },
        {
            "sha": "bc06b406bf43edb506e5c167cfdba76b6cebf273",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -1079,14 +1079,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -1161,9 +1166,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple("
        },
        {
            "sha": "13641ecb37f255fc7f5c8731bfc436e7d010fd39",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -960,14 +960,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -1036,9 +1041,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)"
        },
        {
            "sha": "5eaf50f090fa4913e980ac065b10ce5b5a0d7fb1",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80b774eb2906c34265c54b9534758ffc0d619cb7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=80b774eb2906c34265c54b9534758ffc0d619cb7",
            "patch": "@@ -889,14 +889,19 @@ def forward(\n                 )\n                 use_cache = False\n \n-        use_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache) and not self.training:\n-            use_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.46. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -966,9 +971,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = None\n-        if use_cache:\n-            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)"
        }
    ],
    "stats": {
        "total": 653,
        "additions": 402,
        "deletions": 251
    }
}