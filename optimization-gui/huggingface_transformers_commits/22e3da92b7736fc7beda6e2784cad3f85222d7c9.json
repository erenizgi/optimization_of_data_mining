{
    "author": "kkew3",
    "message": "Fix wrong input shapes in doc-string of models (#37729)\n\n* Fix wrong position_ids shape in doc\n\nSupported by ClvpDecoder.forward, line 1212--1215:\n\nsrc/transformers/models/clvp/modeling_clvp.py:\n  1212\t        if inputs_embeds is None:\n  1213\t            inputs_embeds = self.input_embeds_layer(input_ids)\n  1214\t        position_embeds = self.position_embeds_layer(position_ids)\n  1215\t        inputs_embeds = inputs_embeds + position_embeds\n\n* Fix possibly wrong input_ids shape in doc\n\nSince 'input_ids_length' was mentioned immediately after the shape `(batch_size, sequence_length)`, it doesn't make sense to me for `input_ids` to have such shape---IMO it ought to have shape `(batch_size, input_ids_length)` instead.\n\n* Fix possibly wrong inputs_embeds shape in doc\n\nSupported by CTRLModel.forward, line 448--449:\n\nsrc/transformers/models/ctrl/modeling_ctrl.py:\n   448\t        if inputs_embeds is None:\n   449\t            inputs_embeds = self.w(input_ids)\n\nThis commit is introduced due to commit 6f36b56497828642b65f54ea26aa4064186de57a.\n\n* Fix possibly wrong token_type_ids shape in doc\n\nSupported by CTRLModel.forward, line 441--460:\n\nsrc/transformers/models/ctrl/modeling_ctrl.py:\n   441\t        if token_type_ids is not None:\n   442\t            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n   443\t            token_type_embeds = self.w(token_type_ids)\n   444\t            token_type_embeds *= np.sqrt(self.d_model_size)\n   445\t        else:\n   446\t            token_type_embeds = 0\n   447\n   448\t        if inputs_embeds is None:\n   449\t            inputs_embeds = self.w(input_ids)\n   450\t        # inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\n   451\t        seq_len = input_shape[-1]\n   452\t        mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(device)\n   453\n   454\t        inputs_embeds *= np.sqrt(self.d_model_size)\n   455\n   456\t        # `self.pos_encoding` won't be sent to the correct device along the model, so we do it manually.\n   457\t        self.pos_encoding = self.pos_encoding.to(device)\n   458\t        pos_embeds = self.pos_encoding[position_ids, :]\n   459\n   460\t        hidden_states = inputs_embeds + pos_embeds + token_type_embeds\n\nThis commit is introduced due to commit 6f36b56497828642b65f54ea26aa4064186de57a.\n\n* Fix possibly wrong position_ids shape in doc\n\nSupported by CTRLModel.forward, line 448--460:\n\nsrc/transformers/models/ctrl/modeling_ctrl.py:\n   448\t        if inputs_embeds is None:\n   449\t            inputs_embeds = self.w(input_ids)\n   450\t        # inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\n   451\t        seq_len = input_shape[-1]\n   452\t        mask = torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(device)\n   453\n   454\t        inputs_embeds *= np.sqrt(self.d_model_size)\n   455\n   456\t        # `self.pos_encoding` won't be sent to the correct device along the model, so we do it manually.\n   457\t        self.pos_encoding = self.pos_encoding.to(device)\n   458\t        pos_embeds = self.pos_encoding[position_ids, :]\n   459\n   460\t        hidden_states = inputs_embeds + pos_embeds + token_type_embeds\n\nThis commit is introduced due to commit 6f36b56497828642b65f54ea26aa4064186de57a.\n\n* Fix wrong token_type_ids shape in doc\n\nSupported by TFCTRLMainLayer.call, line 376--394:\n\nsrc/transformers/models/ctrl/modeling_tf_ctrl.py:\n   376\t        if token_type_ids is not None:\n   377\t            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n   378\t            token_type_embeds = self.w(token_type_ids)\n   379\t            token_type_embeds *= tf.math.sqrt(tf.cast(self.d_model_size, dtype=token_type_embeds.dtype))\n   380\t        else:\n   381\t            token_type_embeds = tf.constant(0.0)\n   382\t        position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n   383\n   384\t        if inputs_embeds is None:\n   385\t            check_embeddings_within_bounds(input_ids, self.w.input_dim)\n   386\t            inputs_embeds = self.w(input_ids)\n   387\t        seq_len = input_shape[-1]\n   388\t        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n   389\n   390\t        inputs_embeds *= tf.math.sqrt(tf.cast(self.d_model_size, inputs_embeds.dtype))\n   391\n   392\t        pos_embeds = tf.gather(self.pos_encoding, position_ids)\n   393\t        pos_embeds = tf.cast(pos_embeds, dtype=token_type_embeds.dtype)\n   394\t        hidden_states = inputs_embeds + pos_embeds + token_type_embeds\n\n* Fix wrong position_ids shape in doc\n\nSupported by TFCTRLMainLayer.call, line 384--394:\n\nsrc/transformers/models/ctrl/modeling_tf_ctrl.py:\n   384\t        if inputs_embeds is None:\n   385\t            check_embeddings_within_bounds(input_ids, self.w.input_dim)\n   386\t            inputs_embeds = self.w(input_ids)\n   387\t        seq_len = input_shape[-1]\n   388\t        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n   389\n   390\t        inputs_embeds *= tf.math.sqrt(tf.cast(self.d_model_size, inputs_embeds.dtype))\n   391\n   392\t        pos_embeds = tf.gather(self.pos_encoding, position_ids)\n   393\t        pos_embeds = tf.cast(pos_embeds, dtype=token_type_embeds.dtype)\n   394\t        hidden_states = inputs_embeds + pos_embeds + token_type_embeds\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by TFCTRLMainLayer.call, line 384--394:\n\nsrc/transformers/models/ctrl/modeling_tf_ctrl.py:\n   384\t        if inputs_embeds is None:\n   385\t            check_embeddings_within_bounds(input_ids, self.w.input_dim)\n   386\t            inputs_embeds = self.w(input_ids)\n   387\t        seq_len = input_shape[-1]\n   388\t        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n   389\n   390\t        inputs_embeds *= tf.math.sqrt(tf.cast(self.d_model_size, inputs_embeds.dtype))\n   391\n   392\t        pos_embeds = tf.gather(self.pos_encoding, position_ids)\n   393\t        pos_embeds = tf.cast(pos_embeds, dtype=token_type_embeds.dtype)\n   394\t        hidden_states = inputs_embeds + pos_embeds + token_type_embeds\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by ClvpDecoder.forward, line 1212--1213:\n\nsrc/transformers/models/clvp/modeling_clvp.py:\n  1212\t        if inputs_embeds is None:\n  1213\t            inputs_embeds = self.input_embeds_layer(input_ids)\n\n* Fix wrong position_ids shape in doc\n\nSupported by FlaxGemmaPreTrainedModel.__call__, line 502--508:\n\nsrc/transformers/models/gemma/modeling_flax_gemma.py:\n   502\t        batch_size, sequence_length = input_ids.shape\n   503\n   504\t        if position_ids is None:\n   505\t            if past_key_values is not None:\n   506\t                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n   507\n   508\t            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n* Fix wrong position_ids shape in doc\n\nSupported by FlaxGPT2PreTrainedModel.__call__, line 482--488:\n\nsrc/transformers/models/gpt2/modeling_flax_gpt2.py:\n   482\t        batch_size, sequence_length = input_ids.shape\n   483\n   484\t        if position_ids is None:\n   485\t            if past_key_values is not None:\n   486\t                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n   487\n   488\t            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n* Fix wrong position_ids shape in doc\n\nSupported by GPT2Model.forward, line 918--921:\n\nsrc/transformers/models/gpt2/modeling_gpt2.py:\n   918\t        if inputs_embeds is None:\n   919\t            inputs_embeds = self.wte(input_ids)\n   920\t        position_embeds = self.wpe(position_ids)\n   921\t        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by GPT2Model.forward, line 918--919:\n\nsrc/transformers/models/gpt2/modeling_gpt2.py:\n   918\t        if inputs_embeds is None:\n   919\t            inputs_embeds = self.wte(input_ids)\n\n* Fix wrong labels shape in doc\n\nSupported by GPT2LMHeadModel.forward, line 1156--1157:\n\nsrc/transformers/models/gpt2/modeling_gpt2.py:\n  1156\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n  1157\t            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n\n* Fix wrong labels shape in doc\n\nSupported by GPT2DoubleHeadsModel.forward, line 1314--1315:\n\nsrc/transformers/models/gpt2/modeling_gpt2.py:\n  1314\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n  1315\t            `labels = input_ids`. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to\n\n* Fix wrong token_type_ids shape in doc\n\nSupported by TFGPT2MainLayer.call, line 486--500:\n\nsrc/transformers/models/gpt2/modeling_tf_gpt2.py:\n   486\t        if inputs_embeds is None:\n   487\t            check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n   488\t            inputs_embeds = self.wte(input_ids)\n   489\n   490\t        position_embeds = self.wpe(position_ids)\n   491\n   492\t        if token_type_ids is not None:\n   493\t            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n   494\t            token_type_embeds = self.wte(token_type_ids)\n   495\t        else:\n   496\t            token_type_embeds = tf.constant(0.0)\n   497\n   498\t        position_embeds = tf.cast(position_embeds, dtype=inputs_embeds.dtype)\n   499\t        token_type_embeds = tf.cast(token_type_embeds, dtype=inputs_embeds.dtype)\n   500\t        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n\n* Fix wrong position_ids shape in doc\n\nSupported by TFGPT2MainLayer.call, line 486--500:\n\nsrc/transformers/models/gpt2/modeling_tf_gpt2.py:\n   486\t        if inputs_embeds is None:\n   487\t            check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n   488\t            inputs_embeds = self.wte(input_ids)\n   489\n   490\t        position_embeds = self.wpe(position_ids)\n   491\n   492\t        if token_type_ids is not None:\n   493\t            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n   494\t            token_type_embeds = self.wte(token_type_ids)\n   495\t        else:\n   496\t            token_type_embeds = tf.constant(0.0)\n   497\n   498\t        position_embeds = tf.cast(position_embeds, dtype=inputs_embeds.dtype)\n   499\t        token_type_embeds = tf.cast(token_type_embeds, dtype=inputs_embeds.dtype)\n   500\t        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by TFGPT2MainLayer.call, line 486--488:\n\nsrc/transformers/models/gpt2/modeling_tf_gpt2.py:\n   486\t        if inputs_embeds is None:\n   487\t            check_embeddings_within_bounds(input_ids, self.config.vocab_size)\n   488\t            inputs_embeds = self.wte(input_ids)\n\n* Fix wrong position_ids shape in doc\n\nSupported by GPTBigCodeModel.forward, line 962--965:\n\nsrc/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:\n   962\t        if inputs_embeds is None:\n   963\t            inputs_embeds = self.wte(input_ids)\n   964\t        position_embeds = self.wpe(position_ids)\n   965\t        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by GPTBigCodeModel.forward, line 962--963:\n\nsrc/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:\n   962\t        if inputs_embeds is None:\n   963\t            inputs_embeds = self.wte(input_ids)\n\n* Fix wrong labels shape in doc\n\nSupported by GPTBigCodeForCausalLM.forward, line 1158--1159:\n\nsrc/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:\n  1158\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n  1159\t            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n\n* Fix wrong position_ids shape in doc\n\nSupported by FlaxGPTNeoModule.__call__, line 549--552:\n\nsrc/transformers/models/gpt_neo/modeling_flax_gpt_neo.py:\n   549\t        input_embeds = self.wte(input_ids.astype(\"i4\"))\n   550\t        position_embeds = self.wpe(position_ids.astype(\"i4\"))\n   551\n   552\t        hidden_states = input_embeds + position_embeds\n\n* Fix wrong position_ids shape in doc\n\nSupported by GPTNeoModel.forward, line 685--720:\n\nsrc/transformers/models/gpt_neo/modeling_gpt_neo.py:\n   685\t        if inputs_embeds is None:\n   686\t            inputs_embeds = self.wte(input_ids)\n   687\n   688\t        # kept for BC (non `Cache` `past_key_values` inputs)\n   689\t        return_legacy_cache = False\n   690\t        if use_cache and not isinstance(past_key_values, Cache):\n   691\t            return_legacy_cache = True\n   692\t            if past_key_values is None:\n   693\t                past_key_values = DynamicCache()\n   694\t            else:\n   695\t                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n   696\t                logger.warning_once(\n   697\t                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n   698\t                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n   699\t                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n   700\t                )\n   701\n   702\t        seq_length = inputs_embeds.shape[1]\n   703\t        if cache_position is None:\n   704\t            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n   705\t            cache_position = torch.arange(past_seen_tokens, past_seen_tokens + seq_length, device=inputs_embeds.device)\n   706\n   707\t        if position_ids is None:\n   708\t            position_ids = cache_position.unsqueeze(0)\n   709\n   710\t        causal_mask = self._update_causal_mask(\n   711\t            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n   712\t        )\n   713\n   714\t        # Prepare head mask if needed\n   715\t        # 1.0 in head_mask indicate we keep the head\n   716\t        # attention_probs has shape bsz x num_heads x N x N\n   717\t        # head_mask has shape n_layer x batch x num_heads x N x N\n   718\t        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n   719\t        position_embeds = self.wpe(position_ids)\n   720\t        hidden_states = inputs_embeds + position_embeds\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by GPTNeoModel.forward, line 685--686:\n\nsrc/transformers/models/gpt_neo/modeling_gpt_neo.py:\n   685\t        if inputs_embeds is None:\n   686\t            inputs_embeds = self.wte(input_ids)\n\n* Fix wrong labels shape in doc\n\nSupported by GPTNeoForCausalLM.forward, line 968--969:\n\nsrc/transformers/models/gpt_neo/modeling_gpt_neo.py:\n   968\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n   969\t            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n\n* Fix wrong position_ids shape in doc\n\nSupported by FlaxGPTJPreTrainedModel.__call__, line 455--461:\n\nsrc/transformers/models/gptj/modeling_flax_gptj.py:\n   455\t        batch_size, sequence_length = input_ids.shape\n   456\n   457\t        if position_ids is None:\n   458\t            if past_key_values is not None:\n   459\t                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n   460\n   461\t            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n* Fix wrong token_type_ids shape in doc\n\nSupported by TFGPTJMainLayer.call, line 482--493:\n\nsrc/transformers/models/gptj/modeling_tf_gptj.py:\n   482\t        if inputs_embeds is None:\n   483\t            check_embeddings_within_bounds(input_ids, self.wte.vocab_size)\n   484\t            inputs_embeds = self.wte(input_ids, mode=\"embedding\")\n   485\n   486\t        if token_type_ids is not None:\n   487\t            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n   488\t            token_type_embeds = self.wte(token_type_ids, mode=\"embedding\")\n   489\t        else:\n   490\t            token_type_embeds = tf.constant(0.0)\n   491\n   492\t        token_type_embeds = tf.cast(token_type_embeds, dtype=inputs_embeds.dtype)\n   493\t        hidden_states = inputs_embeds + token_type_embeds\n\n* Fix wrong position_ids shape in doc\n\nSupported by TFGPTJMainLayer.call, line 434--449:\n\nsrc/transformers/models/gptj/modeling_tf_gptj.py:\n   434\t        elif input_ids is not None:\n   435\t            input_shape = shape_list(input_ids)\n   436\t            input_ids = tf.reshape(input_ids, [-1, input_shape[-1]])\n   437\t        elif inputs_embeds is not None:\n   438\t            input_shape = shape_list(inputs_embeds)[:-1]\n   439\t        else:\n   440\t            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n   441\n   442\t        if past_key_values is None:\n   443\t            past_length = 0\n   444\t            past_key_values = [None] * len(self.h)\n   445\t        else:\n   446\t            past_length = shape_list(past_key_values[0][0])[-2]\n   447\n   448\t        if position_ids is None:\n   449\t            position_ids = tf.expand_dims(tf.range(past_length, input_shape[-1] + past_length), axis=0)\n\n* Fix wrong inputs_embeds shape in doc\n\nSupported by TFGPTJMainLayer.call, line 482--484:\n\nsrc/transformers/models/gptj/modeling_tf_gptj.py:\n   482\t        if inputs_embeds is None:\n   483\t            check_embeddings_within_bounds(input_ids, self.wte.vocab_size)\n   484\t            inputs_embeds = self.wte(input_ids, mode=\"embedding\")\n\n* Fix wrong labels shape in doc\n\nSupported by TFGPTJForCausalLM.call, line 812--813:\n\nsrc/transformers/models/gptj/modeling_tf_gptj.py:\n   812\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n   813\t            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n\n* Fix possibly wrong input_ids shape in doc\n\nSince 'input_ids_length' was mentioned immediately after the shape `(batch_size, sequence_length)`, it doesn't make sense to me for `input_ids` to have such shape---IMO it ought to have shape `(batch_size, input_ids_length)` instead.\n\n* Fix possibly wrong token_type_ids shape in doc\n\nSupported by ImageGPTModel.forward, line 773--780:\n\nsrc/transformers/models/imagegpt/modeling_imagegpt.py:\n   773\t        if inputs_embeds is None:\n   774\t            inputs_embeds = self.wte(input_ids)\n   775\t        position_embeds = self.wpe(position_ids)\n   776\t        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n   777\n   778\t        if token_type_ids is not None:\n   779\t            token_type_embeds = self.wte(token_type_ids)\n   780\t            hidden_states = hidden_states + token_type_embeds\n\nThis commit is introduced due to commit 8e594a4143cca79f165b99e4ed4c9f3a90047bf3.\n\n* Fix possibly wrong position_ids shape in doc\n\nSupported by ImageGPTModel.forward, line 773--776:\n\nsrc/transformers/models/imagegpt/modeling_imagegpt.py:\n   773\t        if inputs_embeds is None:\n   774\t            inputs_embeds = self.wte(input_ids)\n   775\t        position_embeds = self.wpe(position_ids)\n   776\t        hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n\nThis commit is introduced due to commit 8e594a4143cca79f165b99e4ed4c9f3a90047bf3.\n\n* Fix possibly wrong inputs_embeds shape in doc\n\nSupported by ImageGPTModel.forward, line 773--774:\n\nsrc/transformers/models/imagegpt/modeling_imagegpt.py:\n   773\t        if inputs_embeds is None:\n   774\t            inputs_embeds = self.wte(input_ids)\n\nThis commit is introduced due to commit 8e594a4143cca79f165b99e4ed4c9f3a90047bf3.\n\n* Fix possibly wrong labels shape in doc\n\nSupported by ImageGPTForCausalImageModeling.forward, line 923--924:\n\nsrc/transformers/models/imagegpt/modeling_imagegpt.py:\n   923\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n   924\t            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n\nThis commit is introduced due to commit 8e594a4143cca79f165b99e4ed4c9f3a90047bf3.\n\n* Fix possibly wrong labels shape in doc\n\nSupported by ImageGPTModel.forward, line 665--666:\n\nsrc/transformers/models/imagegpt/modeling_imagegpt.py:\n   665\t            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n   666\t            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n\nThis commit is introduced due to commit 8e594a4143cca79f165b99e4ed4c9f3a90047bf3.\n\n* Fix wrong position_ids shape in doc\n\nSupported by FlaxLlamaPreTrainedModel.__call__, line 484--490:\n\nsrc/transformers/models/llama/modeling_flax_llama.py:\n   484\t        batch_size, sequence_length = input_ids.shape\n   485\n   486\t        if position_ids is None:\n   487\t            if past_key_values is not None:\n   488\t                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n   489\n   490\t            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n* Fix wrong position_ids shape in doc\n\nSupported by FlaxMistralPreTrainedModel.__call__, line 478--484:\n\nsrc/transformers/models/mistral/modeling_flax_mistral.py:\n   478\t        batch_size, sequence_length = input_ids.shape\n   479\n   480\t        if position_ids is None:\n   481\t            if past_key_values is not None:\n   482\t                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n   483\n   484\t            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))",
    "sha": "22e3da92b7736fc7beda6e2784cad3f85222d7c9",
    "files": [
        {
            "sha": "136e8d18b26d074f3881f0a54f7c9be2857799ef",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -935,7 +935,7 @@ def _init_weights(self, module):\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -946,7 +946,7 @@ def _init_weights(self, module):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix."
        },
        {
            "sha": "5426789c0467db31c9d8ad62623e323cfc389c75",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -249,7 +249,7 @@ def _init_weights(self, module):\n \n CTRL_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n             (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n \n@@ -271,15 +271,15 @@ def _init_weights(self, module):\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n             1]`:\n \n             - 0 corresponds to a *sentence A* token,\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -290,7 +290,7 @@ def _init_weights(self, module):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix."
        },
        {
            "sha": "ddb1be17b29f407907af9cc12eb55176c22f6ed1",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -533,15 +533,15 @@ class TFCTRLPreTrainedModel(TFPreTrainedModel):\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`, *optional*):\n+        token_type_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n             1]`:\n \n             - 0 corresponds to a *sentence A* token,\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -552,7 +552,7 @@ class TFCTRLPreTrainedModel(TFPreTrainedModel):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix."
        },
        {
            "sha": "1b8c3671f00ac97625c7800ae3a02b084dbe7497",
            "filename": "src/transformers/models/gemma/modeling_flax_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -103,7 +103,7 @@\n \n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.n_positions - 1]`.\n "
        },
        {
            "sha": "b6000aed5d77a490d0f2ed9d2ef91a7c91ac7d52",
            "filename": "src/transformers/models/gpt2/modeling_flax_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -90,7 +90,7 @@\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n         past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):"
        },
        {
            "sha": "f89d3b4278a813776fac8a2ae449d1a8bc9a516b",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -710,7 +710,7 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -721,7 +721,7 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n@@ -1297,7 +1297,7 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n@@ -1443,7 +1443,7 @@ def forward(\n         mc_token_ids (`torch.LongTensor` of shape `(batch_size, num_choices)`, *optional*, default to index of the last token of the input):\n             Index of the classification token in each input sequence. Selected in the range `[0, input_ids.size(-1) -\n             1]`.\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids`. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`. All labels set to\n             `-100` are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size - 1]`"
        },
        {
            "sha": "03b16eead54be5bb6c94ff3b6d0df4ea0ecbf3f2",
            "filename": "src/transformers/models/gpt2/modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -705,15 +705,15 @@ class TFGPT2DoubleHeadsModelOutput(ModelOutput):\n             `len(past_key_values) + len(input_ids)`\n \n             [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`, *optional*):\n+        token_type_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n             1]`:\n \n             - 0 corresponds to a *sentence A* token,\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -724,7 +724,7 @@ class TFGPT2DoubleHeadsModelOutput(ModelOutput):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`tf.Tensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix."
        },
        {
            "sha": "03550e09edbdd8161187ee31f1cb22172a5e1518",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -751,7 +751,7 @@ def _init_weights(self, module):\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`torch.Tensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -762,7 +762,7 @@ def _init_weights(self, module):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`torch.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`torch.Tensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n@@ -1154,7 +1154,7 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`torch.Tensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`"
        },
        {
            "sha": "7f74b80001f4087d8dacf9ed79261ebd54566f4b",
            "filename": "src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -88,7 +88,7 @@\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n         past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):"
        },
        {
            "sha": "6233b03895bc2c1c6bfbd0b18f805ca185b55f5f",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -583,7 +583,7 @@ def _init_weights(self, module):\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -594,7 +594,7 @@ def _init_weights(self, module):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n@@ -964,7 +964,7 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`"
        },
        {
            "sha": "01ec3acd50f785715ea8c72003f197f2f0389f24",
            "filename": "src/transformers/models/gptj/modeling_flax_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -89,7 +89,7 @@\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n         past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):"
        },
        {
            "sha": "403ca926edfc12f23608b88918da4f32b4b5c031",
            "filename": "src/transformers/models/gptj/modeling_tf_gptj.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -635,15 +635,15 @@ class TFGPTJPreTrainedModel(TFPreTrainedModel):\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`, *optional*):\n+        token_type_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n             1]`:\n \n             - 0 corresponds to a *sentence A* token,\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -654,7 +654,7 @@ class TFGPTJPreTrainedModel(TFPreTrainedModel):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`tf.Tensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n@@ -808,7 +808,7 @@ def call(\n         training: Optional[bool] = False,\n     ) -> Union[TFCausalLMOutputWithPast, Tuple[tf.Tensor]]:\n         r\"\"\"\n-        labels (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`np.ndarray` or `tf.Tensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`"
        },
        {
            "sha": "ae56f7812a3a9f726716b7cf388412dfeb54215a",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -543,7 +543,7 @@ def _init_weights(self, module):\n \n IMAGEGPT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n             `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n             `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n             sequence tokens in the vocabulary.\n@@ -564,15 +564,15 @@ def _init_weights(self, module):\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n             1]`:\n \n             - 0 corresponds to a *sentence A* token,\n             - 1 corresponds to a *sentence B* token.\n \n             [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.max_position_embeddings - 1]`.\n \n@@ -583,7 +583,7 @@ def _init_weights(self, module):\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n \n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n@@ -661,7 +661,7 @@ def forward(\n         **kwargs: Any,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n@@ -919,7 +919,7 @@ def forward(\n         **kwargs: Any,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        labels (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`"
        },
        {
            "sha": "1fed0a36c140ea186142b1b6bed4bee7ec93cf41",
            "filename": "src/transformers/models/llama/modeling_flax_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -109,7 +109,7 @@\n \n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.n_positions - 1]`.\n "
        },
        {
            "sha": "f02446ae3ebcd6384b0662e4d4f50f29f6045e17",
            "filename": "src/transformers/models/mistral/modeling_flax_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e3da92b7736fc7beda6e2784cad3f85222d7c9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py?ref=22e3da92b7736fc7beda6e2784cad3f85222d7c9",
            "patch": "@@ -108,7 +108,7 @@\n \n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n-        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n+        position_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`, *optional*):\n             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n             config.n_positions - 1]`.\n "
        }
    ],
    "stats": {
        "total": 76,
        "additions": 38,
        "deletions": 38
    }
}