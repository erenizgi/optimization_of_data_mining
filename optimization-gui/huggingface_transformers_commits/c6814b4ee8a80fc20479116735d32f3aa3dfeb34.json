{
    "author": "ydshieh",
    "message": "Update ruff to `0.11.2` (#36962)\n\n* update\n\n* update\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
    "files": [
        {
            "sha": "c7e8fd76365f72b80af42cf5393b929262756002",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -162,7 +162,7 @@\n     \"rhoknp>=1.1.0,<1.3.1\",\n     \"rjieba\",\n     \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n-    \"ruff==0.5.1\",\n+    \"ruff==0.11.2\",\n     \"sacrebleu>=1.4.12,<2.0.0\",\n     \"sacremoses\",\n     \"safetensors>=0.4.3\","
        },
        {
            "sha": "f7ead9f2ebe72bf1b0576ff902925f7dec928729",
            "filename": "src/transformers/agents/tools.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fagents%2Ftools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fagents%2Ftools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftools.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -167,9 +167,9 @@ def validate_arguments(self, do_validate_forward: bool = True):\n                 )\n         for input_name, input_content in self.inputs.items():\n             assert isinstance(input_content, dict), f\"Input '{input_name}' should be a dictionary.\"\n-            assert (\n-                \"type\" in input_content and \"description\" in input_content\n-            ), f\"Input '{input_name}' should have keys 'type' and 'description', has only {list(input_content.keys())}.\"\n+            assert \"type\" in input_content and \"description\" in input_content, (\n+                f\"Input '{input_name}' should have keys 'type' and 'description', has only {list(input_content.keys())}.\"\n+            )\n             if input_content[\"type\"] not in authorized_types:\n                 raise Exception(\n                     f\"Input '{input_name}': type '{input_content['type']}' is not an authorized value, should be one of {authorized_types}.\""
        },
        {
            "sha": "e999532ee3183e9c8dab828d576a01381c9cb763",
            "filename": "src/transformers/commands/add_fast_image_processor.py",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -288,7 +288,7 @@ def add_fast_image_processor_to_dummy(fast_image_processor_name: str):\n     if new_dummy_object not in content:\n         if index_new != len(image_processor_names) - 1:\n             # add the dummy object just before the next ImageProcessorFast\n-            first_line = f\"class {image_processor_names[index_new+1]}(metaclass=DummyObject):\"\n+            first_line = f\"class {image_processor_names[index_new + 1]}(metaclass=DummyObject):\"\n             updated_content = content.replace(first_line, new_dummy_object + \"\\n\\n\" + first_line)\n         else:\n             # add the dummy object at the very end\n@@ -313,11 +313,9 @@ def add_fast_image_processor_to_doc(fast_image_processor_name: str, model_name:\n         raise ValueError(f\"No doc files found for {model_name}\")\n \n     base_doc_string = (\n-        f\"## {fast_image_processor_name[:-4]}\\n\\n\" f\"[[autodoc]] {fast_image_processor_name[:-4]}\\n\" \"    - preprocess\"\n-    )\n-    fast_doc_string = (\n-        f\"## {fast_image_processor_name}\\n\\n\" f\"[[autodoc]] {fast_image_processor_name}\\n\" \"    - preprocess\"\n+        f\"## {fast_image_processor_name[:-4]}\\n\\n[[autodoc]] {fast_image_processor_name[:-4]}\\n    - preprocess\"\n     )\n+    fast_doc_string = f\"## {fast_image_processor_name}\\n\\n[[autodoc]] {fast_image_processor_name}\\n    - preprocess\"\n \n     for doc_file in doc_files:\n         with open(doc_file, \"r\", encoding=\"utf-8\") as f:\n@@ -385,7 +383,7 @@ def replacement_function(match):\n     # add the fast image processor to the imports\n     base_import_string = f\"    from transformers import {fast_image_processor_name[:-4]}\"\n     fast_import_string = (\n-        \"    if is_torchvision_available():\\n\" f\"        from transformers import {fast_image_processor_name}\"\n+        f\"    if is_torchvision_available():\\n        from transformers import {fast_image_processor_name}\"\n     )\n     if fast_import_string not in updated_content:\n         updated_content = updated_content.replace(base_import_string, base_import_string + \"\\n\\n\" + fast_import_string)\n@@ -546,17 +544,17 @@ def add_fast_image_processor_file(\n         \"    # For an example of a fast image processor requiring more complex augmentations, see `LlavaNextImageProcessorFast`.\\n\\n\"\n         \"    # Default values should be checked against the slow image processor\\n\"\n         \"    # None values left after checking can be removed\\n\"\n-        f'    resample = {default_args_dict.get(\"resample\")}\\n'\n-        f'    image_mean = {default_args_dict.get(\"image_mean\")}\\n'\n-        f'    image_std = {default_args_dict.get(\"image_std\")}\\n'\n-        f'    size = {default_args_dict.get(\"size\")}\\n'\n-        f'    default_to_square = {default_args_dict.get(\"default_to_square\")}\\n'\n-        f'    crop_size = {default_args_dict.get(\"crop_size\")}\\n'\n-        f'    do_resize = {default_args_dict.get(\"do_resize\")}\\n'\n-        f'    do_center_crop = {default_args_dict.get(\"do_center_crop\")}\\n'\n-        f'    do_rescale = {default_args_dict.get(\"do_rescale\")}\\n'\n-        f'    do_normalize = {default_args_dict.get(\"do_normalize\")}\\n'\n-        f'    do_convert_rgb = {default_args_dict.get(\"do_convert_rgb\")}\\n\\n\\n'\n+        f\"    resample = {default_args_dict.get('resample')}\\n\"\n+        f\"    image_mean = {default_args_dict.get('image_mean')}\\n\"\n+        f\"    image_std = {default_args_dict.get('image_std')}\\n\"\n+        f\"    size = {default_args_dict.get('size')}\\n\"\n+        f\"    default_to_square = {default_args_dict.get('default_to_square')}\\n\"\n+        f\"    crop_size = {default_args_dict.get('crop_size')}\\n\"\n+        f\"    do_resize = {default_args_dict.get('do_resize')}\\n\"\n+        f\"    do_center_crop = {default_args_dict.get('do_center_crop')}\\n\"\n+        f\"    do_rescale = {default_args_dict.get('do_rescale')}\\n\"\n+        f\"    do_normalize = {default_args_dict.get('do_normalize')}\\n\"\n+        f\"    do_convert_rgb = {default_args_dict.get('do_convert_rgb')}\\n\\n\\n\"\n         f'__all__ = [\"{fast_image_processor_name}\"]\\n'\n     )\n "
        },
        {
            "sha": "922ece8c0f45203d92ad77bf7a8a439ea759c62e",
            "filename": "src/transformers/convert_graph_to_onnx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fconvert_graph_to_onnx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fconvert_graph_to_onnx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_graph_to_onnx.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -189,7 +189,7 @@ def build_shape_dict(name: str, tensor, is_input: bool, seq_len: int):\n                     raise ValueError(f\"Unable to infer tensor axes ({len(tensor.shape)})\")\n             else:\n                 seq_axes = [dim for dim, shape in enumerate(tensor.shape) if shape == seq_len]\n-                axes.update({dim: \"sequence\" for dim in seq_axes})\n+                axes.update(dict.fromkeys(seq_axes, \"sequence\"))\n \n         print(f\"Found {'input' if is_input else 'output'} {name} with shape: {axes}\")\n         return axes"
        },
        {
            "sha": "f83c23bdeecfc4bce37b37be29d71459eed1f05f",
            "filename": "src/transformers/data/metrics/squad_metrics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fdata%2Fmetrics%2Fsquad_metrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fdata%2Fmetrics%2Fsquad_metrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fmetrics%2Fsquad_metrics.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -226,7 +226,7 @@ def squad_evaluate(examples, preds, no_answer_probs=None, no_answer_probability_\n     no_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer]\n \n     if no_answer_probs is None:\n-        no_answer_probs = {k: 0.0 for k in preds}\n+        no_answer_probs = dict.fromkeys(preds, 0.0)\n \n     exact, f1 = get_raw_scores(examples, preds)\n "
        },
        {
            "sha": "cbb4a70ab027fab2c75e2ae68cee1d21fc936de4",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -101,7 +101,7 @@ def gen():\n \n         return tf.data.Dataset.from_generator(\n             gen,\n-            ({k: tf.int32 for k in input_names}, label_type),\n+            (dict.fromkeys(input_names, tf.int32), label_type),\n             ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n         )\n "
        },
        {
            "sha": "0384d0c2943e9e3de157c887a83a0e1478a2b4c8",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -68,7 +68,7 @@\n     \"rhoknp\": \"rhoknp>=1.1.0,<1.3.1\",\n     \"rjieba\": \"rjieba\",\n     \"rouge-score\": \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n-    \"ruff\": \"ruff==0.5.1\",\n+    \"ruff\": \"ruff==0.11.2\",\n     \"sacrebleu\": \"sacrebleu>=1.4.12,<2.0.0\",\n     \"sacremoses\": \"sacremoses\",\n     \"safetensors\": \"safetensors>=0.4.3\","
        },
        {
            "sha": "0a349f31855ece3e8a6289fdbdaf0b1713c2a541",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -2749,9 +2749,7 @@ def compute_ngram_keys(self, ngrams: torch.LongTensor) -> torch.LongTensor:\n             ngram keys (batch_size, num_ngrams, depth).\n         \"\"\"\n         if len(ngrams.shape) != 3:\n-            raise ValueError(\n-                \"Ngrams should be of shape (batch_size, num_ngrams, ngram_len), but\" f\" is {ngrams.shape}\"\n-            )\n+            raise ValueError(f\"Ngrams should be of shape (batch_size, num_ngrams, ngram_len), but is {ngrams.shape}\")\n         if ngrams.shape[2] != self.ngram_len:\n             raise ValueError(\n                 \"Ngrams should be of shape (batch_size, num_ngrams, ngram_len),\"\n@@ -2836,7 +2834,7 @@ def sample_g_values(self, ngram_keys: torch.LongTensor) -> torch.LongTensor:\n     def _check_input_ids_shape(self, input_ids: torch.LongTensor):\n         \"\"\"Checks the shape of input ids.\"\"\"\n         if len(input_ids.shape) != 2:\n-            raise ValueError(\"Input ids should be of shape (batch_size, input_len), but is\" f\" {input_ids.shape}\")\n+            raise ValueError(f\"Input ids should be of shape (batch_size, input_len), but is {input_ids.shape}\")\n \n     def compute_g_values(self, input_ids: torch.LongTensor) -> torch.LongTensor:\n         \"\"\""
        },
        {
            "sha": "458342595d4a7226a3e1d6f24fcbb056e08cbe60",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1678,7 +1678,7 @@ def _get_layer_device_map_for_cache_init(self):\n         if execution_device_map is None:\n             return None\n         elif len(execution_device_map) == 1 and \"\" in execution_device_map:\n-            return {idx: execution_device_map[\"\"] for idx in range(num_hidden_layers)}\n+            return dict.fromkeys(range(num_hidden_layers), execution_device_map[\"\"])\n         layer_device_map = {}\n         for layer in execution_device_map:\n             for idx in range(num_hidden_layers):"
        },
        {
            "sha": "4ff154ee2056c26d5e9005c7fa90a970931d41fe",
            "filename": "src/transformers/integrations/hqq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhqq.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -106,11 +106,11 @@ def prepare_for_hqq_linear(model, quantization_config=None, modules_to_not_conve\n \n     if any(key in linear_tags for key in quant_config.keys()):\n         # If the user doesn't specify a key from get_linear_tags, the layer is not quantized via (key, None)\n-        patch_params = {key: None for key in linear_tags}\n+        patch_params = dict.fromkeys(linear_tags)\n         patch_params.update(quant_config)\n     else:\n         # Same quant_config for all layers\n-        patch_params = {k: quant_config for k in linear_tags}\n+        patch_params = dict.fromkeys(linear_tags, quant_config)\n \n     model, has_been_replaced = _prepare_for_hqq_linear(\n         model, patch_params=patch_params, has_been_replaced=has_been_replaced"
        },
        {
            "sha": "1f7b0df3cd3b905805581fe2cf9c0314c1dda72a",
            "filename": "src/transformers/integrations/tpu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fintegrations%2Ftpu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fintegrations%2Ftpu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftpu.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -21,9 +21,9 @@ def tpu_spmd_dataloader(dataloader: DataLoader):\n     if is_torch_xla_available():\n         import torch_xla.distributed.parallel_loader as pl\n \n-        assert isinstance(\n-            dataloader, pl.MpDeviceLoader\n-        ), \"The dataloader must be a `torch_xla.distributed.parallel_loader.MpDeviceLoader`.\"\n+        assert isinstance(dataloader, pl.MpDeviceLoader), (\n+            \"The dataloader must be a `torch_xla.distributed.parallel_loader.MpDeviceLoader`.\"\n+        )\n \n         # This is to support PyTorch/XLA FSDP via SPMD.\n         # Here we shard the input data's 0th dim across the fsdp axis."
        },
        {
            "sha": "2f9039683201c149bbd37e6b24ec2463077e8403",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -154,7 +154,7 @@ def flax_shard_checkpoint(params, max_shard_size=\"10GB\"):\n     weight_map = {}\n     shards = {}\n     for idx, shard in enumerate(sharded_state_dicts):\n-        shard_file = FLAX_WEIGHTS_NAME.replace(\".msgpack\", f\"-{idx+1:05d}-of-{len(sharded_state_dicts):05d}.msgpack\")\n+        shard_file = FLAX_WEIGHTS_NAME.replace(\".msgpack\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.msgpack\")\n         shards[shard_file] = shard\n         for weight_name in shard.keys():\n             weight_map[weight_name] = shard_file"
        },
        {
            "sha": "d569d97f8553c85a720fa95c8a28d1a932e54692",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -701,7 +701,7 @@ def tf_shard_checkpoint(weights, max_shard_size=\"10GB\", weights_name: str = TF2_\n     weight_map = {}\n     shards = {}\n     for idx, shard in enumerate(sharded_state_dicts):\n-        shard_file = weights_name.replace(\".h5\", f\"-{idx+1:05d}-of-{len(sharded_state_dicts):05d}.h5\")\n+        shard_file = weights_name.replace(\".h5\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.h5\")\n         shard_file = shard_file.replace(\n             \".safetensors\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.safetensors\"\n         )"
        },
        {
            "sha": "be79f5b327c957f6fb6274b6c8de701beccb2389",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -2509,9 +2509,9 @@ def tie_encoder_to_decoder_recursively(\n             total_decoder_name=\"\",\n             total_encoder_name=\"\",\n         ):\n-            assert isinstance(decoder_pointer, nn.Module) and isinstance(\n-                encoder_pointer, nn.Module\n-            ), f\"{decoder_pointer} and {encoder_pointer} have to be of type nn.Module\"\n+            assert isinstance(decoder_pointer, nn.Module) and isinstance(encoder_pointer, nn.Module), (\n+                f\"{decoder_pointer} and {encoder_pointer} have to be of type nn.Module\"\n+            )\n             if hasattr(decoder_pointer, \"weight\"):\n                 assert hasattr(encoder_pointer, \"weight\")\n                 encoder_pointer.weight = decoder_pointer.weight\n@@ -2525,9 +2525,9 @@ def tie_encoder_to_decoder_recursively(\n             encoder_modules = encoder_pointer._modules\n             decoder_modules = decoder_pointer._modules\n             if len(decoder_modules) > 0:\n-                assert (\n-                    len(encoder_modules) > 0\n-                ), f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n+                assert len(encoder_modules) > 0, (\n+                    f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n+                )\n \n                 all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()}\n                 encoder_layer_pos = 0\n@@ -3571,7 +3571,7 @@ def save_pretrained(\n                         f\"Please upgrade accelerate with `pip install -U accelerate`\"\n                     )\n                 # init state_dict for this shard\n-                shard_state_dict = {name: \"\" for name in shard}\n+                shard_state_dict = dict.fromkeys(shard, \"\")\n                 for module_name in shard:\n                     # skip to collect this weight again\n                     if shard_state_dict.get(module_name) != \"\":\n@@ -4814,7 +4814,7 @@ def _load_pretrained_model(\n                 param_device_map = expand_device_map(device_map, checkpoint_keys)\n                 str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n                 if sharded_metadata is None:\n-                    weight_map = {p: checkpoint_files[0] for p in checkpoint_keys}\n+                    weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n                 else:\n                     folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n                     # Fix the weight map keys according to the key mapping\n@@ -5446,9 +5446,9 @@ def forward(\n         Returns:\n             `torch.FloatTensor`: The end logits for SQuAD.\n         \"\"\"\n-        assert (\n-            start_states is not None or start_positions is not None\n-        ), \"One of start_states, start_positions should be not None\"\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n         if start_positions is not None:\n             slen, hsz = hidden_states.shape[-2:]\n             start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n@@ -5514,9 +5514,9 @@ def forward(\n         \"\"\"\n         # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n         hsz = hidden_states.shape[-1]\n-        assert (\n-            start_states is not None or start_positions is not None\n-        ), \"One of start_states, start_positions should be not None\"\n+        assert start_states is not None or start_positions is not None, (\n+            \"One of start_states, start_positions should be not None\"\n+        )\n         if start_positions is not None:\n             start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n             start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)"
        },
        {
            "sha": "8a6c845efacda797971ed05e7715bcf18f24cd2c",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1058,7 +1058,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "f8c8399cb61afd2f4c15d9599a0261f8ae8184ab",
            "filename": "src/transformers/models/bark/convert_suno_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -150,7 +150,7 @@ def _load_model(ckpt_path, device, use_small=False, model_type=\"text\"):\n     model.load_state_dict(state_dict, strict=False)\n     n_params = model.num_parameters(exclude_embeddings=True)\n     val_loss = checkpoint[\"best_val_loss\"].item()\n-    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params, {round(val_loss,3)} loss\")\n+    logger.info(f\"model loaded: {round(n_params / 1e6, 1)}M params, {round(val_loss, 3)} loss\")\n     model.eval()\n     model.to(device)\n     del checkpoint, state_dict"
        },
        {
            "sha": "7d1bf211784a0b193236fd5a30a4b6b8c54ea258",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -103,7 +103,7 @@ def from_pretrained(\n             )\n             if speaker_embeddings_path is None:\n                 logger.warning(\n-                    f\"\"\"`{os.path.join(pretrained_processor_name_or_path,speaker_embeddings_dict_path)}` does not exists\n+                    f\"\"\"`{os.path.join(pretrained_processor_name_or_path, speaker_embeddings_dict_path)}` does not exists\n                     , no preloaded speaker embeddings will be used - Make sure to provide a correct path to the json\n                     dictionnary if wanted, otherwise set `speaker_embeddings_dict_path=None`.\"\"\"\n                 )\n@@ -202,7 +202,7 @@ def _load_voice_preset(self, voice_preset: str = None, **kwargs):\n             )\n             if path is None:\n                 raise ValueError(\n-                    f\"\"\"`{os.path.join(self.speaker_embeddings.get(\"repo_or_path\", \"/\"),voice_preset_paths[key])}` does not exists\n+                    f\"\"\"`{os.path.join(self.speaker_embeddings.get(\"repo_or_path\", \"/\"), voice_preset_paths[key])}` does not exists\n                     , no preloaded voice preset will be used - Make sure to provide correct paths to the {voice_preset}\n                     embeddings.\"\"\"\n                 )"
        },
        {
            "sha": "2ab7d9dfc15b93f4650c4ab7c68a3493d7d06e25",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -329,7 +329,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "018e9044bc5c3a1ee377565a35140b6945940c65",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -234,7 +234,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "66488e401a1a28817e892d3578f425b6c378fb75",
            "filename": "src/transformers/models/clap/convert_clap_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclap%2Fconvert_clap_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclap%2Fconvert_clap_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconvert_clap_original_pytorch_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -74,7 +74,7 @@ def rename_state_dict(state_dict):\n             # replace sequential layers with list\n             sequential_layer = re.match(sequential_layers_pattern, key).group(1)\n \n-            key = key.replace(f\"sequential.{sequential_layer}.\", f\"layers.{int(sequential_layer)//3}.linear.\")\n+            key = key.replace(f\"sequential.{sequential_layer}.\", f\"layers.{int(sequential_layer) // 3}.linear.\")\n         elif re.match(text_projection_pattern, key):\n             projecton_layer = int(re.match(text_projection_pattern, key).group(1))\n "
        },
        {
            "sha": "7898a125fa40ae259229d0df14c591ffab5eff57",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -242,7 +242,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "3f363dd51f4e88d969492145b3a96658b33e4861",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -209,7 +209,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=True\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\n         patch_embeds = patch_embeds.flatten(2).transpose(1, 2)"
        },
        {
            "sha": "b06cd5f6a41e92a9828817ce0e2369dda1063694",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -144,7 +144,7 @@ def from_pretrained(\n         # this is to make sure that we can load only text or speech configs from the nested ClvpConfig.\n         if config_type not in cls.base_config_key:\n             raise ValueError(\n-                f\"We can only load either 'text_config' or 'speech_config' but you are trying to load\" f\"{config_type}\"\n+                f\"We can only load either 'text_config' or 'speech_config' but you are trying to load{config_type}\"\n             )\n \n         # get the text config dict if we are loading from ClvpConfig"
        },
        {
            "sha": "c9d4c34b86ac577f2e6ec753450afdacc0704535",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -190,8 +190,8 @@ def update_post_processor(self):\n         if eos is None and self.add_eos_token:\n             raise ValueError(\"add_eos_token = True but eos_token = None\")\n \n-        single = f\"{(bos+':0 ') if self.add_bos_token else ''}$A:0{(' '+eos+':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' '+bos+':1') if self.add_bos_token else ''} $B:1{(' '+eos+':1') if self.add_eos_token else ''}\"\n+        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n+        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n \n         special_tokens = []\n         if self.add_bos_token:"
        },
        {
            "sha": "c8b0f6d3fed65bcd9722bf88372803607d434c2d",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -198,8 +198,8 @@ def update_post_processor(self):\n         if eos is None and self.add_eos_token:\n             raise ValueError(\"add_eos_token = True but eos_token = None\")\n \n-        single = f\"{(bos+':0 ') if self.add_bos_token else ''}$A:0{(' '+eos+':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' '+bos+':1') if self.add_bos_token else ''} $B:1{(' '+eos+':1') if self.add_eos_token else ''}\"\n+        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n+        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n \n         special_tokens = []\n         if self.add_bos_token:"
        },
        {
            "sha": "93c9afe9f65bd914a725110f34650fb26b462694",
            "filename": "src/transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -127,27 +127,27 @@ def convert_data2vec_checkpoint_to_pytorch(\n \n         # self-attention output\n         self_output: BertSelfOutput = layer.attention.output\n-        assert (\n-            self_output.dense.weight.shape == data2vec_layer.self_attn.out_proj.weight.shape\n-        ), f\"Shape for self_output.dense.weight should be {data2vec_layer.self_attn.out_proj.weight.shape}\"\n+        assert self_output.dense.weight.shape == data2vec_layer.self_attn.out_proj.weight.shape, (\n+            f\"Shape for self_output.dense.weight should be {data2vec_layer.self_attn.out_proj.weight.shape}\"\n+        )\n         self_output.dense.weight = data2vec_layer.self_attn.out_proj.weight\n         self_output.dense.bias = data2vec_layer.self_attn.out_proj.bias\n         self_output.LayerNorm.weight = data2vec_layer.self_attn_layer_norm.weight\n         self_output.LayerNorm.bias = data2vec_layer.self_attn_layer_norm.bias\n \n         # intermediate\n         intermediate: BertIntermediate = layer.intermediate\n-        assert (\n-            intermediate.dense.weight.shape == data2vec_layer.fc1.weight.shape\n-        ), f\"Shape for intermediate.dense.weight should be {data2vec_layer.fc1.weight.shape}\"\n+        assert intermediate.dense.weight.shape == data2vec_layer.fc1.weight.shape, (\n+            f\"Shape for intermediate.dense.weight should be {data2vec_layer.fc1.weight.shape}\"\n+        )\n         intermediate.dense.weight = data2vec_layer.fc1.weight\n         intermediate.dense.bias = data2vec_layer.fc1.bias\n \n         # output\n         bert_output: BertOutput = layer.output\n-        assert (\n-            bert_output.dense.weight.shape == data2vec_layer.fc2.weight.shape\n-        ), f\"Shape for bert_output.dense.weight should be {data2vec_layer.fc2.weight.shape}\"\n+        assert bert_output.dense.weight.shape == data2vec_layer.fc2.weight.shape, (\n+            f\"Shape for bert_output.dense.weight should be {data2vec_layer.fc2.weight.shape}\"\n+        )\n         bert_output.dense.weight = data2vec_layer.fc2.weight\n         bert_output.dense.bias = data2vec_layer.fc2.bias\n         bert_output.LayerNorm.weight = data2vec_layer.final_layer_norm.weight"
        },
        {
            "sha": "56056c43138ec3bd163a0de28a6b131a47b890d8",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1491,7 +1491,7 @@ def __init__(\n                     kernel_size=kernel_size,\n                     padding=\"same\",\n                     dilation=dilation,\n-                    name=f\"conv_module_{i+2}\",\n+                    name=f\"conv_module_{i + 2}\",\n                 )\n             )\n         if self.num_convs == 0:"
        },
        {
            "sha": "1f3d675e091de45137f4ff76394bd92525dd9b8e",
            "filename": "src/transformers/models/deprecated/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fbort%2Fconvert_bort_original_gluonnlp_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -180,9 +180,9 @@ def check_and_map_params(hf_param, gluon_param):\n         gluon_param = to_torch(params[gluon_param])\n         shape_gluon = gluon_param.shape\n \n-        assert (\n-            shape_hf == shape_gluon\n-        ), f\"The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers\"\n+        assert shape_hf == shape_gluon, (\n+            f\"The gluon parameter {gluon_param} has shape {shape_gluon}, but expects shape {shape_hf} for Transformers\"\n+        )\n \n         return gluon_param\n "
        },
        {
            "sha": "a8d5eac1e19bed47f779a36fb73374200d8fef06",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -427,7 +427,7 @@ def __init__(self, vocab, ids_to_tokens, emoji):\n             )\n         keisen = \"\"\n         blocks = \"\"\n-        self.content_trans1 = str.maketrans({k: \"<BLOCK>\" for k in keisen + blocks})\n+        self.content_trans1 = str.maketrans(dict.fromkeys(keisen + blocks, \"<BLOCK>\"))\n \n     def __len__(self):\n         return len(self.ids_to_tokens)"
        },
        {
            "sha": "960c8f6ff574016cbf2e293b08c0000c1620b542",
            "filename": "src/transformers/models/deprecated/jukebox/convert_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -200,7 +200,7 @@ def fix_jukebox_keys(state_dict, model_state_dict, key_prefix, mapping):\n         # handle missmatched shape\n         elif value.shape != model_state_dict[f\"{key_prefix}.{key}\"].shape:\n             val = model_state_dict[f\"{key_prefix}.{key}\"]\n-            print(f\"{original_key}-> {key} : \\nshape {val.shape} and { value.shape}, do not match\")\n+            print(f\"{original_key}-> {key} : \\nshape {val.shape} and {value.shape}, do not match\")\n             key = original_key\n \n         mapping[key] = original_key"
        },
        {
            "sha": "6aa077a001f3c8be8c0fb0a5678467d9b9a18200",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -2366,7 +2366,7 @@ def sample_single_window(self, music_tokens, labels, offset, sampling_kwargs, le\n         new_tokens = sample_tokens - previous_sampled_tokens.shape[1]\n \n         logger.info(\n-            f\"Sampling {sample_tokens} tokens for [{start},{start+sample_tokens}]. Conditioning on\"\n+            f\"Sampling {sample_tokens} tokens for [{start},{start + sample_tokens}]. Conditioning on\"\n             f\" {conditioning_tokens} tokens\"\n         )\n \n@@ -2390,7 +2390,7 @@ def sample_single_window(self, music_tokens, labels, offset, sampling_kwargs, le\n             name = [\"Ancestral\", \"Primed\"][music_tokens_i.shape[1] == 0]\n             iterator.set_description(\n                 f\"[prior level {level}] {name} Sampling {sample_tokens} tokens out of\"\n-                f\" {self.total_length//prior.raw_to_tokens}\",\n+                f\" {self.total_length // prior.raw_to_tokens}\",\n                 refresh=True,\n             )\n             tokens_i = prior.sample("
        },
        {
            "sha": "3a19fd24a4c404819a02a2659d353153dfcbc97c",
            "filename": "src/transformers/models/deprecated/open_llama/configuration_open_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fconfiguration_open_llama.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -154,7 +154,7 @@ def _rope_scaling_validation(self):\n \n         if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n             raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n+                f\"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {self.rope_scaling}\"\n             )\n         rope_scaling_type = self.rope_scaling.get(\"type\", None)\n         rope_scaling_factor = self.rope_scaling.get(\"factor\", None)"
        },
        {
            "sha": "d5bf922f42b0450d45cb141f845f18dd62df29f6",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -139,9 +139,9 @@ def load_tf_weights_in_realm(model, config, tf_checkpoint_path):\n         elif m_name == \"kernel\":\n             array = np.transpose(array)\n         try:\n-            assert (\n-                pointer.shape == array.shape\n-            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+            assert pointer.shape == array.shape, (\n+                f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+            )\n         except AssertionError as e:\n             e.args += (pointer.shape, array.shape)\n             raise"
        },
        {
            "sha": "a383e2937f6fd081124c095c980e53471508018c",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -579,7 +579,7 @@ def forward(\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n-                    \"`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache =\" \" False`...\"\n+                    \"`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\"\n                 )\n                 use_cache = False\n "
        },
        {
            "sha": "3a0f9c5ca4b14f5f0f47406a208a3b0f940b683f",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1095,9 +1095,9 @@ def call(\n                 batch_size, sequence_length = shape_list(input_ids)[:2]\n             else:\n                 batch_size, sequence_length = shape_list(inputs_embeds)[:2]\n-            assert (\n-                self.config.pad_token_id is not None or batch_size == 1\n-            ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+            assert self.config.pad_token_id is not None or batch_size == 1, (\n+                \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+            )\n \n             if not tf.is_tensor(sequence_lengths):\n                 in_logits = logits[0:batch_size, sequence_lengths]"
        },
        {
            "sha": "cbab6f2108dc13a889f3299c6bf7475088e3b7f0",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -155,9 +155,9 @@ def load_tf_weights_in_transfo_xl(model, config, tf_path):\n                 p_i.data = torch.from_numpy(arr_i)\n         else:\n             try:\n-                assert (\n-                    pointer.shape == array.shape\n-                ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+                assert pointer.shape == array.shape, (\n+                    f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+                )\n             except AssertionError as e:\n                 e.args += (pointer.shape, array.shape)\n                 raise\n@@ -1238,9 +1238,9 @@ def forward(\n         else:\n             batch_size, sequence_length = inputs_embeds.shape[:2]\n \n-        assert (\n-            self.config.pad_token_id is not None or batch_size == 1\n-        ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+        assert self.config.pad_token_id is not None or batch_size == 1, (\n+            \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n+        )\n         if self.config.pad_token_id is None:\n             sequence_lengths = -1\n         else:"
        },
        {
            "sha": "35c089599bdfa19fa842f04a8facc3e0bc0db5ff",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -588,9 +588,9 @@ def __init__(self, config: XLMProphetNetConfig) -> None:\n         super().__init__(config.max_position_embeddings, config.hidden_size, config.pad_token_id)\n \n     def forward(self, inputs_shape, device, attention_mask=None, past_key_values=None, position_ids=None):\n-        assert (position_ids is None) or (\n-            self.padding_idx is None\n-        ), \"If position_ids is pre-computed then padding_idx should not be set.\"\n+        assert (position_ids is None) or (self.padding_idx is None), (\n+            \"If position_ids is pre-computed then padding_idx should not be set.\"\n+        )\n \n         if position_ids is None:\n             if past_key_values is not None:\n@@ -784,9 +784,9 @@ def __init__(self, config: XLMProphetNetConfig):\n         self.head_dim = config.hidden_size // self.num_attn_heads\n         self.ngram = config.ngram\n \n-        assert (\n-            self.head_dim * self.num_attn_heads == config.hidden_size\n-        ), \"config.hidden_size must be divisible by num_attn_heads\"\n+        assert self.head_dim * self.num_attn_heads == config.hidden_size, (\n+            \"config.hidden_size must be divisible by num_attn_heads\"\n+        )\n         # key, value, query projection\n         self.key_proj = nn.Linear(config.hidden_size, config.hidden_size)\n         self.value_proj = nn.Linear(config.hidden_size, config.hidden_size)\n@@ -1041,9 +1041,9 @@ def get_predict_relative_pos_embeddings(\n \n         if predict_relative_position_buckets is None:\n             key_sequence_length = attn_weights.shape[-1]\n-            assert (\n-                position_ids[0][0] == key_sequence_length - 1\n-            ), \"`position_ids` are incorrect. They should be of the format 1 2 3 4 5 ... (key_sequence_length - 1)\"\n+            assert position_ids[0][0] == key_sequence_length - 1, (\n+                \"`position_ids` are incorrect. They should be of the format 1 2 3 4 5 ... (key_sequence_length - 1)\"\n+            )\n             relative_positions = (\n                 torch.arange(0, key_sequence_length)\n                 .unsqueeze(0)\n@@ -1313,9 +1313,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_hidden_states = encoder_hidden_states + (hidden_states,)\n@@ -1488,9 +1488,9 @@ def forward(\n \n         # prepare attention mask\n         if past_key_values is not None:\n-            assert (\n-                hidden_states.size(1) == 1\n-            ), \"At the moment `use_cache` is only supported for `decoder_input_ids` of length 1\"\n+            assert hidden_states.size(1) == 1, (\n+                \"At the moment `use_cache` is only supported for `decoder_input_ids` of length 1\"\n+            )\n \n             ngram_hidden_states = [\n                 (ngram_embeddings[ngram - 1] + predicting_stream_pos_embed).repeat(batch_size, 1, 1)"
        },
        {
            "sha": "64b58d552067a4827e1d7aeaa892fd90398c6614",
            "filename": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -114,7 +114,7 @@ def __init__(\n         # scaled_images_ratios is sorted\n         if scaled_images_ratios != sorted(scaled_images_ratios):\n             raise ValueError(\n-                f\"Values in scaled_images_ratios={scaled_images_ratios} \" \"should be sorted from low to high\"\n+                f\"Values in scaled_images_ratios={scaled_images_ratios} should be sorted from low to high\"\n             )\n \n         # scaled_images_ratios, scaled_images_overlap_ratios, scaled_images_feature_dims should be consistent"
        },
        {
            "sha": "683cfc67ee76ffc7d0f1de49a61f11991a74bc38",
            "filename": "src/transformers/models/distilbert/modeling_flax_distilbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_flax_distilbert.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -275,9 +275,9 @@ class FlaxTransformerBlock(nn.Module):\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n     def setup(self):\n-        assert (\n-            self.config.dim % self.config.n_heads == 0\n-        ), f\"Hidden size {self.config.dim} not dividable by number of heads {self.config.n_heads}\"\n+        assert self.config.dim % self.config.n_heads == 0, (\n+            f\"Hidden size {self.config.dim} not dividable by number of heads {self.config.n_heads}\"\n+        )\n \n         self.attention = FlaxMultiHeadSelfAttention(self.config, dtype=self.dtype)\n         self.sa_layer_norm = nn.LayerNorm(epsilon=1e-12, dtype=self.dtype)"
        },
        {
            "sha": "d0ee2f84835d41585f6133148a4596a1253e8604",
            "filename": "src/transformers/models/distilbert/modeling_tf_distilbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -269,9 +269,9 @@ def __init__(self, config, **kwargs):\n         self.activation = config.activation\n         self.output_attentions = config.output_attentions\n \n-        assert (\n-            config.dim % config.n_heads == 0\n-        ), f\"Hidden size {config.dim} not dividable by number of heads {config.n_heads}\"\n+        assert config.dim % config.n_heads == 0, (\n+            f\"Hidden size {config.dim} not dividable by number of heads {config.n_heads}\"\n+        )\n \n         self.attention = TFMultiHeadSelfAttention(config, name=\"attention\")\n         self.sa_layer_norm = keras.layers.LayerNormalization(epsilon=1e-12, name=\"sa_layer_norm\")"
        },
        {
            "sha": "d24c2f01db4e1721e649d97123b085237fbef272",
            "filename": "src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpr%2Fconvert_dpr_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpr%2Fconvert_dpr_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fconvert_dpr_original_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -137,7 +137,7 @@ def convert(comp_type: str, src_file: Path, dest_dir: Path):\n     dest_dir = f\"converted-{src_file.name}\" if args.dest is None else args.dest\n     dest_dir = Path(dest_dir)\n     assert src_file.exists()\n-    assert (\n-        args.type is not None\n-    ), \"Please specify the component type of the DPR model to convert: 'ctx_encoder', 'question_encoder' or 'reader'.\"\n+    assert args.type is not None, (\n+        \"Please specify the component type of the DPR model to convert: 'ctx_encoder', 'question_encoder' or 'reader'.\"\n+    )\n     convert(args.type, src_file, dest_dir)"
        },
        {
            "sha": "f4e7c0fdcdbf72aebb97216927dea52577b8bc4c",
            "filename": "src/transformers/models/dpr/tokenization_dpr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Ftokenization_dpr_fast.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -170,9 +170,9 @@ def __call__(\n         texts = texts if not isinstance(texts, str) else [texts]\n         n_passages = len(titles)\n         questions = questions if not isinstance(questions, str) else [questions] * n_passages\n-        assert len(titles) == len(\n-            texts\n-        ), f\"There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.\"\n+        assert len(titles) == len(texts), (\n+            f\"There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.\"\n+        )\n         encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)[\"input_ids\"]\n         encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n         encoded_inputs = {"
        },
        {
            "sha": "d7dc6d104f4074814a9bafeb82476305e54f5540",
            "filename": "src/transformers/models/dpt/convert_dpt_hybrid_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_hybrid_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_hybrid_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_hybrid_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -119,7 +119,7 @@ def rename_key(name):\n     if \"refinenet\" in name:\n         layer_idx = int(name[len(\"neck.refinenet\") : len(\"neck.refinenet\") + 1])\n         # tricky here: we need to map 4 to 0, 3 to 1, 2 to 2 and 1 to 3\n-        name = name.replace(f\"refinenet{layer_idx}\", f\"fusion_stage.layers.{abs(layer_idx-4)}\")\n+        name = name.replace(f\"refinenet{layer_idx}\", f\"fusion_stage.layers.{abs(layer_idx - 4)}\")\n     if \"out_conv\" in name:\n         name = name.replace(\"out_conv\", \"projection\")\n     if \"resConfUnit1\" in name:"
        },
        {
            "sha": "55e0a444e857b0d386e685845bb4842c9f8794d7",
            "filename": "src/transformers/models/dpt/convert_dpt_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -107,7 +107,7 @@ def rename_key(name):\n     if \"refinenet\" in name:\n         layer_idx = int(name[len(\"neck.refinenet\") : len(\"neck.refinenet\") + 1])\n         # tricky here: we need to map 4 to 0, 3 to 1, 2 to 2 and 1 to 3\n-        name = name.replace(f\"refinenet{layer_idx}\", f\"fusion_stage.layers.{abs(layer_idx-4)}\")\n+        name = name.replace(f\"refinenet{layer_idx}\", f\"fusion_stage.layers.{abs(layer_idx - 4)}\")\n     if \"out_conv\" in name:\n         name = name.replace(\"out_conv\", \"projection\")\n     if \"resConfUnit1\" in name:"
        },
        {
            "sha": "e2b279ca672a45718470850986ccf5f1a259e44d",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -617,8 +617,7 @@ def encode(\n             bandwidth = self.config.target_bandwidths[0]\n         if bandwidth not in self.config.target_bandwidths:\n             raise ValueError(\n-                f\"This model doesn't support the bandwidth {bandwidth}. \"\n-                f\"Select one of {self.config.target_bandwidths}.\"\n+                f\"This model doesn't support the bandwidth {bandwidth}. Select one of {self.config.target_bandwidths}.\"\n             )\n \n         _, channels, input_length = input_values.shape"
        },
        {
            "sha": "b05a603fb29f51e2b870c73fa28899cd91936bdb",
            "filename": "src/transformers/models/esm/openfold_utils/residue_constants.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fresidue_constants.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -399,13 +399,11 @@ def map_structure_with_atom_order(in_list: list, first_call: bool = True) -> lis\n \n \n @functools.lru_cache(maxsize=None)\n-def load_stereo_chemical_props() -> (\n-    Tuple[\n-        Mapping[str, List[Bond]],\n-        Mapping[str, List[Bond]],\n-        Mapping[str, List[BondAngle]],\n-    ]\n-):\n+def load_stereo_chemical_props() -> Tuple[\n+    Mapping[str, List[Bond]],\n+    Mapping[str, List[Bond]],\n+    Mapping[str, List[BondAngle]],\n+]:\n     \"\"\"Load stereo_chemical_props.txt into a nice structure.\n \n     Load literature values for bond lengths and bond angles and translate bond angles into the length of the opposite"
        },
        {
            "sha": "ca08cad4d2836c76f6eafd43c42414ec8d82a29c",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1495,9 +1495,9 @@ def __init__(self, num_blocks: int, num_layers: int, in_size: int, out_size: int\n         blocks = OrderedDict()\n         for i in range(num_blocks):\n             if i == 0:\n-                blocks[f\"block_{i+1}\"] = FlavaImageCodebookBlock(in_size, out_size, num_layers)\n+                blocks[f\"block_{i + 1}\"] = FlavaImageCodebookBlock(in_size, out_size, num_layers)\n             else:\n-                blocks[f\"block_{i+1}\"] = FlavaImageCodebookBlock(out_size, out_size, num_layers)\n+                blocks[f\"block_{i + 1}\"] = FlavaImageCodebookBlock(out_size, out_size, num_layers)\n \n         if use_pool:\n             blocks[\"pool\"] = nn.MaxPool2d(kernel_size=2)"
        },
        {
            "sha": "fa5ec7fdda2668abe952ca45d403be779c84e78c",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -539,9 +539,9 @@ def forward(\n         all_attentions = () if output_attentions else None\n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 x = x.transpose(0, 1)  # T x B x C -> B x T x C\n@@ -960,9 +960,9 @@ def forward(\n         attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n         if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (\n-                self.num_heads,\n-            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            assert layer_head_mask.size() == (self.num_heads,), (\n+                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            )\n             attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n "
        },
        {
            "sha": "5dd1b09088b548b0eb74c9ae85c62864f9bc1255",
            "filename": "src/transformers/models/funnel/configuration_funnel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fconfiguration_funnel.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -113,9 +113,9 @@ def __init__(\n         self.vocab_size = vocab_size\n         self.block_sizes = block_sizes\n         self.block_repeats = [1] * len(block_sizes) if block_repeats is None else block_repeats\n-        assert len(block_sizes) == len(\n-            self.block_repeats\n-        ), \"`block_sizes` and `block_repeats` should have the same length.\"\n+        assert len(block_sizes) == len(self.block_repeats), (\n+            \"`block_sizes` and `block_repeats` should have the same length.\"\n+        )\n         self.num_decoder_layers = num_decoder_layers\n         self.d_model = d_model\n         self.n_head = n_head"
        },
        {
            "sha": "50a1975c763b8e7475ceb3a50813fed80be2b1a3",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -195,7 +195,7 @@ def _rope_scaling_validation(self):\n \n         if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n             raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n+                f\"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {self.rope_scaling}\"\n             )\n         rope_scaling_type = self.rope_scaling.get(\"type\", None)\n         rope_scaling_factor = self.rope_scaling.get(\"factor\", None)"
        },
        {
            "sha": "cb15e47d30a4dbcdefbb7fcbc83c810ff07ea690",
            "filename": "src/transformers/models/gemma/tokenization_gemma_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma_fast.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -136,8 +136,8 @@ def update_post_processor(self):\n         if eos is None and self.add_eos_token:\n             raise ValueError(\"add_eos_token = True but eos_token = None\")\n \n-        single = f\"{(bos+':0 ') if self.add_bos_token else ''}$A:0{(' '+eos+':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' '+bos+':1') if self.add_bos_token else ''} $B:1{(' '+eos+':1') if self.add_eos_token else ''}\"\n+        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n+        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n \n         special_tokens = []\n         if self.add_bos_token:"
        },
        {
            "sha": "232370ee01b6d01c7c6b43e6205711d83d9fe806",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -683,7 +683,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "5d18c3b73a5096c37608837732d6b6235b7daf46",
            "filename": "src/transformers/models/glpn/convert_glpn_to_pytorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fglpn%2Fconvert_glpn_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fglpn%2Fconvert_glpn_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fconvert_glpn_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -40,21 +40,21 @@ def rename_keys(state_dict):\n         if \"patch_embed\" in key:\n             # replace for example patch_embed1 by patch_embeddings.0\n             idx = key[key.find(\"patch_embed\") + len(\"patch_embed\")]\n-            key = key.replace(f\"patch_embed{idx}\", f\"patch_embeddings.{int(idx)-1}\")\n+            key = key.replace(f\"patch_embed{idx}\", f\"patch_embeddings.{int(idx) - 1}\")\n         if \"norm\" in key:\n             key = key.replace(\"norm\", \"layer_norm\")\n         if \"glpn.encoder.layer_norm\" in key:\n             # replace for example layer_norm1 by layer_norm.0\n             idx = key[key.find(\"glpn.encoder.layer_norm\") + len(\"glpn.encoder.layer_norm\")]\n-            key = key.replace(f\"layer_norm{idx}\", f\"layer_norm.{int(idx)-1}\")\n+            key = key.replace(f\"layer_norm{idx}\", f\"layer_norm.{int(idx) - 1}\")\n         if \"layer_norm1\" in key:\n             key = key.replace(\"layer_norm1\", \"layer_norm_1\")\n         if \"layer_norm2\" in key:\n             key = key.replace(\"layer_norm2\", \"layer_norm_2\")\n         if \"block\" in key:\n             # replace for example block1 by block.0\n             idx = key[key.find(\"block\") + len(\"block\")]\n-            key = key.replace(f\"block{idx}\", f\"block.{int(idx)-1}\")\n+            key = key.replace(f\"block{idx}\", f\"block.{int(idx) - 1}\")\n         if \"attn.q\" in key:\n             key = key.replace(\"attn.q\", \"attention.self.query\")\n         if \"attn.proj\" in key:\n@@ -73,7 +73,7 @@ def rename_keys(state_dict):\n         if \"linear_c\" in key:\n             # replace for example linear_c4 by linear_c.3\n             idx = key[key.find(\"linear_c\") + len(\"linear_c\")]\n-            key = key.replace(f\"linear_c{idx}\", f\"linear_c.{int(idx)-1}\")\n+            key = key.replace(f\"linear_c{idx}\", f\"linear_c.{int(idx) - 1}\")\n         if \"bot_conv\" in key:\n             key = key.replace(\"bot_conv\", \"0.convolution\")\n         if \"skip_conv1\" in key:"
        },
        {
            "sha": "767a8a68f63c6d51fc75adf8d1df3ccb5b54328d",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -154,8 +154,8 @@ def update_post_processor(self):\n         if eos is None and self.add_eos_token:\n             raise ValueError(\"add_eos_token = True but eos_token = None\")\n \n-        single = f\"{(bos+':0 ') if self.add_bos_token else ''}$A:0{(' '+eos+':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' '+bos+':1') if self.add_bos_token else ''} $B:1{(' '+eos+':1') if self.add_eos_token else ''}\"\n+        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n+        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n \n         special_tokens = []\n         if self.add_bos_token:"
        },
        {
            "sha": "19b0fd2375c3c89d58b737380c278893a1280a55",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -250,7 +250,7 @@ def __init__(self, vocab, ids_to_tokens, emoji):\n             )\n         keisen = \"\"\n         blocks = \"\"\n-        self.content_trans1 = str.maketrans({k: \"<BLOCK>\" for k in keisen + blocks})\n+        self.content_trans1 = str.maketrans(dict.fromkeys(keisen + blocks, \"<BLOCK>\"))\n \n     def __len__(self):\n         return len(self.ids_to_tokens)"
        },
        {
            "sha": "3550f63958628bd67a6458816c89eaaac7cb86bf",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -587,7 +587,7 @@ def __init__(self, config: HubertConfig, **kwargs: Any) -> None:\n \n         if config.feat_extract_norm == \"group\":\n             conv_layers = [TFHubertGroupNormConvLayer(config, layer_id=0, name=f\"conv_layers.{0}\")] + [\n-                TFHubertNoLayerNormConvLayer(config, layer_id=i + 1, name=f\"conv_layers.{i+1}\")\n+                TFHubertNoLayerNormConvLayer(config, layer_id=i + 1, name=f\"conv_layers.{i + 1}\")\n                 for i in range(config.num_feat_extract_layers - 1)\n             ]\n         elif config.feat_extract_norm == \"layer\":"
        },
        {
            "sha": "d490d555a70282e47a022e85d9909023451d2dc2",
            "filename": "src/transformers/models/ibert/quant_modules.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -171,9 +171,9 @@ def forward(\n             x_min = x_act.data.min()\n             x_max = x_act.data.max()\n \n-            assert (\n-                x_max.isnan().sum() == 0 and x_min.isnan().sum() == 0\n-            ), \"NaN detected when computing min/max of the activation\"\n+            assert x_max.isnan().sum() == 0 and x_min.isnan().sum() == 0, (\n+                \"NaN detected when computing min/max of the activation\"\n+            )\n \n             # Initialization\n             if self.x_min.min() > -1.1e-5 and self.x_max.max() < 1.1e-5:"
        },
        {
            "sha": "c16aab776f19f2ff75321d9216b633f1ba344841",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -451,7 +451,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "892a7c2cf1d98ea48f955b0d807226266463bbd7",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -101,8 +101,7 @@ def __call__(\n         # verify input\n         if self.image_processor.apply_ocr and (boxes is not None):\n             raise ValueError(\n-                \"You cannot provide bounding boxes \"\n-                \"if you initialized the image processor with apply_ocr set to True.\"\n+                \"You cannot provide bounding boxes if you initialized the image processor with apply_ocr set to True.\"\n             )\n \n         if self.image_processor.apply_ocr and (word_labels is not None):"
        },
        {
            "sha": "67eb44503f0f4a3ba6fc660c5d9a7a7f5887d0df",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -130,12 +130,12 @@ def __init__(self, config, layer_id):\n \n         self.layer_id = layer_id\n         attention_window = config.attention_window[self.layer_id]\n-        assert (\n-            attention_window % 2 == 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n-        assert (\n-            attention_window > 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        assert attention_window % 2 == 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n+        )\n+        assert attention_window > 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        )\n \n         self.one_sided_attn_window_size = attention_window // 2\n \n@@ -169,9 +169,9 @@ def forward(\n         value_vectors = self.value(hidden_states)\n \n         seq_len, batch_size, embed_dim = hidden_states.size()\n-        assert (\n-            embed_dim == self.embed_dim\n-        ), f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"\n+        assert embed_dim == self.embed_dim, (\n+            f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"\n+        )\n \n         # normalize query\n         query_vectors /= math.sqrt(self.head_dim)\n@@ -239,9 +239,9 @@ def forward(\n         )  # use fp32 for numerical stability\n \n         if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (\n-                self.num_heads,\n-            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            assert layer_head_mask.size() == (self.num_heads,), (\n+                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            )\n             attn_probs = layer_head_mask.view(1, 1, -1, 1) * attn_probs\n \n         # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n@@ -433,9 +433,9 @@ def _sliding_chunks_query_key_matmul(self, query: torch.Tensor, key: torch.Tenso\n         overlap of size window_overlap\n         \"\"\"\n         batch_size, seq_len, num_heads, head_dim = query.size()\n-        assert (\n-            seq_len % (window_overlap * 2) == 0\n-        ), f\"Sequence length should be multiple of {window_overlap * 2}. Given {seq_len}\"\n+        assert seq_len % (window_overlap * 2) == 0, (\n+            f\"Sequence length should be multiple of {window_overlap * 2}. Given {seq_len}\"\n+        )\n         assert query.size() == key.size()\n \n         chunks_count = torch.div(seq_len, window_overlap, rounding_mode=\"trunc\") - 1\n@@ -706,9 +706,9 @@ def _compute_global_attn_output_from_hidden(\n \n         # apply layer head masking\n         if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (\n-                self.num_heads,\n-            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            assert layer_head_mask.size() == (self.num_heads,), (\n+                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            )\n             global_attn_probs_float = layer_head_mask.view(1, -1, 1, 1) * global_attn_probs_float.view(\n                 batch_size, self.num_heads, max_num_global_attn_indices, seq_len\n             )"
        },
        {
            "sha": "fe6c4a8986ae8e75211c2a6d03f4335f2ce00ed2",
            "filename": "src/transformers/models/led/modeling_tf_led.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -182,12 +182,12 @@ def __init__(self, config, layer_id, **kwargs):\n         self.layer_id = layer_id\n         attention_window = config.attention_window[self.layer_id]\n \n-        assert (\n-            attention_window % 2 == 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n-        assert (\n-            attention_window > 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        assert attention_window % 2 == 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n+        )\n+        assert attention_window > 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        )\n \n         self.one_sided_attn_window_size = attention_window // 2\n "
        },
        {
            "sha": "417a2078d27915213fe8a53148e2d83347c0349b",
            "filename": "src/transformers/models/llama/tokenization_llama_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama_fast.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -192,8 +192,8 @@ def update_post_processor(self):\n         if eos is None and self.add_eos_token:\n             raise ValueError(\"add_eos_token = True but eos_token = None\")\n \n-        single = f\"{(bos+':0 ') if self.add_bos_token else ''}$A:0{(' '+eos+':0') if self.add_eos_token else ''}\"\n-        pair = f\"{single}{(' '+bos+':1') if self.add_bos_token else ''} $B:1{(' '+eos+':1') if self.add_eos_token else ''}\"\n+        single = f\"{(bos + ':0 ') if self.add_bos_token else ''}$A:0{(' ' + eos + ':0') if self.add_eos_token else ''}\"\n+        pair = f\"{single}{(' ' + bos + ':1') if self.add_bos_token else ''} $B:1{(' ' + eos + ':1') if self.add_eos_token else ''}\"\n \n         special_tokens = []\n         if self.add_bos_token:"
        },
        {
            "sha": "ca87b37c6505f20e6498a35472ce9f407e1c5da8",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -510,12 +510,12 @@ def __init__(self, config, layer_id):\n \n         self.layer_id = layer_id\n         attention_window = config.attention_window[self.layer_id]\n-        assert (\n-            attention_window % 2 == 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n-        assert (\n-            attention_window > 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        assert attention_window % 2 == 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n+        )\n+        assert attention_window > 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        )\n \n         self.one_sided_attn_window_size = attention_window // 2\n \n@@ -549,9 +549,9 @@ def forward(\n         value_vectors = self.value(hidden_states)\n \n         seq_len, batch_size, embed_dim = hidden_states.size()\n-        assert (\n-            embed_dim == self.embed_dim\n-        ), f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"\n+        assert embed_dim == self.embed_dim, (\n+            f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"\n+        )\n \n         # normalize query\n         query_vectors /= math.sqrt(self.head_dim)\n@@ -619,9 +619,9 @@ def forward(\n         )  # use fp32 for numerical stability\n \n         if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (\n-                self.num_heads,\n-            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            assert layer_head_mask.size() == (self.num_heads,), (\n+                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            )\n             attn_probs = layer_head_mask.view(1, 1, -1, 1) * attn_probs\n \n         # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n@@ -813,9 +813,9 @@ def _sliding_chunks_query_key_matmul(self, query: torch.Tensor, key: torch.Tenso\n         overlap of size window_overlap\n         \"\"\"\n         batch_size, seq_len, num_heads, head_dim = query.size()\n-        assert (\n-            seq_len % (window_overlap * 2) == 0\n-        ), f\"Sequence length should be multiple of {window_overlap * 2}. Given {seq_len}\"\n+        assert seq_len % (window_overlap * 2) == 0, (\n+            f\"Sequence length should be multiple of {window_overlap * 2}. Given {seq_len}\"\n+        )\n         assert query.size() == key.size()\n \n         chunks_count = torch.div(seq_len, window_overlap, rounding_mode=\"trunc\") - 1\n@@ -1086,9 +1086,9 @@ def _compute_global_attn_output_from_hidden(\n \n         # apply layer head masking\n         if layer_head_mask is not None:\n-            assert layer_head_mask.size() == (\n-                self.num_heads,\n-            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            assert layer_head_mask.size() == (self.num_heads,), (\n+                f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n+            )\n             global_attn_probs_float = layer_head_mask.view(1, -1, 1, 1) * global_attn_probs_float.view(\n                 batch_size, self.num_heads, max_num_global_attn_indices, seq_len\n             )\n@@ -1287,9 +1287,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layer)\n-            ), f\"The head_mask should be specified for {len(self.layer)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layer)), (\n+                f\"The head_mask should be specified for {len(self.layer)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n         for idx, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n@@ -1590,8 +1590,7 @@ def _pad_to_window_size(\n         # this path should be recorded in the ONNX export, it is fine with padding_len == 0 as well\n         if padding_len > 0:\n             logger.warning_once(\n-                f\"Input ids are automatically padded to be a multiple of \"\n-                f\"`config.attention_window`: {attention_window}\"\n+                f\"Input ids are automatically padded to be a multiple of `config.attention_window`: {attention_window}\"\n             )\n             if input_ids is not None:\n                 input_ids = nn.functional.pad(input_ids, (0, padding_len), value=pad_token_id)"
        },
        {
            "sha": "41eb0ae34ba5dbd2f27b86f4ddf7e6e792e42f97",
            "filename": "src/transformers/models/longformer/modeling_tf_longformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -746,12 +746,12 @@ def __init__(self, config, layer_id, **kwargs):\n         self.layer_id = layer_id\n         attention_window = config.attention_window[self.layer_id]\n \n-        assert (\n-            attention_window % 2 == 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n-        assert (\n-            attention_window > 0\n-        ), f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        assert attention_window % 2 == 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be an even value. Given {attention_window}\"\n+        )\n+        assert attention_window > 0, (\n+            f\"`attention_window` for layer {self.layer_id} has to be positive. Given {attention_window}\"\n+        )\n \n         self.one_sided_attn_window_size = attention_window // 2\n "
        },
        {
            "sha": "63a38791182780be017e87d700a02fd515911ecf",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1294,7 +1294,7 @@ def forward(\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting\" \" `use_cache=False`...\"\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n "
        },
        {
            "sha": "6181e94c60a3aebcf8992ab2adb493c7837d3f09",
            "filename": "src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmarian%2Fconvert_marian_tatoeba_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmarian%2Fconvert_marian_tatoeba_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fconvert_marian_tatoeba_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -228,7 +228,7 @@ def write_model_card(self, model_dict, dry_run=False) -> str:\n         # combine with Tatoeba markdown\n         readme_url = f\"{TATOEBA_MODELS_URL}/{model_dict['_name']}/README.md\"\n         extra_markdown = f\"\"\"\n-### {model_dict['_name']}\n+### {model_dict[\"_name\"]}\n \n * source language name: {self.tag2name[a3_src]}\n * target language name: {self.tag2name[a3_tgt]}\n@@ -237,12 +237,12 @@ def write_model_card(self, model_dict, dry_run=False) -> str:\n \n         content = (\n             f\"\"\"\n-* model: {model_dict['modeltype']}\n-* source language code{src_multilingual*'s'}: {', '.join(a2_src_tags)}\n-* target language code{tgt_multilingual*'s'}: {', '.join(a2_tgt_tags)}\n+* model: {model_dict[\"modeltype\"]}\n+* source language code{src_multilingual * \"s\"}: {\", \".join(a2_src_tags)}\n+* target language code{tgt_multilingual * \"s\"}: {\", \".join(a2_tgt_tags)}\n * dataset: opus {backtranslated_data}\n-* release date: {model_dict['release-date']}\n-* pre-processing: {model_dict['pre-processing']}\n+* release date: {model_dict[\"release-date\"]}\n+* pre-processing: {model_dict[\"pre-processing\"]}\n \"\"\"\n             + multilingual_data\n             + tuned"
        },
        {
            "sha": "f68dceee915955b96769cdc74d75ff7bcc503b58",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -741,9 +741,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)"
        },
        {
            "sha": "b1fd6463c68e2587e7bfea232c2a6bab693ac440",
            "filename": "src/transformers/models/marian/tokenization_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Ftokenization_marian.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -339,7 +339,7 @@ def get_tgt_vocab(self):\n     def __getstate__(self) -> Dict:\n         state = self.__dict__.copy()\n         state.update(\n-            {k: None for k in [\"spm_source\", \"spm_target\", \"current_spm\", \"punc_normalizer\", \"target_vocab_file\"]}\n+            dict.fromkeys([\"spm_source\", \"spm_target\", \"current_spm\", \"punc_normalizer\", \"target_vocab_file\"])\n         )\n         return state\n "
        },
        {
            "sha": "3fd28acad39206b34b2c8e219819f57e9d351c0f",
            "filename": "src/transformers/models/mask2former/convert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -523,11 +523,11 @@ def replace_swin_backbone(self, dst_state_dict: StateDict, src_state_dict: State\n                 [\n                     (\n                         f\"{src_prefix}.norm{layer_idx}.weight\",\n-                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx+1}.weight\",\n+                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx + 1}.weight\",\n                     ),\n                     (\n                         f\"{src_prefix}.norm{layer_idx}.bias\",\n-                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx+1}.bias\",\n+                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx + 1}.bias\",\n                     ),\n                 ]\n             )\n@@ -863,9 +863,9 @@ def test(\n         for original_model_feature, our_model_feature in zip(\n             original_model_backbone_features.values(), our_model_output.encoder_hidden_states\n         ):\n-            assert torch.allclose(\n-                original_model_feature, our_model_feature, atol=tolerance\n-            ), \"The backbone features are not the same.\"\n+            assert torch.allclose(original_model_feature, our_model_feature, atol=tolerance), (\n+                \"The backbone features are not the same.\"\n+            )\n \n         # Test pixel decoder\n         mask_features, _, multi_scale_features = original_model.sem_seg_head.pixel_decoder.forward_features(\n@@ -875,9 +875,9 @@ def test(\n         for original_model_feature, our_model_feature in zip(\n             multi_scale_features, our_model_output.pixel_decoder_hidden_states\n         ):\n-            assert torch.allclose(\n-                original_model_feature, our_model_feature, atol=tolerance\n-            ), \"The pixel decoder feature are not the same\"\n+            assert torch.allclose(original_model_feature, our_model_feature, atol=tolerance), (\n+                \"The pixel decoder feature are not the same\"\n+            )\n \n         # Let's test the full model\n         tr_complete = T.Compose(\n@@ -894,12 +894,12 @@ def test(\n \n         assert original_mask_logits.shape == our_mask_logits.shape, \"Output masks shapes are not matching.\"\n         assert original_class_logits.shape == our_class_logits.shape, \"Output class logits shapes are not matching.\"\n-        assert torch.allclose(\n-            original_class_logits, our_class_logits, atol=tolerance\n-        ), \"The class logits are not the same.\"\n-        assert torch.allclose(\n-            original_mask_logits, our_mask_logits, atol=tolerance\n-        ), \"The predicted masks are not the same.\"\n+        assert torch.allclose(original_class_logits, our_class_logits, atol=tolerance), (\n+            \"The class logits are not the same.\"\n+        )\n+        assert torch.allclose(original_mask_logits, our_mask_logits, atol=tolerance), (\n+            \"The predicted masks are not the same.\"\n+        )\n \n         logger.info(\" Test passed!\")\n "
        },
        {
            "sha": "b55b0e871480a35c55f670f7456377f9f458ad9e",
            "filename": "src/transformers/models/maskformer/convert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_original_pytorch_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -581,9 +581,9 @@ def test(original_model, our_model: MaskFormerForInstanceSegmentation, image_pro\n         for original_model_feature, our_model_feature in zip(\n             original_model_backbone_features.values(), our_model_output.encoder_hidden_states\n         ):\n-            assert torch.allclose(\n-                original_model_feature, our_model_feature, atol=1e-3\n-            ), \"The backbone features are not the same.\"\n+            assert torch.allclose(original_model_feature, our_model_feature, atol=1e-3), (\n+                \"The backbone features are not the same.\"\n+            )\n \n         original_model_pixel_out = original_model.sem_seg_head.pixel_decoder.forward_features(\n             original_model_backbone_features\n@@ -602,9 +602,9 @@ def test(original_model, our_model: MaskFormerForInstanceSegmentation, image_pro\n \n         our_segmentation = image_processor.post_process_segmentation(our_model_out, target_size=(384, 384))\n \n-        assert torch.allclose(\n-            original_segmentation, our_segmentation, atol=1e-3\n-        ), \"The segmentation image is not the same.\"\n+        assert torch.allclose(original_segmentation, our_segmentation, atol=1e-3), (\n+            \"The segmentation image is not the same.\"\n+        )\n \n         logger.info(\" Test passed!\")\n "
        },
        {
            "sha": "79ef4917e9d3d9734e917117330b27dbbeb19f19",
            "filename": "src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconvert_maskformer_resnet_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -125,31 +125,31 @@ def create_rename_keys(config):\n             for i in range(3):\n                 rename_keys.append(\n                     (\n-                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i+1}.weight\",\n+                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i + 1}.weight\",\n                         f\"model.pixel_level_module.encoder.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.convolution.weight\",\n                     )\n                 )\n                 rename_keys.append(\n                     (\n-                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i+1}.norm.weight\",\n+                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i + 1}.norm.weight\",\n                         f\"model.pixel_level_module.encoder.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.weight\",\n                     )\n                 )\n                 rename_keys.append(\n                     (\n-                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i+1}.norm.bias\",\n+                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i + 1}.norm.bias\",\n                         f\"model.pixel_level_module.encoder.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.bias\",\n                     )\n                 )\n                 rename_keys.append(\n                     (\n-                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i+1}.norm.running_mean\",\n+                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i + 1}.norm.running_mean\",\n                         f\"model.pixel_level_module.encoder.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.running_mean\",\n                     )\n                 )\n                 rename_keys.append(\n                     (\n-                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i+1}.norm.running_var\",\n+                        f\"backbone.res{stage_idx + 2}.{layer_idx}.conv{i + 1}.norm.running_var\",\n                         f\"model.pixel_level_module.encoder.encoder.stages.{stage_idx}.layers.{layer_idx}.layer.{i}.normalization.running_var\",\n                     )\n                 )"
        },
        {
            "sha": "75702aadd314a8f5cfd6dc51389abe322d6c9328",
            "filename": "src/transformers/models/mimi/convert_mimi_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmimi%2Fconvert_mimi_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmimi%2Fconvert_mimi_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconvert_mimi_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -129,7 +129,7 @@ def permute(w, n_heads, dim1=hidden_size, dim2=hidden_size):\n     hf_model.load_state_dict(state_dict, strict=True)\n     n_params = param_count(hf_model)\n \n-    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params\")\n+    logger.info(f\"model loaded: {round(n_params / 1e6, 1)}M params\")\n \n     hf_model.eval()\n     hf_model.to(device)"
        },
        {
            "sha": "11fde85cf614a3cf3a3600f27622b900e29ee866",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -144,9 +144,9 @@ def load_tf_weights_in_mobilebert(model, config, tf_checkpoint_path):\n         elif m_name == \"kernel\":\n             array = np.transpose(array)\n         try:\n-            assert (\n-                pointer.shape == array.shape\n-            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+            assert pointer.shape == array.shape, (\n+                f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+            )\n         except AssertionError as e:\n             e.args += (pointer.shape, array.shape)\n             raise"
        },
        {
            "sha": "d08642666cdbb771234dc9e214f713b61785b7b4",
            "filename": "src/transformers/models/mobilevitv2/convert_mlcvnets_to_pytorch.py",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -99,9 +99,9 @@ def get_mobilevitv2_config(task_name, orig_cfg_file):\n     orig_config = load_orig_config_file(orig_cfg_file)\n     assert getattr(orig_config, \"model.classification.name\", -1) == \"mobilevit_v2\", \"Invalid model\"\n     config.width_multiplier = getattr(orig_config, \"model.classification.mitv2.width_multiplier\", 1.0)\n-    assert (\n-        getattr(orig_config, \"model.classification.mitv2.attn_norm_layer\", -1) == \"layer_norm_2d\"\n-    ), \"Norm layers other than layer_norm_2d is not supported\"\n+    assert getattr(orig_config, \"model.classification.mitv2.attn_norm_layer\", -1) == \"layer_norm_2d\", (\n+        \"Norm layers other than layer_norm_2d is not supported\"\n+    )\n     config.hidden_act = getattr(orig_config, \"model.classification.activation.name\", \"swish\")\n     # config.image_size == getattr(orig_config,  'sampler.bs.crop_size_width', 256)\n \n@@ -151,19 +151,19 @@ def create_rename_keys(state_dict, base_model=False):\n             k_new = k_new.replace(\"conv_1.\", f\"{model_prefix}conv_stem.\")\n         for i in [1, 2]:\n             if f\"layer_{i}.\" in k:\n-                k_new = k_new.replace(f\"layer_{i}.\", f\"{model_prefix}encoder.layer.{i-1}.layer.\")\n+                k_new = k_new.replace(f\"layer_{i}.\", f\"{model_prefix}encoder.layer.{i - 1}.layer.\")\n         if \".exp_1x1.\" in k:\n             k_new = k_new.replace(\".exp_1x1.\", \".expand_1x1.\")\n         if \".red_1x1.\" in k:\n             k_new = k_new.replace(\".red_1x1.\", \".reduce_1x1.\")\n \n         for i in [3, 4, 5]:\n             if f\"layer_{i}.0.\" in k:\n-                k_new = k_new.replace(f\"layer_{i}.0.\", f\"{model_prefix}encoder.layer.{i-1}.downsampling_layer.\")\n+                k_new = k_new.replace(f\"layer_{i}.0.\", f\"{model_prefix}encoder.layer.{i - 1}.downsampling_layer.\")\n             if f\"layer_{i}.1.local_rep.0.\" in k:\n-                k_new = k_new.replace(f\"layer_{i}.1.local_rep.0.\", f\"{model_prefix}encoder.layer.{i-1}.conv_kxk.\")\n+                k_new = k_new.replace(f\"layer_{i}.1.local_rep.0.\", f\"{model_prefix}encoder.layer.{i - 1}.conv_kxk.\")\n             if f\"layer_{i}.1.local_rep.1.\" in k:\n-                k_new = k_new.replace(f\"layer_{i}.1.local_rep.1.\", f\"{model_prefix}encoder.layer.{i-1}.conv_1x1.\")\n+                k_new = k_new.replace(f\"layer_{i}.1.local_rep.1.\", f\"{model_prefix}encoder.layer.{i - 1}.conv_1x1.\")\n \n         for i in [3, 4, 5]:\n             if i == 3:\n@@ -176,15 +176,17 @@ def create_rename_keys(state_dict, base_model=False):\n             for j in j_in:\n                 if f\"layer_{i}.1.global_rep.{j}.\" in k:\n                     k_new = k_new.replace(\n-                        f\"layer_{i}.1.global_rep.{j}.\", f\"{model_prefix}encoder.layer.{i-1}.transformer.layer.{j}.\"\n+                        f\"layer_{i}.1.global_rep.{j}.\", f\"{model_prefix}encoder.layer.{i - 1}.transformer.layer.{j}.\"\n                     )\n-            if f\"layer_{i}.1.global_rep.{j+1}.\" in k:\n+            if f\"layer_{i}.1.global_rep.{j + 1}.\" in k:\n                 k_new = k_new.replace(\n-                    f\"layer_{i}.1.global_rep.{j+1}.\", f\"{model_prefix}encoder.layer.{i-1}.layernorm.\"\n+                    f\"layer_{i}.1.global_rep.{j + 1}.\", f\"{model_prefix}encoder.layer.{i - 1}.layernorm.\"\n                 )\n \n             if f\"layer_{i}.1.conv_proj.\" in k:\n-                k_new = k_new.replace(f\"layer_{i}.1.conv_proj.\", f\"{model_prefix}encoder.layer.{i-1}.conv_projection.\")\n+                k_new = k_new.replace(\n+                    f\"layer_{i}.1.conv_proj.\", f\"{model_prefix}encoder.layer.{i - 1}.conv_projection.\"\n+                )\n \n         if \"pre_norm_attn.0.\" in k:\n             k_new = k_new.replace(\"pre_norm_attn.0.\", \"layernorm_before.\")"
        },
        {
            "sha": "39d8df0f3fabc4c88ca7ba0dae226e0dde403cf5",
            "filename": "src/transformers/models/moonshine/convert_usefulsensors_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -56,7 +56,7 @@ def _read_h5_weights(group, current_key=\"\", weights={}):\n def _convert_layer_names(name, gated_mlp=False):\n     name = re.sub(\n         r\"layers\\.functional(?:_(\\d+))?\\.layers\",\n-        lambda m: f'layers.{m.group(1) if m.group(1) else \"0\"}',\n+        lambda m: f\"layers.{m.group(1) if m.group(1) else '0'}\",\n         name,\n         count=1,\n     )"
        },
        {
            "sha": "55d8f77ad045e0f1c4967231115481e682c748cb",
            "filename": "src/transformers/models/moshi/convert_moshi_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconvert_moshi_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconvert_moshi_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fconvert_moshi_transformers.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -186,7 +186,7 @@ def permute(w, n_heads, dim1=hidden_size, dim2=hidden_size):\n     hf_model.load_state_dict(state_dict, strict=True)\n     n_params = param_count(hf_model)\n \n-    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params\")\n+    logger.info(f\"model loaded: {round(n_params / 1e6, 1)}M params\")\n \n     hf_model.eval()\n     hf_model.to(device)"
        },
        {
            "sha": "30938c8251c73eab4735fb010ba6f6b7fabe40ec",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -719,9 +719,9 @@ def load_tf_weights_in_mt5(model, config, tf_checkpoint_path):\n             logger.info(f\"Transposing numpy weight of shape {array.shape} for {name}\")\n             array = np.transpose(array)\n         try:\n-            assert (\n-                pointer.shape == array.shape\n-            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+            assert pointer.shape == array.shape, (\n+                f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+            )\n         except AssertionError as e:\n             e.args += (pointer.shape, array.shape)\n             raise"
        },
        {
            "sha": "07a5c51a3c4bcd1062e39c51223f2a46ff8544f0",
            "filename": "src/transformers/models/nemotron/convert_nemotron_nemo_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconvert_nemotron_nemo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconvert_nemotron_nemo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fconvert_nemotron_nemo_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -65,13 +65,13 @@ def get_args():\n         \"--hf_input_path\",\n         type=str,\n         default=None,\n-        help=\"A HF model path, \" \"e.g. a folder containing https://huggingface.co/nvidia/Minitron-8B-Base\",\n+        help=\"A HF model path, e.g. a folder containing https://huggingface.co/nvidia/Minitron-8B-Base\",\n     )\n     parser.add_argument(\n         \"--hf_output_path\",\n         type=str,\n         default=None,\n-        help=\"Output HF model path, \" \"with the same format as above but user's own weights\",\n+        help=\"Output HF model path, with the same format as above but user's own weights\",\n     )\n     parser.add_argument(\n         \"--precision\","
        },
        {
            "sha": "dd995bcbc6b3e0e0a68989be6d1ffbb57c930781",
            "filename": "src/transformers/models/nllb_moe/convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -82,7 +82,7 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weig\n             remove_ignore_keys_(expert_state)\n             expert_state = rename_fairseq_keys(expert_state, expert)\n             save_path = os.path.join(\n-                dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts)+1:05d}-of-???.bin\")\n+                dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts) + 1:05d}-of-???.bin\")\n             )\n             torch.save(expert_state, save_path)\n             sharded_state_dicts.append(expert_state.keys())\n@@ -91,7 +91,9 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weig\n             )\n \n     # Add the last block\n-    save_path = os.path.join(dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts)+1:05d}-of-???.bin\"))\n+    save_path = os.path.join(\n+        dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts) + 1:05d}-of-???.bin\")\n+    )\n     shared_weights = torch.load(switch_checkpoint_path + \"-shared.pt\")[\"model\"]\n     remove_ignore_keys_(shared_weights)\n     shared_weights = rename_fairseq_keys(shared_weights, None)\n@@ -108,8 +110,8 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weig\n     # Otherwise, let's build the index\n     weight_map = {}\n     for idx, shard in enumerate(sharded_state_dicts):\n-        shard_file = weights_name.replace(\".bin\", f\"-{idx+1:05d}-of-{len(sharded_state_dicts):05d}.bin\")\n-        temp_filename = os.path.join(dump_path, weights_name.replace(\".bin\", f\"-{idx+1:05d}-of-???.bin\"))\n+        shard_file = weights_name.replace(\".bin\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin\")\n+        temp_filename = os.path.join(dump_path, weights_name.replace(\".bin\", f\"-{idx + 1:05d}-of-???.bin\"))\n         os.rename(temp_filename, os.path.join(dump_path, shard_file))\n         for key in shard:\n             weight_map[key] = shard_file"
        },
        {
            "sha": "56c97c8870c1e210a65e258dc1e181bba44c8ad7",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1352,7 +1352,7 @@ def forward(\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting\" \" `use_cache=False`...\"\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n "
        },
        {
            "sha": "3c1f396e0f8414c991411b8c508896166d282228",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -5,7 +5,6 @@\n #                          modular_olmo2.py file directly. One of our CI enforces this.\n #                \n \n-\n from ...configuration_utils import PretrainedConfig\n \n "
        },
        {
            "sha": "fbd431532fd70ed75aafdb0382f07c277a83169d",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1,7 +1,7 @@\n from typing import Callable, Optional, Tuple\n \n import torch\n-from torch import nn\n+import torch.nn as nn\n \n from ...cache_utils import Cache\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS"
        },
        {
            "sha": "960634f9f4e89696e020052cc7ef84a48e54ea1a",
            "filename": "src/transformers/models/oneformer/convert_to_hf_oneformer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconvert_to_hf_oneformer.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -394,11 +394,11 @@ def replace_swin_backbone(self, dst_state_dict: StateDict, src_state_dict: State\n                 [\n                     (\n                         f\"{src_prefix}.norm{layer_idx}.weight\",\n-                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx+1}.weight\",\n+                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx + 1}.weight\",\n                     ),\n                     (\n                         f\"{src_prefix}.norm{layer_idx}.bias\",\n-                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx+1}.bias\",\n+                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx + 1}.bias\",\n                     ),\n                 ]\n             )\n@@ -531,11 +531,11 @@ def rename_keys_for_weight_bias(src_prefix: str, dst_prefix: str):\n                 [\n                     (\n                         f\"{src_prefix}.norm{layer_idx}.weight\",\n-                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx+1}.weight\",\n+                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx + 1}.weight\",\n                     ),\n                     (\n                         f\"{src_prefix}.norm{layer_idx}.bias\",\n-                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx+1}.bias\",\n+                        f\"{dst_prefix}.hidden_states_norms.stage{layer_idx + 1}.bias\",\n                     ),\n                 ]\n             )\n@@ -1010,9 +1010,9 @@ def _preprocess_text(text_list=None, max_length=77):\n         for original_model_feature, our_model_feature in zip(\n             original_model_backbone_features.values(), our_model_output.encoder_hidden_states\n         ):\n-            assert torch.allclose(\n-                original_model_feature, our_model_feature, atol=3e-3\n-            ), \"The backbone features are not the same.\"\n+            assert torch.allclose(original_model_feature, our_model_feature, atol=3e-3), (\n+                \"The backbone features are not the same.\"\n+            )\n         mask_features, _, multi_scale_features, _, _ = original_model.sem_seg_head.pixel_decoder.forward_features(\n             original_model_backbone_features\n         )\n@@ -1025,9 +1025,9 @@ def _preprocess_text(text_list=None, max_length=77):\n         for original_model_feature, our_model_feature in zip(\n             original_pixel_decoder_features, our_model_output.pixel_decoder_hidden_states\n         ):\n-            assert torch.allclose(\n-                original_model_feature, our_model_feature, atol=3e-4\n-            ), \"The pixel decoder feature are not the same\"\n+            assert torch.allclose(original_model_feature, our_model_feature, atol=3e-4), (\n+                \"The pixel decoder feature are not the same\"\n+            )\n \n         tr_complete = T.Compose(\n             [\n@@ -1049,9 +1049,9 @@ def _preprocess_text(text_list=None, max_length=77):\n \n         our_segmentation = post_process_sem_seg_output(our_model_out, target_size=(640, 640))[0]\n \n-        assert torch.allclose(\n-            original_segmentation, our_segmentation, atol=1e-3\n-        ), \"The segmentation image is not the same.\"\n+        assert torch.allclose(original_segmentation, our_segmentation, atol=1e-3), (\n+            \"The segmentation image is not the same.\"\n+        )\n \n         logger.info(\" Test passed!\")\n "
        },
        {
            "sha": "3dd0a6b86b47136177d9a7b0825f5d8d160a2c2d",
            "filename": "src/transformers/models/openai/modeling_tf_openai.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -62,9 +62,9 @@ def __init__(self, nx, config, scale=False, **kwargs):\n \n         n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n         # [switch nx => n_state from Block to Attention to keep identical to TF implementation]\n-        assert (\n-            n_state % config.n_head == 0\n-        ), f\"Hidden dimension {n_state} not dividable by number of heads {config.n_head}\"\n+        assert n_state % config.n_head == 0, (\n+            f\"Hidden dimension {n_state} not dividable by number of heads {config.n_head}\"\n+        )\n         self.n_head = config.n_head\n         self.split_size = n_state\n         self.scale = scale"
        },
        {
            "sha": "77ec2c71927e1b9d4ae5d1750c9ba98f666b478f",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -173,7 +173,7 @@ def _preprocess_resize_output_shape(image, output_shape):\n         # multichannel case: append shape of last axis\n         output_shape = output_shape + (image.shape[-1],)\n     elif output_ndim < image.ndim:\n-        raise ValueError(\"output_shape length cannot be smaller than the \" \"image number of dimensions\")\n+        raise ValueError(\"output_shape length cannot be smaller than the image number of dimensions\")\n \n     return image, output_shape\n \n@@ -345,10 +345,10 @@ def resize(\n             else:\n                 anti_aliasing_sigma = np.atleast_1d(anti_aliasing_sigma) * np.ones_like(factors)\n                 if np.any(anti_aliasing_sigma < 0):\n-                    raise ValueError(\"Anti-aliasing standard deviation must be \" \"greater than or equal to zero\")\n+                    raise ValueError(\"Anti-aliasing standard deviation must be greater than or equal to zero\")\n                 elif np.any((anti_aliasing_sigma > 0) & (factors <= 1)):\n                     warnings.warn(\n-                        \"Anti-aliasing standard deviation greater than zero but \" \"not down-sampling along all axes\"\n+                        \"Anti-aliasing standard deviation greater than zero but not down-sampling along all axes\"\n                     )\n             filtered = ndi.gaussian_filter(image, anti_aliasing_sigma, cval=cval, mode=ndi_mode)\n         else:"
        },
        {
            "sha": "059a7933775020d166c25fc0f2a07c87469ac9b3",
            "filename": "src/transformers/models/prompt_depth_anything/convert_prompt_depth_anything_to_hf.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconvert_prompt_depth_anything_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconvert_prompt_depth_anything_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconvert_prompt_depth_anything_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -112,17 +112,17 @@ def transform_qkv_weights(key, value, config):\n     r\"pretrained.blocks.(\\d+).attn.qkv.(weight|bias)\": r\"qkv_transform_\\2_\\1\",\n     # Neck\n     r\"depth_head.projects.(\\d+).(weight|bias)\": r\"neck.reassemble_stage.layers.\\1.projection.\\2\",\n-    r\"depth_head.scratch.layer(\\d+)_rn.weight\": lambda m: f\"neck.convs.{int(m.group(1))-1}.weight\",\n+    r\"depth_head.scratch.layer(\\d+)_rn.weight\": lambda m: f\"neck.convs.{int(m.group(1)) - 1}.weight\",\n     r\"depth_head.resize_layers.(\\d+).(weight|bias)\": r\"neck.reassemble_stage.layers.\\1.resize.\\2\",\n     # Refinenet (with reversed indices)\n-    r\"depth_head.scratch.refinenet(\\d+).out_conv.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.projection.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit1.conv1.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.residual_layer1.convolution1.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit1.conv2.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.residual_layer1.convolution2.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit2.conv1.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.residual_layer2.convolution1.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit2.conv2.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.residual_layer2.convolution2.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit_depth.0.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.prompt_depth_layer.convolution1.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit_depth.2.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.prompt_depth_layer.convolution2.{m.group(2)}\",\n-    r\"depth_head.scratch.refinenet(\\d+).resConfUnit_depth.4.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4-int(m.group(1))}.prompt_depth_layer.convolution3.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).out_conv.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.projection.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit1.conv1.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.residual_layer1.convolution1.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit1.conv2.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.residual_layer1.convolution2.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit2.conv1.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.residual_layer2.convolution1.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit2.conv2.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.residual_layer2.convolution2.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit_depth.0.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.prompt_depth_layer.convolution1.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit_depth.2.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.prompt_depth_layer.convolution2.{m.group(2)}\",\n+    r\"depth_head.scratch.refinenet(\\d+).resConfUnit_depth.4.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{4 - int(m.group(1))}.prompt_depth_layer.convolution3.{m.group(2)}\",\n     # Head\n     r\"depth_head.scratch.output_conv1.(weight|bias)\": r\"head.conv1.\\1\",\n     r\"depth_head.scratch.output_conv2.0.(weight|bias)\": r\"head.conv2.\\1\","
        },
        {
            "sha": "805338511d8a21f5e46998c0f70941659d446025",
            "filename": "src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconvert_prophetnet_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconvert_prophetnet_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fconvert_prophetnet_original_pytorch_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -118,9 +118,9 @@ def convert_prophetnet_checkpoint_to_pytorch(prophetnet_checkpoint_path: str, py\n                 is_key_init = True\n                 break\n             elif attribute == \"position_embeddings\":\n-                assert (\n-                    model.position_embeddings.weight.shape[-1] == old_model.embed_positions.weight.shape[-1]\n-                ), \"Hidden size has to match\"\n+                assert model.position_embeddings.weight.shape[-1] == old_model.embed_positions.weight.shape[-1], (\n+                    \"Hidden size has to match\"\n+                )\n                 assert model.position_embeddings.weight.shape[0] == 512, \"We want 512 position_embeddings.\"\n                 model.position_embeddings.weight = nn.Parameter(old_model.embed_positions.weight[:512, :])\n                 is_key_init = True"
        },
        {
            "sha": "c7230ddc7afd7f2e51218010a2ec905b0c550aa9",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -588,9 +588,9 @@ def __init__(self, config: ProphetNetConfig) -> None:\n         super().__init__(config.max_position_embeddings, config.hidden_size, config.pad_token_id)\n \n     def forward(self, inputs_shape, device, attention_mask=None, past_key_values=None, position_ids=None):\n-        assert (position_ids is None) or (\n-            self.padding_idx is None\n-        ), \"If position_ids is pre-computed then padding_idx should not be set.\"\n+        assert (position_ids is None) or (self.padding_idx is None), (\n+            \"If position_ids is pre-computed then padding_idx should not be set.\"\n+        )\n \n         if position_ids is None:\n             if past_key_values is not None:\n@@ -784,9 +784,9 @@ def __init__(self, config: ProphetNetConfig):\n         self.head_dim = config.hidden_size // self.num_attn_heads\n         self.ngram = config.ngram\n \n-        assert (\n-            self.head_dim * self.num_attn_heads == config.hidden_size\n-        ), \"config.hidden_size must be divisible by num_attn_heads\"\n+        assert self.head_dim * self.num_attn_heads == config.hidden_size, (\n+            \"config.hidden_size must be divisible by num_attn_heads\"\n+        )\n         # key, value, query projection\n         self.key_proj = nn.Linear(config.hidden_size, config.hidden_size)\n         self.value_proj = nn.Linear(config.hidden_size, config.hidden_size)\n@@ -1041,9 +1041,9 @@ def get_predict_relative_pos_embeddings(\n \n         if predict_relative_position_buckets is None:\n             key_sequence_length = attn_weights.shape[-1]\n-            assert (\n-                position_ids[0][0] == key_sequence_length - 1\n-            ), \"`position_ids` are incorrect. They should be of the format 1 2 3 4 5 ... (key_sequence_length - 1)\"\n+            assert position_ids[0][0] == key_sequence_length - 1, (\n+                \"`position_ids` are incorrect. They should be of the format 1 2 3 4 5 ... (key_sequence_length - 1)\"\n+            )\n             relative_positions = (\n                 torch.arange(0, key_sequence_length)\n                 .unsqueeze(0)\n@@ -1313,9 +1313,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_hidden_states = encoder_hidden_states + (hidden_states,)\n@@ -1488,9 +1488,9 @@ def forward(\n \n         # prepare attention mask\n         if past_key_values is not None:\n-            assert (\n-                hidden_states.size(1) == 1\n-            ), \"At the moment `use_cache` is only supported for `decoder_input_ids` of length 1\"\n+            assert hidden_states.size(1) == 1, (\n+                \"At the moment `use_cache` is only supported for `decoder_input_ids` of length 1\"\n+            )\n \n             ngram_hidden_states = [\n                 (ngram_embeddings[ngram - 1] + predicting_stream_pos_embed).repeat(batch_size, 1, 1)"
        },
        {
            "sha": "99002e3d67ce664c520661eb7bb2db664f3fae29",
            "filename": "src/transformers/models/pvt/convert_pvt_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fpvt%2Fconvert_pvt_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fpvt%2Fconvert_pvt_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fconvert_pvt_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -162,7 +162,7 @@ def convert_pvt_checkpoint(pvt_size, pvt_checkpoint, pytorch_dump_folder_path):\n     elif pvt_size == \"large\":\n         config_path = \"Zetatech/pvt-large-224\"\n     else:\n-        raise ValueError(f\"Available model's size: 'tiny', 'small', 'medium', 'large', but \" f\"'{pvt_size}' was given\")\n+        raise ValueError(f\"Available model's size: 'tiny', 'small', 'medium', 'large', but '{pvt_size}' was given\")\n     config = PvtConfig(name_or_path=config_path)\n     # load original model from https://github.com/whai362/PVT\n     state_dict = torch.load(pvt_checkpoint, map_location=\"cpu\")\n@@ -192,7 +192,7 @@ def convert_pvt_checkpoint(pvt_size, pvt_checkpoint, pytorch_dump_folder_path):\n     elif pvt_size == \"large\":\n         expected_slice_logits = torch.tensor([0.3740, -0.7739, -0.4214])\n     else:\n-        raise ValueError(f\"Available model's size: 'tiny', 'small', 'medium', 'large', but \" f\"'{pvt_size}' was given\")\n+        raise ValueError(f\"Available model's size: 'tiny', 'small', 'medium', 'large', but '{pvt_size}' was given\")\n \n     assert torch.allclose(logits[0, :3], expected_slice_logits, atol=1e-4)\n "
        },
        {
            "sha": "b5178cc2e995452e5ef3581318971df9def44ac3",
            "filename": "src/transformers/models/pvt_v2/convert_pvt_v2_to_pytorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fconvert_pvt_v2_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fconvert_pvt_v2_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fconvert_pvt_v2_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -203,8 +203,7 @@ def convert_pvt_v2_checkpoint(pvt_v2_size, pvt_v2_checkpoint, pytorch_dump_folde\n         config_path = \"OpenGVLab/pvt_v2_b5\"\n     else:\n         raise ValueError(\n-            f\"Available model sizes: 'b0', 'b1', 'b2', 'b2-linear', 'b3', 'b4', 'b5', but \"\n-            f\"'{pvt_v2_size}' was given\"\n+            f\"Available model sizes: 'b0', 'b1', 'b2', 'b2-linear', 'b3', 'b4', 'b5', but '{pvt_v2_size}' was given\"\n         )\n     config = PvtV2Config.from_pretrained(config_path)\n     # load original model from https://github.com/whai362/PVT\n@@ -248,9 +247,9 @@ def convert_pvt_v2_checkpoint(pvt_v2_size, pvt_v2_checkpoint, pytorch_dump_folde\n                 f\"'{pvt_v2_size}' was given\"\n             )\n \n-        assert torch.allclose(\n-            logits[0, :3], expected_slice_logits, atol=1e-4\n-        ), \"ImageNet weights not converted successfully.\"\n+        assert torch.allclose(logits[0, :3], expected_slice_logits, atol=1e-4), (\n+            \"ImageNet weights not converted successfully.\"\n+        )\n \n         print(\"ImageNet weights verified, conversion successful.\")\n "
        },
        {
            "sha": "c5d45c9c9d1f2067ff911ae124ee875f18e1e890",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -623,9 +623,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n \n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:"
        },
        {
            "sha": "b6faa9545491bab4dfc8a4d0eaadd36105b78264",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -494,9 +494,9 @@ def __init__(\n         retriever: Optional[RagRetriever] = None,  # or maybe just use a `set_retriever(...)` method\n         **kwargs,\n     ):\n-        assert config is not None or (\n-            question_encoder is not None and generator is not None\n-        ), \"Either a configuration or an question_encoder and a generator has to be provided.\"\n+        assert config is not None or (question_encoder is not None and generator is not None), (\n+            \"Either a configuration or an question_encoder and a generator has to be provided.\"\n+        )\n \n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n@@ -517,9 +517,9 @@ def __init__(\n \n         self.retriever = retriever\n         if self.retriever is not None:\n-            assert isinstance(\n-                retriever, RagRetriever\n-            ), f\"`self.retriever` is of type {type(self.retriever)}, but should be of type `RagRetriever`\"\n+            assert isinstance(retriever, RagRetriever), (\n+                f\"`self.retriever` is of type {type(self.retriever)}, but should be of type `RagRetriever`\"\n+            )\n             self.retriever = retriever\n \n         self.question_encoder = question_encoder\n@@ -660,9 +660,9 @@ def forward(\n                     \" retriever using the `set_retriever(...)` function.\"\n                 )\n \n-        assert (\n-            doc_scores is not None\n-        ), \"Make sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\"\n+        assert doc_scores is not None, (\n+            \"Make sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\"\n+        )\n \n         assert (doc_scores.shape[1] % n_docs) == 0, (\n             f\" The first dimension of `context_input_ids` should be a multiple of `n_docs`={n_docs}, but is\"\n@@ -740,9 +740,9 @@ def __init__(\n         retriever: Optional[RagRetriever] = None,\n         **kwargs,\n     ):\n-        assert config is not None or (\n-            question_encoder is not None and generator is not None\n-        ), \"Either a configuration or an encoder and a generator has to be provided.\"\n+        assert config is not None or (question_encoder is not None and generator is not None), (\n+            \"Either a configuration or an encoder and a generator has to be provided.\"\n+        )\n \n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n@@ -973,9 +973,9 @@ def generate(\n         )\n         num_beams = num_beams if num_beams is not None else self.config.num_beams\n \n-        assert (\n-            input_ids is not None or context_input_ids is not None\n-        ), \" At least one of input_ids or context_input_ids must be given\"\n+        assert input_ids is not None or context_input_ids is not None, (\n+            \" At least one of input_ids or context_input_ids must be given\"\n+        )\n \n         if self.retriever is not None and context_input_ids is None:\n             question_hidden_states = self.question_encoder(input_ids, attention_mask=attention_mask)[0]\n@@ -1138,9 +1138,9 @@ def __init__(\n         retriever: Optional[RagRetriever] = None,\n         **kwargs,\n     ):\n-        assert config is not None or (\n-            question_encoder is not None and generator is not None\n-        ), \"Either a configuration or an encoder and a generator has to be provided.\"\n+        assert config is not None or (question_encoder is not None and generator is not None), (\n+            \"Either a configuration or an encoder and a generator has to be provided.\"\n+        )\n \n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs("
        },
        {
            "sha": "babc83961098e83693913a7f64ad7989d18e9470",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -506,9 +506,9 @@ def __init__(\n         load_weight_prefix: Optional[str] = None,\n         **kwargs,\n     ):\n-        assert config is not None or (\n-            question_encoder is not None and generator is not None\n-        ), \"Either a configuration or an question_encoder and a generator has to be provided.\"\n+        assert config is not None or (question_encoder is not None and generator is not None), (\n+            \"Either a configuration or an question_encoder and a generator has to be provided.\"\n+        )\n \n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n@@ -533,9 +533,9 @@ def __init__(\n \n         self.retriever = retriever\n         if self.retriever is not None:\n-            assert isinstance(\n-                retriever, RagRetriever\n-            ), f\"`self.retriever` is of type {type(self.retriever)}, but should be of type `RagRetriever`\"\n+            assert isinstance(retriever, RagRetriever), (\n+                f\"`self.retriever` is of type {type(self.retriever)}, but should be of type `RagRetriever`\"\n+            )\n             self.retriever = retriever\n \n         self.question_encoder = question_encoder\n@@ -589,9 +589,9 @@ def call(\n         >>> input_ids = input_dict[\"input_ids\"]\n         >>> outputs = model(input_ids)\n         ```\"\"\"\n-        assert (\n-            \"decoder_cached_states\" not in kwargs\n-        ), \"Please use past_key_values to cache intermediate outputs\"  # from modeling_tf_bart.py\n+        assert \"decoder_cached_states\" not in kwargs, (\n+            \"Please use past_key_values to cache intermediate outputs\"\n+        )  # from modeling_tf_bart.py\n \n         # aliasing to minimize code changing\n         n_docs = n_docs if n_docs is not None else self.config.n_docs\n@@ -657,9 +657,9 @@ def call(\n                     \" retriever using the `set_retriever(...)` function.\"\n                 )\n \n-        assert (\n-            doc_scores is not None\n-        ), \"Make sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\"\n+        assert doc_scores is not None, (\n+            \"Make sure that `doc_scores` are passed when passing `encoder_outputs` to the forward function.\"\n+        )\n \n         assert (doc_scores.shape[1] % n_docs) == 0, (\n             f\" The first dimension of `context_input_ids` should be a multiple of `n_docs`={n_docs}, but is\"\n@@ -747,9 +747,9 @@ def __init__(\n         retriever: Optional[RagRetriever] = None,\n         **kwargs,\n     ):\n-        assert config is not None or (\n-            question_encoder is not None and generator is not None\n-        ), \"Either a configuration or an encoder and a generator has to be provided.\"\n+        assert config is not None or (question_encoder is not None and generator is not None), (\n+            \"Either a configuration or an encoder and a generator has to be provided.\"\n+        )\n \n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n@@ -939,9 +939,9 @@ def call(\n         >>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n         ```\"\"\"\n \n-        assert (\n-            \"decoder_cached_states\" not in kwargs\n-        ), \"Please use past_key_values to cache intermediate outputs\"  # from modeling_tf_bart.py\n+        assert \"decoder_cached_states\" not in kwargs, (\n+            \"Please use past_key_values to cache intermediate outputs\"\n+        )  # from modeling_tf_bart.py\n \n         do_marginalize = do_marginalize if do_marginalize else self.config.do_marginalize\n         reduce_loss = reduce_loss if reduce_loss else self.config.reduce_loss\n@@ -1327,9 +1327,9 @@ def __init__(\n         retriever: Optional[RagRetriever] = None,\n         **kwargs,\n     ):\n-        assert config is not None or (\n-            question_encoder is not None and generator is not None\n-        ), \"Either a configuration or an encoder and a generator has to be provided.\"\n+        assert config is not None or (question_encoder is not None and generator is not None), (\n+            \"Either a configuration or an encoder and a generator has to be provided.\"\n+        )\n \n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n@@ -1454,9 +1454,9 @@ def call(\n         >>> generated_string = tokenizer.batch_decode(generated, skip_special_tokens=True)\n         ```\"\"\"\n \n-        assert (\n-            \"decoder_cached_states\" not in kwargs\n-        ), \"Please use past_key_values to cache intermediate outputs\"  # from modeling_tf_bart.py\n+        assert \"decoder_cached_states\" not in kwargs, (\n+            \"Please use past_key_values to cache intermediate outputs\"\n+        )  # from modeling_tf_bart.py\n \n         exclude_bos_score = exclude_bos_score if exclude_bos_score else self.config.exclude_bos_score\n         reduce_loss = reduce_loss if reduce_loss else self.config.reduce_loss\n@@ -1663,9 +1663,9 @@ def generate(\n         )\n         num_beams = num_beams if num_beams is not None else self.config.num_beams\n \n-        assert (\n-            input_ids is not None or context_input_ids is not None\n-        ), \" At least one of input_ids or context_input_ids must be given\"\n+        assert input_ids is not None or context_input_ids is not None, (\n+            \" At least one of input_ids or context_input_ids must be given\"\n+        )\n \n         if self.retriever is not None and context_input_ids is None:\n             question_hidden_states = self.question_encoder(input_ids, attention_mask=attention_mask)[0]"
        },
        {
            "sha": "c7a592a64417f1fa14b3196d0c299958c82fca32",
            "filename": "src/transformers/models/rag/retrieval_rag.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -156,9 +156,9 @@ def _deserialize_index(self):\n             )\n         with open(resolved_meta_path, \"rb\") as metadata_file:\n             self.index_id_to_db_id = pickle.load(metadata_file)\n-        assert (\n-            len(self.index_id_to_db_id) == self.index.ntotal\n-        ), \"Deserialized index_id_to_db_id should match faiss index size\"\n+        assert len(self.index_id_to_db_id) == self.index.ntotal, (\n+            \"Deserialized index_id_to_db_id should match faiss index size\"\n+        )\n \n     def is_initialized(self):\n         return self._index_initialized"
        },
        {
            "sha": "55cad3c8bae16ee23d3296a426bb7424fc5efb14",
            "filename": "src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Freformer%2Fconvert_reformer_trax_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Freformer%2Fconvert_reformer_trax_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fconvert_reformer_trax_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -150,15 +150,15 @@ def set_model_weights_in_torch(weights, torch_model, hidden_size):\n         position_embeddings = torch_model_reformer.embeddings.position_embeddings\n         for emb_idx in range(len(position_embeddings.weights)):\n             emb_weights = np.asarray(weights[3][emb_idx][0])\n-            assert (\n-                position_embeddings.weights[emb_idx].shape == emb_weights.shape\n-            ), f\"{position_embeddings[emb_idx]} emb does not match\"\n+            assert position_embeddings.weights[emb_idx].shape == emb_weights.shape, (\n+                f\"{position_embeddings[emb_idx]} emb does not match\"\n+            )\n             position_embeddings.weights[emb_idx] = nn.Parameter(torch.tensor(emb_weights))\n \n     trax_layer_weights = weights[5]\n-    assert len(torch_model_reformer.encoder.layers) * 4 == len(\n-        trax_layer_weights\n-    ), \"HF and trax model do not have the same number of layers\"\n+    assert len(torch_model_reformer.encoder.layers) * 4 == len(trax_layer_weights), (\n+        \"HF and trax model do not have the same number of layers\"\n+    )\n     for layer_idx, layer in enumerate(torch_model_reformer.encoder.layers):\n         block_weights = trax_layer_weights[4 * layer_idx : 4 * (layer_idx + 1)]\n         set_block_weights_in_torch(block_weights, layer, hidden_size)"
        },
        {
            "sha": "0fe930bd813ffe211713cad7a531f73acdb8a3cf",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -446,12 +446,12 @@ def forward(\n         # free memory\n         del hidden_states\n \n-        assert (\n-            query_key_vectors.shape[-1] == self.attention_head_size\n-        ), f\"last dim of query_key_vectors is {query_key_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n-        assert (\n-            value_vectors.shape[-1] == self.attention_head_size\n-        ), f\"last dim of value_vectors is {value_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        assert query_key_vectors.shape[-1] == self.attention_head_size, (\n+            f\"last dim of query_key_vectors is {query_key_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        )\n+        assert value_vectors.shape[-1] == self.attention_head_size, (\n+            f\"last dim of value_vectors is {value_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        )\n \n         do_standard_self_attention = (sequence_length <= self.chunk_length) or (\n             use_cache and past_buckets_states[1] is not None\n@@ -470,9 +470,9 @@ def forward(\n                 # make sure buckets has correct shape for LSH attention\n                 buckets = buckets.view(batch_size, self.num_attention_heads, num_hashes * sequence_length)\n \n-            assert (\n-                int(buckets.shape[-1]) == num_hashes * sequence_length\n-            ), f\"last dim of buckets is {buckets.shape[-1]}, but should be {num_hashes * sequence_length}\"\n+            assert int(buckets.shape[-1]) == num_hashes * sequence_length, (\n+                f\"last dim of buckets is {buckets.shape[-1]}, but should be {num_hashes * sequence_length}\"\n+            )\n \n             sorted_bucket_idx, undo_sorted_bucket_idx = self._get_sorted_bucket_idx_and_undo_sorted_bucket_idx(\n                 sequence_length, buckets, num_hashes\n@@ -612,18 +612,18 @@ def _hash_vectors(self, vectors, num_hashes, attention_mask, increase_num_bucket\n         # We sample a different random rotation for each round of hashing to\n         # decrease the probability of hash misses.\n         if isinstance(self.num_buckets, int):\n-            assert (\n-                self.num_buckets % 2 == 0\n-            ), f\"There should be an even number of buckets, but `self.num_buckets`: {self.num_buckets}\"\n+            assert self.num_buckets % 2 == 0, (\n+                f\"There should be an even number of buckets, but `self.num_buckets`: {self.num_buckets}\"\n+            )\n             rotation_size = self.num_buckets\n             num_buckets = self.num_buckets\n         else:\n             # Factorize the hash if self.num_buckets is a list or tuple\n             rotation_size, num_buckets = 0, 1\n             for bucket_factor in self.num_buckets:\n-                assert (\n-                    bucket_factor % 2 == 0\n-                ), f\"The number of buckets should be even, but `num_bucket`: {bucket_factor}\"\n+                assert bucket_factor % 2 == 0, (\n+                    f\"The number of buckets should be even, but `num_bucket`: {bucket_factor}\"\n+                )\n                 rotation_size = rotation_size + bucket_factor\n                 num_buckets = num_buckets * bucket_factor\n \n@@ -1090,15 +1090,15 @@ def forward(\n         key_vectors = self._split_hidden_size_dim(key_vectors, self.num_attention_heads, self.attention_head_size)\n         value_vectors = self._split_hidden_size_dim(value_vectors, self.num_attention_heads, self.attention_head_size)\n \n-        assert (\n-            query_vectors.shape[-1] == self.attention_head_size\n-        ), f\"last dim of query_key_vectors is {query_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n-        assert (\n-            key_vectors.shape[-1] == self.attention_head_size\n-        ), f\"last dim of query_key_vectors is {key_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n-        assert (\n-            value_vectors.shape[-1] == self.attention_head_size\n-        ), f\"last dim of query_key_vectors is {value_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        assert query_vectors.shape[-1] == self.attention_head_size, (\n+            f\"last dim of query_key_vectors is {query_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        )\n+        assert key_vectors.shape[-1] == self.attention_head_size, (\n+            f\"last dim of query_key_vectors is {key_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        )\n+        assert value_vectors.shape[-1] == self.attention_head_size, (\n+            f\"last dim of query_key_vectors is {value_vectors.shape[-1]} but should be {self.attention_head_size}.\"\n+        )\n \n         if self.chunk_length is None:\n             assert self.num_chunks_before == 0 and self.num_chunks_after == 0, (\n@@ -1976,9 +1976,9 @@ class ReformerModel(ReformerPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.config = config\n-        assert (\n-            self.config.num_hidden_layers > 0\n-        ), \"`config.attn_layers` is empty. Select at least one attn layer form ['lsh', 'local']\"\n+        assert self.config.num_hidden_layers > 0, (\n+            \"`config.attn_layers` is empty. Select at least one attn layer form ['lsh', 'local']\"\n+        )\n \n         self.embeddings = ReformerEmbeddings(config)\n         self.encoder = ReformerEncoder(config)\n@@ -2039,9 +2039,9 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        assert (\n-            len(input_shape) == 2\n-        ), f\"`input_ids` have be of shape `[batch_size, sequence_length]`, but got shape: {input_shape}\"\n+        assert len(input_shape) == 2, (\n+            f\"`input_ids` have be of shape `[batch_size, sequence_length]`, but got shape: {input_shape}\"\n+        )\n \n         if past_buckets_states is not None:\n             assert not self.training, \"`past_buckets_states` can only be used for inference, not for training`.\""
        },
        {
            "sha": "7e0cc5d562b97559878f86b7b6316fa2c403b675",
            "filename": "src/transformers/models/regnet/modeling_tf_regnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_tf_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_tf_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_tf_regnet.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -311,7 +311,7 @@ def __init__(\n         self.layers = [\n             # downsampling is done in the first layer with stride of 2\n             layer(config, in_channels, out_channels, stride=stride, name=\"layers.0\"),\n-            *[layer(config, out_channels, out_channels, name=f\"layers.{i+1}\") for i in range(depth - 1)],\n+            *[layer(config, out_channels, out_channels, name=f\"layers.{i + 1}\") for i in range(depth - 1)],\n         ]\n \n     def call(self, hidden_state):\n@@ -346,7 +346,7 @@ def __init__(self, config: RegNetConfig, **kwargs):\n         )\n         in_out_channels = zip(config.hidden_sizes, config.hidden_sizes[1:])\n         for i, ((in_channels, out_channels), depth) in enumerate(zip(in_out_channels, config.depths[1:])):\n-            self.stages.append(TFRegNetStage(config, in_channels, out_channels, depth=depth, name=f\"stages.{i+1}\"))\n+            self.stages.append(TFRegNetStage(config, in_channels, out_channels, depth=depth, name=f\"stages.{i + 1}\"))\n \n     def call(\n         self, hidden_state: tf.Tensor, output_hidden_states: bool = False, return_dict: bool = True"
        },
        {
            "sha": "7bef416ec3757f4eb172f0c57135cb2ba58d3540",
            "filename": "src/transformers/models/seamless_m4t/convert_fairseq2_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fconvert_fairseq2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fconvert_fairseq2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fconvert_fairseq2_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -206,7 +206,7 @@ def filter_func(item):\n     hf_model.load_state_dict(state_dict, strict=False)\n     n_params = param_count(hf_model)\n \n-    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params\")\n+    logger.info(f\"model loaded: {round(n_params / 1e6, 1)}M params\")\n \n     hf_model.eval()\n     hf_model.to(device)"
        },
        {
            "sha": "5cbdf0960d39d0cfe48b96d80e8339cb208b196b",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -2869,7 +2869,7 @@ def generate(\n                 if tgt_lang not in self.generation_config.text_decoder_lang_to_code_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model. Please specify a `tgt_lang` in\n-                        {', '.join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n+                        {\", \".join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n@@ -3140,7 +3140,7 @@ def generate(\n                 if tgt_lang not in self.generation_config.text_decoder_lang_to_code_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model. Please specify a `tgt_lang` in\n-                        {', '.join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n+                        {\", \".join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n@@ -3407,7 +3407,7 @@ def generate(\n                 elif tgt_lang not in lang_code_to_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n-                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n+                    Please specify a `tgt_lang` in {\",\".join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n                     more languages for text translation than for speech synthesis.\"\"\"\n                     )\n \n@@ -3736,7 +3736,7 @@ def generate(\n                 elif tgt_lang not in lang_code_to_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n-                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n+                    Please specify a `tgt_lang` in {\",\".join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n                     more languages for text translation than for speech synthesis.\"\"\"\n                     )\n \n@@ -4151,7 +4151,7 @@ def generate(\n                 elif tgt_lang not in lang_code_to_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n-                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n+                    Please specify a `tgt_lang` in {\",\".join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n                     more languages for text translation than for speech synthesis.\"\"\"\n                     )\n "
        },
        {
            "sha": "c75b7c8139d3eab6e05d17d21ee88f6f7ab84747",
            "filename": "src/transformers/models/seamless_m4t_v2/convert_fairseq2_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fconvert_fairseq2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fconvert_fairseq2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fconvert_fairseq2_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -207,7 +207,7 @@ def filter_func(item):\n     hf_model.load_state_dict(state_dict, strict=False)\n     n_params = param_count(hf_model)\n \n-    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params\")\n+    logger.info(f\"model loaded: {round(n_params / 1e6, 1)}M params\")\n \n     hf_model.eval()\n     hf_model.to(device)"
        },
        {
            "sha": "1b48297a6f1f20096de99a05c5ab61edc43f036a",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -3149,7 +3149,7 @@ def generate(\n                 if tgt_lang not in self.generation_config.text_decoder_lang_to_code_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model. Please specify a `tgt_lang` in\n-                        {', '.join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n+                        {\", \".join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n@@ -3430,7 +3430,7 @@ def generate(\n                 if tgt_lang not in self.generation_config.text_decoder_lang_to_code_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model. Please specify a `tgt_lang` in\n-                        {', '.join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n+                        {\", \".join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n@@ -3707,7 +3707,7 @@ def generate(\n                 elif tgt_lang not in lang_code_to_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n-                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4Tv2 supports\n+                    Please specify a `tgt_lang` in {\",\".join(lang_code_to_id.keys())}. Note that SeamlessM4Tv2 supports\n                     more languages for text translation than for speech synthesis.\"\"\"\n                     )\n \n@@ -4078,7 +4078,7 @@ def generate(\n                 elif tgt_lang not in lang_code_to_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n-                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4Tv2 supports\n+                    Please specify a `tgt_lang` in {\",\".join(lang_code_to_id.keys())}. Note that SeamlessM4Tv2 supports\n                     more languages for text translation than for speech synthesis.\"\"\"\n                     )\n \n@@ -4539,7 +4539,7 @@ def generate(\n                 elif tgt_lang not in lang_code_to_id:\n                     raise ValueError(\n                         f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n-                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4Tv2 supports\n+                    Please specify a `tgt_lang` in {\",\".join(lang_code_to_id.keys())}. Note that SeamlessM4Tv2 supports\n                     more languages for text translation than for speech synthesis.\"\"\"\n                     )\n "
        },
        {
            "sha": "3bbc86e433b0dd5b72ebe8c76961b92908599ac4",
            "filename": "src/transformers/models/segformer/convert_segformer_original_to_pytorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fsegformer%2Fconvert_segformer_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fsegformer%2Fconvert_segformer_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fconvert_segformer_original_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -47,21 +47,21 @@ def rename_keys(state_dict, encoder_only=False):\n         if \"patch_embed\" in key:\n             # replace for example patch_embed1 by patch_embeddings.0\n             idx = key[key.find(\"patch_embed\") + len(\"patch_embed\")]\n-            key = key.replace(f\"patch_embed{idx}\", f\"patch_embeddings.{int(idx)-1}\")\n+            key = key.replace(f\"patch_embed{idx}\", f\"patch_embeddings.{int(idx) - 1}\")\n         if \"norm\" in key:\n             key = key.replace(\"norm\", \"layer_norm\")\n         if \"segformer.encoder.layer_norm\" in key:\n             # replace for example layer_norm1 by layer_norm.0\n             idx = key[key.find(\"segformer.encoder.layer_norm\") + len(\"segformer.encoder.layer_norm\")]\n-            key = key.replace(f\"layer_norm{idx}\", f\"layer_norm.{int(idx)-1}\")\n+            key = key.replace(f\"layer_norm{idx}\", f\"layer_norm.{int(idx) - 1}\")\n         if \"layer_norm1\" in key:\n             key = key.replace(\"layer_norm1\", \"layer_norm_1\")\n         if \"layer_norm2\" in key:\n             key = key.replace(\"layer_norm2\", \"layer_norm_2\")\n         if \"block\" in key:\n             # replace for example block1 by block.0\n             idx = key[key.find(\"block\") + len(\"block\")]\n-            key = key.replace(f\"block{idx}\", f\"block.{int(idx)-1}\")\n+            key = key.replace(f\"block{idx}\", f\"block.{int(idx) - 1}\")\n         if \"attn.q\" in key:\n             key = key.replace(\"attn.q\", \"attention.self.query\")\n         if \"attn.proj\" in key:\n@@ -80,7 +80,7 @@ def rename_keys(state_dict, encoder_only=False):\n         if \"linear_c\" in key:\n             # replace for example linear_c4 by linear_c.3\n             idx = key[key.find(\"linear_c\") + len(\"linear_c\")]\n-            key = key.replace(f\"linear_c{idx}\", f\"linear_c.{int(idx)-1}\")\n+            key = key.replace(f\"linear_c{idx}\", f\"linear_c.{int(idx) - 1}\")\n         if key.startswith(\"head\"):\n             key = key.replace(\"head\", \"classifier\")\n         new_state_dict[key] = value"
        },
        {
            "sha": "bdd57c84f544dc58031b0cc00b6e4f6498c997fd",
            "filename": "src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconvert_mbart_wav2vec2_seq2seq_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconvert_mbart_wav2vec2_seq2seq_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconvert_mbart_wav2vec2_seq2seq_original_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -192,41 +192,41 @@ def load_adapter(full_name, value, adapter, unused_weights):\n         if \"proj_ln\" in full_name:\n             # has to be layer norm\n             if \"bias\" in name:\n-                assert (\n-                    value.shape == adapter.proj_layer_norm.bias.data.shape\n-                ), f\"{full_name} has size {value.shape}, but {adapter.proj_layer_norm.bias.data.shape} was found.\"\n+                assert value.shape == adapter.proj_layer_norm.bias.data.shape, (\n+                    f\"{full_name} has size {value.shape}, but {adapter.proj_layer_norm.bias.data.shape} was found.\"\n+                )\n                 adapter.proj_layer_norm.bias.data = value\n                 logger.info(f\"Adapter proj layer norm bias was initialized from {full_name}.\")\n             if \"weight\" in name:\n-                assert (\n-                    value.shape == adapter.proj_layer_norm.weight.data.shape\n-                ), f\"{full_name} has size {value.shape}, but {adapter.proj_layer_norm.weight.data.shape} was found.\"\n+                assert value.shape == adapter.proj_layer_norm.weight.data.shape, (\n+                    f\"{full_name} has size {value.shape}, but {adapter.proj_layer_norm.weight.data.shape} was found.\"\n+                )\n                 adapter.proj_layer_norm.weight.data = value\n         else:\n             # has to be projection layer\n             if \"bias\" in name:\n-                assert (\n-                    value.shape == adapter.proj.bias.data.shape\n-                ), f\"{full_name} has size {value.shape}, but {adapter.proj.bias.data.shape} was found.\"\n+                assert value.shape == adapter.proj.bias.data.shape, (\n+                    f\"{full_name} has size {value.shape}, but {adapter.proj.bias.data.shape} was found.\"\n+                )\n                 adapter.proj.bias.data = value\n                 logger.info(f\"Adapter proj layer bias was initialized from {full_name}.\")\n             if \"weight\" in name:\n-                assert (\n-                    value.shape == adapter.proj.weight.data.shape\n-                ), f\"{full_name} has size {value.shape}, but {adapter.proj.weight.data.shape} was found.\"\n+                assert value.shape == adapter.proj.weight.data.shape, (\n+                    f\"{full_name} has size {value.shape}, but {adapter.proj.weight.data.shape} was found.\"\n+                )\n                 adapter.proj.weight.data = value\n                 logger.info(f\"Adapter proj layer weight was initialized from {full_name}.\")\n     elif isinstance(layer_id, int):\n         if \"bias\" in name:\n-            assert (\n-                value.shape == adapter.layers[layer_id].conv.bias.data.shape\n-            ), f\"{full_name} has size {value.shape}, but {adapter.layers[layer_id].conv.bias.data.shape} was found.\"\n+            assert value.shape == adapter.layers[layer_id].conv.bias.data.shape, (\n+                f\"{full_name} has size {value.shape}, but {adapter.layers[layer_id].conv.bias.data.shape} was found.\"\n+            )\n             adapter.layers[layer_id].conv.bias.data = value\n             logger.info(f\"Adapter layer {layer_id} bias was initialized from {full_name}.\")\n         elif \"weight\" in name:\n-            assert (\n-                value.shape == adapter.layers[layer_id].conv.weight.data.shape\n-            ), f\"{full_name} has size {value.shape}, but {adapter.layers[layer_id].conv.weight.data.shape} was found.\"\n+            assert value.shape == adapter.layers[layer_id].conv.weight.data.shape, (\n+                f\"{full_name} has size {value.shape}, but {adapter.layers[layer_id].conv.weight.data.shape} was found.\"\n+            )\n             adapter.layers[layer_id].conv.weight.data = value\n             logger.info(f\"Adapter layer {layer_id} bias was initialized from {full_name}.\")\n     else:"
        },
        {
            "sha": "ccff216c98c151775fdccb86733ffcdb742a146b",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -774,9 +774,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n \n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:"
        },
        {
            "sha": "29fe2e3e25de1ff73abbe4b3600f9df63ca7e411",
            "filename": "src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -224,9 +224,9 @@ def convert_swin2sr_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to\n             [[-0.5238, -0.5557, -0.6321], [-0.6016, -0.5903, -0.6391], [-0.6244, -0.6334, -0.6889]]\n         )\n \n-    assert (\n-        outputs.reconstruction.shape == expected_shape\n-    ), f\"Shape of reconstruction should be {expected_shape}, but is {outputs.reconstruction.shape}\"\n+    assert outputs.reconstruction.shape == expected_shape, (\n+        f\"Shape of reconstruction should be {expected_shape}, but is {outputs.reconstruction.shape}\"\n+    )\n     assert torch.allclose(outputs.reconstruction[0, 0, :3, :3], expected_slice, atol=1e-3)\n     print(\"Looks ok!\")\n "
        },
        {
            "sha": "70652c10cf15129176ff92fc5791d0d25193560e",
            "filename": "src/transformers/models/switch_transformers/convert_big_switch.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -103,7 +103,7 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, max_shard_size, dtype, w\n         # If this weight is going to tip up over the maximal size, we split.\n         if current_block_size + weight_size > max_shard_size:\n             save_path = os.path.join(\n-                dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts)+1:05d}-of-???.bin\")\n+                dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts) + 1:05d}-of-???.bin\")\n             )\n             rename_and_save_block(current_block, save_path)\n             sharded_state_dicts.append(current_block.keys())\n@@ -116,7 +116,9 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, max_shard_size, dtype, w\n         total_size += weight_size\n \n     # Add the last block\n-    save_path = os.path.join(dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts)+1:05d}-of-???.bin\"))\n+    save_path = os.path.join(\n+        dump_path, weights_name.replace(\".bin\", f\"-{len(sharded_state_dicts) + 1:05d}-of-???.bin\")\n+    )\n     rename_and_save_block(current_block, save_path)\n     sharded_state_dicts.append(current_block.keys())\n \n@@ -129,9 +131,9 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, max_shard_size, dtype, w\n     shards = {}\n     for idx, shard in enumerate(sharded_state_dicts):\n         shard_file = weights_name.replace(\n-            \".bin\", f\"-{idx+1:05d}-of-{len(sharded_state_dicts):05d}.bin\"\n+            \".bin\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.bin\"\n         )  # len(sharded_state_dicts):05d}\n-        temp_filename = os.path.join(dump_path, weights_name.replace(\".bin\", f\"-{idx+1:05d}-of-???.bin\"))\n+        temp_filename = os.path.join(dump_path, weights_name.replace(\".bin\", f\"-{idx + 1:05d}-of-???.bin\"))\n         os.rename(temp_filename, os.path.join(dump_path, shard_file))\n         shards[shard_file] = shard\n         for key in shard:"
        },
        {
            "sha": "84f5b2a636129ab69f58341faf30df135128d13e",
            "filename": "src/transformers/models/t5/modeling_tf_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -363,9 +363,9 @@ def call(\n         real_seq_length = seq_length\n \n         if past_key_value is not None:\n-            assert (\n-                len(past_key_value) == 2\n-            ), f\"past_key_value should have 2 past states: keys and values. Got {len(past_key_value)} past states\"\n+            assert len(past_key_value) == 2, (\n+                f\"past_key_value should have 2 past states: keys and values. Got {len(past_key_value)} past states\"\n+            )\n             real_seq_length += shape_list(past_key_value[0])[2] if query_length is None else query_length\n \n         key_length = real_seq_length if key_value_states is None else shape_list(key_value_states)[1]"
        },
        {
            "sha": "53d9a9d6baeb8ae0352e743adefb16d721335918",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1284,9 +1284,9 @@ def forward(\n                 aggregate_mask = None\n             else:\n                 if float_answer is not None:\n-                    assert (\n-                        labels.shape[0] == float_answer.shape[0]\n-                    ), \"Make sure the answers are a FloatTensor of shape (batch_size,)\"\n+                    assert labels.shape[0] == float_answer.shape[0], (\n+                        \"Make sure the answers are a FloatTensor of shape (batch_size,)\"\n+                    )\n                     # <float32>[batch_size]\n                     aggregate_mask = _calculate_aggregate_mask(\n                         float_answer,\n@@ -1336,9 +1336,9 @@ def forward(\n                 if is_supervised:\n                     # Note that `aggregate_mask` is None if the setting is supervised.\n                     if aggregation_labels is not None:\n-                        assert (\n-                            labels.shape[0] == aggregation_labels.shape[0]\n-                        ), \"Make sure the aggregation labels are a LongTensor of shape (batch_size,)\"\n+                        assert labels.shape[0] == aggregation_labels.shape[0], (\n+                            \"Make sure the aggregation labels are a LongTensor of shape (batch_size,)\"\n+                        )\n                         per_example_additional_loss = _calculate_aggregation_loss(\n                             logits_aggregation,\n                             aggregate_mask,"
        },
        {
            "sha": "82430deebfe0a414b3d1594349bbd198a5dfd3be",
            "filename": "src/transformers/models/tapas/modeling_tf_tapas.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1562,9 +1562,9 @@ def call(\n                 aggregate_mask = None\n             else:\n                 if float_answer is not None:\n-                    assert (\n-                        shape_list(labels)[0] == shape_list(float_answer)[0]\n-                    ), \"Make sure the answers are a FloatTensor of shape (batch_size,)\"\n+                    assert shape_list(labels)[0] == shape_list(float_answer)[0], (\n+                        \"Make sure the answers are a FloatTensor of shape (batch_size,)\"\n+                    )\n                     # <float32>[batch_size]\n                     aggregate_mask = _calculate_aggregate_mask(\n                         float_answer,\n@@ -1615,9 +1615,9 @@ def call(\n                 if is_supervised:\n                     # Note that `aggregate_mask` is None if the setting is supervised.\n                     if aggregation_labels is not None:\n-                        assert (\n-                            shape_list(labels)[0] == shape_list(aggregation_labels)[0]\n-                        ), \"Make sure the aggregation labels are a LongTensor of shape (batch_size,)\"\n+                        assert shape_list(labels)[0] == shape_list(aggregation_labels)[0], (\n+                            \"Make sure the aggregation labels are a LongTensor of shape (batch_size,)\"\n+                        )\n                         per_example_additional_loss = _calculate_aggregation_loss(\n                             logits_aggregation,\n                             aggregate_mask,"
        },
        {
            "sha": "16dd5c0a37520be501a3b38114e2f60687016cac",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -773,7 +773,7 @@ def forward(self, pixel_values, interpolate_pad_encoding: bool = False):\n \n \n @add_start_docstrings(\n-    \"The bare Tvp Model transformer outputting BaseModelOutputWithPooling object without any specific head on\" \" top.\",\n+    \"The bare Tvp Model transformer outputting BaseModelOutputWithPooling object without any specific head on top.\",\n     TVP_START_DOCSTRING,\n )\n class TvpModel(TvpPreTrainedModel):"
        },
        {
            "sha": "93d128562e418c593720dc727b2b70ed63c72f39",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -407,8 +407,7 @@ def forward(self, pixel_values):\n         batch_size, num_channels, height, width = pixel_values.shape\n         if height != self.image_size[0] or width != self.image_size[1]:\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\"\n-                f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n             )\n         embeddings = self.proj(pixel_values)\n         embeddings = embeddings.flatten(2).transpose(1, 2)"
        },
        {
            "sha": "59733539415865a1dcd7f5a2266f92d643744daa",
            "filename": "src/transformers/models/visual_bert/convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fconvert_visual_bert_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fconvert_visual_bert_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fconvert_visual_bert_original_pytorch_checkpoint_to_pytorch.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -84,9 +84,9 @@ def convert_visual_bert_checkpoint(checkpoint_path, pytorch_dump_folder_path):\n     Copy/paste/tweak model's weights to our VisualBERT structure.\n     \"\"\"\n \n-    assert (\n-        checkpoint_path.split(\"/\")[-1] in ACCEPTABLE_CHECKPOINTS\n-    ), f\"The checkpoint provided must be in {ACCEPTABLE_CHECKPOINTS}.\"\n+    assert checkpoint_path.split(\"/\")[-1] in ACCEPTABLE_CHECKPOINTS, (\n+        f\"The checkpoint provided must be in {ACCEPTABLE_CHECKPOINTS}.\"\n+    )\n \n     # Get Config\n     if \"pre\" in checkpoint_path:"
        },
        {
            "sha": "0d36e332a4f0de1971ce5e69330f7e6799e9f65d",
            "filename": "src/transformers/models/vitpose/convert_vitpose_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -229,7 +229,7 @@ def write_model(model_name, model_path, push_to_hub, check_logits=True):\n         elif re.search(\"head\", new_key) and not config.use_simple_decoder:\n             # Pattern for deconvolution layers\n             deconv_pattern = r\"deconv_layers\\.(0|3)\\.weight\"\n-            new_key = re.sub(deconv_pattern, lambda m: f\"deconv{int(m.group(1))//3 + 1}.weight\", new_key)\n+            new_key = re.sub(deconv_pattern, lambda m: f\"deconv{int(m.group(1)) // 3 + 1}.weight\", new_key)\n             # Pattern for batch normalization layers\n             bn_patterns = [\n                 (r\"deconv_layers\\.(\\d+)\\.weight\", r\"batchnorm\\1.weight\"),"
        },
        {
            "sha": "238a723dfab3a230bf920f5f8006217f81ed8d55",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -72,8 +72,7 @@ def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         batch_size, num_frames, num_channels, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Image image size ({height}*{width}) doesn't match model\"\n-                f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n+                f\"Image image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n             )\n \n         # permute to (batch_size, num_channels, num_frames, height, width)"
        },
        {
            "sha": "1cfbeb43a5e6a3117b5a4e63f0a5f35151f84622",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -614,7 +614,7 @@ def __init__(self, config: Wav2Vec2Config, **kwargs: Any) -> None:\n \n         if config.feat_extract_norm == \"group\":\n             conv_layers = [TFWav2Vec2GroupNormConvLayer(config, layer_id=0, name=f\"conv_layers.{0}\")] + [\n-                TFWav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1, name=f\"conv_layers.{i+1}\")\n+                TFWav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1, name=f\"conv_layers.{i + 1}\")\n                 for i in range(config.num_feat_extract_layers - 1)\n             ]\n         elif config.feat_extract_norm == \"layer\":"
        },
        {
            "sha": "33510654dcca5ec24940f744653df1e909db3fae",
            "filename": "src/transformers/models/wav2vec2_bert/convert_wav2vec2_seamless_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -113,7 +113,7 @@ def _convert_model(\n     hf_model.load_state_dict(state_dict, strict=True)\n     n_params = param_count(hf_model)\n \n-    logger.info(f\"model loaded: {round(n_params/1e6,1)}M params\")\n+    logger.info(f\"model loaded: {round(n_params / 1e6, 1)}M params\")\n \n     hf_model.eval()\n     del state_dict"
        },
        {
            "sha": "727a68f8571cb8f768850989dec65462e4472f40",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1043,9 +1043,9 @@ def forward(\n \n         # check if head_mask has a correct number of layers specified if desired\n         if head_mask is not None:\n-            assert head_mask.size()[0] == (\n-                len(self.layers)\n-            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            assert head_mask.size()[0] == (len(self.layers)), (\n+                f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n+            )\n \n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:"
        },
        {
            "sha": "4f1aa6e322333e03b693c5c033df2986fe40b2a3",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -167,7 +167,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         batch_size, _, height, width = pixel_values.shape\n         if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n             raise ValueError(\n-                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n             )\n         target_dtype = self.patch_embedding.weight.dtype\n         patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]"
        },
        {
            "sha": "9d1adf737010f5c5e3ee0a0693ca54a88ff39009",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -601,8 +601,7 @@ def forward(\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n                 logger.warning_once(\n-                    \"`use_cache = True` is incompatible with gradient checkpointing`. Setting `use_cache =\"\n-                    \" False`...\"\n+                    \"`use_cache = True` is incompatible with gradient checkpointing`. Setting `use_cache = False`...\"\n                 )\n                 use_cache = False\n "
        },
        {
            "sha": "f689e417bfe8cdaf29a6d402275e38f7b87356b3",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -164,25 +164,25 @@ def load_tf_weights_in_xlnet(model, config, tf_path):\n             array = np.transpose(array)\n         if isinstance(pointer, list):\n             # Here we will split the TF weights\n-            assert (\n-                len(pointer) == array.shape[0]\n-            ), f\"Pointer length {len(pointer)} and array length {array.shape[0]} mismatched\"\n+            assert len(pointer) == array.shape[0], (\n+                f\"Pointer length {len(pointer)} and array length {array.shape[0]} mismatched\"\n+            )\n             for i, p_i in enumerate(pointer):\n                 arr_i = array[i, ...]\n                 try:\n-                    assert (\n-                        p_i.shape == arr_i.shape\n-                    ), f\"Pointer shape {p_i.shape} and array shape {arr_i.shape} mismatched\"\n+                    assert p_i.shape == arr_i.shape, (\n+                        f\"Pointer shape {p_i.shape} and array shape {arr_i.shape} mismatched\"\n+                    )\n                 except AssertionError as e:\n                     e.args += (p_i.shape, arr_i.shape)\n                     raise\n                 logger.info(f\"Initialize PyTorch weight {name} for layer {i}\")\n                 p_i.data = torch.from_numpy(arr_i)\n         else:\n             try:\n-                assert (\n-                    pointer.shape == array.shape\n-                ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+                assert pointer.shape == array.shape, (\n+                    f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n+                )\n             except AssertionError as e:\n                 e.args += (pointer.shape, array.shape)\n                 raise"
        },
        {
            "sha": "46d99a322720518b71587c7f178f2c14b1ed350a",
            "filename": "src/transformers/models/zamba/configuration_zamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fconfiguration_zamba.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -203,9 +203,9 @@ def __init__(\n \n         self.layers_block_type = self._layers_block_type(num_hidden_layers, attn_layer_period, attn_layer_offset)\n \n-        assert (\n-            self.mamba_expand * self.hidden_size\n-        ) % self.n_mamba_heads == 0, \"`intermediate_size` should be divisible by `n_mamba_heads`.\"\n+        assert (self.mamba_expand * self.hidden_size) % self.n_mamba_heads == 0, (\n+            \"`intermediate_size` should be divisible by `n_mamba_heads`.\"\n+        )\n \n         super().__init__(\n             pad_token_id=pad_token_id,"
        },
        {
            "sha": "899a7cc53908a8a42048f4a54d400501244c6556",
            "filename": "src/transformers/pipelines/document_question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdocument_question_answering.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -339,7 +339,7 @@ def preprocess(\n             )\n \n         if self.model_type == ModelType.VisionEncoderDecoder:\n-            task_prompt = f'<s_docvqa><s_question>{input[\"question\"]}</s_question><s_answer>'\n+            task_prompt = f\"<s_docvqa><s_question>{input['question']}</s_question><s_answer>\"\n             # Adapted from https://huggingface.co/spaces/nielsr/donut-docvqa/blob/main/app.py\n             encoding = {\n                 \"inputs\": image_features[\"pixel_values\"],"
        },
        {
            "sha": "0adefdffb9fff0ad85ffba2cbeb0eca7a5fee487",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -290,7 +290,7 @@ def check_inputs(self, input_length: int, min_length: int, max_length: int) -> b\n             logger.warning(\n                 f\"Your max_length is set to {max_length}, but your input_length is only {input_length}. Since this is \"\n                 \"a summarization task, where outputs shorter than the input are typically wanted, you might \"\n-                f\"consider decreasing max_length manually, e.g. summarizer('...', max_length={input_length//2})\"\n+                f\"consider decreasing max_length manually, e.g. summarizer('...', max_length={input_length // 2})\"\n             )\n \n "
        },
        {
            "sha": "69c55d01bdf065f3d85babf8bc5e5f5dcbc01f43",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -104,8 +104,7 @@ def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n         elif torch_dtype == torch.float16:\n             raise ValueError(\n-                \"You cannot use FP8 with torch_dtype=torch.float16.\"\n-                \"We recommend you passing torch_dtype=torch.bfloat16\"\n+                \"You cannot use FP8 with torch_dtype=torch.float16.We recommend you passing torch_dtype=torch.bfloat16\"\n             )\n         return torch_dtype\n "
        },
        {
            "sha": "81c0710288e3174750e3daf50cf12c0902d9cd88",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -257,8 +257,7 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n     def is_serializable(self, safe_serialization=None) -> bool:\n         if safe_serialization:\n             logger.warning(\n-                \"torchao quantized model does not support safe serialization, \"\n-                \"please set `safe_serialization` to False\"\n+                \"torchao quantized model does not support safe serialization, please set `safe_serialization` to False\"\n             )\n             return False\n         _is_torchao_serializable = version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse("
        },
        {
            "sha": "548c4887816488a39a09928579b16e215732e3e4",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -868,7 +868,7 @@ class SpecialTokensMixin:\n     def __init__(self, verbose=False, **kwargs):\n         self._pad_token_type_id = 0\n         self.verbose = verbose\n-        self._special_tokens_map = {attr: None for attr in self.SPECIAL_TOKENS_ATTRIBUTES}\n+        self._special_tokens_map = dict.fromkeys(self.SPECIAL_TOKENS_ATTRIBUTES)\n         self._special_tokens_map[\"additional_special_tokens\"] = []  # for BC where it defaults to empty list\n \n         # We directly set the hidden value to allow initialization with special tokens\n@@ -881,9 +881,9 @@ def __init__(self, verbose=False, **kwargs):\n             if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n                 if key == \"additional_special_tokens\":\n                     assert isinstance(value, (list, tuple)), f\"Value {value} is not a list or tuple\"\n-                    assert all(\n-                        isinstance(t, (str, AddedToken)) for t in value\n-                    ), \"One of the tokens is not a string or an AddedToken\"\n+                    assert all(isinstance(t, (str, AddedToken)) for t in value), (\n+                        \"One of the tokens is not a string or an AddedToken\"\n+                    )\n                     setattr(self, key, value)\n                 elif isinstance(value, (str, AddedToken)):\n                     setattr(self, key, value)\n@@ -967,9 +967,9 @@ def add_special_tokens(\n                 logger.info(f\"Assigning {value} to the {key} key of the tokenizer\")\n \n             if key == \"additional_special_tokens\":\n-                assert isinstance(value, (list, tuple)) and all(\n-                    isinstance(t, (str, AddedToken)) for t in value\n-                ), f\"Tokens {value} for key {key} should all be str or AddedToken instances\"\n+                assert isinstance(value, (list, tuple)) and all(isinstance(t, (str, AddedToken)) for t in value), (\n+                    f\"Tokens {value} for key {key} should all be str or AddedToken instances\"\n+                )\n \n                 to_add = []\n                 for token in value:\n@@ -3379,9 +3379,9 @@ def pad(\n             return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n \n         batch_size = len(required_input)\n-        assert all(\n-            len(v) == batch_size for v in encoded_inputs.values()\n-        ), \"Some items in the output dictionary have a different batch size than others.\"\n+        assert all(len(v) == batch_size for v in encoded_inputs.values()), (\n+            \"Some items in the output dictionary have a different batch size than others.\"\n+        )\n \n         if padding_strategy == PaddingStrategy.LONGEST:\n             max_length = max(len(inputs) for inputs in required_input)"
        },
        {
            "sha": "7a4892159f549c1635256cfc09099f95044df5b5",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -749,12 +749,12 @@ def on_train_begin(self, args, state, control, **kwargs):\n                 \"Using EarlyStoppingCallback without load_best_model_at_end=True. \"\n                 \"Once training is finished, the best model will not be loaded automatically.\"\n             )\n-        assert (\n-            args.metric_for_best_model is not None\n-        ), \"EarlyStoppingCallback requires metric_for_best_model to be defined\"\n-        assert (\n-            args.eval_strategy != IntervalStrategy.NO\n-        ), \"EarlyStoppingCallback requires IntervalStrategy of steps or epoch\"\n+        assert args.metric_for_best_model is not None, (\n+            \"EarlyStoppingCallback requires metric_for_best_model to be defined\"\n+        )\n+        assert args.eval_strategy != IntervalStrategy.NO, (\n+            \"EarlyStoppingCallback requires IntervalStrategy of steps or epoch\"\n+        )\n \n     def on_evaluate(self, args, state, control, metrics, **kwargs):\n         metric_to_check = args.metric_for_best_model"
        },
        {
            "sha": "b0f5635cf5bc0f9b294536269354acd15a4ceed0",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -121,9 +121,9 @@ def nested_concat(tensors, new_tensors, padding_index=-100):\n     nested list/tuples/dict of tensors.\n     \"\"\"\n     if not (isinstance(tensors, torch.Tensor) and isinstance(new_tensors, torch.Tensor)):\n-        assert (\n-            type(tensors) is type(new_tensors)\n-        ), f\"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n+        assert type(tensors) is type(new_tensors), (\n+            f\"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n+        )\n     if isinstance(tensors, (list, tuple)):\n         return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n     elif isinstance(tensors, torch.Tensor):\n@@ -381,15 +381,15 @@ def __iter__(self):\n \n         # add extra samples to make it evenly divisible\n         indices += indices[: (self.total_size - len(indices))]\n-        assert (\n-            len(indices) == self.total_size\n-        ), f\"Indices length {len(indices)} and total size {self.total_size} mismatched\"\n+        assert len(indices) == self.total_size, (\n+            f\"Indices length {len(indices)} and total size {self.total_size} mismatched\"\n+        )\n \n         # subsample\n         indices = indices[self.rank * self.num_samples : (self.rank + 1) * self.num_samples]\n-        assert (\n-            len(indices) == self.num_samples\n-        ), f\"Indices length {len(indices)} and sample number {self.num_samples} mismatched\"\n+        assert len(indices) == self.num_samples, (\n+            f\"Indices length {len(indices)} and sample number {self.num_samples} mismatched\"\n+        )\n \n         return iter(indices)\n \n@@ -506,9 +506,9 @@ def _nested_set_tensors(self, storage, arrays):\n         if isinstance(arrays, (list, tuple)):\n             result = [self._nested_set_tensors(x, y) for x, y in zip(storage, arrays)]\n             return result[0][0], type(arrays)(r[1] for r in result)\n-        assert (\n-            arrays.shape[0] % self.world_size == 0\n-        ), f\"Arrays passed should all have a first dimension multiple of {self.world_size}, found {arrays.shape[0]}.\"\n+        assert arrays.shape[0] % self.world_size == 0, (\n+            f\"Arrays passed should all have a first dimension multiple of {self.world_size}, found {arrays.shape[0]}.\"\n+        )\n \n         slice_len = arrays.shape[0] // self.world_size\n         for i in range(self.world_size):"
        },
        {
            "sha": "e101fadc2a009599f527cb9f17d85841fb0c4c7a",
            "filename": "src/transformers/utils/chat_template_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fchat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fchat_template_utils.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -412,7 +412,7 @@ def activate_tracker(self, rendered_blocks: List[int], generation_indices: List[\n \n     if version.parse(jinja2.__version__) < version.parse(\"3.1.0\"):\n         raise ImportError(\n-            \"apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is \" f\"{jinja2.__version__}.\"\n+            f\"apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is {jinja2.__version__}.\"\n         )\n \n     def raise_exception(message):"
        },
        {
            "sha": "34c3999d9bb020ec15b635359293f14af8f6a312",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -513,7 +513,9 @@ def cached_files(\n             return None\n         # Now we raise for missing entries\n         revision_ = \"main\" if revision is None else revision\n-        msg = f\"a file named {missing_entries[0]}\" if len(missing_entries) == 1 else f\"files named {*missing_entries,}\"\n+        msg = (\n+            f\"a file named {missing_entries[0]}\" if len(missing_entries) == 1 else f\"files named {(*missing_entries,)}\"\n+        )\n         raise EnvironmentError(\n             f\"{path_or_repo_id} does not appear to have {msg}. Checkout 'https://huggingface.co/{path_or_repo_id}/tree/{revision_}'\"\n             \"for available files.\""
        },
        {
            "sha": "150cf8e132abf287b49b9f013a49326a62ddaa8b",
            "filename": "src/transformers/utils/logging.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Flogging.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Flogging.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Flogging.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -65,7 +65,7 @@ def _get_default_logging_level():\n         else:\n             logging.getLogger().warning(\n                 f\"Unknown option TRANSFORMERS_VERBOSITY={env_level_str}, \"\n-                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n+                f\"has to be one of: {', '.join(log_levels.keys())}\"\n             )\n     return _default_log_level\n "
        },
        {
            "sha": "94fc1990b57b2ea70f6c2444d20f1122ea35a982",
            "filename": "src/transformers/utils/notebook.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fnotebook.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fnotebook.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fnotebook.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -186,7 +186,7 @@ def update_bar(self, value, comment=None):\n             if self.average_time_per_item == 0:\n                 self.label += \", +inf it/s\"\n             else:\n-                self.label += f\", {1/self.average_time_per_item:.2f} it/s\"\n+                self.label += f\", {1 / self.average_time_per_item:.2f} it/s\"\n \n         self.label += \"]\" if self.comment is None or len(self.comment) == 0 else f\", {self.comment}]\"\n         self.display()"
        },
        {
            "sha": "32c249fb51116f4e886768927906d2892474640a",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1636,16 +1636,16 @@ def to_dict(self):\n     def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n         \"\"\"Create configuration from a dictionary.\"\"\"\n         ao_verison = cls._get_ao_version()\n-        assert ao_verison >= version.parse(\n-            \"0.10.0\"\n-        ), \"TorchAoConfig requires torchao >= 0.10.0 for construction from dict\"\n+        assert ao_verison >= version.parse(\"0.10.0\"), (\n+            \"TorchAoConfig requires torchao >= 0.10.0 for construction from dict\"\n+        )\n         config_dict = config_dict.copy()\n         quant_type = config_dict.pop(\"quant_type\")\n         # Check if we only have one key which is \"default\"\n         # In the future we may update this\n-        assert (\n-            len(quant_type) == 1 and \"default\" in quant_type\n-        ), \"Expected only one key 'default' in quant_type dictionary\"\n+        assert len(quant_type) == 1 and \"default\" in quant_type, (\n+            \"Expected only one key 'default' in quant_type dictionary\"\n+        )\n         quant_type = quant_type[\"default\"]\n \n         # Deserialize quant_type if needed"
        },
        {
            "sha": "fd9fea1f741a2d27f93fb1ea0e4fd63b1c10c599",
            "filename": "tests/models/fuyu/test_image_processing_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Ffuyu%2Ftest_image_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Ffuyu%2Ftest_image_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_image_processing_fuyu.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -42,9 +42,9 @@ def test_patches(self):\n         expected_num_patches = self.processor.get_num_patches(image_height=self.height, image_width=self.width)\n \n         patches_final = self.processor.patchify_image(image=self.image_input)\n-        assert (\n-            patches_final.shape[1] == expected_num_patches\n-        ), f\"Expected {expected_num_patches} patches, got {patches_final.shape[1]}.\"\n+        assert patches_final.shape[1] == expected_num_patches, (\n+            f\"Expected {expected_num_patches} patches, got {patches_final.shape[1]}.\"\n+        )\n \n     def test_scale_to_target_aspect_ratio(self):\n         # (h:450, w:210) fitting (160, 320) -> (160, 210*160/450)"
        },
        {
            "sha": "b58859a642b2001fe0d6831b10482b9484150655",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -431,9 +431,9 @@ def create_and_check_cached_forward_with_and_without_attention_mask(self, config\n         model.eval()\n \n         # We want this for SDPA, eager works with a `None` attention mask\n-        assert (\n-            model.config._attn_implementation == \"sdpa\"\n-        ), \"This test assumes the model to have the SDPA implementation for its attention calculations.\"\n+        assert model.config._attn_implementation == \"sdpa\", (\n+            \"This test assumes the model to have the SDPA implementation for its attention calculations.\"\n+        )\n \n         # Prepare cache and non_cache input, needs a full attention mask\n         cached_len = input_ids.shape[-1] // 2"
        },
        {
            "sha": "45906d60777c04a52ae52031353b47bdb2078799",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -222,9 +222,9 @@ def create_and_check_cached_forward_with_and_without_attention_mask(self, config\n         model.eval()\n \n         # We want this for SDPA, eager works with a `None` attention mask\n-        assert (\n-            model.config._attn_implementation == \"sdpa\"\n-        ), \"This test assumes the model to have the SDPA implementation for its attention calculations.\"\n+        assert model.config._attn_implementation == \"sdpa\", (\n+            \"This test assumes the model to have the SDPA implementation for its attention calculations.\"\n+        )\n \n         # Prepare cache and non_cache input, needs a full attention mask\n         cached_len = input_ids.shape[-1] // 2"
        },
        {
            "sha": "f2c5cd779469f43c68c105a190c64dc24f38c4eb",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -315,7 +315,7 @@ def get_instance_segmentation_and_mapping(annotation):\n             inst2class = {}\n             for label in class_labels:\n                 instance_ids = np.unique(instance_seg[class_id_map == label])\n-                inst2class.update({i: label for i in instance_ids})\n+                inst2class.update(dict.fromkeys(instance_ids, label))\n \n             return instance_seg, inst2class\n "
        },
        {
            "sha": "d97522261c0ca9e74367d89c5c10e3e4f0ba8706",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -269,7 +269,7 @@ def get_instance_segmentation_and_mapping(annotation):\n             inst2class = {}\n             for label in class_labels:\n                 instance_ids = np.unique(instance_seg[class_id_map == label])\n-                inst2class.update({i: label for i in instance_ids})\n+                inst2class.update(dict.fromkeys(instance_ids, label))\n \n             return instance_seg, inst2class\n "
        },
        {
            "sha": "4633f497f9032a01cfa5b50a46c68cfa80e48c55",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -1458,9 +1458,9 @@ def test_input_parameter_passthrough(self):\n \n         chunked_output = speech_recognizer(inputs.copy(), chunk_length_s=30)\n         non_chunked_output = speech_recognizer(inputs.copy())\n-        assert (\n-            chunked_output.keys() == non_chunked_output.keys()\n-        ), \"The output structure should be the same for chunked vs non-chunked versions of asr pipelines.\"\n+        assert chunked_output.keys() == non_chunked_output.keys(), (\n+            \"The output structure should be the same for chunked vs non-chunked versions of asr pipelines.\"\n+        )\n \n     @require_torch\n     def test_return_timestamps_ctc_fast(self):"
        },
        {
            "sha": "e48717914e081578099a81686bd8d08b8213398c",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -145,9 +145,9 @@ def __post_init__(self):\n             train_extension = self.train_file.split(\".\")[-1]\n             assert train_extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n             validation_extension = self.validation_file.split(\".\")[-1]\n-            assert (\n-                validation_extension == train_extension\n-            ), \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n+            assert validation_extension == train_extension, (\n+                \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n+            )\n \n \n @dataclass\n@@ -265,9 +265,9 @@ def main():\n             if data_args.test_file is not None:\n                 train_extension = data_args.train_file.split(\".\")[-1]\n                 test_extension = data_args.test_file.split(\".\")[-1]\n-                assert (\n-                    test_extension == train_extension\n-                ), \"`test_file` should have the same extension (csv or json) as `train_file`.\"\n+                assert test_extension == train_extension, (\n+                    \"`test_file` should have the same extension (csv or json) as `train_file`.\"\n+                )\n                 data_files[\"test\"] = data_args.test_file\n             else:\n                 raise ValueError(\"Need either a GLUE task or a test file for `do_predict`.\")"
        },
        {
            "sha": "5454140a688e6380b82537570a849a778ad958ee",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -3234,9 +3234,9 @@ def test_model_is_small(self):\n         for model_class in self.all_model_classes:\n             model = model_class(config)\n             num_params = model.num_parameters()\n-            assert (\n-                num_params < 1000000\n-            ), f\"{model_class} is too big for the common tests ({num_params})! It should have 1M max.\"\n+            assert num_params < 1000000, (\n+                f\"{model_class} is too big for the common tests ({num_params})! It should have 1M max.\"\n+            )\n \n     @require_flash_attn\n     @require_torch_gpu"
        },
        {
            "sha": "c0eea11012d3676772a29fb2c5b89844a383702f",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -3005,9 +3005,9 @@ def test_load_best_model_with_save(self):\n         )\n         trainer.train()\n         # Check that we have the last known step:\n-        assert os.path.exists(\n-            os.path.join(tmp_dir, f\"checkpoint-{trainer.state.max_steps}\")\n-        ), f\"Could not find checkpoint-{trainer.state.max_steps}\"\n+        assert os.path.exists(os.path.join(tmp_dir, f\"checkpoint-{trainer.state.max_steps}\")), (\n+            f\"Could not find checkpoint-{trainer.state.max_steps}\"\n+        )\n         # And then check the last step\n         assert os.path.exists(os.path.join(tmp_dir, \"checkpoint-9\")), \"Could not find checkpoint-9\"\n "
        },
        {
            "sha": "d43edd19fffce41fb40dd53297bc8dd6f7cdb326",
            "filename": "tests/trainer/test_trainer_seq2seq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_seq2seq.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -180,9 +180,9 @@ def prepare_data(examples):\n         for num_return_sequences in range(3, 0, -1):\n             gen_config.num_return_sequences = num_return_sequences\n             metrics = trainer.evaluate(eval_dataset=prepared_dataset, generation_config=gen_config)\n-            assert (\n-                metrics[\"eval_samples\"] == dataset_len * num_return_sequences\n-            ), f\"Got {metrics['eval_samples']}, expected: {dataset_len * num_return_sequences}\"\n+            assert metrics[\"eval_samples\"] == dataset_len * num_return_sequences, (\n+                f\"Got {metrics['eval_samples']}, expected: {dataset_len * num_return_sequences}\"\n+            )\n \n     @require_torch\n     def test_bad_generation_config_fail_early(self):"
        },
        {
            "sha": "1c4ff3ddc9064df48f3b199d66b85cc1e6468e6f",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -736,7 +736,7 @@ def replace_default_in_arg_description(description: str, default: Any) -> str:\n         elif _re_parse_description.search(description) is None:\n             idx = description.find(OPTIONAL_KEYWORD)\n             len_optional = len(OPTIONAL_KEYWORD)\n-            description = f\"{description[:idx + len_optional]}, defaults to {str_default}\"\n+            description = f\"{description[: idx + len_optional]}, defaults to {str_default}\"\n         else:\n             description = _re_parse_description.sub(rf\"*optional*, defaults to {str_default}\", description)\n "
        },
        {
            "sha": "64cd3752024aaf11e117541ce27fcc24c521b845",
            "filename": "utils/download_glue_data.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fdownload_glue_data.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fdownload_glue_data.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fdownload_glue_data.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -79,9 +79,11 @@ def format_mrpc(data_dir, path_to_data):\n         for row in ids_fh:\n             dev_ids.append(row.strip().split(\"\\t\"))\n \n-    with open(mrpc_train_file, encoding=\"utf8\") as data_fh, open(\n-        os.path.join(mrpc_dir, \"train.tsv\"), \"w\", encoding=\"utf8\"\n-    ) as train_fh, open(os.path.join(mrpc_dir, \"dev.tsv\"), \"w\", encoding=\"utf8\") as dev_fh:\n+    with (\n+        open(mrpc_train_file, encoding=\"utf8\") as data_fh,\n+        open(os.path.join(mrpc_dir, \"train.tsv\"), \"w\", encoding=\"utf8\") as train_fh,\n+        open(os.path.join(mrpc_dir, \"dev.tsv\"), \"w\", encoding=\"utf8\") as dev_fh,\n+    ):\n         header = data_fh.readline()\n         train_fh.write(header)\n         dev_fh.write(header)\n@@ -92,9 +94,10 @@ def format_mrpc(data_dir, path_to_data):\n             else:\n                 train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n \n-    with open(mrpc_test_file, encoding=\"utf8\") as data_fh, open(\n-        os.path.join(mrpc_dir, \"test.tsv\"), \"w\", encoding=\"utf8\"\n-    ) as test_fh:\n+    with (\n+        open(mrpc_test_file, encoding=\"utf8\") as data_fh,\n+        open(os.path.join(mrpc_dir, \"test.tsv\"), \"w\", encoding=\"utf8\") as test_fh,\n+    ):\n         header = data_fh.readline()\n         test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n         for idx, row in enumerate(data_fh):"
        },
        {
            "sha": "155d859bba65dbee0863eae5a5c379adb7193161",
            "filename": "utils/get_github_job_time.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fget_github_job_time.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fget_github_job_time.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_github_job_time.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -68,4 +68,4 @@ def get_job_time(workflow_run_id, token=None):\n     job_time = dict(sorted(job_time.items(), key=lambda item: item[1][\"duration\"], reverse=True))\n \n     for k, v in job_time.items():\n-        print(f'{k}: {v[\"duration\"]}')\n+        print(f\"{k}: {v['duration']}\")"
        },
        {
            "sha": "55e71c4cc91b01ce2451b3fda836b5076fb18d81",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -513,7 +513,7 @@ def forward(...):\n     all_dependencies = set()\n     all_dependencies_with_parent = []\n     checked_dependencies = set(initial_checked_dependencies)\n-    parents = {initial_dep: start_entity for initial_dep in initial_dependencies}\n+    parents = dict.fromkeys(initial_dependencies, start_entity)\n     while len(dependency_queue) > 0:\n         # Pick element to visit\n         current = dependency_queue.popleft()\n@@ -524,7 +524,7 @@ def forward(...):\n             if current in dependency_mapping.keys():\n                 # Update dependency queue\n                 dependency_queue.extend(dependency_mapping[current])\n-                parents.update({dep: current for dep in dependency_mapping[current]})\n+                parents.update(dict.fromkeys(dependency_mapping[current], current))\n             # add visited node to the list\n             checked_dependencies.add(current)\n "
        },
        {
            "sha": "66db34e00c227acbea6a1262493e9810ce3e5117",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -665,7 +665,7 @@ def get_reply_blocks(self, job_name, job_result, failures, device, text):\n \n         failure_text = \"\"\n         for idx, error in enumerate(failures):\n-            new_text = failure_text + f'*{error[\"line\"]}*\\n_{error[\"trace\"]}_\\n\\n'\n+            new_text = failure_text + f\"*{error['line']}*\\n_{error['trace']}_\\n\\n\"\n             if len(new_text) > MAX_ERROR_TEXT:\n                 # `failure_text` here has length <= 3000\n                 failure_text = failure_text + \"[Truncated]\"\n@@ -728,7 +728,7 @@ def get_new_model_failure_blocks(self, with_header=True, to_truncate=True):\n                         if error[\"line\"] in prev_error_lines:\n                             continue\n \n-                        new_text = f'{error[\"line\"]}\\n\\n'\n+                        new_text = f\"{error['line']}\\n\\n\"\n \n                         if new_text not in all_failure_lines:\n                             all_failure_lines[new_text] = []\n@@ -794,7 +794,7 @@ def post_reply(self):\n                         job_result,\n                         failures,\n                         device,\n-                        text=f'Number of failures: {job_result[\"failed\"][device]}',\n+                        text=f\"Number of failures: {job_result['failed'][device]}\",\n                     )\n \n                     print(\"Sending the following reply\")"
        },
        {
            "sha": "f15aa68f907bc07ae8936aac18c9370a9338e7ba",
            "filename": "utils/notification_service_quantization.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fnotification_service_quantization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fnotification_service_quantization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service_quantization.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -152,7 +152,7 @@ def post_reply(self):\n                         job_result,\n                         failures,\n                         device,\n-                        text=f'Number of failures: {job_result[\"failed\"][device]}',\n+                        text=f\"Number of failures: {job_result['failed'][device]}\",\n                     )\n \n                     print(\"Sending the following reply\")\n@@ -203,7 +203,7 @@ def post_reply(self):\n             \"job_link\": {},\n         }\n         for quant in quantization_matrix\n-        if f\"run_quantization_torch_gpu_{ quant }_test_reports\" in available_artifacts\n+        if f\"run_quantization_torch_gpu_{quant}_test_reports\" in available_artifacts\n     }\n \n     github_actions_jobs = get_jobs(\n@@ -220,7 +220,7 @@ def post_reply(self):\n                 break\n \n     for quant in quantization_results.keys():\n-        for artifact_path in available_artifacts[f\"run_quantization_torch_gpu_{ quant }_test_reports\"].paths:\n+        for artifact_path in available_artifacts[f\"run_quantization_torch_gpu_{quant}_test_reports\"].paths:\n             artifact = retrieve_artifact(artifact_path[\"path\"], artifact_path[\"gpu\"])\n             if \"stats\" in artifact:\n                 # Link to the GitHub Action job"
        },
        {
            "sha": "858f7184d707e4158821cb259b6869ac44612333",
            "filename": "utils/past_ci_versions.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fpast_ci_versions.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fpast_ci_versions.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fpast_ci_versions.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -116,8 +116,8 @@\n \n     info = past_versions_testing[args.framework][args.version]\n \n-    os.system(f'echo \"export INSTALL_CMD=\\'{info[\"install\"]}\\'\" >> ~/.profile')\n-    print(f'echo \"export INSTALL_CMD=\\'{info[\"install\"]}\\'\" >> ~/.profile')\n+    os.system(f\"echo \\\"export INSTALL_CMD='{info['install']}'\\\" >> ~/.profile\")\n+    print(f\"echo \\\"export INSTALL_CMD='{info['install']}'\\\" >> ~/.profile\")\n \n     cuda = \"\"\n     if args.framework == \"pytorch\":"
        },
        {
            "sha": "eb61f6d586e536130fef31a0f0764b27afec519f",
            "filename": "utils/process_circleci_workflow_test_reports.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fprocess_circleci_workflow_test_reports.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fprocess_circleci_workflow_test_reports.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fprocess_circleci_workflow_test_reports.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -37,12 +37,12 @@\n     for job in jobs:\n         project_slug = job[\"project_slug\"]\n         if job[\"name\"].startswith((\"tests_\", \"examples_\", \"pipelines_\")):\n-            url = f'https://circleci.com/api/v2/project/{project_slug}/{job[\"job_number\"]}/artifacts'\n+            url = f\"https://circleci.com/api/v2/project/{project_slug}/{job['job_number']}/artifacts\"\n             r = requests.get(url, headers={\"Circle-Token\": os.environ.get(\"CIRCLE_TOKEN\", \"\")})\n             job_artifacts = r.json()[\"items\"]\n \n             os.makedirs(job[\"name\"], exist_ok=True)\n-            os.makedirs(f'outputs/{job[\"name\"]}', exist_ok=True)\n+            os.makedirs(f\"outputs/{job['name']}\", exist_ok=True)\n \n             job_test_summaries = {}\n             for artifact in job_artifacts:\n@@ -67,7 +67,7 @@\n             workflow_summary[job[\"name\"]] = summary\n \n             # collected version\n-            with open(f'outputs/{job[\"name\"]}/test_summary.json', \"w\") as fp:\n+            with open(f\"outputs/{job['name']}/test_summary.json\", \"w\") as fp:\n                 json.dump(summary, fp, indent=4)\n \n     new_workflow_summary = {}"
        },
        {
            "sha": "d8df28c2a3252384220be338747fcb2a4acf5469",
            "filename": "utils/update_metadata.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fupdate_metadata.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6814b4ee8a80fc20479116735d32f3aa3dfeb34/utils%2Fupdate_metadata.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fupdate_metadata.py?ref=c6814b4ee8a80fc20479116735d32f3aa3dfeb34",
            "patch": "@@ -247,7 +247,7 @@ def update_pipeline_and_auto_class_table(table: Dict[str, Tuple[str, str]]) -> D\n                     model_names.extend(list(name))\n \n             # Add pipeline tag and auto model class for those models\n-            table.update({model_name: (pipeline_tag, cls) for model_name in model_names})\n+            table.update(dict.fromkeys(model_names, (pipeline_tag, cls)))\n \n     return table\n "
        }
    ],
    "stats": {
        "total": 1213,
        "additions": 604,
        "deletions": 609
    }
}