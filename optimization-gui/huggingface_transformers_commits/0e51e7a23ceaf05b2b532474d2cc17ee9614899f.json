{
    "author": "Sai-Suraj-27",
    "message": "Fixed failing batch_generation test for `opt` model (#42693)\n\n* Fixed batch_generation test for opt model.\n\n* Removed unused variable.",
    "sha": "0e51e7a23ceaf05b2b532474d2cc17ee9614899f",
    "files": [
        {
            "sha": "d12fbd0812f9eaf23808dcc52821f4246bca2677",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e51e7a23ceaf05b2b532474d2cc17ee9614899f/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e51e7a23ceaf05b2b532474d2cc17ee9614899f/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=0e51e7a23ceaf05b2b532474d2cc17ee9614899f",
            "patch": "@@ -474,22 +474,22 @@ def test_batch_generation(self):\n         outputs = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_new_tokens=10,\n         )\n \n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_new_tokens=10)\n \n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+        output_padded = model.generate(input_ids=inputs_padded, max_new_tokens=10)\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n         padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n \n         expected_output_sentence = [\n-            \"Hello, my dog is a little bit of a dork.\\nI'm a little bit\",\n-            \"Today, I was in the middle of a conversation with a friend about the\",\n+            \"Hello, my dog is a little bit of a dork.\\nI'm a\",\n+            \"Today, I was in the middle of a conversation with a friend\",\n         ]\n         self.assertListEqual(expected_output_sentence, batch_out_sentence)\n         self.assertListEqual(batch_out_sentence, [non_padded_sentence, padded_sentence])"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}