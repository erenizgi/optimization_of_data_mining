{
    "author": "SunMarc",
    "message": "Remove `past_index` (#41384)\n\n* remove-tpu-num-cores\n\n* fix\n\n* rm past index\n\n* Revert \"fix\"\n\nThis reverts commit 7608a6c059210957d3a77812e66178c8b79a9313.\n\n* Revert \"remove-tpu-num-cores\"\n\nThis reverts commit ef08a51d71389849851518d67d8ad6c9ea8f04fc.",
    "sha": "b450d55a91828a0868ad4345b53316e49000c9db",
    "files": [
        {
            "sha": "b7fe4db2164bdc0c2bae39741fc61fa6e55a3869",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/b450d55a91828a0868ad4345b53316e49000c9db/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b450d55a91828a0868ad4345b53316e49000c9db/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=b450d55a91828a0868ad4345b53316e49000c9db",
            "patch": "@@ -2434,10 +2434,6 @@ def _inner_training_loop(\n             if hasattr(epoch_dataloader, \"set_epoch\"):\n                 epoch_dataloader.set_epoch(epoch)\n \n-            # Reset the past mems state at the beginning of each epoch if necessary.\n-            if args.past_index >= 0:\n-                self._past = None\n-\n             steps_in_epoch = (\n                 len(epoch_dataloader)\n                 if len_dataloader is not None\n@@ -2646,10 +2642,6 @@ def _inner_training_loop(\n             if self.control.should_training_stop:\n                 break\n \n-        if args.past_index and hasattr(self, \"_past\"):\n-            # Clean the state at the end of training\n-            delattr(self, \"_past\")\n-\n         logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n         if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n             self._load_best_model()\n@@ -3619,8 +3611,6 @@ def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[s\n                 \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n                 f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\n             )\n-        if self.args.past_index >= 0 and self._past is not None:\n-            inputs[\"mems\"] = self._past\n \n         return inputs\n \n@@ -3878,10 +3868,6 @@ def compute_loss(\n                 kwargs[\"num_items_in_batch\"] = num_items_in_batch\n             inputs = {**inputs, **kwargs}\n         outputs = model(**inputs)\n-        # Save past state if it exists\n-        # TODO: this needs to be fixed and made cleaner later.\n-        if self.args.past_index >= 0:\n-            self._past = outputs[self.args.past_index]\n \n         # User-defined compute_loss function\n         if self.compute_loss_func is not None:\n@@ -4422,9 +4408,6 @@ def evaluation_loop(\n         # Do this before wrapping.\n         eval_dataset = getattr(dataloader, \"dataset\", None)\n \n-        if args.past_index >= 0:\n-            self._past = None\n-\n         # Initialize containers\n         all_losses = EvalLoopContainer(self.args.eval_do_concat_batches, padding_index=-100)\n         all_preds = EvalLoopContainer(self.args.eval_do_concat_batches, padding_index=-100)\n@@ -4509,9 +4492,6 @@ def evaluation_loop(\n \n         # After all calls to `.gather_function`, reset to `gather_for_metrics`:\n         self.gather_function = self.accelerator.gather_for_metrics\n-        if args.past_index and hasattr(self, \"_past\"):\n-            # Clean the state at the end of the evaluation loop\n-            delattr(self, \"_past\")\n \n         # Gather all remaining tensors and put them back on the CPU\n         all_losses = all_losses.get_arrays()\n@@ -4682,9 +4662,6 @@ def prediction_step(\n                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n                     else:\n                         logits = outputs\n-                    # TODO: this needs to be fixed and made cleaner later.\n-                    if self.args.past_index >= 0:\n-                        self._past = outputs[self.args.past_index - 1]\n \n         if prediction_loss_only:\n             return (loss, None, None)"
        },
        {
            "sha": "ccc5d04682d0505a1a2b8d284d028d1bd24ec6f9",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b450d55a91828a0868ad4345b53316e49000c9db/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b450d55a91828a0868ad4345b53316e49000c9db/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=b450d55a91828a0868ad4345b53316e49000c9db",
            "patch": "@@ -418,11 +418,6 @@ class TrainingArguments:\n         dataloader_num_workers (`int`, *optional*, defaults to 0):\n             Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n             main process.\n-        past_index (`int`, *optional*, defaults to -1):\n-            Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n-            the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n-            use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n-            training step under the keyword argument `mems`.\n         run_name (`str`, *optional*, defaults to `output_dir`):\n             A descriptor for the run. Typically used for [trackio](https://github.com/gradio-app/trackio),\n             [wandb](https://www.wandb.com/), [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and\n@@ -1098,10 +1093,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    past_index: int = field(\n-        default=-1,\n-        metadata={\"help\": \"If >=0, uses the corresponding part of the output as the past state for next step.\"},\n-    )\n \n     run_name: Optional[str] = field(\n         default=None,"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 0,
        "deletions": 32
    }
}