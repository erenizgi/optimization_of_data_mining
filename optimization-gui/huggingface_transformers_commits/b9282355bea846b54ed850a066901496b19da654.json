{
    "author": "bozheng-hit",
    "message": "Adding Support for Qwen3-Next (#40771)\n\n* Add Qwen3-Next.\n\n* fix\n\n* style\n\n* doc\n\n* simplify\n\n* fix name\n\n* lazy cache init to allow multi-gpu inference\n\n* simplify\n\n* fix config to support different hybrid ratio.\n\n* remove last commit (redundant)\n\n* tests\n\n* fix test\n\n---------\n\nCo-authored-by: bozheng-hit <dsoul0621@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "b9282355bea846b54ed850a066901496b19da654",
    "files": [
        {
            "sha": "94f443c35bcb80f5b649cfa2a7be96f01811c014",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -657,6 +657,8 @@\n         title: Qwen3\n       - local: model_doc/qwen3_moe\n         title: Qwen3MoE\n+      - local: model_doc/qwen3_next\n+        title: Qwen3Next\n       - local: model_doc/rag\n         title: RAG\n       - local: model_doc/realm"
        },
        {
            "sha": "f2e003182ee795b843e08bd4390b5314db7078c0",
            "filename": "docs/source/en/model_doc/qwen3_next.md",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -0,0 +1,97 @@\n+<!--Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+## Overview\n+\n+The Qwen3-Next series represents our next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency. \n+The series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\n+- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.  \n+- **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers â€” drastically reducing FLOPs per token while preserving model capacity.\n+- **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference.\n+- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.  \n+\n+Built on this architecture, we trained and open-sourced Qwen3-Next-80B-A3B â€” 80B total parameters, only 3B active â€” achieving extreme sparsity and efficiency.\n+\n+Despite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks â€” while requiring **less than 1/10 of the training cost**. \n+Moreover, it delivers over **10x higher inference throughput** than Qwen3-32B when handling contexts longer than 32K tokens.\n+\n+For more details, please visit our blog [Qwen3-Next](qwen3_next) ([blog post](https://qwenlm.github.io/blog/qwen3_next/)).\n+## Usage examples\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n+\n+# load the tokenizer and the model\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    dtype=\"auto\",\n+    device_map=\"auto\"\n+)\n+\n+# prepare the model input\n+prompt = \"Give me a short introduction to large language model.\"\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+text = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=False,\n+    add_generation_prompt=True,\n+)\n+model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n+\n+# conduct text completion\n+generated_ids = model.generate(\n+    **model_inputs,\n+    max_new_tokens=512\n+)\n+output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n+\n+content = tokenizer.decode(output_ids, skip_special_tokens=True)\n+\n+print(\"content:\", content)\n+```\n+\n+## Qwen3NextConfig\n+\n+[[autodoc]] Qwen3NextConfig\n+\n+## Qwen3NextModel\n+\n+[[autodoc]] Qwen3NextModel\n+    - forward\n+\n+## Qwen3NextForCausalLM\n+\n+[[autodoc]] Qwen3NextForCausalLM\n+    - forward\n+\n+## Qwen3NextForSequenceClassification\n+\n+[[autodoc]] Qwen3NextForSequenceClassification\n+    - forward\n+\n+## Qwen3NextForQuestionAnswering\n+\n+[[autodoc]] Qwen3NextForQuestionAnswering\n+    - forward\n+\n+## Qwen3NextForTokenClassification\n+\n+[[autodoc]] Qwen3NextForTokenClassification\n+    - forward"
        },
        {
            "sha": "79ccf866c4a9d34adfcf6a6e02af2c8590187822",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -276,6 +276,7 @@\n     from .qwen2_vl import *\n     from .qwen3 import *\n     from .qwen3_moe import *\n+    from .qwen3_next import *\n     from .rag import *\n     from .recurrent_gemma import *\n     from .reformer import *"
        },
        {
            "sha": "e8dfb7ef5ea060d0937b31f6022eb0aa1df561e0",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -323,6 +323,7 @@\n         (\"qwen2_vl_text\", \"Qwen2VLTextConfig\"),\n         (\"qwen3\", \"Qwen3Config\"),\n         (\"qwen3_moe\", \"Qwen3MoeConfig\"),\n+        (\"qwen3_next\", \"Qwen3NextConfig\"),\n         (\"rag\", \"RagConfig\"),\n         (\"realm\", \"RealmConfig\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaConfig\"),\n@@ -759,6 +760,7 @@\n         (\"qwen2_vl_text\", \"Qwen2VL\"),\n         (\"qwen3\", \"Qwen3\"),\n         (\"qwen3_moe\", \"Qwen3MoE\"),\n+        (\"qwen3_next\", \"Qwen3Next\"),\n         (\"rag\", \"RAG\"),\n         (\"realm\", \"REALM\"),\n         (\"recurrent_gemma\", \"RecurrentGemma\"),"
        },
        {
            "sha": "2fb18514d51dd03949ecf0298080a7f61c1c6788",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -317,6 +317,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen2_vl_text\", \"Qwen2VLTextModel\"),\n         (\"qwen3\", \"Qwen3Model\"),\n         (\"qwen3_moe\", \"Qwen3MoeModel\"),\n+        (\"qwen3_next\", \"Qwen3NextModel\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaModel\"),\n         (\"reformer\", \"ReformerModel\"),\n         (\"regnet\", \"RegNetModel\"),\n@@ -713,6 +714,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen2_moe\", \"Qwen2MoeForCausalLM\"),\n         (\"qwen3\", \"Qwen3ForCausalLM\"),\n         (\"qwen3_moe\", \"Qwen3MoeForCausalLM\"),\n+        (\"qwen3_next\", \"Qwen3NextForCausalLM\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaForCausalLM\"),\n         (\"reformer\", \"ReformerModelWithLMHead\"),\n         (\"rembert\", \"RemBertForCausalLM\"),\n@@ -1263,6 +1265,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen2_moe\", \"Qwen2MoeForSequenceClassification\"),\n         (\"qwen3\", \"Qwen3ForSequenceClassification\"),\n         (\"qwen3_moe\", \"Qwen3MoeForSequenceClassification\"),\n+        (\"qwen3_next\", \"Qwen3NextForSequenceClassification\"),\n         (\"reformer\", \"ReformerForSequenceClassification\"),\n         (\"rembert\", \"RemBertForSequenceClassification\"),\n         (\"roberta\", \"RobertaForSequenceClassification\"),\n@@ -1352,6 +1355,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen2_moe\", \"Qwen2MoeForQuestionAnswering\"),\n         (\"qwen3\", \"Qwen3ForQuestionAnswering\"),\n         (\"qwen3_moe\", \"Qwen3MoeForQuestionAnswering\"),\n+        (\"qwen3_next\", \"Qwen3NextForQuestionAnswering\"),\n         (\"reformer\", \"ReformerForQuestionAnswering\"),\n         (\"rembert\", \"RemBertForQuestionAnswering\"),\n         (\"roberta\", \"RobertaForQuestionAnswering\"),\n@@ -1467,6 +1471,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"qwen2_moe\", \"Qwen2MoeForTokenClassification\"),\n         (\"qwen3\", \"Qwen3ForTokenClassification\"),\n         (\"qwen3_moe\", \"Qwen3MoeForTokenClassification\"),\n+        (\"qwen3_next\", \"Qwen3NextForTokenClassification\"),\n         (\"rembert\", \"RemBertForTokenClassification\"),\n         (\"roberta\", \"RobertaForTokenClassification\"),\n         (\"roberta-prelayernorm\", \"RobertaPreLayerNormForTokenClassification\"),"
        },
        {
            "sha": "592cbe95424810d2236e31b5f58da9467d9e867f",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -575,6 +575,13 @@\n                 \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\n+            \"qwen3_next\",\n+            (\n+                \"Qwen2Tokenizer\",\n+                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\"rag\", (\"RagTokenizer\", None)),\n         (\"realm\", (\"RealmTokenizer\", \"RealmTokenizerFast\" if is_tokenizers_available() else None)),\n         ("
        },
        {
            "sha": "e4f87f71c621d8da2dc9caffd688cf9f1d19b11c",
            "filename": "src/transformers/models/qwen3_next/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2F__init__.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_qwen3_next import *\n+    from .modeling_qwen3_next import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "465cc66d6a64d7c747c4def95bdd01ff7c7cde23",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "added",
            "additions": 270,
            "deletions": 0,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -0,0 +1,270 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Qwen3-Next model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen3NextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Qwen3NextModel`]. It is used to instantiate a\n+    Qwen3-Next model according to the specified arguments, defining the model architecture.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of\n+    Qwen3-Next-80B-A3B-Instruct [Qwen/Qwen3-Next-80B-A3B-Instruct](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 151936):\n+            Vocabulary size of the model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids`.\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 5632):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 48):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 2):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `32`.\n+        hidden_act (`str`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 32768):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        partial_rotary_factor (`float`, *optional*, defaults to 0.25):\n+            Percentage of the query and keys which will have rotary embedding.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        head_dim (`int`, *optional*, defaults to 256):\n+            Projection weights dimension in multi-head attention.\n+        linear_conv_kernel_dim (`int`, *optional*, defaults to 4):\n+            Kernel size of the convolution used in linear attention layers.\n+        linear_key_head_dim (`int`, *optional*, defaults to 128):\n+            Dimension of each key head in linear attention.\n+        linear_value_head_dim (`int`, *optional*, defaults to 128):\n+            Dimension of each value head in linear attention.\n+        linear_num_key_heads (`int`, *optional*, defaults to 16):\n+            Number of key heads used in linear attention layers.\n+        linear_num_value_heads (`int`, *optional*, defaults to 32):\n+            Number of value heads used in linear attention layers.\n+        decoder_sparse_step (`int`, *optional*, defaults to 1):\n+            The frequency of the MoE layer.\n+        moe_intermediate_size (`int`, *optional*, defaults to 512):\n+            Intermediate size of the routed expert.\n+        shared_expert_intermediate_size (`int`, *optional*, defaults to 512):\n+            Intermediate size of the shared expert.\n+        num_experts_per_tok (`int`, *optional*, defaults to 10):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 512):\n+            Number of routed experts.\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the topk probabilities.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n+        mlp_only_layers (`list[int]`, *optional*, defaults to `[]`):\n+            Indicate which layers use Qwen3NextMLP rather than Qwen3NextSparseMoeBlock\n+            The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n+            If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n+        layer_types (`list[str]`, *optional*):\n+            Types of each layer (attention or linear).\n+\n+    ```python\n+    >>> from transformers import Qwen3NextModel, Qwen3NextConfig\n+\n+    >>> # Initializing a Qwen3Next style configuration\n+    >>> configuration =  Qwen3NextConfig()\n+\n+    >>> # Initializing a model from the Qwen3-Next-80B-A3B style configuration\n+    >>> model = Qwen3NextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"qwen3_next\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=151936,\n+        hidden_size=2048,\n+        intermediate_size=5632,\n+        num_hidden_layers=48,\n+        num_attention_heads=16,\n+        num_key_value_heads=2,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=32768,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        partial_rotary_factor=0.25,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        head_dim=256,\n+        linear_conv_kernel_dim=4,\n+        linear_key_head_dim=128,\n+        linear_value_head_dim=128,\n+        linear_num_key_heads=16,\n+        linear_num_value_heads=32,\n+        decoder_sparse_step=1,\n+        moe_intermediate_size=512,\n+        shared_expert_intermediate_size=512,\n+        num_experts_per_tok=10,\n+        num_experts=512,\n+        norm_topk_prob=True,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.001,\n+        mlp_only_layers=[],\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.partial_rotary_factor = partial_rotary_factor\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.head_dim = head_dim\n+        rope_config_validation(self)\n+\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"linear_attention\" if bool((i + 1) % 4) else \"full_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n+        # linear attention part\n+        self.linear_conv_kernel_dim = linear_conv_kernel_dim\n+        self.linear_key_head_dim = linear_key_head_dim\n+        self.linear_value_head_dim = linear_value_head_dim\n+        self.linear_num_key_heads = linear_num_key_heads\n+        self.linear_num_value_heads = linear_num_value_heads\n+\n+        # MoE arguments\n+        self.decoder_sparse_step = decoder_sparse_step\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.shared_expert_intermediate_size = shared_expert_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.norm_topk_prob = norm_topk_prob\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+        self.mlp_only_layers = mlp_only_layers\n+\n+\n+__all__ = [\"Qwen3NextConfig\"]"
        },
        {
            "sha": "55b23945488b80193459b8322a4ae79f36fc92e4",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "added",
            "additions": 1267,
            "deletions": 0,
            "changes": 1267,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -0,0 +1,1267 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen3_next/modular_qwen3_next.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen3_next.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.import_utils import (\n+    is_causal_conv1d_available,\n+    is_flash_linear_attention_available,\n+)\n+from .configuration_qwen3_next import Qwen3NextConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_update, causal_conv1d_fn = None, None\n+\n+if is_flash_linear_attention_available():\n+    from fla.modules import FusedRMSNormGated\n+    from fla.ops.gated_delta_rule import chunk_gated_delta_rule, fused_recurrent_gated_delta_rule\n+else:\n+    chunk_gated_delta_rule, fused_recurrent_gated_delta_rule = None, None\n+    FusedRMSNormGated = None\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen3NextRMSNormGated(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6, **kwargs):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states, gate=None):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        # Norm before gate\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        hidden_states = self.weight * hidden_states.to(input_dtype)\n+        hidden_states = hidden_states * F.silu(gate.to(torch.float32))\n+\n+        return hidden_states.to(input_dtype)\n+\n+\n+class Qwen3NextDynamicCache:\n+    \"\"\"\n+    A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the linear attention\n+    cache (which has a constant shape regardless of seq_len).\n+\n+    This cache has two sets of lists of tensors: `key_cache` and `value_cache` for attention cache and `conv_states`\n+    and `ssm_states` for gated deltanet cache. Each of these lists has `num_layers` tensors. The expected shape for each tensor\n+    For attention layers, `key_cache` and `value_cache` have a shape of `(batch_size, num_heads, seq_len, head_dim)`,\n+    while `conv_states` and `ssm_states` have a shape of `(batch_size, 0)` (empty tensors).\n+    For linear attention layers, `key_cache` and `value_cache` have a shape of `(batch_size, 0)` (empty tensors),\n+    while `conv_states` represents the convolution state and has a shape of `(batch_size, d_inner, d_conv)`,\n+    and `recurrent_states` represents the recurrent state and has a shape of `(batch_size, d_inner, d_state)`.\n+    \"\"\"\n+\n+    is_compileable = False\n+\n+    def __init__(self, config: Qwen3NextConfig):\n+        super().__init__()\n+        self.layer_types = config.layer_types\n+        self.transformer_layers = [\n+            i for i in range(config.num_hidden_layers) if self.layer_types[i] == \"full_attention\"\n+        ]\n+        self.last_linear_layer = len(self.layer_types) - 1 - self.layer_types[::-1].index(\"linear_attention\")\n+\n+        # Initialize everything to None -> will be lazy initialized to allow multi-gpu (device_map) inference\n+        self.conv_states = [None for _ in range(config.num_hidden_layers)]\n+        self.recurrent_states = [None for _ in range(config.num_hidden_layers)]\n+        self.key_cache = [None for _ in range(config.num_hidden_layers)]\n+        self.value_cache = [None for _ in range(config.num_hidden_layers)]\n+\n+    def __len__(self):\n+        return len(self.layer_types)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        if self.key_cache[layer_idx] is None:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            if self.key_cache[layer_idx] is not None:\n+                device = self.key_cache[layer_idx].device\n+                beam_idx = beam_idx.to(device)\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx)\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx)\n+\n+            if self.conv_states[layer_idx] is not None:\n+                device = self.conv_states[layer_idx].device\n+                beam_idx = beam_idx.to(device)\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx)\n+                self.recurrent_states[layer_idx] = self.recurrent_states[layer_idx].index_select(0, beam_idx)\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx] is None:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns for each layer.\n+        \"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length(layer_idx)\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, kv_offset\n+\n+    @property\n+    def has_previous_state(self):\n+        \"\"\"We have a previous state if the last linear (conv) layer was already updated.\"\"\"\n+        return self.conv_states[self.last_linear_layer] is not None\n+\n+\n+class Qwen3NextRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Qwen3NextConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Qwen3NextRMSNorm(nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.zeros(dim))\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        output = self._norm(x.float())\n+        # Llama does x.to(float16) * w whilst Qwen3Next is (x * w).to(float16)\n+        # See https://github.com/huggingface/transformers/pull/29402\n+        output = output * (1.0 + self.weight.float())\n+        return output.type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+# Adapted from transformers.models.glm.modular_glm.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Removes the interleaving of cos and sin from GLM\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    # Keep half or full tensor for later concatenation\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n+\n+    # Concatenate back to full shape\n+    q_embed = torch.cat([q_embed, q_pass], dim=-1)\n+    k_embed = torch.cat([k_embed, k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Qwen3NextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim * 2, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = Qwen3NextRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n+        self.k_norm = Qwen3NextRMSNorm(\n+            self.head_dim, eps=config.rms_norm_eps\n+        )  # thus post q_norm does not need reshape\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states, gate = torch.chunk(\n+            self.q_proj(hidden_states).view(*input_shape, -1, self.head_dim * 2), 2, dim=-1\n+        )\n+        gate = gate.reshape(*input_shape, -1)\n+\n+        query_states = self.q_norm(query_states.view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = attn_output * torch.sigmoid(gate)\n+\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+def apply_mask_to_padding_states(hidden_states, attention_mask):\n+    \"\"\"\n+    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n+        dtype = hidden_states.dtype\n+        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+    return hidden_states\n+\n+\n+is_fast_path_available = all(\n+    (causal_conv1d_fn, causal_conv1d_update, chunk_gated_delta_rule, fused_recurrent_gated_delta_rule)\n+)\n+\n+\n+def torch_causal_conv1d_update(\n+    hidden_states,\n+    conv_state,\n+    weight,\n+    bias=None,\n+    activation=None,\n+):\n+    _, hidden_size, seq_len = hidden_states.shape\n+    state_len = conv_state.shape[-1]\n+\n+    hidden_states_new = torch.cat([conv_state, hidden_states], dim=-1).to(weight.dtype)\n+    conv_state.copy_(hidden_states_new[:, :, -state_len:])\n+    out = F.conv1d(hidden_states_new, weight.unsqueeze(1), bias, padding=0, groups=hidden_size)\n+    out = F.silu(out[:, :, -seq_len:])\n+    out = out.to(hidden_states.dtype)\n+    return out\n+\n+\n+def torch_chunk_gated_delta_rule(\n+    query,\n+    key,\n+    value,\n+    g,\n+    beta,\n+    chunk_size=64,\n+    initial_state=None,\n+    output_final_state=False,\n+    use_qk_l2norm_in_kernel=False,\n+):\n+    initial_dtype = query.dtype\n+    if use_qk_l2norm_in_kernel:\n+        query = F.normalize(query, p=2, dim=-1)\n+        key = F.normalize(key, p=2, dim=-1)\n+    query, key, value, beta, g = [\n+        x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n+    ]\n+\n+    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    v_head_dim = value.shape[-1]\n+    pad_size = (chunk_size - num_heads % chunk_size) % chunk_size\n+    query = F.pad(query, (0, 0, 0, pad_size))\n+    key = F.pad(key, (0, 0, 0, pad_size))\n+    value = F.pad(value, (0, 0, 0, pad_size))\n+    beta = F.pad(beta, (0, pad_size))\n+    g = F.pad(g, (0, pad_size))\n+    tot_heads = num_heads + pad_size\n+    scale = 1 / (query.shape[-1] ** 0.5)\n+    query = query * scale\n+\n+    v_beta = value * beta.unsqueeze(-1)\n+    k_beta = key * beta.unsqueeze(-1)\n+    # reshape to chunks\n+    query, key, value, k_beta, v_beta = [\n+        x.reshape(x.shape[0], x.shape[1], -1, chunk_size, x.shape[-1]) for x in (query, key, value, k_beta, v_beta)\n+    ]\n+    g = g.reshape(g.shape[0], g.shape[1], -1, chunk_size)\n+    mask = torch.triu(torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=query.device), diagonal=0)\n+\n+    # chunk decay\n+    g = g.cumsum(dim=-1)\n+    decay_mask = ((g.unsqueeze(-1) - g.unsqueeze(-2)).tril().exp().float()).tril()\n+    attn = -((k_beta @ key.transpose(-1, -2)) * decay_mask).masked_fill(mask, 0)\n+    for i in range(1, chunk_size):\n+        row = attn[..., i, :i].clone()\n+        sub = attn[..., :i, :i].clone()\n+        attn[..., i, :i] = row + (row.unsqueeze(-1) * sub).sum(-2)\n+    attn = attn + torch.eye(chunk_size, dtype=attn.dtype, device=attn.device)\n+    value = attn @ v_beta\n+    k_cumdecay = attn @ (k_beta * g.exp().unsqueeze(-1))\n+    last_recurrent_state = (\n+        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        if initial_state is None\n+        else initial_state.to(value)\n+    )\n+    core_attn_out = torch.zeros_like(value)\n+    mask = torch.triu(torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=query.device), diagonal=1)\n+\n+    # for each chunk\n+    for i in range(0, tot_heads // chunk_size):\n+        q_i, k_i, v_i = query[:, :, i], key[:, :, i], value[:, :, i]\n+        attn = (q_i @ k_i.transpose(-1, -2) * decay_mask[:, :, i]).masked_fill_(mask, 0)\n+        v_prime = (k_cumdecay[:, :, i]) @ last_recurrent_state\n+        v_new = v_i - v_prime\n+        attn_inter = (q_i * g[:, :, i, :, None].exp()) @ last_recurrent_state\n+        core_attn_out[:, :, i] = attn_inter + attn @ v_new\n+        last_recurrent_state = (\n+            last_recurrent_state * g[:, :, i, -1, None, None].exp()\n+            + (k_i * (g[:, :, i, -1, None] - g[:, :, i]).exp()[..., None]).transpose(-1, -2) @ v_new\n+        )\n+\n+    if not output_final_state:\n+        last_recurrent_state = None\n+    core_attn_out = core_attn_out.reshape(core_attn_out.shape[0], core_attn_out.shape[1], -1, core_attn_out.shape[-1])\n+    core_attn_out = core_attn_out[:, :, :num_heads]\n+    core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype)\n+    return core_attn_out, last_recurrent_state\n+\n+\n+def torch_recurrent_gated_delta_rule(\n+    query, key, value, g, beta, initial_state, output_final_state, use_qk_l2norm_in_kernel=False\n+):\n+    initial_dtype = query.dtype\n+    if use_qk_l2norm_in_kernel:\n+        query = F.normalize(query, p=2, dim=-1)\n+        key = F.normalize(key, p=2, dim=-1)\n+    query, key, value, beta, g = [\n+        x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n+    ]\n+\n+    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    v_head_dim = value.shape[-1]\n+    scale = 1 / (query.shape[-1] ** 0.5)\n+    query = query * scale\n+\n+    core_attn_out = torch.zeros(batch_size, sequence_length, num_heads, v_head_dim).to(value)\n+    last_recurrent_state = (\n+        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        if initial_state is None\n+        else initial_state.to(value)\n+    )\n+\n+    for i in range(num_heads):\n+        q_t = query[:, :, i]\n+        k_t = key[:, :, i]\n+        v_t = value[:, :, i]\n+        g_t = g[:, :, i].exp().unsqueeze(-1).unsqueeze(-1)\n+        beta_t = beta[:, :, i].unsqueeze(-1)\n+\n+        last_recurrent_state = last_recurrent_state * g_t\n+        kv_mem = (last_recurrent_state * k_t.unsqueeze(-1)).sum(dim=-2)\n+        delta = (v_t - kv_mem) * beta_t\n+        last_recurrent_state = last_recurrent_state + k_t.unsqueeze(-1) * delta.unsqueeze(-2)\n+        core_attn_out[:, :, i] = (last_recurrent_state * q_t.unsqueeze(-1)).sum(dim=-2)\n+\n+    if not output_final_state:\n+        last_recurrent_state = None\n+    core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype)\n+    return core_attn_out, last_recurrent_state\n+\n+\n+class Qwen3NextGatedDeltaNet(nn.Module):\n+    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.num_v_heads = config.linear_num_value_heads\n+        self.num_k_heads = config.linear_num_key_heads\n+        self.head_k_dim = config.linear_key_head_dim\n+        self.head_v_dim = config.linear_value_head_dim\n+        self.key_dim = self.head_k_dim * self.num_k_heads\n+        self.value_dim = self.head_v_dim * self.num_v_heads\n+\n+        self.conv_kernel_size = config.linear_conv_kernel_dim\n+        self.layer_idx = layer_idx\n+        self.activation = config.hidden_act\n+        self.act = ACT2FN[config.hidden_act]\n+        self.layer_norm_epsilon = config.rms_norm_eps\n+\n+        # QKV\n+        self.conv_dim = self.key_dim * 2 + self.value_dim\n+        self.conv1d = nn.Conv1d(\n+            in_channels=self.conv_dim,\n+            out_channels=self.conv_dim,\n+            bias=False,\n+            kernel_size=self.conv_kernel_size,\n+            groups=self.conv_dim,\n+            padding=self.conv_kernel_size - 1,\n+        )\n+\n+        # projection of the input hidden states\n+        projection_size_qkvz = self.key_dim * 2 + self.value_dim * 2\n+        projection_size_ba = self.num_v_heads * 2\n+        self.in_proj_qkvz = nn.Linear(self.hidden_size, projection_size_qkvz, bias=False)\n+        self.in_proj_ba = nn.Linear(self.hidden_size, projection_size_ba, bias=False)\n+\n+        # time step projection (discretization)\n+        # instantiate once and copy inv_dt in init_weights of PretrainedModel\n+        self.dt_bias = nn.Parameter(torch.ones(self.num_v_heads))\n+\n+        A = torch.empty(self.num_v_heads).uniform_(0, 16)\n+        self.A_log = nn.Parameter(torch.log(A))\n+\n+        self.norm = (\n+            Qwen3NextRMSNormGated(self.head_v_dim, eps=self.layer_norm_epsilon)\n+            if FusedRMSNormGated is None\n+            else FusedRMSNormGated(\n+                self.head_v_dim,\n+                eps=self.layer_norm_epsilon,\n+                activation=self.activation,\n+                device=torch.cuda.current_device(),\n+                dtype=config.dtype if config.dtype is not None else torch.get_current_dtype(),\n+            )\n+        )\n+\n+        self.out_proj = nn.Linear(self.value_dim, self.hidden_size, bias=False)\n+\n+        self.causal_conv1d_fn = causal_conv1d_fn\n+        self.causal_conv1d_update = causal_conv1d_update or torch_causal_conv1d_update\n+        self.chunk_gated_delta_rule = chunk_gated_delta_rule or torch_chunk_gated_delta_rule\n+        self.recurrent_gated_delta_rule = fused_recurrent_gated_delta_rule or torch_recurrent_gated_delta_rule\n+\n+        if not is_fast_path_available:\n+            logger.warning_once(\n+                \"The fast path is not available because one of the required library is not installed. Falling back to \"\n+                \"torch implementation. To install follow https://github.com/fla-org/flash-linear-attention#installation and\"\n+                \" https://github.com/Dao-AILab/causal-conv1d\"\n+            )\n+\n+    def fix_query_key_value_ordering(self, mixed_qkvz, mixed_ba):\n+        \"\"\"\n+        Derives `query`, `key` and `value` tensors from `mixed_qkvz` and `mixed_ba`.\n+        \"\"\"\n+\n+        new_tensor_shape_qkvz = mixed_qkvz.size()[:-1] + (\n+            self.num_k_heads,\n+            2 * self.head_k_dim + 2 * self.head_v_dim * self.num_v_heads // self.num_k_heads,\n+        )\n+        new_tensor_shape_ba = mixed_ba.size()[:-1] + (self.num_k_heads, 2 * self.num_v_heads // self.num_k_heads)\n+\n+        mixed_qkvz = mixed_qkvz.view(*new_tensor_shape_qkvz)\n+        mixed_ba = mixed_ba.view(*new_tensor_shape_ba)\n+        split_arg_list_qkvz = [\n+            self.head_k_dim,\n+            self.head_k_dim,\n+            (self.num_v_heads // self.num_k_heads * self.head_v_dim),\n+            (self.num_v_heads // self.num_k_heads * self.head_v_dim),\n+        ]\n+        split_arg_list_ba = [self.num_v_heads // self.num_k_heads, self.num_v_heads // self.num_k_heads]\n+        query, key, value, z = torch.split(mixed_qkvz, split_arg_list_qkvz, dim=3)\n+        b, a = torch.split(mixed_ba, split_arg_list_ba, dim=3)\n+        # [b, sq, ng, np/ng * hn] -> [b, sq, np, hn]\n+        value = value.reshape(value.size(0), value.size(1), -1, self.head_v_dim)\n+        z = z.reshape(z.size(0), z.size(1), -1, self.head_v_dim)\n+        b = b.reshape(b.size(0), b.size(1), self.num_v_heads)\n+        a = a.reshape(a.size(0), a.size(1), self.num_v_heads)\n+        return query, key, value, z, b, a\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cache_params: Optional[Qwen3NextDynamicCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n+\n+        # Set up dimensions for reshapes later\n+        batch_size, seq_len, _ = hidden_states.shape\n+\n+        use_precomputed_states = (\n+            cache_params is not None\n+            and cache_params.has_previous_state\n+            and seq_len == 1\n+            and cache_position is not None\n+        )\n+\n+        # getting projected states from cache if it exists\n+        if cache_params is not None:\n+            conv_state = cache_params.conv_states[self.layer_idx]\n+            recurrent_state = cache_params.recurrent_states[self.layer_idx]\n+\n+        projected_states_qkvz = self.in_proj_qkvz(hidden_states)\n+        projected_states_ba = self.in_proj_ba(hidden_states)\n+        query, key, value, z, b, a = self.fix_query_key_value_ordering(projected_states_qkvz, projected_states_ba)\n+        query, key, value = (x.reshape(x.shape[0], x.shape[1], -1) for x in (query, key, value))\n+\n+        mixed_qkv = torch.cat((query, key, value), dim=-1)\n+        mixed_qkv = mixed_qkv.transpose(1, 2)\n+\n+        if use_precomputed_states:\n+            # 2. Convolution sequence transformation\n+            # NOTE: the conv state is updated in `causal_conv1d_update`\n+            mixed_qkv = self.causal_conv1d_update(\n+                mixed_qkv,\n+                conv_state,\n+                self.conv1d.weight.squeeze(1),\n+                self.conv1d.bias,\n+                self.activation,\n+            )\n+        else:\n+            if cache_params is not None:\n+                conv_state = F.pad(mixed_qkv, (self.conv_kernel_size - mixed_qkv.shape[-1], 0))\n+                cache_params.conv_states[self.layer_idx] = conv_state\n+            if self.causal_conv1d_fn is not None:\n+                mixed_qkv = self.causal_conv1d_fn(\n+                    x=mixed_qkv,\n+                    weight=self.conv1d.weight.squeeze(1),\n+                    bias=self.conv1d.bias,\n+                    activation=self.activation,\n+                    seq_idx=None,\n+                )\n+            else:\n+                mixed_qkv = F.silu(self.conv1d(mixed_qkv)[:, :, :seq_len])\n+\n+        mixed_qkv = mixed_qkv.transpose(1, 2)\n+        query, key, value = torch.split(\n+            mixed_qkv,\n+            [\n+                self.key_dim,\n+                self.key_dim,\n+                self.value_dim,\n+            ],\n+            dim=-1,\n+        )\n+        query = query.reshape(query.shape[0], query.shape[1], -1, self.head_k_dim)\n+        key = key.reshape(key.shape[0], key.shape[1], -1, self.head_k_dim)\n+        value = value.reshape(value.shape[0], value.shape[1], -1, self.head_v_dim)\n+\n+        beta = b.sigmoid()\n+        # If the model is loaded in fp16, without the .float() here, A might be -inf\n+        g = -self.A_log.float().exp() * F.softplus(a.float() + self.dt_bias)\n+        if self.num_v_heads // self.num_k_heads > 1:\n+            query = query.repeat_interleave(self.num_v_heads // self.num_k_heads, dim=2)\n+            key = key.repeat_interleave(self.num_v_heads // self.num_k_heads, dim=2)\n+\n+        if not use_precomputed_states:\n+            core_attn_out, last_recurrent_state = self.chunk_gated_delta_rule(\n+                query,\n+                key,\n+                value,\n+                g=g,\n+                beta=beta,\n+                initial_state=None,\n+                output_final_state=cache_params is not None,\n+                use_qk_l2norm_in_kernel=True,\n+            )\n+\n+        else:\n+            core_attn_out, last_recurrent_state = self.recurrent_gated_delta_rule(\n+                query,\n+                key,\n+                value,\n+                g=g,\n+                beta=beta,\n+                initial_state=recurrent_state,\n+                output_final_state=cache_params is not None,\n+                use_qk_l2norm_in_kernel=True,\n+            )\n+\n+        # Update cache\n+        if cache_params is not None:\n+            cache_params.recurrent_states[self.layer_idx] = last_recurrent_state\n+\n+        z_shape_og = z.shape\n+        # reshape input data into 2D tensor\n+        core_attn_out = core_attn_out.reshape(-1, core_attn_out.shape[-1])\n+        z = z.reshape(-1, z.shape[-1])\n+        core_attn_out = self.norm(core_attn_out, z)\n+        core_attn_out = core_attn_out.reshape(z_shape_og)\n+        core_attn_out = core_attn_out.reshape(core_attn_out.shape[0], core_attn_out.shape[1], -1)\n+\n+        output = self.out_proj(core_attn_out)\n+        return output\n+\n+\n+class Qwen3NextMLP(nn.Module):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Qwen3NextSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+        # gating\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = nn.ModuleList(\n+            [Qwen3NextMLP(config, intermediate_size=config.moe_intermediate_size) for _ in range(self.num_experts)]\n+        )\n+\n+        self.shared_expert = Qwen3NextMLP(config, intermediate_size=config.shared_expert_intermediate_size)\n+        self.shared_expert_gate = torch.nn.Linear(config.hidden_size, 1, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        \"\"\" \"\"\"\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits = self.gate(hidden_states)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        if self.norm_topk_prob:\n+            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        # we cast back to the input dtype\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+\n+        shared_expert_output = self.shared_expert(hidden_states)\n+        shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states)) * shared_expert_output\n+\n+        final_hidden_states = final_hidden_states + shared_expert_output\n+\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states, router_logits\n+\n+\n+class Qwen3NextDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        # token mixer\n+        self.layer_type = config.layer_types[layer_idx]\n+        if self.layer_type == \"linear_attention\":\n+            self.linear_attn = Qwen3NextGatedDeltaNet(config, layer_idx)\n+        elif self.layer_type == \"full_attention\":\n+            self.self_attn = Qwen3NextAttention(config, layer_idx)\n+\n+        if (layer_idx not in config.mlp_only_layers) and (\n+            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n+        ):\n+            self.mlp = Qwen3NextSparseMoeBlock(config)\n+        else:\n+            self.mlp = Qwen3NextMLP(config, intermediate_size=config.intermediate_size)\n+\n+        self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n+                `(batch, sequence_length)` where padding elements are indicated by 0.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_router_logits (`bool`, *optional*):\n+                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n+                and should not be returned during inference.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Token Mixer\n+        if self.layer_type == \"linear_attention\":\n+            hidden_states = self.linear_attn(\n+                hidden_states=hidden_states,\n+                cache_params=past_key_values,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        elif self.layer_type == \"full_attention\":\n+            # Self Attention\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        # For the MoE layers, we need to unpack\n+        if isinstance(hidden_states, tuple):\n+            hidden_states, _ = hidden_states\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+class Qwen3NextPreTrainedModel(PreTrainedModel):\n+    config: Qwen3NextConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Qwen3NextDecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _keys_to_ignore_on_load_unexpected = [r\"^mtp.*\"]\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(Qwen3NextSparseMoeBlock, index=1),\n+        \"hidden_states\": Qwen3NextDecoderLayer,\n+        \"attentions\": Qwen3NextAttention,\n+    }\n+    _is_stateful = True\n+\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Qwen3NextGatedDeltaNet):\n+            module.dt_bias.data.fill_(1.0)\n+            module.A_log.data.uniform_(0, 16).log_()\n+\n+\n+class Qwen3NextModel(Qwen3NextPreTrainedModel):\n+    def __init__(self, config: Qwen3NextConfig):\n+        super().__init__(config)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n+        self.layers = nn.ModuleList(\n+            [Qwen3NextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen3NextRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = Qwen3NextDynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+        linear_attn_mask = self._update_linear_attn_mask(attention_mask, cache_position)\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = linear_attn_mask if decoder_layer.layer_type == \"linear_attention\" else causal_mask\n+\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=layer_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+    def _update_linear_attn_mask(self, attention_mask, cache_position):\n+        \"\"\"\n+        NOTE: Left-padding is used for linear attention mask.\n+        No need for zeroing states when\n+            1. Cached forward\n+            2. Attending to all inputs\n+        \"\"\"\n+        linear_attn_mask = attention_mask\n+        if cache_position[0] > 0 or (attention_mask is not None and torch.all(attention_mask == 1)):\n+            linear_attn_mask = None\n+        return linear_attn_mask\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n+@auto_docstring\n+class Qwen3NextForCausalLM(Qwen3NextPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen3NextModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.num_experts\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Qwen3NextDynamicCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Qwen3NextForCausalLM\n+\n+        >>> model = Qwen3NextForCausalLM.from_pretrained(\"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: MoeModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_router_logits=output_router_logits,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+class Qwen3NextForSequenceClassification(GenericForSequenceClassification, Qwen3NextPreTrainedModel):\n+    pass\n+\n+\n+class Qwen3NextForTokenClassification(GenericForTokenClassification, Qwen3NextPreTrainedModel):\n+    pass\n+\n+\n+class Qwen3NextForQuestionAnswering(GenericForQuestionAnswering, Qwen3NextPreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n+\n+\n+__all__ = [\n+    \"Qwen3NextForCausalLM\",\n+    \"Qwen3NextForQuestionAnswering\",\n+    \"Qwen3NextModel\",\n+    \"Qwen3NextPreTrainedModel\",\n+    \"Qwen3NextForSequenceClassification\",\n+    \"Qwen3NextForTokenClassification\",\n+]"
        },
        {
            "sha": "b4c179c50d721da5499cc4e569909506d583170d",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "added",
            "additions": 876,
            "deletions": 0,
            "changes": 876,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -0,0 +1,876 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Qwen3-Next model.\"\"\"\n+\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.import_utils import (\n+    is_causal_conv1d_available,\n+    is_flash_linear_attention_available,\n+)\n+from ..bamba.modeling_bamba import apply_mask_to_padding_states, apply_rotary_pos_emb\n+from ..gemma3.modeling_gemma3 import Gemma3RMSNorm\n+from ..llama.modeling_llama import (\n+    LlamaForQuestionAnswering,\n+    LlamaForSequenceClassification,\n+    LlamaForTokenClassification,\n+)\n+from ..mixtral.modeling_mixtral import MixtralForCausalLM\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock\n+from ..qwen3_moe.modeling_qwen3_moe import (\n+    Qwen3MoeAttention,\n+    Qwen3MoeDecoderLayer,\n+    Qwen3MoeMLP,\n+    Qwen3MoeRotaryEmbedding,\n+    eager_attention_forward,\n+)\n+from .configuration_qwen3_next import Qwen3NextConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_update, causal_conv1d_fn = None, None\n+\n+if is_flash_linear_attention_available():\n+    from fla.modules import FusedRMSNormGated\n+    from fla.ops.gated_delta_rule import chunk_gated_delta_rule, fused_recurrent_gated_delta_rule\n+else:\n+    chunk_gated_delta_rule, fused_recurrent_gated_delta_rule = None, None\n+    FusedRMSNormGated = None\n+\n+\n+is_fast_path_available = all(\n+    (causal_conv1d_fn, causal_conv1d_update, chunk_gated_delta_rule, fused_recurrent_gated_delta_rule)\n+)\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen3NextRMSNormGated(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6, **kwargs):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states, gate=None):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        # Norm before gate\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        hidden_states = self.weight * hidden_states.to(input_dtype)\n+        hidden_states = hidden_states * F.silu(gate.to(torch.float32))\n+\n+        return hidden_states.to(input_dtype)\n+\n+\n+class Qwen3NextDynamicCache:\n+    \"\"\"\n+    A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the linear attention\n+    cache (which has a constant shape regardless of seq_len).\n+\n+    This cache has two sets of lists of tensors: `key_cache` and `value_cache` for attention cache and `conv_states`\n+    and `ssm_states` for gated deltanet cache. Each of these lists has `num_layers` tensors. The expected shape for each tensor\n+    For attention layers, `key_cache` and `value_cache` have a shape of `(batch_size, num_heads, seq_len, head_dim)`,\n+    while `conv_states` and `ssm_states` have a shape of `(batch_size, 0)` (empty tensors).\n+    For linear attention layers, `key_cache` and `value_cache` have a shape of `(batch_size, 0)` (empty tensors),\n+    while `conv_states` represents the convolution state and has a shape of `(batch_size, d_inner, d_conv)`,\n+    and `recurrent_states` represents the recurrent state and has a shape of `(batch_size, d_inner, d_state)`.\n+    \"\"\"\n+\n+    is_compileable = False\n+\n+    def __init__(self, config: Qwen3NextConfig):\n+        super().__init__()\n+        self.layer_types = config.layer_types\n+        self.transformer_layers = [\n+            i for i in range(config.num_hidden_layers) if self.layer_types[i] == \"full_attention\"\n+        ]\n+        self.last_linear_layer = len(self.layer_types) - 1 - self.layer_types[::-1].index(\"linear_attention\")\n+\n+        # Initialize everything to None -> will be lazy initialized to allow multi-gpu (device_map) inference\n+        self.conv_states = [None for _ in range(config.num_hidden_layers)]\n+        self.recurrent_states = [None for _ in range(config.num_hidden_layers)]\n+        self.key_cache = [None for _ in range(config.num_hidden_layers)]\n+        self.value_cache = [None for _ in range(config.num_hidden_layers)]\n+\n+    def __len__(self):\n+        return len(self.layer_types)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        if self.key_cache[layer_idx] is None:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            if self.key_cache[layer_idx] is not None:\n+                device = self.key_cache[layer_idx].device\n+                beam_idx = beam_idx.to(device)\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx)\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx)\n+\n+            if self.conv_states[layer_idx] is not None:\n+                device = self.conv_states[layer_idx].device\n+                beam_idx = beam_idx.to(device)\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx)\n+                self.recurrent_states[layer_idx] = self.recurrent_states[layer_idx].index_select(0, beam_idx)\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx] is None:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns for each layer.\n+        \"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length(layer_idx)\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, kv_offset\n+\n+    @property\n+    def has_previous_state(self):\n+        \"\"\"We have a previous state if the last linear (conv) layer was already updated.\"\"\"\n+        return self.conv_states[self.last_linear_layer] is not None\n+\n+\n+class Qwen3NextRotaryEmbedding(Qwen3MoeRotaryEmbedding):\n+    pass\n+\n+\n+class Qwen3NextRMSNorm(Gemma3RMSNorm):\n+    pass\n+\n+\n+class Qwen3NextAttention(Qwen3MoeAttention):\n+    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim * 2, bias=config.attention_bias\n+        )\n+        del self.sliding_window\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states, gate = torch.chunk(\n+            self.q_proj(hidden_states).view(*input_shape, -1, self.head_dim * 2), 2, dim=-1\n+        )\n+        gate = gate.reshape(*input_shape, -1)\n+\n+        query_states = self.q_norm(query_states.view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = attn_output * torch.sigmoid(gate)\n+\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+def torch_causal_conv1d_update(\n+    hidden_states,\n+    conv_state,\n+    weight,\n+    bias=None,\n+    activation=None,\n+):\n+    _, hidden_size, seq_len = hidden_states.shape\n+    state_len = conv_state.shape[-1]\n+\n+    hidden_states_new = torch.cat([conv_state, hidden_states], dim=-1).to(weight.dtype)\n+    conv_state.copy_(hidden_states_new[:, :, -state_len:])\n+    out = F.conv1d(hidden_states_new, weight.unsqueeze(1), bias, padding=0, groups=hidden_size)\n+    out = F.silu(out[:, :, -seq_len:])\n+    out = out.to(hidden_states.dtype)\n+    return out\n+\n+\n+def torch_chunk_gated_delta_rule(\n+    query,\n+    key,\n+    value,\n+    g,\n+    beta,\n+    chunk_size=64,\n+    initial_state=None,\n+    output_final_state=False,\n+    use_qk_l2norm_in_kernel=False,\n+):\n+    initial_dtype = query.dtype\n+    if use_qk_l2norm_in_kernel:\n+        query = F.normalize(query, p=2, dim=-1)\n+        key = F.normalize(key, p=2, dim=-1)\n+    query, key, value, beta, g = [\n+        x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n+    ]\n+\n+    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    v_head_dim = value.shape[-1]\n+    pad_size = (chunk_size - num_heads % chunk_size) % chunk_size\n+    query = F.pad(query, (0, 0, 0, pad_size))\n+    key = F.pad(key, (0, 0, 0, pad_size))\n+    value = F.pad(value, (0, 0, 0, pad_size))\n+    beta = F.pad(beta, (0, pad_size))\n+    g = F.pad(g, (0, pad_size))\n+    tot_heads = num_heads + pad_size\n+    scale = 1 / (query.shape[-1] ** 0.5)\n+    query = query * scale\n+\n+    v_beta = value * beta.unsqueeze(-1)\n+    k_beta = key * beta.unsqueeze(-1)\n+    # reshape to chunks\n+    query, key, value, k_beta, v_beta = [\n+        x.reshape(x.shape[0], x.shape[1], -1, chunk_size, x.shape[-1]) for x in (query, key, value, k_beta, v_beta)\n+    ]\n+    g = g.reshape(g.shape[0], g.shape[1], -1, chunk_size)\n+    mask = torch.triu(torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=query.device), diagonal=0)\n+\n+    # chunk decay\n+    g = g.cumsum(dim=-1)\n+    decay_mask = ((g.unsqueeze(-1) - g.unsqueeze(-2)).tril().exp().float()).tril()\n+    attn = -((k_beta @ key.transpose(-1, -2)) * decay_mask).masked_fill(mask, 0)\n+    for i in range(1, chunk_size):\n+        row = attn[..., i, :i].clone()\n+        sub = attn[..., :i, :i].clone()\n+        attn[..., i, :i] = row + (row.unsqueeze(-1) * sub).sum(-2)\n+    attn = attn + torch.eye(chunk_size, dtype=attn.dtype, device=attn.device)\n+    value = attn @ v_beta\n+    k_cumdecay = attn @ (k_beta * g.exp().unsqueeze(-1))\n+    last_recurrent_state = (\n+        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        if initial_state is None\n+        else initial_state.to(value)\n+    )\n+    core_attn_out = torch.zeros_like(value)\n+    mask = torch.triu(torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=query.device), diagonal=1)\n+\n+    # for each chunk\n+    for i in range(0, tot_heads // chunk_size):\n+        q_i, k_i, v_i = query[:, :, i], key[:, :, i], value[:, :, i]\n+        attn = (q_i @ k_i.transpose(-1, -2) * decay_mask[:, :, i]).masked_fill_(mask, 0)\n+        v_prime = (k_cumdecay[:, :, i]) @ last_recurrent_state\n+        v_new = v_i - v_prime\n+        attn_inter = (q_i * g[:, :, i, :, None].exp()) @ last_recurrent_state\n+        core_attn_out[:, :, i] = attn_inter + attn @ v_new\n+        last_recurrent_state = (\n+            last_recurrent_state * g[:, :, i, -1, None, None].exp()\n+            + (k_i * (g[:, :, i, -1, None] - g[:, :, i]).exp()[..., None]).transpose(-1, -2) @ v_new\n+        )\n+\n+    if not output_final_state:\n+        last_recurrent_state = None\n+    core_attn_out = core_attn_out.reshape(core_attn_out.shape[0], core_attn_out.shape[1], -1, core_attn_out.shape[-1])\n+    core_attn_out = core_attn_out[:, :, :num_heads]\n+    core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype)\n+    return core_attn_out, last_recurrent_state\n+\n+\n+def torch_recurrent_gated_delta_rule(\n+    query, key, value, g, beta, initial_state, output_final_state, use_qk_l2norm_in_kernel=False\n+):\n+    initial_dtype = query.dtype\n+    if use_qk_l2norm_in_kernel:\n+        query = F.normalize(query, p=2, dim=-1)\n+        key = F.normalize(key, p=2, dim=-1)\n+    query, key, value, beta, g = [\n+        x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n+    ]\n+\n+    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    v_head_dim = value.shape[-1]\n+    scale = 1 / (query.shape[-1] ** 0.5)\n+    query = query * scale\n+\n+    core_attn_out = torch.zeros(batch_size, sequence_length, num_heads, v_head_dim).to(value)\n+    last_recurrent_state = (\n+        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        if initial_state is None\n+        else initial_state.to(value)\n+    )\n+\n+    for i in range(num_heads):\n+        q_t = query[:, :, i]\n+        k_t = key[:, :, i]\n+        v_t = value[:, :, i]\n+        g_t = g[:, :, i].exp().unsqueeze(-1).unsqueeze(-1)\n+        beta_t = beta[:, :, i].unsqueeze(-1)\n+\n+        last_recurrent_state = last_recurrent_state * g_t\n+        kv_mem = (last_recurrent_state * k_t.unsqueeze(-1)).sum(dim=-2)\n+        delta = (v_t - kv_mem) * beta_t\n+        last_recurrent_state = last_recurrent_state + k_t.unsqueeze(-1) * delta.unsqueeze(-2)\n+        core_attn_out[:, :, i] = (last_recurrent_state * q_t.unsqueeze(-1)).sum(dim=-2)\n+\n+    if not output_final_state:\n+        last_recurrent_state = None\n+    core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype)\n+    return core_attn_out, last_recurrent_state\n+\n+\n+class Qwen3NextGatedDeltaNet(nn.Module):\n+    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.num_v_heads = config.linear_num_value_heads\n+        self.num_k_heads = config.linear_num_key_heads\n+        self.head_k_dim = config.linear_key_head_dim\n+        self.head_v_dim = config.linear_value_head_dim\n+        self.key_dim = self.head_k_dim * self.num_k_heads\n+        self.value_dim = self.head_v_dim * self.num_v_heads\n+\n+        self.conv_kernel_size = config.linear_conv_kernel_dim\n+        self.layer_idx = layer_idx\n+        self.activation = config.hidden_act\n+        self.act = ACT2FN[config.hidden_act]\n+        self.layer_norm_epsilon = config.rms_norm_eps\n+\n+        # QKV\n+        self.conv_dim = self.key_dim * 2 + self.value_dim\n+        self.conv1d = nn.Conv1d(\n+            in_channels=self.conv_dim,\n+            out_channels=self.conv_dim,\n+            bias=False,\n+            kernel_size=self.conv_kernel_size,\n+            groups=self.conv_dim,\n+            padding=self.conv_kernel_size - 1,\n+        )\n+\n+        # projection of the input hidden states\n+        projection_size_qkvz = self.key_dim * 2 + self.value_dim * 2\n+        projection_size_ba = self.num_v_heads * 2\n+        self.in_proj_qkvz = nn.Linear(self.hidden_size, projection_size_qkvz, bias=False)\n+        self.in_proj_ba = nn.Linear(self.hidden_size, projection_size_ba, bias=False)\n+\n+        # time step projection (discretization)\n+        # instantiate once and copy inv_dt in init_weights of PretrainedModel\n+        self.dt_bias = nn.Parameter(torch.ones(self.num_v_heads))\n+\n+        A = torch.empty(self.num_v_heads).uniform_(0, 16)\n+        self.A_log = nn.Parameter(torch.log(A))\n+\n+        self.norm = (\n+            Qwen3NextRMSNormGated(self.head_v_dim, eps=self.layer_norm_epsilon)\n+            if FusedRMSNormGated is None\n+            else FusedRMSNormGated(\n+                self.head_v_dim,\n+                eps=self.layer_norm_epsilon,\n+                activation=self.activation,\n+                device=torch.cuda.current_device(),\n+                dtype=config.dtype if config.dtype is not None else torch.get_current_dtype(),\n+            )\n+        )\n+\n+        self.out_proj = nn.Linear(self.value_dim, self.hidden_size, bias=False)\n+\n+        self.causal_conv1d_fn = causal_conv1d_fn\n+        self.causal_conv1d_update = causal_conv1d_update or torch_causal_conv1d_update\n+        self.chunk_gated_delta_rule = chunk_gated_delta_rule or torch_chunk_gated_delta_rule\n+        self.recurrent_gated_delta_rule = fused_recurrent_gated_delta_rule or torch_recurrent_gated_delta_rule\n+\n+        if not is_fast_path_available:\n+            logger.warning_once(\n+                \"The fast path is not available because one of the required library is not installed. Falling back to \"\n+                \"torch implementation. To install follow https://github.com/fla-org/flash-linear-attention#installation and\"\n+                \" https://github.com/Dao-AILab/causal-conv1d\"\n+            )\n+\n+    def fix_query_key_value_ordering(self, mixed_qkvz, mixed_ba):\n+        \"\"\"\n+        Derives `query`, `key` and `value` tensors from `mixed_qkvz` and `mixed_ba`.\n+        \"\"\"\n+\n+        new_tensor_shape_qkvz = mixed_qkvz.size()[:-1] + (\n+            self.num_k_heads,\n+            2 * self.head_k_dim + 2 * self.head_v_dim * self.num_v_heads // self.num_k_heads,\n+        )\n+        new_tensor_shape_ba = mixed_ba.size()[:-1] + (self.num_k_heads, 2 * self.num_v_heads // self.num_k_heads)\n+\n+        mixed_qkvz = mixed_qkvz.view(*new_tensor_shape_qkvz)\n+        mixed_ba = mixed_ba.view(*new_tensor_shape_ba)\n+        split_arg_list_qkvz = [\n+            self.head_k_dim,\n+            self.head_k_dim,\n+            (self.num_v_heads // self.num_k_heads * self.head_v_dim),\n+            (self.num_v_heads // self.num_k_heads * self.head_v_dim),\n+        ]\n+        split_arg_list_ba = [self.num_v_heads // self.num_k_heads, self.num_v_heads // self.num_k_heads]\n+        query, key, value, z = torch.split(mixed_qkvz, split_arg_list_qkvz, dim=3)\n+        b, a = torch.split(mixed_ba, split_arg_list_ba, dim=3)\n+        # [b, sq, ng, np/ng * hn] -> [b, sq, np, hn]\n+        value = value.reshape(value.size(0), value.size(1), -1, self.head_v_dim)\n+        z = z.reshape(z.size(0), z.size(1), -1, self.head_v_dim)\n+        b = b.reshape(b.size(0), b.size(1), self.num_v_heads)\n+        a = a.reshape(a.size(0), a.size(1), self.num_v_heads)\n+        return query, key, value, z, b, a\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cache_params: Optional[Qwen3NextDynamicCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n+\n+        # Set up dimensions for reshapes later\n+        batch_size, seq_len, _ = hidden_states.shape\n+\n+        use_precomputed_states = (\n+            cache_params is not None\n+            and cache_params.has_previous_state\n+            and seq_len == 1\n+            and cache_position is not None\n+        )\n+\n+        # getting projected states from cache if it exists\n+        if cache_params is not None:\n+            conv_state = cache_params.conv_states[self.layer_idx]\n+            recurrent_state = cache_params.recurrent_states[self.layer_idx]\n+\n+        projected_states_qkvz = self.in_proj_qkvz(hidden_states)\n+        projected_states_ba = self.in_proj_ba(hidden_states)\n+        query, key, value, z, b, a = self.fix_query_key_value_ordering(projected_states_qkvz, projected_states_ba)\n+        query, key, value = (x.reshape(x.shape[0], x.shape[1], -1) for x in (query, key, value))\n+\n+        mixed_qkv = torch.cat((query, key, value), dim=-1)\n+        mixed_qkv = mixed_qkv.transpose(1, 2)\n+\n+        if use_precomputed_states:\n+            # 2. Convolution sequence transformation\n+            # NOTE: the conv state is updated in `causal_conv1d_update`\n+            mixed_qkv = self.causal_conv1d_update(\n+                mixed_qkv,\n+                conv_state,\n+                self.conv1d.weight.squeeze(1),\n+                self.conv1d.bias,\n+                self.activation,\n+            )\n+        else:\n+            if cache_params is not None:\n+                conv_state = F.pad(mixed_qkv, (self.conv_kernel_size - mixed_qkv.shape[-1], 0))\n+                cache_params.conv_states[self.layer_idx] = conv_state\n+            if self.causal_conv1d_fn is not None:\n+                mixed_qkv = self.causal_conv1d_fn(\n+                    x=mixed_qkv,\n+                    weight=self.conv1d.weight.squeeze(1),\n+                    bias=self.conv1d.bias,\n+                    activation=self.activation,\n+                    seq_idx=None,\n+                )\n+            else:\n+                mixed_qkv = F.silu(self.conv1d(mixed_qkv)[:, :, :seq_len])\n+\n+        mixed_qkv = mixed_qkv.transpose(1, 2)\n+        query, key, value = torch.split(\n+            mixed_qkv,\n+            [\n+                self.key_dim,\n+                self.key_dim,\n+                self.value_dim,\n+            ],\n+            dim=-1,\n+        )\n+        query = query.reshape(query.shape[0], query.shape[1], -1, self.head_k_dim)\n+        key = key.reshape(key.shape[0], key.shape[1], -1, self.head_k_dim)\n+        value = value.reshape(value.shape[0], value.shape[1], -1, self.head_v_dim)\n+\n+        beta = b.sigmoid()\n+        # If the model is loaded in fp16, without the .float() here, A might be -inf\n+        g = -self.A_log.float().exp() * F.softplus(a.float() + self.dt_bias)\n+        if self.num_v_heads // self.num_k_heads > 1:\n+            query = query.repeat_interleave(self.num_v_heads // self.num_k_heads, dim=2)\n+            key = key.repeat_interleave(self.num_v_heads // self.num_k_heads, dim=2)\n+\n+        if not use_precomputed_states:\n+            core_attn_out, last_recurrent_state = self.chunk_gated_delta_rule(\n+                query,\n+                key,\n+                value,\n+                g=g,\n+                beta=beta,\n+                initial_state=None,\n+                output_final_state=cache_params is not None,\n+                use_qk_l2norm_in_kernel=True,\n+            )\n+\n+        else:\n+            core_attn_out, last_recurrent_state = self.recurrent_gated_delta_rule(\n+                query,\n+                key,\n+                value,\n+                g=g,\n+                beta=beta,\n+                initial_state=recurrent_state,\n+                output_final_state=cache_params is not None,\n+                use_qk_l2norm_in_kernel=True,\n+            )\n+\n+        # Update cache\n+        if cache_params is not None:\n+            cache_params.recurrent_states[self.layer_idx] = last_recurrent_state\n+\n+        z_shape_og = z.shape\n+        # reshape input data into 2D tensor\n+        core_attn_out = core_attn_out.reshape(-1, core_attn_out.shape[-1])\n+        z = z.reshape(-1, z.shape[-1])\n+        core_attn_out = self.norm(core_attn_out, z)\n+        core_attn_out = core_attn_out.reshape(z_shape_og)\n+        core_attn_out = core_attn_out.reshape(core_attn_out.shape[0], core_attn_out.shape[1], -1)\n+\n+        output = self.out_proj(core_attn_out)\n+        return output\n+\n+\n+class Qwen3NextMLP(Qwen3MoeMLP):\n+    pass\n+\n+\n+class Qwen3NextSparseMoeBlock(Qwen2MoeSparseMoeBlock):\n+    pass\n+\n+\n+class Qwen3NextDecoderLayer(Qwen3MoeDecoderLayer):\n+    def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n+        nn.Module.__init__(self)\n+        self.hidden_size = config.hidden_size\n+\n+        # token mixer\n+        self.layer_type = config.layer_types[layer_idx]\n+        if self.layer_type == \"linear_attention\":\n+            self.linear_attn = Qwen3NextGatedDeltaNet(config, layer_idx)\n+        elif self.layer_type == \"full_attention\":\n+            self.self_attn = Qwen3NextAttention(config, layer_idx)\n+\n+        if (layer_idx not in config.mlp_only_layers) and (\n+            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n+        ):\n+            self.mlp = Qwen3NextSparseMoeBlock(config)\n+        else:\n+            self.mlp = Qwen3NextMLP(config, intermediate_size=config.intermediate_size)\n+\n+        self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[tuple[torch.Tensor]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Token Mixer\n+        if self.layer_type == \"linear_attention\":\n+            hidden_states = self.linear_attn(\n+                hidden_states=hidden_states,\n+                cache_params=past_key_values,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        elif self.layer_type == \"full_attention\":\n+            # Self Attention\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        # For the MoE layers, we need to unpack\n+        if isinstance(hidden_states, tuple):\n+            hidden_states, _ = hidden_states\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+class Qwen3NextPreTrainedModel(PreTrainedModel):\n+    config: Qwen3NextConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Qwen3NextDecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _keys_to_ignore_on_load_unexpected = [r\"^mtp.*\"]\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(Qwen3NextSparseMoeBlock, index=1),\n+        \"hidden_states\": Qwen3NextDecoderLayer,\n+        \"attentions\": Qwen3NextAttention,\n+    }\n+    _is_stateful = True\n+\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Qwen3NextGatedDeltaNet):\n+            module.dt_bias.data.fill_(1.0)\n+            module.A_log.data.uniform_(0, 16).log_()\n+\n+\n+class Qwen3NextModel(Qwen3NextPreTrainedModel):\n+    def __init__(self, config: Qwen3NextConfig):\n+        super().__init__(config)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n+        self.layers = nn.ModuleList(\n+            [Qwen3NextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen3NextRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = Qwen3NextDynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+        linear_attn_mask = self._update_linear_attn_mask(attention_mask, cache_position)\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = linear_attn_mask if decoder_layer.layer_type == \"linear_attention\" else causal_mask\n+\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=layer_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+    def _update_linear_attn_mask(self, attention_mask, cache_position):\n+        \"\"\"\n+        NOTE: Left-padding is used for linear attention mask.\n+        No need for zeroing states when\n+            1. Cached forward\n+            2. Attending to all inputs\n+        \"\"\"\n+        linear_attn_mask = attention_mask\n+        if cache_position[0] > 0 or (attention_mask is not None and torch.all(attention_mask == 1)):\n+            linear_attn_mask = None\n+        return linear_attn_mask\n+\n+\n+class Qwen3NextForCausalLM(MixtralForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_experts = config.num_experts\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Qwen3NextDynamicCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Qwen3NextForCausalLM\n+\n+        >>> model = Qwen3NextForCausalLM.from_pretrained(\"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Next-80B-A3B-Instruct\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        return super().forward(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            output_router_logits=output_router_logits,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+\n+class Qwen3NextForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+class Qwen3NextForTokenClassification(LlamaForTokenClassification):\n+    pass\n+\n+\n+class Qwen3NextForQuestionAnswering(LlamaForQuestionAnswering):\n+    pass\n+\n+\n+__all__ = [\n+    \"Qwen3NextForCausalLM\",\n+    \"Qwen3NextForQuestionAnswering\",\n+    \"Qwen3NextModel\",\n+    \"Qwen3NextPreTrainedModel\",\n+    \"Qwen3NextForSequenceClassification\",\n+    \"Qwen3NextForTokenClassification\",\n+]"
        },
        {
            "sha": "3e71d925c57146ae778507bd65a6290f8b0a6dd7",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -1637,7 +1637,6 @@ def assert_screenout(out, what):\n \n \n def set_model_tester_for_less_flaky_test(test_case):\n-    target_num_hidden_layers = 1\n     # TODO (if possible): Avoid exceptional cases\n     exceptional_classes = [\n         \"ZambaModelTester\",\n@@ -1646,9 +1645,12 @@ def set_model_tester_for_less_flaky_test(test_case):\n         \"AriaVisionText2TextModelTester\",\n         \"GPTNeoModelTester\",\n         \"DPTModelTester\",\n+        \"Qwen3NextModelTester\",\n     ]\n     if test_case.model_tester.__class__.__name__ in exceptional_classes:\n-        target_num_hidden_layers = None\n+        return\n+\n+    target_num_hidden_layers = 1\n     if hasattr(test_case.model_tester, \"out_features\") or hasattr(test_case.model_tester, \"out_indices\"):\n         target_num_hidden_layers = None\n "
        },
        {
            "sha": "21ddf2a9a3f00fc84db6613d277808ca7cf88a84",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -159,6 +159,7 @@ def _generate_supported_model_class_names(\n     \"qwen2\",\n     \"qwen2_moe\",\n     \"qwen3\",\n+    \"qwen3_next\",\n     \"qwen3_moe\",\n     \"resnet\",\n     \"roberta\","
        },
        {
            "sha": "8155a5f1188e6e9aa2e3ecd5412a93c3f13aa344",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -586,6 +586,21 @@ def is_mamba_2_ssm_available() -> bool:\n     return False\n \n \n+def is_flash_linear_attention_available():\n+    if is_torch_available():\n+        import torch\n+\n+        if not torch.cuda.is_available():\n+            return False\n+        else:\n+            if _is_package_available(\"fla\"):\n+                import fla\n+\n+                if version.parse(fla.__version__) >= version.parse(\"0.2.2\"):\n+                    return True\n+    return False\n+\n+\n def is_causal_conv1d_available() -> Union[tuple[bool, str], bool]:\n     if is_torch_available():\n         import torch"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/qwen3_next/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/tests%2Fmodels%2Fqwen3_next%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/tests%2Fmodels%2Fqwen3_next%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2F__init__.py?ref=b9282355bea846b54ed850a066901496b19da654"
        },
        {
            "sha": "272d9a9f5ec45bdba5f97fe04d3a8bb4557545af",
            "filename": "tests/models/qwen3_next/test_modeling_qwen3_next.py",
            "status": "added",
            "additions": 390,
            "deletions": 0,
            "changes": 390,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9282355bea846b54ed850a066901496b19da654/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9282355bea846b54ed850a066901496b19da654/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py?ref=b9282355bea846b54ed850a066901496b19da654",
            "patch": "@@ -0,0 +1,390 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import copy\n+import tempfile\n+import unittest\n+\n+import pytest\n+from parameterized import parameterized\n+\n+from transformers import Qwen3NextConfig, is_torch_available\n+from transformers.testing_utils import require_torch, require_torch_multi_gpu, slow, torch_device\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Qwen3NextForCausalLM,\n+        Qwen3NextForQuestionAnswering,\n+        Qwen3NextForSequenceClassification,\n+        Qwen3NextForTokenClassification,\n+        Qwen3NextModel,\n+    )\n+    from transformers.models.qwen3_next.modeling_qwen3_next import Qwen3NextDynamicCache\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...generation.test_utils import has_similar_generate_outputs\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    _config_zero_init,\n+    _test_eager_matches_sdpa_inference,\n+)\n+\n+\n+class Qwen3NextModelTester(CausalLMModelTester):\n+    config_class = Qwen3NextConfig\n+    if is_torch_available():\n+        base_model_class = Qwen3NextModel\n+        causal_lm_class = Qwen3NextForCausalLM\n+        sequence_class = Qwen3NextForSequenceClassification\n+        token_class = Qwen3NextForTokenClassification\n+        question_answering_class = Qwen3NextForQuestionAnswering\n+\n+    def __init__(self, parent):\n+        super().__init__(parent=parent)\n+        self.layer_types = [\"linear_attention\", \"full_attention\"]\n+        self.linear_conv_kernel_dim = 2\n+        self.linear_key_head_dim = 16\n+        self.linear_value_head_dim = 16\n+        self.linear_num_key_heads = 4\n+        self.linear_num_value_heads = 8\n+\n+\n+@require_torch\n+class Qwen3NextModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            Qwen3NextModel,\n+            Qwen3NextForCausalLM,\n+            Qwen3NextForSequenceClassification,\n+            Qwen3NextForTokenClassification,\n+            Qwen3NextForQuestionAnswering,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Qwen3NextModel,\n+            \"text-classification\": Qwen3NextForSequenceClassification,\n+            \"token-classification\": Qwen3NextForTokenClassification,\n+            \"text-generation\": Qwen3NextForCausalLM,\n+            \"question-answering\": Qwen3NextForQuestionAnswering,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = Qwen3NextModelTester\n+\n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        \"Qwen3-Next has a special Cache as it alternates with gated deltanet layers\"\n+        self.assertIsInstance(decoder_past_key_values, Qwen3NextDynamicCache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_shape = (\n+            batch_size,\n+            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            cache_length,\n+            config.hidden_size // config.num_attention_heads,\n+        )\n+\n+        attention_layer_indices = decoder_past_key_values.transformer_layers\n+        self.assertListEqual(\n+            [decoder_past_key_values.key_cache[idx].shape for idx in attention_layer_indices],\n+            [expected_shape] * len(attention_layer_indices),\n+        )\n+        self.assertListEqual(\n+            [decoder_past_key_values.value_cache[idx].shape for idx in attention_layer_indices],\n+            [expected_shape] * len(attention_layer_indices),\n+        )\n+\n+    @pytest.mark.generate\n+    def test_past_key_values_format(self):\n+        \"Needs to be overwritten as Qwen3-Next alternates between attention layers and gated deltanet layers.\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device)\n+            model = model.eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+\n+    @pytest.mark.generate\n+    def test_generate_continue_from_past_key_values(self):\n+        \"Needs to be overwritten as Qwen3-Next has non-standard cache.\"\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            generate_kwargs = {\n+                \"pad_token_id\": -1,\n+                \"eos_token_id\": -1,\n+                \"forced_eos_token_id\": None,\n+                \"encoder_no_repeat_ngram_size\": 0,\n+                \"use_cache\": True,\n+                \"do_sample\": False,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=4)\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[-1]\n+\n+            inputs[\"input_ids\"] = outputs_cached.sequences\n+            if \"attention_mask\" in inputs:\n+                inputs[\"attention_mask\"] = torch.nn.functional.pad(\n+                    inputs[\"attention_mask\"],\n+                    (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+            first_caches_scores = outputs_cached.scores\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n+            full_cached_scores = first_caches_scores + outputs_cached.scores\n+            outputs_cached.scores = full_cached_scores\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self.assertTrue(has_similar_generate_outputs(outputs, outputs_cached))\n+            for layer_idx in range(len(outputs_cached.past_key_values)):\n+                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n+                    # Diff with the main test: we need to skip layers where it stays None\n+                    if outputs.past_key_values[layer_idx][kv_idx] is not None:\n+                        self.assertTrue(\n+                            torch.allclose(\n+                                outputs.past_key_values[layer_idx][kv_idx],\n+                                outputs_cached.past_key_values[layer_idx][kv_idx],\n+                            )\n+                        )\n+\n+    @pytest.mark.generate\n+    def test_generate_continue_from_inputs_embeds(self):\n+        \"Needs to be overwritten as Qwen3-Next has non-standard cache.\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            model = model_class(config).to(torch_device).eval()\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n+            model.generation_config.forced_eos_token_id = None\n+            model.config.is_decoder = True\n+            model.generation_config.use_cache = True\n+\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"do_sample\": False,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values.\n+            input_embeds = model.get_input_embeddings()(input_ids)\n+            outputs = model.generate(inputs_embeds=input_embeds, max_new_tokens=4, **generation_kwargs)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens)\n+            initial_output = model.generate(inputs_embeds=input_embeds, max_new_tokens=3, **generation_kwargs)\n+            continued_embeds = torch.cat([input_embeds, model.get_input_embeddings()(initial_output.sequences)], dim=1)\n+            cached_output = model.generate(\n+                inputs_embeds=continued_embeds,\n+                max_new_tokens=1,\n+                past_key_values=initial_output.past_key_values,\n+                **generation_kwargs,\n+            )\n+\n+            # Combine the (3 + 1) generated tokens and verify it matches with full generation.\n+            combined_output_sequences = torch.concat([initial_output.sequences, cached_output.sequences], axis=1)\n+            self.assertListEqual(outputs.sequences.tolist(), combined_output_sequences.tolist())\n+            # The two sets of past kv should be equal to each other\n+            for layer_idx in range(len(cached_output.past_key_values)):\n+                for kv_idx in range(len(cached_output.past_key_values[layer_idx])):\n+                    # Diff with the main test: we need to skip layers where it stays None\n+                    if outputs.past_key_values[layer_idx][kv_idx] is not None:\n+                        self.assertTrue(\n+                            torch.allclose(\n+                                outputs.past_key_values[layer_idx][kv_idx],\n+                                cached_output.past_key_values[layer_idx][kv_idx],\n+                            )\n+                        )\n+\n+    def test_attention_outputs(self):\n+        \"Needs to be overwritten as Qwen3-Next alternates between attention layers and gated deltanet layers.\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    def test_initialization(self):\n+        \"Some parameters need to be skipped.\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=copy.deepcopy(configs_no_init))\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    # this one need to be skipped, it's initialized as log(uniform(0, 16))\n+                    if \"A_log\" in name:\n+                        continue\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    @unittest.skip(\"Redundant with `test_initialization`, and fails because of the same param (`A_log`)\")\n+    def test_mismatched_shapes_have_properly_initialized_weights(self):\n+        pass\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    def test_eager_matches_sdpa_inference(\n+        self,\n+        name,\n+        dtype,\n+        padding_side,\n+        use_attention_mask,\n+        output_attentions,\n+        enable_kernels,\n+    ):\n+        \"\"\"\n+        We need to overwrite this without the fp16 part of the dtype, because the slow path `torch_chunk_gated_delta_rule`\n+        is not robust enough (flaky test) in fp16 due to upscaling in fp32 and then downscaling to fp16 at the end\n+        \"\"\"\n+        if dtype == \"fp16\":\n+            self.skipTest(\"Not robust in fp16\")\n+        _test_eager_matches_sdpa_inference(\n+            self,\n+            name,\n+            dtype,\n+            padding_side,\n+            use_attention_mask,\n+            output_attentions,\n+            enable_kernels,\n+        )\n+\n+    @unittest.skip(\"The specific cache format cannot be instantiated from dp/ddp data.\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @require_torch_multi_gpu\n+    def test_can_use_device_map(self):\n+        \"\"\"\n+        Test that this model can be dispatched on multiple gpus. It's not obvious as the Cache is not standard,\n+        ant each layer need to use the correct device on which it reside (i.e. it needs to be lazy initialized).\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            inputs_dict = {k: v.to(0) if isinstance(v, torch.Tensor) else v for k, v in inputs_dict.items()}\n+            # We want the linear attention layer to reside on device 1 with the device map (i.e. not the first/default device),\n+            # to check if cache initialization is on the correct device\n+            config.layer_types = [\"full_attention\", \"linear_attention\"]\n+            model = model_class(config).eval()\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                del model\n+                model = model_class.from_pretrained(\n+                    tmpdirname,\n+                    device_map={\n+                        \"lm_head\": 0,\n+                        \"model.embed_tokens\": 0,\n+                        \"model.norm\": 0,\n+                        \"model.layers.0\": 0,\n+                        \"model.layers.1\": 1,\n+                    },\n+                )\n+\n+                # Check that we indeed use 2 different devices for each layer\n+                self.assertTrue({param.device for param in model.model.layers[0].parameters()} == {torch.device(0)})\n+                self.assertTrue({param.device for param in model.model.layers[1].parameters()} == {torch.device(1)})\n+\n+                # This should not crash\n+                _ = model.generate(**inputs_dict, max_new_tokens=5, min_new_tokens=5)\n+\n+\n+@slow\n+class Qwen3NextIntegrationTest(unittest.TestCase):\n+    pass"
        }
    ],
    "stats": {
        "total": 2966,
        "additions": 2964,
        "deletions": 2
    }
}