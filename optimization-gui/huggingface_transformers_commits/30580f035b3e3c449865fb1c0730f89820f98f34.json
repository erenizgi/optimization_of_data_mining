{
    "author": "yonigozlan",
    "message": "Fix Mistral3 tests (#36797)\n\n* fix processor tests\n\n* fix modeling tests\n\n* fix test processor chat template\n\n* revert modeling test changes",
    "sha": "30580f035b3e3c449865fb1c0730f89820f98f34",
    "files": [
        {
            "sha": "4efdb6415599fe8b496a47fecb4904235f918561",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/30580f035b3e3c449865fb1c0730f89820f98f34/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/30580f035b3e3c449865fb1c0730f89820f98f34/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=30580f035b3e3c449865fb1c0730f89820f98f34",
            "patch": "@@ -54,7 +54,7 @@ Here is how you can use the `image-text-to-text` pipeline to perform inference w\n ...     },\n ... ]\n \n->>> pipe = pipeline(\"image-text-to-text\", model=\"../mistral3_weights\", torch_dtype=torch.bfloat16)\n+>>> pipe = pipeline(\"image-text-to-text\", model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", torch_dtype=torch.bfloat16)\n >>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)\n >>> outputs[0][\"generated_text\"]\n 'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'"
        },
        {
            "sha": "0a5eeaa99c7f6ac536f127a10636da2c7d3e1d77",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 65,
            "deletions": 3,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/30580f035b3e3c449865fb1c0730f89820f98f34/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/30580f035b3e3c449865fb1c0730f89820f98f34/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=30580f035b3e3c449865fb1c0730f89820f98f34",
            "patch": "@@ -51,17 +51,75 @@ def setUpClass(cls):\n \n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()\n-        processor = PixtralProcessor.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n+        processor = self.processor_class.from_pretrained(\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\")\n         processor.save_pretrained(self.tmpdirname)\n \n+    def get_processor(self):\n+        return self.processor_class.from_pretrained(self.tmpdirname)\n+\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n+    def test_chat_template_accepts_processing_kwargs(self):\n+        # override to use slow image processor to return numpy arrays\n+        processor = self.processor_class.from_pretrained(self.tmpdirname, use_fast=False)\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            max_length=50,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            truncation=True,\n+            max_length=5,\n+        )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n+\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            return_tensors=\"np\",\n+        )\n+        self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)\n+\n     def test_chat_template(self):\n-        processor = self.processor_class.from_pretrained(self.tmpdirname)\n-        expected_prompt = \"<s>[INST][IMG]What is shown in this image?[/INST]\"\n+        processor = self.processor_class.from_pretrained(self.tmpdirname, use_fast=False)\n+        expected_prompt = \"<s>[SYSTEM_PROMPT][/SYSTEM_PROMPT][INST][IMG]What is shown in this image?[/INST]\"\n \n         messages = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": \"\",\n+            },\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n@@ -81,6 +139,10 @@ def test_image_token_filling(self):\n         image_token_index = 10\n \n         messages = [\n+            {\n+                \"role\": \"system\",\n+                \"content\": \"\",\n+            },\n             {\n                 \"role\": \"user\",\n                 \"content\": ["
        }
    ],
    "stats": {
        "total": 70,
        "additions": 66,
        "deletions": 4
    }
}