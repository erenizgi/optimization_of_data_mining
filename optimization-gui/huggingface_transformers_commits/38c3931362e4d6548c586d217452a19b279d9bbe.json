{
    "author": "gante",
    "message": "[server] add tests and fix passing a custom `generation_config` (#39230)\n\n* add tests; fix passing a custom generation_config\n\n* tool integration test\n\n* add install step\n\n* add accelerate as dep to serving\n\n* add todo",
    "sha": "38c3931362e4d6548c586d217452a19b279d9bbe",
    "files": [
        {
            "sha": "f8c9d8af5e1b813a24e478ed4f20a1c3ccf5552c",
            "filename": ".circleci/create_circleci_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c3931362e4d6548c586d217452a19b279d9bbe/.circleci%2Fcreate_circleci_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c3931362e4d6548c586d217452a19b279d9bbe/.circleci%2Fcreate_circleci_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fcreate_circleci_config.py?ref=38c3931362e4d6548c586d217452a19b279d9bbe",
            "patch": "@@ -303,7 +303,7 @@ def job_name(self):\n     docker_image=[{\"image\": \"huggingface/transformers-torch-light\"}],\n     # networkx==3.3 (after #36957) cause some issues\n     # TODO: remove this once it works directly\n-    install_steps=[\"uv venv && uv pip install .\"],\n+    install_steps=[\"uv venv && uv pip install .[serving]\"],\n     marker=\"not generate\",\n     parallelism=6,\n )"
        },
        {
            "sha": "349257ab28a6fe820fc63adbfe0747101150cce8",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c3931362e4d6548c586d217452a19b279d9bbe/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c3931362e4d6548c586d217452a19b279d9bbe/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=38c3931362e4d6548c586d217452a19b279d9bbe",
            "patch": "@@ -313,7 +313,7 @@ def run(self):\n \n extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n \n-extras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\")\n+extras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\") + extras[\"torch\"]\n extras[\"audio\"] = deps_list(\n     \"librosa\",\n     \"pyctcdecode\","
        },
        {
            "sha": "e74970f694f2b241146a9e7845bf6854c6095b66",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c3931362e4d6548c586d217452a19b279d9bbe/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c3931362e4d6548c586d217452a19b279d9bbe/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=38c3931362e4d6548c586d217452a19b279d9bbe",
            "patch": "@@ -14,6 +14,7 @@\n \n \n import asyncio\n+import copy\n import json\n import os\n import platform\n@@ -451,11 +452,13 @@ def is_number(s: str) -> bool:\n             )\n         return processed_generate_flags\n \n-    def get_generation_parameterization(self, args: ChatArguments) -> tuple[GenerationConfig, dict]:\n+    def get_generation_parameterization(\n+        self, args: ChatArguments, model_generation_config: GenerationConfig\n+    ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         Returns a GenerationConfig object holding the generation parameters for the CLI command.\n         \"\"\"\n-        # No generation config arg provided -> use base generation config, apply CLI defaults\n+        # No generation config arg provided -> use model's default generation config, then apply CLI defaults\n         if args.generation_config is not None:\n             if \".json\" in args.generation_config:  # is a local file\n                 dirname = os.path.dirname(args.generation_config)\n@@ -467,7 +470,8 @@ def get_generation_parameterization(self, args: ChatArguments) -> tuple[Generati\n             # !!!!!!!!!\n             # This is a chat session, so we have a few non-standard defaults\n             # !!!!!!!!!\n-            generation_config = GenerationConfig(do_sample=True, max_new_tokens=256)\n+            generation_config = copy.deepcopy(model_generation_config)\n+            generation_config.update({\"do_sample\": True, \"max_new_tokens\": 256})\n \n         # Finally: parse and apply `generate_flags`\n         parsed_generate_flags = self.parse_generate_flags(args.generate_flags)\n@@ -675,7 +679,8 @@ async def _inner_run(self):\n         else:\n             user = args.user\n \n-        generation_config, model_kwargs = self.get_generation_parameterization(args)\n+        model_generation_config = GenerationConfig.from_pretrained(args.model_name_or_path)\n+        generation_config, model_kwargs = self.get_generation_parameterization(args, model_generation_config)\n \n         interface = RichInterface(model_name=args.model_name_or_path, user_name=user)\n         interface.clear()\n@@ -715,7 +720,7 @@ async def _inner_run(self):\n                     stream=True,\n                     extra_body={\n                         \"request_id\": request_id,\n-                        \"generation_config\": {**generation_config.to_dict()},\n+                        \"generation_config\": generation_config.to_json_string(),\n                         \"model\": model,\n                     },\n                 )"
        },
        {
            "sha": "a7be1d4545f662feb973c1482db6f048bb9c6061",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 51,
            "deletions": 41,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c3931362e4d6548c586d217452a19b279d9bbe/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c3931362e4d6548c586d217452a19b279d9bbe/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=38c3931362e4d6548c586d217452a19b279d9bbe",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import copy\n import functools\n import json\n import re\n@@ -20,12 +21,7 @@\n from threading import Thread\n from typing import Any, Optional\n \n-from huggingface_hub import (\n-    ChatCompletionStreamOutputDeltaToolCall,\n-    ChatCompletionStreamOutputFunction,\n-    ModelInfo,\n-    model_info,\n-)\n+from huggingface_hub import ModelInfo, model_info\n \n from transformers.utils.import_utils import is_fastapi_available, is_pydantic_available, is_uvicorn_available\n \n@@ -86,6 +82,9 @@ class ChatCompletionInput(BaseModel):\n         # tool_prompt: Optional[str] = None\n         # top_logprobs: Optional[int] = None\n \n+        # transformers-specific request fields\n+        generation_config: Optional[str] = None\n+\n \n logger = logging.get_logger(__name__)\n \n@@ -110,26 +109,35 @@ def serve_command_factory(args: Namespace):\n     return ServeCommand(args)\n \n \n-def create_generation_config_from_req(req: \"ChatCompletionInput\", **kwargs) -> \"GenerationConfig\":\n+def create_generation_config_from_req(\n+    req: \"ChatCompletionInput\", model_generation_config: \"GenerationConfig\", **kwargs\n+) -> \"GenerationConfig\":\n     \"\"\"\n-    Creates a generation config from the parameters of the request. Note that we can pass a `GenerationConfig`\n-    (serialized into a `dict`) in `extra_body`, for full `generate` parameterization.\n+    Creates a generation config from the parameters of the request. If a generation config is passed in the request,\n+    it will be used as a baseline for parameterization. Otherwise, we will use the model's default generation config.\n+    Other parameters in the request will be applied on top of the baseline.\n \n     Args:\n-        req (`ChatCompletionInput`): The request which may optionally contain generation parameters.\n+        req (`ChatCompletionInput`):\n+            The request which may optionally contain generation parameters.\n+        model_generation_config (`GenerationConfig`):\n+            The model's default generation config.\n \n     Returns:\n         The prepared `GenerationConfig` object.\n     \"\"\"\n-    if req.extra_body is not None and \"generation_config\" in req.extra_body:\n-        for key in req.extra_body[\"generation_config\"].keys():\n-            if key in ChatCompletionInput.base_field_names.keys():\n-                raise ValueError(\"error: Duplicated key in the root request and in the passed generation config.\")\n-\n-    if req.extra_body is not None and \"generation_config\" in req.extra_body:\n-        generation_config = GenerationConfig(**(req.extra_body[\"generation_config\"]), **kwargs)\n+    # If there is a generation config in the request, it is a json string serialization from a `GenerationConfig`\n+    # object. For simplicity, flags set here take precedence over all other flags.\n+    if req.generation_config is not None:\n+        generation_config = GenerationConfig(**json.loads(req.generation_config))\n     else:\n-        generation_config = GenerationConfig(**kwargs)\n+        generation_config = copy.deepcopy(model_generation_config)\n+\n+    non_standard_kwargs = generation_config.update(**kwargs)\n+    # Set extra kwargs that are not in the `GenerationConfig` class (e.g. continuous batching flags)\n+    for k, v in non_standard_kwargs.items():\n+        if v is not None:\n+            setattr(generation_config, k, v)\n \n     if req.frequency_penalty is not None:\n         generation_config.repetition_penalty = float(req.frequency_penalty)\n@@ -267,7 +275,7 @@ def build_chunk(\n         content: Optional[str] = None,\n         role: Optional[str] = None,\n         finish_reason: Optional[str] = None,\n-        tool_calls: Optional[list[ChatCompletionStreamOutputDeltaToolCall]] = None,\n+        tool_calls: Optional[list[dict]] = None,\n     ) -> str:\n         \"\"\"\n         Builds a chunk of a streaming response.\n@@ -284,7 +292,7 @@ def build_chunk(\n                 The role of the next content, until a new role is defined.\n             finish_reason (`str`, *optional*):\n                 The reason the generation by the model has finished.\n-            tool_calls (`list[ChatCompletionStreamOutputDeltaToolCall]`, *optional*):\n+            tool_calls (`list[dict]`, *optional*):\n                 Data about the tool calls, when they are triggered.\n \n         Returns:\n@@ -380,6 +388,7 @@ def _serve(req: \"ChatCompletionInput\"):\n \n             generation_config = create_generation_config_from_req(\n                 req,\n+                model_generation_config=self.model.generation_config,\n                 eos_token_id=self.tokenizer.eos_token_id,\n                 pad_token_id=self.tokenizer.pad_token_id,\n                 use_cache=False,\n@@ -413,6 +422,10 @@ def stream_response(_inputs):\n                     )\n                     queue_is_flushed = False\n \n+                    # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n+                    # they come from the assistant.\n+                    yield self.build_chunk(request_id, role=\"assistant\")\n+\n                     for result in self.running_continuous_batching_manager:\n                         if result.request_id != request_id:\n                             continue\n@@ -424,14 +437,12 @@ def stream_response(_inputs):\n                                 queue_is_flushed = True\n \n                         finish_reason = \"stop\" if result.status == RequestStatus.FINISHED else None\n-                        yield self.build_chunk(\n-                            request_id=request_id, content=result.next_token, finish_reason=finish_reason\n-                        )\n-\n                         if result.status == RequestStatus.FINISHED:\n+                            yield self.build_chunk(request_id, finish_reason=finish_reason)\n                             break\n+                        else:\n+                            yield self.build_chunk(request_id=request_id, content=result.next_token)\n \n-                    yield \"data: [DONE]\\n\\n\"\n                 except Exception as e:\n                     logger.error(str(e))\n                     yield f'data: {{\"error\": \"{str(e)}\"}}'\n@@ -507,7 +518,10 @@ def _serve(req: \"ChatCompletionInput\"):\n \n             generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n \n-            generation_config = create_generation_config_from_req(req)\n+            generation_config = create_generation_config_from_req(\n+                req,\n+                model_generation_config=self.model.generation_config,\n+            )\n             max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 1024\n             generation_config.max_new_tokens = max_new_tokens\n \n@@ -570,14 +584,12 @@ def generate_with_cache(**kwargs):\n                                     else:\n                                         tool_name = tool_name.group(1)\n                                     tool_state.has_tool_name_defined = True\n-                                    tool = ChatCompletionStreamOutputDeltaToolCall(\n-                                        function=ChatCompletionStreamOutputFunction(\n-                                            name=tool_name,\n-                                        ),\n-                                        index=0,\n-                                        type=\"function\",\n-                                        id=_request_id + \"_tool_call\",  # Only the first tool call delta has an id\n-                                    )\n+                                    tool = {\n+                                        \"function\": {\"name\": tool_name},\n+                                        \"index\": 0,\n+                                        \"type\": \"function\",\n+                                        \"id\": _request_id + \"_tool_call\",  # Only the first tool call delta has an id\n+                                    }\n \n                                 # Second step: extract tool arguments. The tool arguments can be seen as a json string\n                                 # within the tool json string. We emit a delta for the arguments.\n@@ -597,13 +609,11 @@ def generate_with_cache(**kwargs):\n                                     if tool_state.arg_nesting_level < 0:\n                                         result = \"\".join(result.split(\"}\")[:-2]) + \"}\"  # e.g. \"4}}\\n\" -> \"4}\"\n \n-                                    tool = ChatCompletionStreamOutputDeltaToolCall(\n-                                        function=ChatCompletionStreamOutputFunction(\n-                                            arguments=result,\n-                                        ),\n-                                        index=0,\n-                                        type=\"function\",\n-                                    )\n+                                    tool = {\n+                                        \"function\": {\"arguments\": result},\n+                                        \"index\": 0,\n+                                        \"type\": \"function\",\n+                                    }\n \n                                 yield self.build_chunk(_request_id, tool_calls=[tool])\n                                 continue"
        },
        {
            "sha": "8f871a150f74e6b28ac5fdc4537a933d6f8b7be5",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c3931362e4d6548c586d217452a19b279d9bbe/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c3931362e4d6548c586d217452a19b279d9bbe/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=38c3931362e4d6548c586d217452a19b279d9bbe",
            "patch": "@@ -640,7 +640,7 @@ def compute_optimal_blocks(\n     memory_per_token = 2 * num_kv_heads * head_dim * dtype_size * num_hidden_layers  # For K and V caches\n \n     # Estimate sequence length requirements\n-    tokens_to_generate = getattr(generation_config, \"max_new_tokens\", 20)\n+    tokens_to_generate = getattr(generation_config, \"max_new_tokens\") or 20\n \n     if median_prefill_length is None and inputs:\n         non_empty_inputs = [len(seq) for seq in inputs if seq]"
        },
        {
            "sha": "ae9fe16e68d8405d622dc2fc704902e94a09f68f",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 256,
            "deletions": 4,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/38c3931362e4d6548c586d217452a19b279d9bbe/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38c3931362e4d6548c586d217452a19b279d9bbe/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=38c3931362e4d6548c586d217452a19b279d9bbe",
            "patch": "@@ -11,22 +11,32 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import asyncio\n+import time\n import unittest\n+from threading import Thread\n from unittest.mock import patch\n \n+import aiohttp.client_exceptions\n+from huggingface_hub import AsyncInferenceClient\n+from parameterized import parameterized\n+\n import transformers.commands.transformers_cli as cli\n-from transformers.commands.serving import ServeCommand\n-from transformers.testing_utils import CaptureStd\n+from transformers import GenerationConfig\n+from transformers.commands.serving import ServeArguments, ServeCommand\n+from transformers.testing_utils import CaptureStd, slow\n \n \n class ServeCLITest(unittest.TestCase):\n     def test_help(self):\n+        \"\"\"Minimal test: we can invoke the help command.\"\"\"\n         with patch(\"sys.argv\", [\"transformers\", \"serve\", \"--help\"]), CaptureStd() as cs:\n             with self.assertRaises(SystemExit):\n                 cli.main()\n         self.assertIn(\"serve\", cs.out.lower())\n \n     def test_parsed_args(self):\n+        \"\"\"Minimal test: we can set arguments through the CLI.\"\"\"\n         with (\n             patch.object(ServeCommand, \"__init__\", return_value=None) as init_mock,\n             patch.object(ServeCommand, \"run\") as run_mock,\n@@ -39,9 +49,251 @@ def test_parsed_args(self):\n         self.assertEqual(parsed_args.host, \"0.0.0.0\")\n         self.assertEqual(parsed_args.port, 9000)\n \n-    def test_build_chunk(self):\n+    def test_completions_build_chunk(self):\n+        \"\"\"Tests that the chunks are correctly built for the Completions API.\"\"\"\n         dummy = ServeCommand.__new__(ServeCommand)\n         dummy.args = type(\"Args\", (), {})()\n-        chunk = ServeCommand.build_chunk(dummy, \"hello\", \"req0\", finish_reason=\"stop\")\n+\n+        # Case 1: most fields are provided\n+        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\")\n+        self.assertIn(\"chat.completion.chunk\", chunk)\n+        self.assertIn(\"data:\", chunk)\n+        self.assertIn(\n+            '\"choices\": [{\"delta\": {\"content\": \"hello\", \"role\": \"user\"}, \"index\": 0, \"finish_reason\": \"stop\"}]', chunk\n+        )\n+\n+        # Case 2: only the role is provided -- other fields in 'choices' are omitted\n+        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", role=\"user\")\n+        self.assertIn(\"chat.completion.chunk\", chunk)\n+        self.assertIn(\"data:\", chunk)\n+        self.assertIn('\"choices\": [{\"delta\": {\"role\": \"user\"}, \"index\": 0}]', chunk)\n+\n+        # Case 3: only the content is provided -- other fields in 'choices' are omitted\n+        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", content=\"hello\")\n+        self.assertIn(\"chat.completion.chunk\", chunk)\n+        self.assertIn(\"data:\", chunk)\n+        self.assertIn('\"choices\": [{\"delta\": {\"content\": \"hello\"}, \"index\": 0}]', chunk)\n+\n+        # Case 4: tool calls support a list of nested dictionaries\n+        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", tool_calls=[{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}])\n         self.assertIn(\"chat.completion.chunk\", chunk)\n         self.assertIn(\"data:\", chunk)\n+        self.assertIn('\"choices\": [{\"delta\": {\"tool_calls\": [{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}]}, \"index\": 0}]', chunk)\n+\n+\n+def async_retry(fn, max_attempts=5, delay=2):\n+    \"\"\"\n+    Retry a function up to `max_attempts` times with a `delay` between attempts.\n+    Useful for testing async functions that may fail due to server not being ready.\n+    \"\"\"\n+\n+    async def wrapper(*args, **kwargs):\n+        for _ in range(max_attempts):\n+            try:\n+                return await fn(*args, **kwargs)\n+            except aiohttp.client_exceptions.ClientConnectorError:\n+                time.sleep(delay)\n+\n+    return wrapper\n+\n+\n+class ServeCompletionsMixin:\n+    \"\"\"\n+    Mixin class for the Completions API tests, to seamlessly replicate tests across the two versions of the API\n+    (`generate` and `continuous_batching`).\n+    \"\"\"\n+\n+    @async_retry\n+    async def run_server(self, request):\n+        client = AsyncInferenceClient(\"http://localhost:8000\")\n+        stream = client.chat_completion(**request)\n+\n+        all_payloads = []\n+        async for payload in await stream:\n+            all_payloads.append(payload)\n+\n+        await client.close()\n+        return all_payloads\n+\n+    @parameterized.expand(\n+        [\n+            (\"default_request\", {}),\n+            (\"one_token\", {\"max_tokens\": 1}),\n+            #  TODO: CB fails next case, seems like it is unable to switch models. fix me\n+            # (\"different_model\", {\"model\": \"HuggingFaceTB/SmolLM2-135M-Instruct\"}),\n+            (\n+                \"tool_call\",\n+                {\n+                    \"tools\": [\n+                        {\n+                            \"function\": {\n+                                \"name\": \"foo_bar\",\n+                                \"parameters\": {\"type\": \"object\"},\n+                                \"description\": \"Foo bar\",\n+                            },\n+                            \"type\": \"function\",\n+                        }\n+                    ]\n+                },\n+            ),\n+        ]\n+    )\n+    def test_requests(self, test_name: str, request_flags: dict):\n+        \"\"\"Tests that the completions app gracefully handles GOOD requests, producing the expected output payloads.\"\"\"\n+\n+        request = {\n+            \"model\": \"Qwen/Qwen3-0.6B\",\n+            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n+            \"stream\": True,  # We don't support \"stream\": False yet\n+            \"max_tokens\": 5,  # Small generation by default\n+        }\n+        request.update(request_flags)\n+        all_payloads = asyncio.run(self.run_server(request))\n+\n+        # If a request is successful, the returned payload needs to follow the schema, which we test here.\n+        # NOTE: the output of our server is wrapped by `AsyncInferenceClient`, which sends fields even when they\n+        # are empty.\n+\n+        # Finish reason: the last payload should have a finish reason of \"stop\", all others should be empty\n+        # TODO: we may add other finish reasons in the future, and this may need more logic\n+        finish_reasons = [payload.choices[0].finish_reason for payload in all_payloads]\n+        self.assertEqual(finish_reasons[-1], \"stop\")\n+        self.assertTrue(all(reason is None for reason in finish_reasons[:-1]))\n+\n+        # Role: the first payload should have a role of \"assistant\", all others should be empty\n+        roles = [payload.choices[0].delta.role for payload in all_payloads]\n+        self.assertEqual(roles[0], \"assistant\")\n+        self.assertTrue(all(role is None for role in roles[1:]))\n+\n+        # Content: the first and the last payload shouldn't have content (role and finish reason). It may be empty\n+        # in some other payload positions, e.g. tool calls.\n+        contents = [payload.choices[0].delta.content for payload in all_payloads]\n+        self.assertTrue(contents[0] is None and contents[-1] is None)\n+        self.assertTrue(any(content is not None for content in contents[1:-1]))\n+        # TODO: add \"usage\" field to output and test it\n+\n+    def test_generation_config_in_request(self):\n+        \"\"\"Tests that the generation config is correctly passed into the generation call.\"\"\"\n+        generation_config = GenerationConfig(do_sample=False, temperature=0.0)\n+        request = {\n+            \"model\": \"Qwen/Qwen3-0.6B\",\n+            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n+            \"stream\": True,\n+            \"max_tokens\": 10,\n+            \"extra_body\": {\n+                \"generation_config\": generation_config.to_json_string(),\n+            },\n+        }\n+        all_payloads = asyncio.run(self.run_server(request))\n+        contents = [payload.choices[0].delta.content for payload in all_payloads]\n+        output_text = \"\".join([text for text in contents if text is not None])\n+        # The generation config sets greedy decoding, so the output is reproducible. By default, `Qwen/Qwen3-0.6B`\n+        # sets `do_sample=True`\n+        self.assertEqual(output_text, '<think>\\nOkay, the user just asked, \"')\n+\n+    # TODO: implement API-compliant error handling, and then test it\n+    # See https://platform.openai.com/docs/guides/error-codes,\n+    # TODO: one test for each request flag, to confirm it is working as expected\n+    # TODO: speed-based test to confirm that KV cache is working across requests\n+\n+\n+class ServeCompletionsGenerateTest(ServeCompletionsMixin, unittest.TestCase):\n+    \"\"\"Tests the `generate` version of the Completions API.\"\"\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"Starts a server for tests to connect to.\"\"\"\n+        args = ServeArguments()\n+        serve_command = ServeCommand(args)\n+        thread = Thread(target=serve_command.run)\n+        thread.daemon = True\n+        thread.start()\n+\n+    @slow\n+    def test_tool_call(self):\n+        \"\"\"Tests that the tool call is correctly handled and that the payloads are correctly structured.\"\"\"\n+        # TODO: move to the mixin when CB also supports tool calls\n+\n+        request = {\n+            # This model is a small model that's very eager to call tools\n+            # TODO: this is a 4B model. Find a smaller model that's eager to call tools\n+            \"model\": \"Menlo/Jan-nano\",\n+            # The request should produce a tool call\n+            \"messages\": [{\"role\": \"user\", \"content\": \"Generate an image of a cat.\"}],\n+            \"stream\": True,\n+            \"max_tokens\": 50,\n+            # Reproducibility\n+            \"temperature\": 0.0,\n+            # This tool is a copy from the tool in the original tiny-agents demo\n+            \"tools\": [\n+                {\n+                    \"function\": {\n+                        \"name\": \"flux1_schnell_infer\",\n+                        \"parameters\": {\n+                            \"type\": \"object\",\n+                            \"properties\": {\n+                                \"prompt\": {\"type\": \"string\"},\n+                                \"seed\": {\"type\": \"number\", \"description\": \"numeric value between 0 and 2147483647\"},\n+                                \"randomize_seed\": {\"type\": \"boolean\", \"default\": True},\n+                                \"width\": {\n+                                    \"type\": \"number\",\n+                                    \"description\": \"numeric value between 256 and 2048\",\n+                                    \"default\": 1024,\n+                                },\n+                                \"height\": {\n+                                    \"type\": \"number\",\n+                                    \"description\": \"numeric value between 256 and 2048\",\n+                                    \"default\": 1024,\n+                                },\n+                                \"num_inference_steps\": {\n+                                    \"type\": \"number\",\n+                                    \"description\": \"numeric value between 1 and 16\",\n+                                    \"default\": 4,\n+                                },\n+                            },\n+                        },\n+                        \"description\": \"Generate an image using the Flux 1 Schnell Image Generator.\",\n+                    },\n+                    \"type\": \"function\",\n+                }\n+            ],\n+        }\n+        all_payloads = asyncio.run(self.run_server(request))\n+\n+        # The first payload should contain the role\n+        roles = [payload.choices[0].delta.role for payload in all_payloads]\n+        self.assertEqual(roles[0], \"assistant\")\n+        self.assertTrue(all(role is None for role in roles[1:]))\n+\n+        # All other payloads (except the last one) should be tool call related, for this specific request\n+        contents = [payload.choices[0].delta.content for payload in all_payloads]\n+        self.assertTrue(all(content is None for content in contents))\n+\n+        # The first tool call delta should contain the tool name. The other tool call deltas should contain the tool\n+        # arguments.\n+        tool_calls = [payload.choices[0].delta.tool_calls[0] for payload in all_payloads[1:-1]]\n+        first_tool_call = tool_calls[0]\n+        self.assertEqual(first_tool_call[\"function\"][\"name\"], \"flux1_schnell_infer\")\n+        self.assertEqual(first_tool_call[\"function\"][\"arguments\"], None)\n+        other_tool_calls = tool_calls[1:]\n+        self.assertTrue(all(tool_call[\"function\"][\"name\"] is None for tool_call in other_tool_calls))\n+        self.assertTrue(all(tool_call[\"function\"][\"arguments\"] is not None for tool_call in other_tool_calls))\n+\n+        # Finally, the last payload should contain a finish reason\n+        finish_reasons = [payload.choices[0].finish_reason for payload in all_payloads]\n+        # TODO: I think the finish reason for a tool call is different? double check this\n+        self.assertEqual(finish_reasons[-1], \"stop\")\n+        self.assertTrue(all(reason is None for reason in finish_reasons[:-1]))\n+\n+\n+class ServeCompletionsContinuousBatchingTest(ServeCompletionsMixin, unittest.TestCase):\n+    \"\"\"Tests the `continuous_batching` version of the Completions API.\"\"\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"Starts a server for tests to connect to.\"\"\"\n+        args = ServeArguments(attn_implementation=\"sdpa_paged\")  # important: toggle continuous batching\n+        serve_command = ServeCommand(args)\n+        thread = Thread(target=serve_command.run)\n+        thread.daemon = True\n+        thread.start()"
        }
    ],
    "stats": {
        "total": 373,
        "additions": 320,
        "deletions": 53
    }
}