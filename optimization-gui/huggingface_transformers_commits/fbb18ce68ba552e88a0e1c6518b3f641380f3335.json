{
    "author": "SunMarc",
    "message": "Update config.torch_dtype correctly (#36679)\n\n* fix\n\n* style\n\n* new test",
    "sha": "fbb18ce68ba552e88a0e1c6518b3f641380f3335",
    "files": [
        {
            "sha": "207ddafa971615e6ce915de3c511bf63e3089a9e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbb18ce68ba552e88a0e1c6518b3f641380f3335/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbb18ce68ba552e88a0e1c6518b3f641380f3335/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=fbb18ce68ba552e88a0e1c6518b3f641380f3335",
            "patch": "@@ -1223,10 +1223,12 @@ def _get_torch_dtype(\n                     )\n             elif hasattr(torch, torch_dtype):\n                 torch_dtype = getattr(torch, torch_dtype)\n+                config.torch_dtype = torch_dtype\n                 for sub_config_key in config.sub_configs.keys():\n                     sub_config = getattr(config, sub_config_key)\n                     sub_config.torch_dtype = torch_dtype\n         elif isinstance(torch_dtype, torch.dtype):\n+            config.torch_dtype = torch_dtype\n             for sub_config_key in config.sub_configs.keys():\n                 sub_config = getattr(config, sub_config_key)\n                 sub_config.torch_dtype = torch_dtype"
        },
        {
            "sha": "7d690731477ac41b4ae49ea87840dddcab2e618c",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbb18ce68ba552e88a0e1c6518b3f641380f3335/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbb18ce68ba552e88a0e1c6518b3f641380f3335/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=fbb18ce68ba552e88a0e1c6518b3f641380f3335",
            "patch": "@@ -36,6 +36,7 @@\n     AutoModel,\n     AutoModelForImageClassification,\n     AutoModelForSequenceClassification,\n+    CLIPTextModelWithProjection,\n     DynamicCache,\n     LlavaForConditionalGeneration,\n     MistralForCausalLM,\n@@ -617,6 +618,14 @@ def remove_torch_dtype(model_path):\n         model = AutoModel.from_pretrained(TINY_BERT_FOR_TOKEN_CLASSIFICATION, torch_dtype=\"auto\")\n         self.assertEqual(model.dtype, torch.float32)\n \n+        # test model that init the model with _from_config\n+        model = CLIPTextModelWithProjection.from_pretrained(\n+            \"hf-internal-testing/diffusers-stable-diffusion-tiny-all\",\n+            subfolder=\"text_encoder\",\n+            torch_dtype=torch.bfloat16,\n+        )\n+        self.assertEqual(model.dtype, torch.bfloat16)\n+\n     def test_model_from_pretrained_attn_implementation(self):\n         # test that the model can be instantiated with attn_implementation of either\n         # 1. explicit from_pretrained's attn_implementation argument"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 11,
        "deletions": 0
    }
}