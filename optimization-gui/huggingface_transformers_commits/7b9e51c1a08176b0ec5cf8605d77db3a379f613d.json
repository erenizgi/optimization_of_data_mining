{
    "author": "tibor-reiss",
    "message": "Feature: print tokens per second during training (#34507)\n\n* Log tokens per second during training\r\n\r\n* Nitpicks\r\n\r\n* Move logic into _maybe_log_save_evaluate\r\n\r\n* Use speed_metrics",
    "sha": "7b9e51c1a08176b0ec5cf8605d77db3a379f613d",
    "files": [
        {
            "sha": "129398e374be73a2d5e349c9d91f36c0da658033",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b9e51c1a08176b0ec5cf8605d77db3a379f613d/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b9e51c1a08176b0ec5cf8605d77db3a379f613d/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=7b9e51c1a08176b0ec5cf8605d77db3a379f613d",
            "patch": "@@ -1671,21 +1671,21 @@ def num_examples(self, dataloader: DataLoader) -> int:\n         except (NameError, AttributeError, TypeError):  # no dataset or length, estimate by length of dataloader\n             return len(dataloader) * self.args.per_device_train_batch_size\n \n-    def num_tokens(self, train_dl: DataLoader, max_steps: Optional[int] = None) -> int:\n+    @staticmethod\n+    def num_tokens(train_dl: DataLoader, max_steps: Optional[int] = None) -> int:\n         \"\"\"\n         Helper to get number of tokens in a [`~torch.utils.data.DataLoader`] by enumerating dataloader.\n         \"\"\"\n         train_tokens = 0\n         try:\n-            for step, batch in enumerate(train_dl):\n+            for batch in train_dl:\n                 tokens = batch[\"input_ids\"].numel()\n                 if max_steps is not None:\n                     return tokens * max_steps\n                 train_tokens += tokens\n-            return train_tokens\n         except KeyError:\n             logger.warning(\"Cannot get num_tokens from dataloader\")\n-            return train_tokens\n+        return train_tokens\n \n     def _hp_search_setup(self, trial: Union[\"optuna.Trial\", Dict[str, Any]]):\n         \"\"\"HP search setup code\"\"\"\n@@ -2439,7 +2439,6 @@ def _inner_training_loop(\n             epoch_iterator = iter(epoch_dataloader)\n             # We chunkify the epoch iterator into gradient accumulation steps `n` batches\n             remainder = num_examples % args.gradient_accumulation_steps\n-            num_items_in_batch = None\n             if remainder == 0:\n                 remainder = args.gradient_accumulation_steps\n             update_step = -1\n@@ -2562,7 +2561,9 @@ def _inner_training_loop(\n                         self.state.global_step += 1\n                         self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n                         self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n-                        self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n+                        self._maybe_log_save_evaluate(\n+                            tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time\n+                        )\n                     else:\n                         self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n \n@@ -2587,7 +2588,7 @@ def _inner_training_loop(\n                 self.control.should_training_stop = True\n \n             self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n-            self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n+            self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\n \n             if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                 if is_torch_xla_available():\n@@ -2992,7 +2993,7 @@ def _evaluate(self, trial, ignore_keys_for_eval, skip_scheduler=False):\n                 ) from exc\n         return metrics\n \n-    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval):\n+    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time):\n         if self.control.should_log and self.state.global_step > self._globalstep_last_logged:\n             if is_torch_xla_available():\n                 xm.mark_step()\n@@ -3014,7 +3015,7 @@ def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, igno\n             self._globalstep_last_logged = self.state.global_step\n             self.store_flos()\n \n-            self.log(logs)\n+            self.log(logs, start_time)\n \n         metrics = None\n         if self.control.should_evaluate:\n@@ -3517,7 +3518,7 @@ def hyperparameter_search(\n         self.hp_search_backend = None\n         return best_run\n \n-    def log(self, logs: Dict[str, float]) -> None:\n+    def log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None:\n         \"\"\"\n         Log `logs` on the various objects watching training.\n \n@@ -3526,11 +3527,15 @@ def log(self, logs: Dict[str, float]) -> None:\n         Args:\n             logs (`Dict[str, float]`):\n                 The values to log.\n+            start_time (`Optional[float]`):\n+                The start of training.\n         \"\"\"\n         if self.state.epoch is not None:\n             logs[\"epoch\"] = self.state.epoch\n         if self.args.include_num_input_tokens_seen:\n             logs[\"num_input_tokens_seen\"] = self.state.num_input_tokens_seen\n+            if start_time is not None:\n+                speed_metrics(\"train\", start_time, num_tokens=self.state.num_input_tokens_seen)\n \n         output = {**logs, **{\"step\": self.state.global_step}}\n         self.state.log_history.append(output)"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 15,
        "deletions": 10
    }
}