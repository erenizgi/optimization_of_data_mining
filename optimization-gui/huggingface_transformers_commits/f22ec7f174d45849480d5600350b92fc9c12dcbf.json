{
    "author": "ahadnagy",
    "message": "Benchmarking V2: framework impl (#40486)\n\n* Start revamping benchmarking\n\n* Start refactoring benchmarking\n\n* Use Pandas for CSV\n\n* import fix\n\n* Remove benchmark files\n\n* Remove sample data\n\n* Address review comments\n\n* Benchmarking v2\n\n* Fix llama bench parameters\n\n* Working checkpoint\n\n* Readme touchups\n\n* Remove unnecessary test\n\n* Massage the framework a bit\n\n* Small cleanup\n\n* Remove unnecessary flushes\n\n* Remove references to mock benchmark\n\n* Take commit ID from CLI\n\n* Address review comments\n\n* Use Events for thread comms\n\n* Tiny renaming",
    "sha": "f22ec7f174d45849480d5600350b92fc9c12dcbf",
    "files": [
        {
            "sha": "2f3040f513f24ba696b7d457130dfbf0cb876b53",
            "filename": "benchmark_v2/.gitignore",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2F.gitignore",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2F.gitignore",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2F.gitignore?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1 @@\n+benchmark_results/\n\\ No newline at end of file"
        },
        {
            "sha": "9a0102b387fce113d5a06071d00a66f93c4bf170",
            "filename": "benchmark_v2/README.md",
            "status": "added",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2FREADME.md?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1,98 @@\n+# Benchmarking v2\n+\n+A comprehensive benchmarking framework for transformer models that supports multiple execution modes (eager, compiled, kernelized), detailed performance metrics collection, and structured output format.\n+\n+\n+## Quick Start\n+\n+### Running All Benchmarks\n+\n+```bash\n+# Run all benchmarks with default settings\n+python run_benchmarks.py\n+\n+# Specify output directory\n+python run_benchmarks.py --output-dir my_results\n+\n+# Run with custom parameters\n+python run_benchmarks.py \\\n+    --warmup-iterations 5 \\\n+    --measurement-iterations 10 \\\n+    --num-tokens-to-generate 200\n+```\n+\n+### Running Specific Benchmarks\n+\n+```bash\n+# Include only specific benchmarks\n+python run_benchmarks.py --include llama\n+\n+# Exclude specific benchmarks\n+python run_benchmarks.py --exclude old_benchmark\n+\n+## Output Format\n+\n+Results are saved as JSON files with the following structure:\n+\n+```json\n+{\n+  \"model_name\": \"llama_2_7b\",\n+  \"benchmark_scenarios\": [\n+    {\n+      \"scenario_name\": \"eager_variant\",\n+      \"metadata\": {\n+        \"timestamp\": \"2025-01-XX...\",\n+        \"commit_id\": \"abc123...\",\n+        \"hardware_info\": {\n+          \"gpu_name\": \"NVIDIA A100\",\n+          \"gpu_memory_total\": 40960,\n+          \"cpu_count\": 64\n+        },\n+        \"config\": {\n+          \"variant\": \"eager\",\n+          \"warmup_iterations\": 3,\n+          \"measurement_iterations\": 5\n+        }\n+      },\n+      \"measurements\": {\n+        \"latency\": {\n+          \"mean\": 2.45,\n+          \"median\": 2.43,\n+          \"std\": 0.12,\n+          \"min\": 2.31,\n+          \"max\": 2.67,\n+          \"p95\": 2.61,\n+          \"p99\": 2.65\n+        },\n+        \"time_to_first_token\": {\n+          \"mean\": 0.15,\n+          \"std\": 0.02\n+        },\n+        \"tokens_per_second\": {\n+          \"mean\": 87.3,\n+          \"unit\": \"tokens/sec\"\n+        }\n+      },\n+      \"gpu_metrics\": {\n+        \"gpu_utilization_mean\": 85.2,\n+        \"gpu_memory_used_mean\": 12450\n+      }\n+    }\n+  ]\n+}\n+```\n+\n+### Debug Mode\n+\n+```bash\n+python run_benchmarks.py --log-level DEBUG\n+```\n+\n+## Contributing\n+\n+To add new benchmarks:\n+\n+1. Create a new file in `benches/`\n+2. Implement the `ModelBenchmark` interface\n+3. Add a runner function (`run_<benchmark_name>` or `run_benchmark`)\n+4. run_benchmarks.py\n\\ No newline at end of file"
        },
        {
            "sha": "6e70a5add84ada32e3b8fe5031ad606335d56c10",
            "filename": "benchmark_v2/benches/__init__.py",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Fbenches%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Fbenches%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenches%2F__init__.py?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1 @@\n+# Benchmark implementations directory \n\\ No newline at end of file"
        },
        {
            "sha": "7075f5834c050642e9241fbca2e6aef0f348dd12",
            "filename": "benchmark_v2/benches/llama.py",
            "status": "added",
            "additions": 156,
            "deletions": 0,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenches%2Fllama.py?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1,156 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+import logging\n+from typing import Dict, Any, List\n+\n+from benchmark_framework import ModelBenchmark\n+\n+import torch\n+\n+os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n+os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n+torch.set_float32_matmul_precision(\"high\")\n+\n+class LLaMABenchmark(ModelBenchmark):\n+    \"\"\"Simplified LLaMA model benchmark implementation using the ModelBenchmark base class.\"\"\"\n+    \n+    def __init__(self, logger: logging.Logger):\n+        super().__init__(logger)\n+        self._default_prompt = \"Why dogs are so cute?\"  # Custom prompt for LLaMA\n+    \n+\n+    \n+    def get_scenario_configs(self) -> List[Dict[str, Any]]:\n+        \"\"\"\n+        Get LLaMA-specific scenario configurations.\n+        \n+        Returns:\n+            List of scenario configuration dictionaries\n+        \"\"\"\n+        return [\n+            # Eager variants\n+            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n+            \n+            # Compiled variants\n+            {\"variant\": \"compiled\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Compiled with max autotune\"},\n+            \n+            # Kernelized variant (if available)\n+            {\"variant\": \"kernelized\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Kernelized execution\"},\n+        ]\n+    \n+    def _is_kernelization_available(self) -> bool:\n+        \"\"\"Check if kernelization is available for LLaMA.\"\"\"\n+        try:\n+            from kernels import Mode, kernelize\n+            return True\n+        except ImportError:\n+            self.logger.debug(\"Kernelization not available: kernels module not found\")\n+            return False\n+    \n+    def get_default_generation_config(self) -> Dict[str, Any]:\n+        \"\"\"Get LLaMA-specific generation configuration.\"\"\"\n+        return {\n+            \"do_sample\": False,\n+            \"top_p\": 1.0,\n+            \"temperature\": 1.0,\n+            \"repetition_penalty\": 1.0,\n+            \"max_new_tokens\": None,  # Will be set per scenario\n+        }\n+    \n+    def get_model_init_kwargs(self, config) -> Dict[str, Any]:\n+        \"\"\"Get LLaMA-specific model initialization kwargs.\"\"\"\n+        from benchmark_framework import BenchmarkConfig\n+        return {\n+            \"torch_dtype\": getattr(torch, config.torch_dtype),\n+            \"attn_implementation\": config.attn_implementation,\n+            \"use_cache\": True,\n+        }\n+    \n+    def get_default_torch_dtype(self) -> str:\n+        \"\"\"Get default torch dtype for LLaMA.\"\"\"\n+        return \"float16\"  # LLaMA works well with float16\n+    \n+    def get_default_device(self) -> str:\n+        \"\"\"Get default device for LLaMA.\"\"\"\n+        return \"cuda\"  # LLaMA prefers CUDA\n+\n+\n+def run_llama(logger, output_dir, **kwargs):\n+    \"\"\"\n+    Run LLaMA benchmark with the given configuration.\n+    \n+    Args:\n+        logger: Logger instance\n+        output_dir: Output directory for results\n+        **kwargs: Additional configuration options\n+        \n+    Returns:\n+        Path to output file if successful\n+    \"\"\"\n+    from benchmark_framework import BenchmarkRunner\n+    \n+    # Extract parameters with defaults\n+    model_id = kwargs.get('model_id', 'meta-llama/Llama-2-7b-hf')\n+    warmup_iterations = kwargs.get('warmup_iterations', 3)\n+    measurement_iterations = kwargs.get('measurement_iterations', 5)\n+    num_tokens_to_generate = kwargs.get('num_tokens_to_generate', 100)\n+    include_sdpa_variants = kwargs.get('include_sdpa_variants', True)\n+    device = kwargs.get('device', 'cuda')\n+    torch_dtype = kwargs.get('torch_dtype', 'float16')\n+    batch_size = kwargs.get('batch_size', 1)\n+    commit_id = kwargs.get('commit_id', None)\n+    \n+    logger.info(f\"Starting LLaMA benchmark for model: {model_id}\")\n+    logger.info(f\"Configuration: warmup={warmup_iterations}, measurement={measurement_iterations}, tokens={num_tokens_to_generate}\")\n+    \n+    try:\n+        # Create benchmark instance\n+        benchmark = LLaMABenchmark(logger)\n+        \n+        # Create scenarios\n+        scenarios = benchmark.create_scenarios(\n+            model_id=model_id,\n+            warmup_iterations=warmup_iterations,\n+            measurement_iterations=measurement_iterations,\n+            num_tokens_to_generate=num_tokens_to_generate,\n+            include_sdpa_variants=include_sdpa_variants,\n+            device=device,\n+            torch_dtype=torch_dtype,\n+            batch_size=batch_size\n+        )\n+        \n+        logger.info(f\"Created {len(scenarios)} benchmark scenarios\")\n+        \n+        # Create runner and execute benchmarks\n+        runner = BenchmarkRunner(logger, output_dir)\n+        results = runner.run_benchmark(benchmark, scenarios, commit_id=commit_id)\n+        \n+        if not results:\n+            logger.warning(\"No successful benchmark results\")\n+            return None\n+        \n+        # Save results\n+        model_name = model_id.split('/')[-1]  # Extract model name from ID\n+        output_file = runner.save_results(model_name, results)\n+        \n+        logger.info(f\"LLaMA benchmark completed successfully. Results saved to: {output_file}\")\n+        return output_file\n+        \n+    except Exception as e:\n+        logger.error(f\"LLaMA benchmark failed: {e}\")\n+        import traceback\n+        logger.debug(traceback.format_exc())\n+        raise\n\\ No newline at end of file"
        },
        {
            "sha": "f152c28c15f0532d3d98eb60309cb796966912cf",
            "filename": "benchmark_v2/benchmark_framework.py",
            "status": "added",
            "additions": 1204,
            "deletions": 0,
            "changes": 1204,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Fbenchmark_framework.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Fbenchmark_framework.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenchmark_framework.py?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1,1204 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import json\n+import os\n+import subprocess\n+import sys\n+import time\n+import statistics\n+import threading\n+from abc import ABC, abstractmethod\n+from contextlib import nullcontext\n+from dataclasses import dataclass, field, asdict\n+from datetime import datetime\n+from typing import Any, Callable, Dict, List, Optional, Union, TypedDict\n+import logging\n+\n+import numpy as np\n+import psutil\n+import gpustat\n+\n+import torch\n+\n+\n+class GPUMetrics(TypedDict):\n+    \"\"\"GPU monitoring result with GPU metrics.\"\"\"\n+    gpu_utilization_mean: float\n+    gpu_utilization_max: float\n+    gpu_utilization_min: float\n+    gpu_memory_used_mean: float\n+    gpu_memory_used_max: float\n+    gpu_memory_used_min: float\n+    sample_count: int\n+    gpu_monitoring_status: str\n+\n+\n+class NoGPU(TypedDict):\n+    \"\"\"GPU monitoring result without GPU metrics.\"\"\"\n+    gpu_monitoring_status: str\n+    gpu_monitoring_reason: str\n+\n+\n+class ArchAwareTimer:\n+    \"\"\"Architecture-aware timer for supposedly better prescision\"\"\"\n+    \n+    def __init__(self, device: Optional[str] = None):\n+        \"\"\"\n+        Initialize architecture-aware timer.\n+        \n+        Args:\n+            device: Device to use. If None, uses current device.\n+        \"\"\"\n+        self.device = device\n+        self.use_cuda = torch.cuda.is_available()\n+        \n+        if self.use_cuda:\n+            if device and device != \"cpu\":\n+                self.device_obj = torch.device(device)\n+            else:\n+                # Fall back to CPU timing if device is CPU or CUDA not available\n+                self.use_cuda = False\n+        \n+        if self.use_cuda:\n+            try:\n+                # Create CUDA events for timing\n+                self.start_event = torch.cuda.Event(enable_timing=True)\n+                self.end_event = torch.cuda.Event(enable_timing=True)\n+            except RuntimeError:\n+                # Fall back to CPU timing if CUDA events fail\n+                self.use_cuda = False\n+        \n+        if not self.use_cuda:\n+            self.start_time = None\n+            self.end_time = None\n+    \n+    def start(self):\n+        \"\"\"Start timing.\"\"\"\n+        if self.use_cuda:\n+            torch.cuda.synchronize(self.device_obj)\n+            self.start_event.record(stream=torch.cuda.current_stream(self.device_obj))\n+        else:\n+            self.start_time = time.perf_counter()\n+    \n+    def stop(self):\n+        \"\"\"Stop timing.\"\"\"\n+        if self.use_cuda:\n+            self.end_event.record(stream=torch.cuda.current_stream(self.device_obj))\n+            torch.cuda.synchronize(self.device_obj)\n+        else:\n+            self.end_time = time.perf_counter()\n+    \n+    def elapsed_time(self) -> float:\n+        \"\"\"\n+        Get elapsed time in seconds.\n+        \n+        Returns:\n+            Elapsed time in seconds\n+        \"\"\"\n+        if self.use_cuda:\n+            # CUDA events return time in milliseconds, convert to seconds\n+            return self.start_event.elapsed_time(self.end_event) / 1000.0\n+        else:\n+            if self.start_time is None or self.end_time is None:\n+                raise RuntimeError(\"Timer not properly started/stopped\")\n+            return self.end_time - self.start_time\n+    \n+    @property\n+    def timing_method(self) -> str:\n+        \"\"\"Get the timing method being used.\"\"\"\n+        return \"CUDA Events\" if self.use_cuda else \"CPU perf_counter\"\n+    \n+    def __enter__(self):\n+        \"\"\"Context manager entry.\"\"\"\n+        self.start()\n+        return self\n+    \n+    def __exit__(self, exc_type, exc_val, exc_tb):\n+        \"\"\"Context manager exit.\"\"\"\n+        self.stop()\n+\n+\n+@dataclass\n+class BenchmarkConfig:\n+    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n+    name: str\n+    model_id: str\n+    variant: str = \"eager\"  # \"eager\", \"compiled\", \"kernelized\"\n+    warmup_iterations: int = 3\n+    measurement_iterations: int = 10\n+    num_tokens_to_generate: int = 100\n+    device: str = \"cuda\"\n+    torch_dtype: str = \"float16\"\n+    compile_mode: Optional[str] = None  # None, \"default\", \"reduce-overhead\", \"max-autotune\"\n+    compile_options: Dict[str, Any] = field(default_factory=dict)\n+    use_cache: bool = True\n+    batch_size: int = 1\n+    sequence_length: Optional[int] = None\n+    attn_implementation: str = \"sdpa\"  # \"eager\", \"sdpa\", \"flash_attention_2\"\n+    sdpa_backend: Optional[str] = None  # None, \"math\", \"flash_attention\", \"efficient_attention\", \"cudnn_attention\"\n+    custom_params: Dict[str, Any] = field(default_factory=dict)\n+\n+\n+class BenchmarkScenario:\n+    \"\"\"\n+    A benchmark scenario that encapsulates both configuration and setup logic.\n+    This makes it easier to define and adapt benchmarks for different models.\n+    \"\"\"\n+    \n+    def __init__(self, name: str, config: BenchmarkConfig, description: str = \"\"):\n+        self.name = name\n+        self.config = config\n+        self.description = description\n+        self._setup_callbacks = []\n+        self._teardown_callbacks = []\n+    \n+    def add_setup_callback(self, callback: callable):\n+        \"\"\"Add a callback to be executed during scenario setup.\"\"\"\n+        self._setup_callbacks.append(callback)\n+    \n+    def add_teardown_callback(self, callback: callable):\n+        \"\"\"Add a callback to be executed during scenario teardown.\"\"\"\n+        self._teardown_callbacks.append(callback)\n+    \n+    def setup(self, model, tokenizer, logger=None):\n+        \"\"\"Execute setup callbacks for this scenario.\"\"\"\n+        for callback in self._setup_callbacks:\n+            try:\n+                callback(model, tokenizer, self.config, logger)\n+            except Exception as e:\n+                if logger:\n+                    logger.warning(f\"Setup callback failed for scenario {self.name}: {e}\")\n+    \n+    def teardown(self, model, tokenizer, logger=None):\n+        \"\"\"Execute teardown callbacks for this scenario.\"\"\"\n+        for callback in self._teardown_callbacks:\n+            try:\n+                callback(model, tokenizer, self.config, logger)\n+            except Exception as e:\n+                if logger:\n+                    logger.warning(f\"Teardown callback failed for scenario {self.name}: {e}\")\n+    \n+    def __repr__(self):\n+        return f\"BenchmarkScenario(name='{self.name}', variant='{self.config.variant}')\"\n+\n+\n+\n+\n+@dataclass\n+class TimingResult:\n+    \"\"\"Result from a timing measurement.\"\"\"\n+    time_to_first_token_seconds: Optional[float] = None\n+    latency_seconds: float = 0.0\n+    tokens_per_second: Optional[float] = None\n+    time_per_output_token_seconds: Optional[float] = None\n+    total_tokens_generated: int = 0\n+    metadata: Dict[str, Any] = field(default_factory=dict)\n+\n+\n+@dataclass\n+class BenchmarkStatistics:\n+    \"\"\"Statistical analysis of benchmark measurements.\"\"\"\n+    name: str\n+    measurements: List[float]\n+    mean: float\n+    median: float\n+    std: float\n+    min: float\n+    max: float\n+    p25: float  # 25th percentile\n+    p75: float  # 75th percentile\n+    p90: float  # 90th percentile\n+    p95: float  # 95th percentile\n+    p99: float  # 99th percentile\n+    unit: str = \"seconds\"\n+\n+    @classmethod\n+    def from_measurements(cls, name: str, measurements: List[float], unit: str = \"seconds\") -> 'BenchmarkStatistics':\n+        \"\"\"Create statistics from a list of measurements.\"\"\"\n+        if not measurements:\n+            raise ValueError(\"Cannot create statistics from empty measurements\")\n+        \n+        measurements_array = np.array(measurements)\n+        \n+        return cls(\n+            name=name,\n+            measurements=measurements,\n+            mean=float(np.mean(measurements_array)),\n+            median=float(np.median(measurements_array)),\n+            std=float(np.std(measurements_array)),\n+            min=float(np.min(measurements_array)),\n+            max=float(np.max(measurements_array)),\n+            p25=float(np.percentile(measurements_array, 25)),\n+            p75=float(np.percentile(measurements_array, 75)),\n+            p90=float(np.percentile(measurements_array, 90)),\n+            p95=float(np.percentile(measurements_array, 95)),\n+            p99=float(np.percentile(measurements_array, 99)),\n+            unit=unit\n+        )\n+\n+\n+@dataclass \n+class HardwareInfo:\n+    \"\"\"Hardware information collected during benchmarking.\"\"\"\n+    gpu_name: str\n+    gpu_memory_total_mb: int\n+    cpu_count: int\n+    memory_total_mb: int\n+    python_version: str\n+    torch_version: Optional[str] = None\n+    cuda_version: Optional[str] = None\n+\n+\n+@dataclass\n+class BenchmarkMetadata:\n+    \"\"\"Metadata collected for each benchmark run.\"\"\"\n+    timestamp: str\n+    commit_id: str\n+    hardware_info: HardwareInfo\n+    config: BenchmarkConfig\n+\n+\n+class GPUMonitor:\n+    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n+    \n+    def __init__(self, sample_interval: float = 0.1, logger: logging.Logger = None):\n+        self.sample_interval = sample_interval\n+        self.logger = logger or logging.getLogger(__name__)\n+        self.stop_event = threading.Event()\n+        self.thread = None\n+        self.gpu_utilization = []\n+        self.gpu_memory_used = []\n+        self.timestamps = []\n+        self.gpu_available = False\n+        self.warning_logged = False\n+        \n+        # Test GPU availability on initialization\n+        self._test_gpu_availability()\n+        \n+    def _test_gpu_availability(self):\n+        \"\"\"Test if GPU monitoring is available.\"\"\"\n+        try:\n+            gpu_stats = gpustat.GPUStatCollection.new_query()\n+            if gpu_stats and len(gpu_stats) > 0:\n+                self.gpu_available = True\n+                self.logger.debug(f\"GPU monitoring available: {len(gpu_stats)} GPU(s) detected\")\n+            else:\n+                self.gpu_available = False\n+                self.logger.debug(\"No GPUs detected by gpustat\")\n+        except Exception as e:\n+            self.gpu_available = False\n+            self.logger.debug(f\"GPU monitoring not available: {e}\")\n+        \n+    def start(self):\n+        \"\"\"Start monitoring GPU metrics.\"\"\"\n+        if not self.gpu_available:\n+            self.logger.debug(\"GPU monitoring disabled: no GPUs available\")\n+            return\n+            \n+        # Clear the stop event to enable monitoring\n+        self.stop_event.clear()\n+        self.gpu_utilization = []\n+        self.gpu_memory_used = []\n+        self.timestamps = []\n+        self.warning_logged = False  # Reset warning flag for new monitoring session\n+        self.thread = threading.Thread(target=self._monitor_loop)\n+        self.thread.start()\n+        self.logger.debug(\"GPU monitoring started\")\n+        \n+    def stop_and_collect(self) -> Union[GPUMetrics, NoGPU]:\n+        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n+        if not self.gpu_available:\n+            return NoGPU(\n+                gpu_monitoring_status=\"disabled\",\n+                gpu_monitoring_reason=\"no_gpus_available\"\n+            )\n+            \n+        # Signal the monitoring thread to stop\n+        self.stop_event.set()\n+        if self.thread:\n+            self.thread.join()\n+        \n+        if self.gpu_utilization:\n+            metrics = GPUMetrics(\n+                gpu_utilization_mean=statistics.mean(self.gpu_utilization),\n+                gpu_utilization_max=max(self.gpu_utilization),\n+                gpu_utilization_min=min(self.gpu_utilization),\n+                gpu_memory_used_mean=statistics.mean(self.gpu_memory_used),\n+                gpu_memory_used_max=max(self.gpu_memory_used),\n+                gpu_memory_used_min=min(self.gpu_memory_used),\n+                sample_count=len(self.gpu_utilization),\n+                gpu_monitoring_status=\"success\"\n+            )\n+            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n+            return metrics\n+        else:\n+            return NoGPU(\n+                gpu_monitoring_status=\"failed\",\n+                gpu_monitoring_reason=\"no_samples_collected\"\n+            )\n+    \n+    def _monitor_loop(self):\n+        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n+        consecutive_failures = 0\n+        max_consecutive_failures = 5\n+        \n+        # Continue monitoring until stop_event is set\n+        while not self.stop_event.is_set():\n+            try:\n+                gpu_stats = gpustat.GPUStatCollection.new_query()\n+                if gpu_stats and len(gpu_stats) > 0:\n+                    gpu = gpu_stats[0]\n+                    self.gpu_utilization.append(gpu[\"utilization.gpu\"])\n+                    self.gpu_memory_used.append(gpu[\"memory.used\"])\n+                    self.timestamps.append(time.time())\n+                    consecutive_failures = 0  # Reset failure counter on success\n+                else:\n+                    consecutive_failures += 1\n+                    if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n+                        self.logger.warning(\"GPU monitoring: No GPU data returned by gpustat\")\n+                        self.warning_logged = True\n+                        \n+            except Exception as e:\n+                consecutive_failures += 1\n+                if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n+                    self.logger.warning(f\"GPU monitoring failed after {max_consecutive_failures} attempts: {e}\")\n+                    self.warning_logged = True\n+            \n+            # Use Event.wait() with timeout instead of time.sleep()\n+            # This allows for immediate response to stop signal while still maintaining sample interval\n+            if self.stop_event.wait(timeout=self.sample_interval):\n+                # Event was set, break out of loop immediately\n+                break\n+\n+\n+def get_hardware_info() -> HardwareInfo:\n+    \"\"\"Collect hardware information.\"\"\"\n+    gpu_name = \"unknown\"\n+    gpu_memory_total = 0\n+    \n+    try:\n+        gpu_stats = gpustat.GPUStatCollection.new_query()\n+        if gpu_stats and len(gpu_stats) > 0:\n+            gpu = gpu_stats[0]\n+            gpu_name = gpu[\"name\"]\n+            gpu_memory_total = gpu[\"memory.total\"]\n+    except Exception:\n+        pass\n+    \n+    torch_version = torch.__version__\n+    cuda_version = None\n+    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n+        cuda_version = torch.version.cuda\n+    \n+    return HardwareInfo(\n+        gpu_name=gpu_name,\n+        gpu_memory_total_mb=gpu_memory_total,\n+        cpu_count=psutil.cpu_count(),\n+        memory_total_mb=int(psutil.virtual_memory().total / (1024 * 1024)),\n+        python_version=f\"{sys.version.split()[0]}\",\n+        torch_version=torch_version,\n+        cuda_version=cuda_version\n+    )\n+\n+\n+def flush_memory():\n+    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n+    gc.collect()\n+    if hasattr(torch, 'cuda') and torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+        torch.cuda.reset_max_memory_allocated()\n+        torch.cuda.reset_peak_memory_stats()\n+        torch.cuda.synchronize()\n+\n+\n+def get_sdpa_backend(backend_name: Optional[str]):\n+    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n+    if backend_name is None:\n+        return None\n+    \n+    try:\n+        backend_map = {\n+            \"math\": torch.nn.attention.SDPBackend.MATH,\n+            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n+            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n+            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n+        }\n+        return backend_map.get(backend_name.lower())\n+    except AttributeError:\n+        # torch.nn.attention.SDPBackend not available in older torch versions\n+        return None\n+\n+\n+\n+\n+\n+class SDPAContext:\n+    \"\"\"Context manager for SDPA kernel selection.\"\"\"\n+    \n+    def __init__(self, backend_name: Optional[str], logger: logging.Logger = None):\n+        self.backend_name = backend_name\n+        self.logger = logger or logging.getLogger(__name__)\n+        self.backend = get_sdpa_backend(backend_name) if backend_name else None\n+        self.context = None\n+        \n+    def __enter__(self):\n+        if self.backend is not None:\n+            try:\n+                self.context = torch.nn.attention.sdpa_kernel(self.backend)\n+                self.context.__enter__()\n+                if self.logger:\n+                    self.logger.debug(f\"Using SDPA backend: {self.backend_name}\")\n+            except Exception as e:\n+                if self.logger:\n+                    self.logger.warning(f\"Failed to set SDPA backend {self.backend_name}: {e}\")\n+                self.context = None\n+        elif self.backend_name and self.logger:\n+            self.logger.debug(f\"SDPA backend '{self.backend_name}' requested but not using kernel context (backend={self.backend})\")\n+        return self\n+        \n+    def __exit__(self, exc_type, exc_val, exc_tb):\n+        if self.context is not None:\n+            try:\n+                self.context.__exit__(exc_type, exc_val, exc_tb)\n+            except Exception as e:\n+                if self.logger:\n+                    self.logger.warning(f\"Error exiting SDPA context: {e}\")\n+        return False\n+\n+\n+class AbstractModelBenchmark(ABC):\n+    \"\"\"Abstract base class for model benchmarks.\"\"\"\n+    \n+    def __init__(self, logger: logging.Logger):\n+        self.logger = logger\n+        self.model = None\n+        self.tokenizer = None\n+        self.device = None\n+        self.scenarios = {}  # Map of scenario_name -> BenchmarkScenario\n+        \n+    @abstractmethod\n+    def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n+        \"\"\"Create and return a dictionary of benchmark scenarios.\"\"\"\n+        pass\n+        \n+    @abstractmethod\n+    def setup_model(self, config: BenchmarkConfig) -> None:\n+        \"\"\"Setup the model for benchmarking with the given configuration.\"\"\"\n+        pass\n+    \n+    @abstractmethod\n+    def cleanup_model(self) -> None:\n+        \"\"\"Cleanup model resources.\"\"\"\n+        pass\n+    \n+    @abstractmethod\n+    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n+        \"\"\"Measure time to first token generation.\"\"\"\n+        pass\n+    \n+    @abstractmethod\n+    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n+        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n+        pass\n+    \n+    def prepare_inputs(self, config: BenchmarkConfig) -> Any:\n+        \"\"\"Prepare inputs for the model. Override if needed.\"\"\"\n+        return None\n+    \n+    def get_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n+        \"\"\"Get benchmark scenarios. Creates them if they don't exist.\"\"\"\n+        if not self.scenarios:\n+            self.scenarios = self.create_scenarios(**kwargs)\n+        return self.scenarios\n+\n+\n+class ModelBenchmark(AbstractModelBenchmark):\n+    \"\"\"\n+    Base class for HuggingFace Transformers model benchmarks.\n+    \n+    This class provides common scenario creation logic and handles the standard\n+    patterns for eager, compiled, and kernelized execution variants with different\n+    attention implementations and SDPA backends.\n+    \"\"\"\n+    \n+    def __init__(self, logger: logging.Logger):\n+        super().__init__(logger)\n+        self.inputs = None\n+        self.compiled_model = None\n+        self.past_key_values = None\n+        self.config = None\n+        self._default_prompt = \"Why dogs are so cute?\"\n+        \n+    @property\n+    def default_prompt(self) -> str:\n+        \"\"\"Default prompt for text generation. Override in subclasses if needed.\"\"\"\n+        return self._default_prompt\n+    \n+\n+    \n+    def get_attention_configs(self, include_sdpa_variants: bool = True) -> List[Dict[str, Any]]:\n+        \"\"\"\n+        Get attention implementation configurations.\n+        \n+        Args:\n+            include_sdpa_variants: Whether to include SDPA backend variants\n+            \n+        Returns:\n+            List of attention configuration dictionaries\n+        \"\"\"\n+        attention_configs = [\n+            {\"attn_implementation\": \"eager\", \"sdpa_backends\": [None], \"desc_suffix\": \" with eager attention\"},\n+        ]\n+        \n+        # Add SDPA variants if requested\n+        if include_sdpa_variants:\n+            attention_configs.append({\n+                \"attn_implementation\": \"sdpa\", \n+                \"sdpa_backends\": [None, \"math\", \"flash_attention\", \"efficient_attention\"],\n+                \"desc_suffix\": \"\"\n+            })\n+        \n+        return attention_configs\n+    \n+    def get_scenario_configs(self) -> List[Dict[str, Any]]:\n+        \"\"\"\n+        Get base scenario configurations. Override in subclasses to customize.\n+        \n+        Returns:\n+            List of scenario configuration dictionaries\n+        \"\"\"\n+        return [\n+            # Eager variants\n+            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n+            \n+            # Compiled variants\n+            {\"variant\": \"compiled\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Compiled with max autotune\"},\n+            \n+            # Kernelized variant (if available)\n+            {\"variant\": \"kernelized\", \"compile_mode\": \"max-autotune\", \"use_cache\": True, \"description\": \"Kernelized execution\"},\n+        ]\n+    \n+    def _is_kernelization_available(self) -> bool:\n+        \"\"\"Check if kernelization is available. Override in subclasses.\"\"\"\n+        try:\n+            from kernels import Mode, kernelize\n+            return True\n+        except ImportError:\n+            return False\n+    \n+    def get_default_generation_config(self) -> Dict[str, Any]:\n+        \"\"\"Get default generation configuration. Override in subclasses for model-specific defaults.\"\"\"\n+        return {\n+            \"do_sample\": False,\n+            \"top_p\": 1.0,\n+            \"temperature\": 1.0\n+        }\n+    \n+    def get_model_init_kwargs(self, config: BenchmarkConfig) -> Dict[str, Any]:\n+        \"\"\"Get model initialization kwargs. Override in subclasses for model-specific parameters.\"\"\"\n+        return {\n+            \"torch_dtype\": getattr(torch, config.torch_dtype),\n+            \"attn_implementation\": config.attn_implementation\n+        }\n+    \n+    def get_default_torch_dtype(self) -> str:\n+        \"\"\"Get default torch dtype. Override in subclasses.\"\"\"\n+        return \"float16\"\n+    \n+    def get_default_device(self) -> str:\n+        \"\"\"Get default device. Override in subclasses.\"\"\"\n+        return \"cuda\"\n+    \n+    def create_scenarios(self, **kwargs) -> Dict[str, 'BenchmarkScenario']:\n+        \"\"\"Create benchmark scenarios for HuggingFace models.\"\"\"\n+        scenarios = {}\n+        \n+        # Extract parameters with model-specific defaults\n+        model_id = kwargs.get('model_id', 'microsoft/DialoGPT-medium')\n+        warmup_iterations = kwargs.get('warmup_iterations', 3)\n+        measurement_iterations = kwargs.get('measurement_iterations', 5)\n+        num_tokens_to_generate = kwargs.get('num_tokens_to_generate', 100)\n+        include_sdpa_variants = kwargs.get('include_sdpa_variants', True)\n+        device = kwargs.get('device', self.get_default_device())\n+        torch_dtype = kwargs.get('torch_dtype', self.get_default_torch_dtype())\n+        batch_size = kwargs.get('batch_size', 1)\n+        \n+        # Get configurations\n+        attention_configs = self.get_attention_configs(include_sdpa_variants)\n+        scenario_configs = self.get_scenario_configs()\n+        \n+        # Create scenarios for each attention config and variant combination\n+        for attn_config in attention_configs:\n+            attn_implementation = attn_config[\"attn_implementation\"]\n+            sdpa_backends = attn_config[\"sdpa_backends\"]\n+            desc_suffix = attn_config[\"desc_suffix\"]\n+            \n+            for scenario_config in scenario_configs:\n+                for sdpa_backend in sdpa_backends:\n+                    # Skip kernelized if not available\n+                    if scenario_config[\"variant\"] == \"kernelized\" and not self._is_kernelization_available():\n+                        continue\n+                    \n+                    # Create unique config for this scenario\n+                    config = BenchmarkConfig(\n+                        name=scenario_config['variant'],\n+                        model_id=model_id,\n+                        variant=scenario_config[\"variant\"],\n+                        compile_mode=scenario_config[\"compile_mode\"],\n+                        use_cache=scenario_config[\"use_cache\"],\n+                        warmup_iterations=warmup_iterations,\n+                        measurement_iterations=measurement_iterations,\n+                        num_tokens_to_generate=num_tokens_to_generate,\n+                        device=device,\n+                        torch_dtype=torch_dtype,\n+                        batch_size=batch_size,\n+                        attn_implementation=attn_implementation,\n+                        sdpa_backend=sdpa_backend if attn_implementation == \"sdpa\" else None\n+                    )\n+                    \n+                    # Create scenario name\n+                    scenario_name_parts = [scenario_config[\"variant\"]]\n+                    if scenario_config[\"compile_mode\"]:\n+                        scenario_name_parts.append(f\"compile_{scenario_config['compile_mode']}\")\n+                    \n+                    # Add attention implementation to name\n+                    if attn_implementation == \"eager\":\n+                        scenario_name_parts.append(\"eager_attn\")\n+                    elif attn_implementation == \"sdpa\":\n+                        if sdpa_backend:\n+                            scenario_name_parts.append(f\"sdpa_{sdpa_backend}\")\n+                        else:\n+                            scenario_name_parts.append(\"sdpa_default\")\n+                    \n+                    scenario_name = \"_\".join(scenario_name_parts)\n+                    \n+                    # Create description\n+                    description = scenario_config[\"description\"]\n+                    if attn_implementation == \"sdpa\" and sdpa_backend:\n+                        description += f\" with SDPA {sdpa_backend} backend\"\n+                    elif attn_implementation == \"sdpa\":\n+                        description += \" with SDPA default backend\"\n+                    else:\n+                        description += desc_suffix\n+                    \n+                    # Create scenario\n+                    scenario = BenchmarkScenario(\n+                        name=scenario_name,\n+                        config=config,\n+                        description=description\n+                    )\n+                    \n+                    # Add setup callbacks based on variant\n+                    if scenario_config[\"variant\"] == \"compiled\":\n+                        scenario.add_setup_callback(self._setup_compilation_callback)\n+                    elif scenario_config[\"variant\"] == \"kernelized\":\n+                        scenario.add_setup_callback(self._setup_kernelization_callback)\n+                    \n+                    scenarios[scenario_name] = scenario\n+        \n+        return scenarios\n+    \n+    def _setup_compilation_callback(self, model, tokenizer, config, logger):\n+        \"\"\"Setup callback for compilation scenarios.\"\"\"\n+        if logger:\n+            logger.info(f\"Setting up compilation with mode: {config.compile_mode}\")\n+        \n+        # Perform torch.compile\n+        if config.compile_mode is not None:\n+            self.compiled_model = torch.compile(\n+                model, \n+                mode=config.compile_mode, \n+                **config.compile_options\n+            )\n+        else:\n+            self.compiled_model = torch.compile(model, **config.compile_options)\n+        \n+        # Setup static cache for compiled mode if needed\n+        if config.use_cache and hasattr(self, 'inputs') and self.inputs is not None:\n+            self._setup_static_cache(config)\n+    \n+    def _setup_kernelization_callback(self, model, tokenizer, config, logger):\n+        \"\"\"Setup callback for kernelization scenarios.\"\"\" \n+        if logger:\n+            logger.info(\"Setting up kernelization\")\n+        \n+        try:\n+            from kernels import Mode, kernelize\n+            self.compiled_model = kernelize(\n+                model,\n+                mode=Mode.INFERENCE\n+            )\n+        except Exception as e:\n+            if logger:\n+                logger.warning(f\"Failed to setup kernelized mode: {e}\")\n+                logger.warning(\"Falling back to eager mode\")\n+            config.variant = \"eager\"\n+    \n+    def _setup_static_cache(self, config: BenchmarkConfig):\n+        \"\"\"Setup static cache for compiled models. Override if needed.\"\"\"\n+        if hasattr(self, 'inputs') and self.inputs is not None:\n+            try:\n+                from transformers import StaticCache\n+                seq_length = self.inputs[\"input_ids\"].shape[1]\n+                \n+                # Get the actual device the model is on\n+                if hasattr(self.model, 'device'):\n+                    cache_device = self.model.device\n+                else:\n+                    cache_device = self.device\n+                \n+                self.past_key_values = StaticCache(\n+                    config=self.model.config,\n+                    max_batch_size=config.batch_size,\n+                    max_cache_len=seq_length + config.num_tokens_to_generate,\n+                    device=cache_device,\n+                    dtype=getattr(torch, config.torch_dtype)\n+                )\n+                self.logger.debug(f\"StaticCache created on device: {cache_device}\")\n+            except (ImportError, TypeError) as e:\n+                # StaticCache not available or incompatible, continue without it\n+                self.logger.debug(f\"StaticCache setup failed: {e}, continuing without cache\")\n+                self.past_key_values = None\n+    \n+    def setup_model(self, config: BenchmarkConfig) -> None:\n+        \"\"\"Setup the HuggingFace model for benchmarking with the given configuration.\"\"\"\n+        \n+        self.logger.info(f\"Setting up model: {config.model_id} with variant: {config.variant}\")\n+        self.device = config.device\n+        self.config = config\n+        \n+        # Load model and tokenizer\n+        self._load_model_and_tokenizer(config)\n+        \n+        # Prepare inputs\n+        self._prepare_model_inputs(config)\n+        \n+        # Configure generation settings\n+        self._configure_generation(config)\n+        \n+        self.logger.info(\"Model setup complete\")\n+    \n+    def _load_model_and_tokenizer(self, config: BenchmarkConfig):\n+        \"\"\"Load the model and tokenizer. Override in subclasses for custom loading.\"\"\"\n+\n+        \n+        from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n+        \n+        # Load tokenizer\n+        self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n+        if self.tokenizer.pad_token is None:\n+            self.tokenizer.pad_token = self.tokenizer.eos_token\n+        \n+        # Prepare generation config\n+        generation_config_dict = self.get_default_generation_config()\n+        gen_config = GenerationConfig(**generation_config_dict)\n+        \n+        # Load model\n+        self.logger.info(\"Loading model...\")\n+        \n+        target_device = config.device    \n+        # Get model initialization kwargs\n+        model_init_kwargs = self.get_model_init_kwargs(config)\n+        model_init_kwargs.update({\n+            \"generation_config\": gen_config\n+        })\n+            \n+        self.model = AutoModelForCausalLM.from_pretrained(\n+            config.model_id, \n+            **model_init_kwargs\n+        ).eval()\n+        \n+        # Move model to target device\n+        self.logger.info(f\"Moving model to device: {target_device}\")\n+        self.model.to(target_device)\n+        self.device = target_device  # Update device to match actual device used\n+    \n+    def _prepare_model_inputs(self, config: BenchmarkConfig):\n+        \"\"\"Prepare model inputs. Override in subclasses for custom inputs.\"\"\"\n+        # Prepare inputs\n+        self.inputs = self.tokenizer(self.default_prompt, return_tensors=\"pt\")\n+        \n+        # Move inputs to the same device as the model\n+        if hasattr(self.model, 'device'):\n+            # Model is on a single device\n+            model_device = self.model.device\n+        else:\n+            # Model might be distributed, use self.device which was set during model loading\n+            model_device = self.device\n+            \n+        self.inputs = {k: v.to(model_device) for k, v in self.inputs.items()}\n+        self.logger.debug(f\"Moved inputs to device: {model_device}\")\n+    \n+    def _configure_generation(self, config: BenchmarkConfig):\n+        \"\"\"Configure generation settings.\"\"\"\n+        seq_length = self.inputs[\"input_ids\"].shape[1]\n+        self.model.generation_config.max_length = seq_length + config.num_tokens_to_generate\n+    \n+    def cleanup_model(self) -> None:\n+        \"\"\"Cleanup model resources.\"\"\"\n+        if hasattr(self, 'model') and self.model is not None:\n+            del self.model\n+            self.model = None\n+        if hasattr(self, 'compiled_model') and self.compiled_model is not None:\n+            del self.compiled_model\n+            self.compiled_model = None\n+        if hasattr(self, 'tokenizer') and self.tokenizer is not None:\n+            del self.tokenizer\n+            self.tokenizer = None\n+        if hasattr(self, 'past_key_values') and self.past_key_values is not None:\n+            del self.past_key_values\n+            self.past_key_values = None\n+        \n+        # Clear CUDA cache\n+        flush_memory()\n+    \n+    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n+        \"\"\"Measure time to first token generation.\"\"\"\n+        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n+        \n+        # Prepare generation kwargs\n+        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=1)\n+        \n+        # Use CUDA timer for high-precision measurement\n+        with ArchAwareTimer(device=config.device) as timer:\n+            # Use SDPA context if specified\n+            with SDPAContext(config.sdpa_backend, self.logger):\n+                with torch.no_grad():\n+                    outputs = model_to_use.generate(**generation_kwargs)\n+        \n+        return timer.elapsed_time()\n+    \n+    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n+        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n+        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n+        \n+        # Prepare generation kwargs\n+        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=config.num_tokens_to_generate)\n+        \n+        # Use CUDA timer for high-precision measurement\n+        with ArchAwareTimer(device=config.device) as timer:\n+            # Use SDPA context if specified\n+            with SDPAContext(config.sdpa_backend, self.logger):\n+                with torch.no_grad():\n+                    outputs = model_to_use.generate(**generation_kwargs)\n+        \n+        # Calculate metrics\n+        latency = timer.elapsed_time()\n+        input_length = self.inputs[\"input_ids\"].shape[1]\n+        output_length = outputs.shape[1]\n+        tokens_generated = output_length - input_length\n+        \n+        tokens_per_second = tokens_generated / latency if latency > 0 else 0\n+        time_per_output_token = latency / tokens_generated if tokens_generated > 0 else None\n+        \n+        return TimingResult(\n+            latency_seconds=latency,\n+            tokens_per_second=tokens_per_second,\n+            time_per_output_token_seconds=time_per_output_token,\n+            total_tokens_generated=tokens_generated,\n+            metadata={\n+                \"input_length\": input_length,\n+                \"output_length\": output_length,\n+                \"variant\": config.variant,\n+                \"compile_mode\": config.compile_mode,\n+                \"attn_implementation\": config.attn_implementation,\n+                \"sdpa_backend\": config.sdpa_backend\n+            }\n+        )\n+    \n+    def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -> Dict[str, Any]:\n+        \"\"\"Get generation kwargs. Override in subclasses for custom generation.\"\"\"\n+        generation_config_dict = self.get_default_generation_config()\n+        generation_kwargs = {\n+            **self.inputs,\n+            \"max_new_tokens\": max_new_tokens,\n+            \"do_sample\": generation_config_dict.get(\"do_sample\", False),\n+            \"temperature\": generation_config_dict.get(\"temperature\", 1.0),\n+            \"top_p\": generation_config_dict.get(\"top_p\", 1.0),\n+            \"pad_token_id\": self.tokenizer.pad_token_id,\n+        }\n+        \n+        # Handle static cache for compiled models\n+        if self.past_key_values is not None and config.variant == \"compiled\":\n+            try:\n+                from transformers import StaticCache\n+                # Reset cache for each measurement\n+                seq_length = self.inputs[\"input_ids\"].shape[1]\n+                \n+                # Get the actual device the model is on\n+                if hasattr(self.model, 'device'):\n+                    cache_device = self.model.device\n+                else:\n+                    cache_device = self.device\n+                \n+                fresh_cache = StaticCache(\n+                    config=self.model.config,\n+                    max_batch_size=config.batch_size,\n+                    max_cache_len=seq_length + max_new_tokens,\n+                    device=cache_device,\n+                    dtype=getattr(torch, config.torch_dtype)\n+                )\n+                generation_kwargs[\"past_key_values\"] = fresh_cache\n+            except (ImportError, TypeError) as e:\n+                self.logger.debug(f\"Fresh StaticCache creation failed: {e}\")\n+                pass\n+        \n+        return generation_kwargs\n+\n+\n+class BenchmarkRunner:\n+    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n+    \n+    def __init__(self, logger: logging.Logger, output_dir: str = \"benchmark_results\"):\n+        self.logger = logger\n+        self.output_dir = output_dir\n+        os.makedirs(output_dir, exist_ok=True)\n+        \n+\n+    def run_benchmark(\n+        self, \n+        benchmark: ModelBenchmark, \n+        scenarios: Dict[str, BenchmarkScenario],\n+        collect_gpu_metrics: bool = True,\n+        commit_id: Optional[str] = None\n+    ) -> Dict[str, Dict[str, Any]]:\n+        \"\"\"\n+        Run benchmarks using scenarios.\n+        \n+        Args:\n+            benchmark: The benchmark instance to run\n+            scenarios: Dictionary mapping scenario names to BenchmarkScenario instances\n+            collect_gpu_metrics: Whether to collect GPU utilization metrics\n+            commit_id: Git commit ID for metadata (if not provided, will auto-detect from git)\n+            \n+        Returns:\n+            Dictionary mapping scenario names to results with statistics\n+        \"\"\"\n+        all_results = {}\n+        \n+        for scenario_name, scenario in scenarios.items():\n+            self.logger.info(f\"Running benchmark scenario: {scenario_name}\")\n+            config = scenario.config\n+            \n+            try:\n+                # Setup model for this configuration\n+                benchmark.setup_model(config)\n+                \n+                # Run scenario setup callbacks\n+                scenario.setup(benchmark.model, benchmark.tokenizer, self.logger)\n+                \n+                # Quick validation: try one measurement first to see if this scenario works\n+                try:\n+                    flush_memory()\n+                    test_result = benchmark.measure_time_to_first_token(config)\n+                    if test_result is None or test_result <= 0:\n+                        raise ValueError(\"Invalid measurement result\")\n+                except Exception as validation_error:\n+                    self.logger.warning(f\"Skipping scenario {scenario_name}: validation failed - {validation_error}\")\n+                    # Clean up and skip this scenario\n+                    try:\n+                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n+                        benchmark.cleanup_model()\n+                    except Exception:\n+                        pass\n+                    continue\n+                \n+                # Collect metadata\n+                metadata = BenchmarkMetadata(\n+                    timestamp=datetime.utcnow().isoformat(),\n+                    commit_id=commit_id,\n+                    hardware_info=get_hardware_info(),\n+                    config=config\n+                )\n+                \n+                # Initialize GPU monitor\n+                gpu_monitor = None\n+                if collect_gpu_metrics:\n+                    gpu_monitor = GPUMonitor(logger=self.logger)\n+                \n+                # Warmup runs\n+                self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n+                warmup_failures = 0\n+                for i in range(config.warmup_iterations):\n+                    try:\n+                        _ = benchmark.measure_latency(config)\n+                    except Exception as e:\n+                        warmup_failures += 1\n+                        self.logger.warning(f\"Warmup iteration {i+1} failed: {e}\")\n+                \n+                # If more than half the warmup iterations failed, skip this scenario\n+                if warmup_failures > config.warmup_iterations // 2:\n+                    self.logger.warning(f\"Skipping scenario {scenario_name}: too many warmup failures ({warmup_failures}/{config.warmup_iterations})\")\n+                    try:\n+                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n+                        benchmark.cleanup_model()\n+                    except Exception:\n+                        pass\n+                    continue\n+                \n+                # Start GPU monitoring\n+                if gpu_monitor:\n+                    gpu_monitor.start()\n+                \n+                # Measurement runs for latency\n+                self.logger.info(f\"Measuring latency with {config.measurement_iterations} iterations...\")\n+                latency_measurements = []\n+                ttft_measurements = []\n+                tokens_per_sec_measurements = []\n+                itl_measurements = []  # Inter-Token Latency\n+                measurement_failures = 0\n+                \n+                for i in range(config.measurement_iterations):\n+                    try:                        \n+                        # Measure time to first token\n+                        ttft = benchmark.measure_time_to_first_token(config)\n+                        ttft_measurements.append(ttft)\n+                        \n+                        # Measure full latency\n+                        timing_result = benchmark.measure_latency(config)\n+                        latency_measurements.append(timing_result.latency_seconds)\n+                        \n+                        if timing_result.tokens_per_second is not None:\n+                            tokens_per_sec_measurements.append(timing_result.tokens_per_second)\n+                        \n+                        if timing_result.time_per_output_token_seconds is not None:\n+                            itl_measurements.append(timing_result.time_per_output_token_seconds)\n+                        \n+                        itl_str = f\", itl={timing_result.time_per_output_token_seconds:.4f}s/token\" if timing_result.time_per_output_token_seconds else \"\"\n+                        self.logger.debug(f\"Iteration {i+1}: latency={timing_result.latency_seconds:.4f}s, ttft={ttft:.4f}s{itl_str}\")\n+                        \n+                    except Exception as e:\n+                        measurement_failures += 1\n+                        self.logger.warning(f\"Measurement iteration {i+1} failed: {e}\")\n+                \n+                # Stop GPU monitoring\n+                gpu_metrics = {}\n+                if gpu_monitor:\n+                    gpu_metrics = gpu_monitor.stop_and_collect()\n+                \n+                # If we don't have enough successful measurements, skip this scenario\n+                if not latency_measurements or len(latency_measurements) < config.measurement_iterations // 2:\n+                    self.logger.warning(f\"Skipping scenario {scenario_name}: insufficient successful measurements ({len(latency_measurements)}/{config.measurement_iterations})\")\n+                    try:\n+                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n+                        benchmark.cleanup_model()\n+                    except Exception:\n+                        pass\n+                    continue\n+                \n+                # Calculate statistics\n+                scenario_results = {\n+                    \"metadata\": asdict(metadata),\n+                    \"measurements\": {},\n+                    \"gpu_metrics\": gpu_metrics,\n+                    \"scenario_description\": scenario.description\n+                }\n+                \n+                if latency_measurements:\n+                    latency_stats = BenchmarkStatistics.from_measurements(\"latency_seconds\", latency_measurements)\n+                    scenario_results[\"measurements\"][\"latency_seconds\"] = asdict(latency_stats)\n+                \n+                if ttft_measurements:\n+                    ttft_stats = BenchmarkStatistics.from_measurements(\"time_to_first_token_seconds\", ttft_measurements)\n+                    scenario_results[\"measurements\"][\"time_to_first_token_seconds\"] = asdict(ttft_stats)\n+                \n+                if tokens_per_sec_measurements:\n+                    tps_stats = BenchmarkStatistics.from_measurements(\"tokens_per_second\", tokens_per_sec_measurements, \"tokens/sec\")\n+                    scenario_results[\"measurements\"][\"tokens_per_second\"] = asdict(tps_stats)\n+                \n+                if itl_measurements:\n+                    itl_stats = BenchmarkStatistics.from_measurements(\"time_per_output_token_seconds\", itl_measurements, \"seconds/token\")\n+                    scenario_results[\"measurements\"][\"time_per_output_token_seconds\"] = asdict(itl_stats)\n+                \n+                # Log summary\n+                if latency_measurements:\n+                    self.logger.info(f\"Latency: {latency_stats.mean:.4f}{latency_stats.std:.4f}s (meanstd)\")\n+                if ttft_measurements:\n+                    self.logger.info(f\"TTFT: {ttft_stats.mean:.4f}{ttft_stats.std:.4f}s (meanstd)\")\n+                if tokens_per_sec_measurements:\n+                    self.logger.info(f\"Throughput: {tps_stats.mean:.2f}{tps_stats.std:.2f} tokens/sec (meanstd)\")\n+                if itl_measurements:\n+                    self.logger.info(f\"ITL: {itl_stats.mean:.4f}{itl_stats.std:.4f}s/token (meanstd)\")\n+                \n+                # Add note about partial results if some measurements failed\n+                if measurement_failures > 0:\n+                    scenario_results[\"warnings\"] = [f\"Some measurements failed ({measurement_failures} failures)\"]\n+                    self.logger.info(f\"Scenario completed with {measurement_failures} measurement failures\")\n+                \n+                # Run scenario teardown callbacks\n+                scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n+                \n+                # Cleanup model\n+                benchmark.cleanup_model()\n+                \n+                all_results[scenario_name] = scenario_results\n+                \n+            except Exception as e:\n+                self.logger.warning(f\"Skipping scenario {scenario_name}: setup failed - {e}\")\n+                import traceback\n+                self.logger.debug(traceback.format_exc())\n+                \n+                # Try to clean up if possible\n+                try:\n+                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n+                    benchmark.cleanup_model()\n+                except Exception:\n+                    pass\n+                # Skip storing failed scenarios - just continue to the next one\n+            finally:\n+                try:\n+                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n+                    benchmark.cleanup_model()\n+                except Exception as cleanup_error:\n+                    self.logger.warning(f\"Cleanup failed for scenario {scenario_name}: {cleanup_error}\")\n+                \n+                flush_memory()\n+        \n+        return all_results\n+    \n+    def save_results(self, model_name: str, results: Dict[str, Dict[str, Any]]) -> str:\n+        \"\"\"Save benchmark results to JSON file.\"\"\"\n+        # Create model-specific subdirectory\n+        model_dir = os.path.join(self.output_dir, model_name)\n+        os.makedirs(model_dir, exist_ok=True)\n+        \n+        # Create filename with timestamp\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n+        filepath = os.path.join(model_dir, filename)\n+        \n+        # Prepare output structure\n+        output_data = {\n+            \"model_name\": model_name,\n+            \"benchmark_scenarios\": []\n+        }\n+        \n+        for config_name, config_results in results.items():\n+            scenario = {\n+                \"scenario_name\": config_name,\n+                \"metadata\": config_results[\"metadata\"],\n+                \"measurements\": config_results[\"measurements\"],\n+                \"gpu_metrics\": config_results.get(\"gpu_metrics\", {})\n+            }\n+            output_data[\"benchmark_scenarios\"].append(scenario)\n+        \n+        # Save to JSON file\n+        with open(filepath, 'w') as f:\n+            json.dump(output_data, f, indent=2, default=str)\n+        \n+        self.logger.info(f\"Results saved to {filepath}\")\n+        return filepath\n+ \n\\ No newline at end of file"
        },
        {
            "sha": "a7a435958cf73377a87f1799dbf57f01523da4a7",
            "filename": "benchmark_v2/requirements.txt",
            "status": "added",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frequirements.txt?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1,6 @@\n+numpy>=1.21.0\n+psutil>=5.8.0\n+gpustat>=1.0.0\n+torch>=2.0.0\n+transformers>=4.30.0\n+datasets>=2.10.0 \n\\ No newline at end of file"
        },
        {
            "sha": "9a147b5dde6ed0284f5a3cac53919fb92e71ee55",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "added",
            "additions": 385,
            "deletions": 0,
            "changes": 385,
            "blob_url": "https://github.com/huggingface/transformers/blob/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f22ec7f174d45849480d5600350b92fc9c12dcbf/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=f22ec7f174d45849480d5600350b92fc9c12dcbf",
            "patch": "@@ -0,0 +1,385 @@\n+#!/usr/bin/env python3\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Top-level benchmarking script that automatically discovers and runs all benchmarks \n+in the ./benches directory, organizing outputs into model-specific subfolders.\n+\"\"\"\n+\n+import argparse\n+import importlib.util\n+import logging\n+import os\n+import sys\n+import json\n+from datetime import datetime\n+from pathlib import Path\n+from typing import Dict, List, Any, Optional\n+\n+\n+def setup_logging(log_level: str = \"INFO\", enable_file_logging: bool = False) -> logging.Logger:\n+    \"\"\"Setup logging configuration.\"\"\"\n+    numeric_level = getattr(logging, log_level.upper(), None)\n+    if not isinstance(numeric_level, int):\n+        raise ValueError(f'Invalid log level: {log_level}')\n+    \n+    handlers = [logging.StreamHandler(sys.stdout)]\n+    \n+    if enable_file_logging:\n+        handlers.append(\n+            logging.FileHandler(f'benchmark_run_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n+        )\n+    \n+    logging.basicConfig(\n+        level=numeric_level,\n+        format='[%(levelname)s - %(asctime)s] %(name)s: %(message)s',\n+        handlers=handlers\n+    )\n+    \n+    return logging.getLogger(__name__)\n+\n+\n+def discover_benchmarks(benches_dir: str) -> List[Dict[str, Any]]:\n+    \"\"\"\n+    Discover all benchmark modules in the benches directory.\n+    \n+    Returns:\n+        List of dictionaries containing benchmark module info\n+    \"\"\"\n+    benchmarks = []\n+    benches_path = Path(benches_dir)\n+    \n+    if not benches_path.exists():\n+        raise FileNotFoundError(f\"Benches directory not found: {benches_dir}\")\n+    \n+    for py_file in benches_path.glob(\"*.py\"):\n+        if py_file.name.startswith(\"__\"):\n+            continue\n+            \n+        module_name = py_file.stem\n+        \n+        try:\n+            # Import the module\n+            spec = importlib.util.spec_from_file_location(module_name, py_file)\n+            module = importlib.util.module_from_spec(spec)\n+            spec.loader.exec_module(module)\n+            \n+            # Check if it has a benchmark runner function\n+            if hasattr(module, f'run_{module_name}'):\n+                benchmarks.append({\n+                    'name': module_name,\n+                    'path': str(py_file),\n+                    'module': module,\n+                    'runner_function': getattr(module, f'run_{module_name}')\n+                })\n+            elif hasattr(module, 'run_benchmark'):\n+                benchmarks.append({\n+                    'name': module_name,\n+                    'path': str(py_file),\n+                    'module': module,\n+                    'runner_function': getattr(module, 'run_benchmark')\n+                })\n+            else:\n+                logging.warning(f\"No runner function found in {py_file}\")\n+                \n+        except Exception as e:\n+            logging.error(f\"Failed to import {py_file}: {e}\")\n+            \n+    return benchmarks\n+\n+\n+def run_single_benchmark(\n+    benchmark_info: Dict[str, Any], \n+    output_dir: str,\n+    logger: logging.Logger,\n+    **kwargs\n+) -> Optional[str]:\n+    \"\"\"\n+    Run a single benchmark and return the output file path.\n+    \n+    Args:\n+        benchmark_info: Dictionary containing benchmark module info\n+        output_dir: Base output directory\n+        logger: Logger instance\n+        **kwargs: Additional arguments to pass to the benchmark\n+        \n+    Returns:\n+        Path to the output file if successful, None otherwise\n+    \"\"\"\n+    benchmark_name = benchmark_info['name']\n+    runner_func = benchmark_info['runner_function']\n+    \n+    logger.info(f\"Running benchmark: {benchmark_name}\")\n+    \n+    try:\n+        # Check function signature to determine what arguments to pass\n+        import inspect\n+        sig = inspect.signature(runner_func)\n+        \n+        # Prepare arguments based on function signature\n+        func_kwargs = {\n+            'logger': logger,\n+            'output_dir': output_dir\n+        }\n+        \n+        # Add other kwargs if the function accepts them\n+        for param_name in sig.parameters:\n+            if param_name in kwargs:\n+                func_kwargs[param_name] = kwargs[param_name]\n+        \n+        # Filter kwargs to only include parameters the function accepts\n+        # If function has **kwargs, include all provided kwargs\n+        has_var_kwargs = any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values())\n+        if has_var_kwargs:\n+            valid_kwargs = {**func_kwargs, **kwargs}\n+        else:\n+            valid_kwargs = {k: v for k, v in func_kwargs.items() \n+                           if k in sig.parameters}\n+        \n+        # Run the benchmark\n+        result = runner_func(**valid_kwargs)\n+        \n+        if isinstance(result, str):\n+            # Function returned a file path\n+            return result\n+        else:\n+            logger.info(f\"Benchmark {benchmark_name} completed successfully\")\n+            return \"completed\"\n+            \n+    except Exception as e:\n+        logger.error(f\"Benchmark {benchmark_name} failed: {e}\")\n+        import traceback\n+        logger.debug(traceback.format_exc())\n+        return None\n+\n+\n+def generate_summary_report(\n+    output_dir: str, \n+    benchmark_results: Dict[str, Any],\n+    logger: logging.Logger\n+) -> str:\n+    \"\"\"Generate a summary report of all benchmark runs.\"\"\"\n+    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+    summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.json\")\n+    \n+    summary_data = {\n+        \"run_metadata\": {\n+            \"timestamp\": datetime.utcnow().isoformat(),\n+            \"total_benchmarks\": len(benchmark_results),\n+            \"successful_benchmarks\": len([r for r in benchmark_results.values() if r is not None]),\n+            \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None])\n+        },\n+        \"benchmark_results\": benchmark_results,\n+        \"output_directory\": output_dir\n+    }\n+    \n+    with open(summary_file, 'w') as f:\n+        json.dump(summary_data, f, indent=2, default=str)\n+    \n+    logger.info(f\"Summary report saved to: {summary_file}\")\n+    return summary_file\n+\n+\n+def main():\n+    \"\"\"Main entry point for the benchmarking script.\"\"\"\n+    parser = argparse.ArgumentParser(\n+        description=\"Run all benchmarks in the ./benches directory\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--output-dir\",\n+        type=str,\n+        default=\"benchmark_results\",\n+        help=\"Base output directory for benchmark results (default: benchmark_results)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--benches-dir\",\n+        type=str,\n+        default=\"./benches\",\n+        help=\"Directory containing benchmark implementations (default: ./benches)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--log-level\",\n+        type=str,\n+        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n+        default=\"INFO\",\n+        help=\"Logging level (default: INFO)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--model-id\",\n+        type=str,\n+        help=\"Specific model ID to benchmark (if supported by benchmarks)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--warmup-iterations\",\n+        type=int,\n+        default=3,\n+        help=\"Number of warmup iterations (default: 3)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--measurement-iterations\",\n+        type=int,\n+        default=5,\n+        help=\"Number of measurement iterations (default: 5)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--num-tokens-to-generate\",\n+        type=int,\n+        default=100,\n+        help=\"Number of tokens to generate in benchmarks (default: 100)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--include\",\n+        type=str,\n+        nargs=\"*\",\n+        help=\"Only run benchmarks matching these names\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--exclude\",\n+        type=str,\n+        nargs=\"*\",\n+        help=\"Exclude benchmarks matching these names\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--enable-mock\",\n+        action=\"store_true\",\n+        help=\"Enable mock benchmark (skipped by default)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--enable-file-logging\",\n+        action=\"store_true\",\n+        help=\"Enable file logging (disabled by default)\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--commit-id\",\n+        type=str,\n+        help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n+    )\n+    \n+    args = parser.parse_args()\n+    \n+    # Setup logging\n+    logger = setup_logging(args.log_level, args.enable_file_logging)\n+    \n+    logger.info(\"Starting benchmark discovery and execution\")\n+    logger.info(f\"Output directory: {args.output_dir}\")\n+    logger.info(f\"Benches directory: {args.benches_dir}\")\n+    \n+    # Create output directory\n+    os.makedirs(args.output_dir, exist_ok=True)\n+    \n+    try:\n+        # Discover benchmarks\n+        benchmarks = discover_benchmarks(args.benches_dir)\n+        logger.info(f\"Discovered {len(benchmarks)} benchmark(s): {[b['name'] for b in benchmarks]}\")\n+        \n+        if not benchmarks:\n+            logger.warning(\"No benchmarks found!\")\n+            return 1\n+        \n+        # Filter benchmarks based on include/exclude\n+        filtered_benchmarks = benchmarks\n+        \n+        if args.include:\n+            filtered_benchmarks = [b for b in filtered_benchmarks \n+                                 if any(pattern in b['name'] for pattern in args.include)]\n+            logger.info(f\"Filtered to include: {[b['name'] for b in filtered_benchmarks]}\")\n+        \n+        if args.exclude:\n+            filtered_benchmarks = [b for b in filtered_benchmarks \n+                                 if not any(pattern in b['name'] for pattern in args.exclude)]\n+            logger.info(f\"After exclusion: {[b['name'] for b in filtered_benchmarks]}\")\n+        \n+        if not filtered_benchmarks:\n+            logger.warning(\"No benchmarks remaining after filtering!\")\n+            return 1\n+        \n+        # Prepare common kwargs for benchmarks\n+        benchmark_kwargs = {\n+            'warmup_iterations': args.warmup_iterations,\n+            'measurement_iterations': args.measurement_iterations,\n+            'num_tokens_to_generate': args.num_tokens_to_generate\n+        }\n+        \n+        if args.model_id:\n+            benchmark_kwargs['model_id'] = args.model_id\n+        \n+        # Add enable_mock flag for mock benchmark\n+        benchmark_kwargs['enable_mock'] = args.enable_mock\n+        \n+        # Add commit_id if provided\n+        if args.commit_id:\n+            benchmark_kwargs['commit_id'] = args.commit_id\n+        \n+        # Run benchmarks\n+        benchmark_results = {}\n+        successful_count = 0\n+        \n+        for benchmark_info in filtered_benchmarks:\n+            result = run_single_benchmark(\n+                benchmark_info,\n+                args.output_dir,\n+                logger,\n+                **benchmark_kwargs\n+            )\n+            \n+            benchmark_results[benchmark_info['name']] = result\n+            \n+            if result is not None:\n+                successful_count += 1\n+        \n+        # Generate summary report\n+        summary_file = generate_summary_report(args.output_dir, benchmark_results, logger)\n+        \n+        # Final summary\n+        total_benchmarks = len(filtered_benchmarks)\n+        failed_count = total_benchmarks - successful_count\n+        \n+        logger.info(\"=\" * 60)\n+        logger.info(\"BENCHMARK RUN SUMMARY\")\n+        logger.info(\"=\" * 60)\n+        logger.info(f\"Total benchmarks: {total_benchmarks}\")\n+        logger.info(f\"Successful: {successful_count}\")\n+        logger.info(f\"Failed: {failed_count}\")\n+        logger.info(f\"Output directory: {args.output_dir}\")\n+        logger.info(f\"Summary report: {summary_file}\")\n+        \n+        if failed_count > 0:\n+            logger.warning(f\"{failed_count} benchmark(s) failed. Check logs for details.\")\n+            return 1\n+        else:\n+            logger.info(\"All benchmarks completed successfully!\")\n+            return 0\n+            \n+    except Exception as e:\n+        logger.error(f\"Benchmark run failed: {e}\")\n+        import traceback\n+        logger.debug(traceback.format_exc())\n+        return 1\n+\n+\n+if __name__ == \"__main__\":\n+    sys.exit(main()) \n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 1851,
        "additions": 1851,
        "deletions": 0
    }
}