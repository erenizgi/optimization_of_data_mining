{
    "author": "keetrap",
    "message": "Add Fast Yolos Processor (#37292)\n\n* Add Fast Yolos Processor\n\n* Update modular file\n\n* Fix copies\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "f6c79f767c3cd4ac61abea727e8456c0e7d78043",
    "files": [
        {
            "sha": "c4a9e212287371c5c53dab402aba2cdfb4395261",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6c79f767c3cd4ac61abea727e8456c0e7d78043/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6c79f767c3cd4ac61abea727e8456c0e7d78043/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=f6c79f767c3cd4ac61abea727e8456c0e7d78043",
            "patch": "@@ -92,6 +92,11 @@ Use [`YolosImageProcessor`] for preparing images (and optional targets) for the\n \n [[autodoc]] YolosImageProcessor\n     - preprocess\n+\n+## YolosImageProcessorFast\n+\n+[[autodoc]] YolosImageProcessorFast\n+    - preprocess\n     - pad\n     - post_process_object_detection\n "
        },
        {
            "sha": "b5a87308e4f2f1c7a58faa0b7ddfb4c2b6edff15",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=f6c79f767c3cd4ac61abea727e8456c0e7d78043",
            "patch": "@@ -167,7 +167,7 @@\n             (\"vit_msn\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vitmatte\", (\"VitMatteImageProcessor\",)),\n             (\"xclip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n-            (\"yolos\", (\"YolosImageProcessor\",)),\n+            (\"yolos\", (\"YolosImageProcessor\", \"YolosImageProcessorFast\")),\n             (\"zoedepth\", (\"ZoeDepthImageProcessor\",)),\n         ]\n     )"
        },
        {
            "sha": "c7ce96987e40700eb32f32e3aa0ae6c29d850b54",
            "filename": "src/transformers/models/yolos/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fyolos%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fyolos%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2F__init__.py?ref=f6c79f767c3cd4ac61abea727e8456c0e7d78043",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_yolos import *\n     from .feature_extraction_yolos import *\n     from .image_processing_yolos import *\n+    from .image_processing_yolos_fast import *\n     from .modeling_yolos import *\n else:\n     import sys"
        },
        {
            "sha": "d048b3bb0e2621ca1f850bce79780af95a29d90f",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "added",
            "additions": 893,
            "deletions": 0,
            "changes": 893,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=f6c79f767c3cd4ac61abea727e8456c0e7d78043",
            "patch": "@@ -0,0 +1,893 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/yolos/modular_yolos.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_yolos.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import pathlib\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    safe_squeeze,\n+)\n+from ...image_transforms import center_to_corners_format, corners_to_center_format\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    validate_annotations,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from ...utils.import_utils import requires\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class YolosFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[Dict[str, int]]\n+    return_segmentation_masks: Optional[bool]\n+\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n+\n+\n+# inspired by https://github.com/facebookresearch/yolos/blob/master/datasets/coco.py#L33\n+def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n+    \"\"\"\n+    Convert a COCO polygon annotation to a mask.\n+\n+    Args:\n+        segmentations (`List[List[float]]`):\n+            List of polygons, each polygon represented by a list of x-y coordinates.\n+        height (`int`):\n+            Height of the mask.\n+        width (`int`):\n+            Width of the mask.\n+    \"\"\"\n+    try:\n+        from pycocotools import mask as coco_mask\n+    except ImportError:\n+        raise ImportError(\"Pycocotools is not installed in your environment.\")\n+\n+    masks = []\n+    for polygons in segmentations:\n+        rles = coco_mask.frPyObjects(polygons, height, width)\n+        mask = coco_mask.decode(rles)\n+        if len(mask.shape) < 3:\n+            mask = mask[..., None]\n+        mask = torch.as_tensor(mask, dtype=torch.uint8, device=device)\n+        mask = torch.any(mask, axis=2)\n+        masks.append(mask)\n+    if masks:\n+        masks = torch.stack(masks, axis=0)\n+    else:\n+        masks = torch.zeros((0, height, width), dtype=torch.uint8, device=device)\n+\n+    return masks\n+\n+\n+# inspired by https://github.com/facebookresearch/yolos/blob/master/datasets/coco.py#L50\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by YOLOS.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n+\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n+\n+    if keypoints:\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    if return_segmentation_masks:\n+        segmentation_masks = [obj[\"segmentation\"] for obj in annotations]\n+        masks = convert_coco_poly_to_mask(segmentation_masks, image_height, image_width, device=image.device)\n+        new_target[\"masks\"] = masks[keep]\n+\n+    return new_target\n+\n+\n+def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute the bounding boxes around the provided panoptic segmentation masks.\n+\n+    Args:\n+        masks: masks in format `[number_masks, height, width]` where N is the number of masks\n+\n+    Returns:\n+        boxes: bounding boxes in format `[number_masks, 4]` in xyxy format\n+    \"\"\"\n+    if masks.numel() == 0:\n+        return torch.zeros((0, 4), device=masks.device)\n+\n+    h, w = masks.shape[-2:]\n+    y = torch.arange(0, h, dtype=torch.float32, device=masks.device)\n+    x = torch.arange(0, w, dtype=torch.float32, device=masks.device)\n+    # see https://github.com/pytorch/pytorch/issues/50276\n+    y, x = torch.meshgrid(y, x, indexing=\"ij\")\n+\n+    x_mask = masks * torch.unsqueeze(x, 0)\n+    x_max = x_mask.view(x_mask.shape[0], -1).max(-1)[0]\n+    x_min = (\n+        torch.where(masks, x.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    y_mask = masks * torch.unsqueeze(y, 0)\n+    y_max = y_mask.view(y_mask.shape[0], -1).max(-1)[0]\n+    y_min = (\n+        torch.where(masks, y.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    return torch.stack([x_min, y_min, x_max, y_max], 1)\n+\n+\n+# 2 functions below adapted from https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py\n+# Copyright (c) 2018, Alexander Kirillov\n+# All rights reserved.\n+def rgb_to_id(color):\n+    \"\"\"\n+    Converts RGB color to unique ID.\n+    \"\"\"\n+    if isinstance(color, torch.Tensor) and len(color.shape) == 3:\n+        if color.dtype == torch.uint8:\n+            color = color.to(torch.int32)\n+        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n+    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n+\n+\n+def prepare_coco_panoptic_annotation(\n+    image: torch.Tensor,\n+    target: Dict,\n+    masks_path: Union[str, pathlib.Path],\n+    return_masks: bool = True,\n+    input_data_format: Union[ChannelDimension, str] = None,\n+) -> Dict:\n+    \"\"\"\n+    Prepare a coco panoptic annotation for YOLOS.\n+    \"\"\"\n+    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n+    annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n+\n+    new_target = {}\n+    new_target[\"image_id\"] = torch.as_tensor(\n+        [target[\"image_id\"] if \"image_id\" in target else target[\"id\"]], dtype=torch.int64, device=image.device\n+    )\n+    new_target[\"size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+    new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+\n+    if \"segments_info\" in target:\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(dtype=torch.int32, device=image.device)\n+        masks = rgb_to_id(masks)\n+\n+        ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)\n+        masks = masks == ids[:, None, None]\n+        masks = masks.to(torch.bool)\n+        if return_masks:\n+            new_target[\"masks\"] = masks\n+        new_target[\"boxes\"] = masks_to_boxes(masks)\n+        new_target[\"class_labels\"] = torch.as_tensor(\n+            [segment_info[\"category_id\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"iscrowd\"] = torch.as_tensor(\n+            [segment_info[\"iscrowd\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"area\"] = torch.as_tensor(\n+            [segment_info[\"area\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.float32,\n+            device=image.device,\n+        )\n+\n+    return new_target\n+\n+\n+def get_size_with_aspect_ratio(\n+    image_size: Tuple[int, int], size: int, max_size: Optional[int] = None, mod_size: int = 16\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image size and the desired output size with multiple of divisible_size.\n+\n+    Args:\n+        image_size (`Tuple[int, int]`):\n+            The input image size.\n+        size (`int`):\n+            The desired output size.\n+        max_size (`int`, *optional*):\n+            The maximum allowed output size.\n+        mod_size (`int`, *optional*):\n+            The size to make multiple of mod_size.\n+    \"\"\"\n+    height, width = image_size\n+    raw_size = None\n+    if max_size is not None:\n+        min_original_size = float(min((height, width)))\n+        max_original_size = float(max((height, width)))\n+        if max_original_size / min_original_size * size > max_size:\n+            raw_size = max_size * min_original_size / max_original_size\n+            size = int(round(raw_size))\n+\n+    if width < height:\n+        ow = size\n+        if max_size is not None and raw_size is not None:\n+            oh = int(raw_size * height / width)\n+        else:\n+            oh = int(size * height / width)\n+    elif (height <= width and height == size) or (width <= height and width == size):\n+        oh, ow = height, width\n+    else:\n+        oh = size\n+        if max_size is not None and raw_size is not None:\n+            ow = int(raw_size * width / height)\n+        else:\n+            ow = int(size * width / height)\n+\n+    if mod_size is not None:\n+        ow_mod = torch.remainder(torch.tensor(ow), mod_size).item()\n+        oh_mod = torch.remainder(torch.tensor(oh), mod_size).item()\n+        ow = ow - ow_mod\n+        oh = oh - oh_mod\n+\n+    return (oh, ow)\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Yolos image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the YOLOS model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n+    \"\"\",\n+)\n+@requires(backends=(\"torchvision\", \"torch\"))\n+class YolosImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    format = AnnotationFormat.COCO_DETECTION\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_pad = True\n+    size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+    default_to_square = False\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = YolosFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[YolosFastImageProcessorKwargs]) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        size = kwargs.pop(\"size\", None)\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n+                \"Please specify in `size['longest_edge'] instead`.\",\n+            )\n+            max_size = kwargs.pop(\"max_size\")\n+        else:\n+            max_size = None if size is None else 1333\n+\n+        size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+        self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        # Backwards compatibility\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n+        do_normalize = kwargs.get(\"do_normalize\", None)\n+        if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n+            self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n+\n+        super().__init__(**kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `YolosImageProcessorFast.from_pretrained(checkpoint, size=600,\n+        max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: Optional[bool] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        \"\"\"\n+        Prepare an annotation for feeding into YOLOS model.\n+        \"\"\"\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        elif format == AnnotationFormat.COCO_PANOPTIC:\n+            return_segmentation_masks = True if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_panoptic_annotation(\n+                image,\n+                target,\n+                masks_path=masks_path,\n+                return_masks=return_segmentation_masks,\n+                input_data_format=input_data_format,\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    def resize_annotation(\n+        self,\n+        annotation: Dict[str, Any],\n+        orig_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+        threshold: float = 0.5,\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Resizes an annotation to a target size.\n+\n+        Args:\n+            annotation (`Dict[str, Any]`):\n+                The annotation dictionary.\n+            orig_size (`Tuple[int, int]`):\n+                The original size of the input image.\n+            target_size (`Tuple[int, int]`):\n+                The target size of the image, as returned by the preprocessing `resize` step.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The threshold used to binarize the segmentation masks.\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+                The resampling filter to use when resizing the masks.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n+\n+        new_annotation = {}\n+        new_annotation[\"size\"] = target_size\n+\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                scaled_boxes = boxes * torch.as_tensor(\n+                    [ratio_width, ratio_height, ratio_width, ratio_height], dtype=torch.float32, device=boxes.device\n+                )\n+                new_annotation[\"boxes\"] = scaled_boxes\n+            elif key == \"area\":\n+                area = value\n+                scaled_area = area * (ratio_width * ratio_height)\n+                new_annotation[\"area\"] = scaled_area\n+            elif key == \"masks\":\n+                masks = value[:, None]\n+                masks = [F.resize(mask, target_size, interpolation=interpolation) for mask in masks]\n+                masks = torch.stack(masks).to(torch.float32)\n+                masks = masks[:, 0] > threshold\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = target_size\n+            else:\n+                new_annotation[key] = value\n+\n+        return new_annotation\n+\n+    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+        image_height, image_width = image_size\n+        norm_annotation = {}\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                boxes = corners_to_center_format(boxes)\n+                boxes /= torch.as_tensor(\n+                    [image_width, image_height, image_width, image_height], dtype=torch.float32, device=boxes.device\n+                )\n+                norm_annotation[key] = boxes\n+            else:\n+                norm_annotation[key] = value\n+        return norm_annotation\n+\n+    def _update_annotation_for_padded_image(\n+        self,\n+        annotation: Dict,\n+        input_image_size: Tuple[int, int],\n+        output_image_size: Tuple[int, int],\n+        padding,\n+        update_bboxes,\n+    ) -> Dict:\n+        \"\"\"\n+        Update the annotation for a padded image.\n+        \"\"\"\n+        new_annotation = {}\n+        new_annotation[\"size\"] = output_image_size\n+        ratio_height, ratio_width = (input / output for output, input in zip(output_image_size, input_image_size))\n+\n+        for key, value in annotation.items():\n+            if key == \"masks\":\n+                masks = value\n+                masks = F.pad(\n+                    masks,\n+                    padding,\n+                    fill=0,\n+                )\n+                masks = safe_squeeze(masks, 1)\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"boxes\" and update_bboxes:\n+                boxes = value\n+                boxes *= torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height], device=boxes.device)\n+                new_annotation[\"boxes\"] = boxes\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = output_image_size\n+            else:\n+                new_annotation[key] = value\n+        return new_annotation\n+\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: Tuple[int, int],\n+        annotation: Optional[Dict[str, Any]] = None,\n+        update_bboxes: bool = True,\n+        fill: int = 0,\n+    ):\n+        original_size = image.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            image = F.pad(image, padding, fill=fill)\n+            if annotation is not None:\n+                annotation = self._update_annotation_for_padded_image(\n+                    annotation, original_size, padded_size, padding, update_bboxes\n+                )\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask, annotation\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+            List of annotations associated with the image or batch of images. If annotation is for object\n+            detection, the annotations should be a dictionary with the following keys:\n+            - \"image_id\" (`int`): The image id.\n+            - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                dictionary. An image can have no annotations, in which case the list should be empty.\n+            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+            - \"image_id\" (`int`): The image id.\n+            - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                An image can have no segments, in which case the list should be empty.\n+            - \"file_name\" (`str`): The file name of the image.\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the YOLOS model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n+        masks_path (`str` or `pathlib.Path`, *optional*):\n+            Path to the directory containing the segmentation masks.\n+        \"\"\",\n+    )\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[YolosFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+            logger.warning_once(\n+                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n+                \"use `do_pad` instead.\"\n+            )\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n+                \" `size['longest_edge']` instead.\"\n+            )\n+            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n+\n+        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n+        return_segmentation_masks: bool,\n+        masks_path: Optional[Union[str, pathlib.Path]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        do_convert_annotations: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        do_pad: bool,\n+        pad_size: Optional[Dict[str, int]],\n+        format: Optional[Union[str, AnnotationFormat]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+        \"\"\"\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        if (\n+            masks_path is not None\n+            and format == AnnotationFormat.COCO_PANOPTIC\n+            and not isinstance(masks_path, (pathlib.Path, str))\n+        ):\n+            raise ValueError(\n+                \"The path to the directory containing the mask PNG files should be provided as a\"\n+                f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n+            )\n+\n+        data = {}\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> YOLOS target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n+                    image,\n+                    annotation,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=ChannelDimension.FIRST,\n+                )\n+\n+            if do_resize:\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n+                    )\n+                image = resized_image\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n+\n+        if do_pad:\n+            # depends on all resized image shapes so we need another loop\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            padded_images = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                image, pixel_mask, annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n+                pixel_masks.append(pixel_mask)\n+            images = padded_images\n+            annotations = padded_annotations if annotations is not None else None\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`YolosObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n+                original image size (before any data augmentation). For visualization, this should be the image size\n+                after data augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = out_logits.sigmoid()\n+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`YolosObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            top_k (`int`, *optional*, defaults to 100):\n+                Keep only top k bounding boxes before filtering by thresholding.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = out_logits.sigmoid()\n+        prob = prob.view(out_logits.shape[0], -1)\n+        k_value = min(top_k, prob.size(1))\n+        topk_values, topk_indexes = torch.topk(prob, k_value, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results\n+\n+\n+__all__ = [\"YolosImageProcessorFast\"]"
        },
        {
            "sha": "5ffdbe2e2ba46d8fc64c9d73d8cb0bfa3340f21b",
            "filename": "src/transformers/models/yolos/modular_yolos.py",
            "status": "added",
            "additions": 193,
            "deletions": 0,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6c79f767c3cd4ac61abea727e8456c0e7d78043/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py?ref=f6c79f767c3cd4ac61abea727e8456c0e7d78043",
            "patch": "@@ -0,0 +1,193 @@\n+from typing import List, Optional, Tuple, Union\n+\n+from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast\n+\n+from ...image_transforms import center_to_corners_format\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    logging,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_size_with_aspect_ratio(\n+    image_size: Tuple[int, int], size: int, max_size: Optional[int] = None, mod_size: int = 16\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image size and the desired output size with multiple of divisible_size.\n+\n+    Args:\n+        image_size (`Tuple[int, int]`):\n+            The input image size.\n+        size (`int`):\n+            The desired output size.\n+        max_size (`int`, *optional*):\n+            The maximum allowed output size.\n+        mod_size (`int`, *optional*):\n+            The size to make multiple of mod_size.\n+    \"\"\"\n+    height, width = image_size\n+    raw_size = None\n+    if max_size is not None:\n+        min_original_size = float(min((height, width)))\n+        max_original_size = float(max((height, width)))\n+        if max_original_size / min_original_size * size > max_size:\n+            raw_size = max_size * min_original_size / max_original_size\n+            size = int(round(raw_size))\n+\n+    if width < height:\n+        ow = size\n+        if max_size is not None and raw_size is not None:\n+            oh = int(raw_size * height / width)\n+        else:\n+            oh = int(size * height / width)\n+    elif (height <= width and height == size) or (width <= height and width == size):\n+        oh, ow = height, width\n+    else:\n+        oh = size\n+        if max_size is not None and raw_size is not None:\n+            ow = int(raw_size * width / height)\n+        else:\n+            ow = int(size * width / height)\n+\n+    if mod_size is not None:\n+        ow_mod = torch.remainder(torch.tensor(ow), mod_size).item()\n+        oh_mod = torch.remainder(torch.tensor(oh), mod_size).item()\n+        ow = ow - ow_mod\n+        oh = oh - oh_mod\n+\n+    return (oh, ow)\n+\n+\n+class YolosImageProcessorFast(DetrImageProcessorFast):\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`YolosObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n+                original image size (before any data augmentation). For visualization, this should be the image size\n+                after data augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = out_logits.sigmoid()\n+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 100, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`YolosForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`YolosObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            top_k (`int`, *optional*, defaults to 100):\n+                Keep only top k bounding boxes before filtering by thresholding.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = out_logits.sigmoid()\n+        prob = prob.view(out_logits.shape[0], -1)\n+        k_value = min(top_k, prob.size(1))\n+        topk_values, topk_indexes = torch.topk(prob, k_value, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results\n+\n+    def post_process_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_instance():\n+        raise NotImplementedError(\"Instance post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_panoptic():\n+        raise NotImplementedError(\"Panoptic post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_instance_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_semantic_segmentation():\n+        raise NotImplementedError(\"Semantic segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+    def post_process_panoptic_segmentation():\n+        raise NotImplementedError(\"Panoptic segmentation post-processing is not implemented for Deformable DETR yet.\")\n+\n+\n+__all__ = [\"YolosImageProcessorFast\"]"
        },
        {
            "sha": "2ef2f197aecb63ca4f496d523fa0817cf7a498dc",
            "filename": "tests/models/yolos/test_image_processing_yolos.py",
            "status": "modified",
            "additions": 294,
            "deletions": 281,
            "changes": 575,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6c79f767c3cd4ac61abea727e8456c0e7d78043/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6c79f767c3cd4ac61abea727e8456c0e7d78043/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py?ref=f6c79f767c3cd4ac61abea727e8456c0e7d78043",
            "patch": "@@ -21,7 +21,7 @@\n from parameterized import parameterized\n \n from transformers.testing_utils import require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -34,6 +34,9 @@\n \n     from transformers import YolosImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import YolosImageProcessorFast\n+\n \n class YolosImageProcessingTester:\n     def __init__(\n@@ -143,6 +146,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class YolosImageProcessingTest(AnnotationFormatTestMixin, ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = YolosImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = YolosImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -153,23 +157,25 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.do_pad, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.do_pad, True)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.do_pad, False)\n \n     def test_equivalence_padding(self):\n         # Initialize image_processings\n@@ -199,21 +205,22 @@ def test_equivalence_padding(self):\n         ]\n     )\n     def test_resize_max_size_respected(self, image_size, longest_edge, shortest_edge):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-\n-        # create torch tensors as image\n-        image = torch.randint(0, 256, image_size, dtype=torch.uint8)\n-        processed_image = image_processor(\n-            image,\n-            size={\"longest_edge\": longest_edge, \"shortest_edge\": shortest_edge},\n-            do_pad=False,\n-            return_tensors=\"pt\",\n-        )[\"pixel_values\"]\n-\n-        shape = list(processed_image.shape[-2:])\n-        max_size, min_size = max(shape), min(shape)\n-        self.assertTrue(max_size <= 1333, f\"Expected max_size <= 1333, got image shape {shape}\")\n-        self.assertTrue(min_size <= 800, f\"Expected min_size <= 800, got image shape {shape}\")\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            # create torch tensors as image\n+            image = torch.randint(0, 256, image_size, dtype=torch.uint8)\n+            processed_image = image_processor(\n+                image,\n+                size={\"longest_edge\": longest_edge, \"shortest_edge\": shortest_edge},\n+                do_pad=False,\n+                return_tensors=\"pt\",\n+            )[\"pixel_values\"]\n+\n+            shape = list(processed_image.shape[-2:])\n+            max_size, min_size = max(shape), min(shape)\n+            self.assertTrue(max_size <= 1333, f\"Expected max_size <= 1333, got image shape {shape}\")\n+            self.assertTrue(min_size <= 800, f\"Expected min_size <= 800, got image shape {shape}\")\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):\n@@ -224,40 +231,41 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = YolosImageProcessor.from_pretrained(\"hustvl/yolos-small\")\n-        encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1056])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n-\n-        # verify area\n-        expected_area = torch.tensor([5832.7256, 11144.6689, 484763.2500, 829269.8125, 146579.4531, 164177.6250])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n-        # verify size\n-        expected_size = torch.tensor([800, 1056])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"hustvl/yolos-small\")\n+            encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1056])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+            # verify area\n+            expected_area = torch.tensor([5832.7256, 11144.6689, 484763.2500, 829269.8125, 146579.4531, 164177.6250])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n+            # verify size\n+            expected_size = torch.tensor([800, 1056])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -270,43 +278,45 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n \n         masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        # encode them\n-        image_processing = YolosImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1056])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n-\n-        # verify area\n-        expected_area = torch.tensor([146591.5000, 163974.2500, 480092.2500, 11187.0000, 5824.5000, 7562.5000])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n-        # verify masks\n-        expected_masks_sum = 815161\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n-        # verify size\n-        expected_size = torch.tensor([800, 1056])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1056])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+            # verify area\n+            expected_area = torch.tensor([146591.5000, 163974.2500, 480092.2500, 11187.0000, 5824.5000, 7562.5000])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n+            # verify masks\n+            expected_masks_sum = 815161\n+            relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n+            self.assertTrue(relative_error < 1e-3)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n+            # verify size\n+            expected_size = torch.tensor([800, 1056])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     # Output size is slight different from DETR as yolos takes mod of 16\n     @slow\n@@ -336,96 +346,97 @@ def test_batched_coco_detection_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotations_0, annotations_1]\n \n-        image_processing = YolosImageProcessor()\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            return_tensors=\"pt\",  # do_convert_annotations=True\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                return_tensors=\"pt\",  # do_convert_annotations=True\n+            )\n \n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1056\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.6879, 0.4609, 0.0755, 0.3691],\n-                [0.2118, 0.3359, 0.2601, 0.1566],\n-                [0.5011, 0.5000, 0.9979, 1.0000],\n-                [0.5010, 0.5020, 0.9979, 0.9959],\n-                [0.3284, 0.5944, 0.5884, 0.8112],\n-                [0.8394, 0.5445, 0.3213, 0.9110],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.4169, 0.2765, 0.0458, 0.2215],\n-                [0.1284, 0.2016, 0.1576, 0.0940],\n-                [0.3792, 0.4933, 0.7559, 0.9865],\n-                [0.3794, 0.5002, 0.7563, 0.9955],\n-                [0.1990, 0.5456, 0.3566, 0.8646],\n-                [0.5845, 0.4115, 0.3462, 0.7161],\n-            ]\n-        )\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3, atol=1e-3)\n-        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3, atol=1e-3)\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1056]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1056]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1, atol=1)\n-        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1, atol=1)\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1056\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.6879, 0.4609, 0.0755, 0.3691],\n+                    [0.2118, 0.3359, 0.2601, 0.1566],\n+                    [0.5011, 0.5000, 0.9979, 1.0000],\n+                    [0.5010, 0.5020, 0.9979, 0.9959],\n+                    [0.3284, 0.5944, 0.5884, 0.8112],\n+                    [0.8394, 0.5445, 0.3213, 0.9110],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.4169, 0.2765, 0.0458, 0.2215],\n+                    [0.1284, 0.2016, 0.1576, 0.0940],\n+                    [0.3792, 0.4933, 0.7559, 0.9865],\n+                    [0.3794, 0.5002, 0.7563, 0.9955],\n+                    [0.1990, 0.5456, 0.3566, 0.8646],\n+                    [0.5845, 0.4115, 0.3462, 0.7161],\n+                ]\n+            )\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3, atol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3, atol=1e-3)\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1056]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1056]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1, atol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1, atol=1)\n \n     # Output size is slight different from DETR as yolos takes mod of 16\n     def test_batched_coco_panoptic_annotations(self):\n@@ -457,98 +468,100 @@ def test_batched_coco_panoptic_annotations(self):\n         annotations = [annotation_0, annotation_1]\n \n         # encode them\n-        image_processing = YolosImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_tensors=\"pt\",\n-            return_segmentation_masks=True,\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            image_processing = YolosImageProcessor(format=\"coco_panoptic\")\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_tensors=\"pt\",\n+                return_segmentation_masks=True,\n+            )\n \n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1056\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.2625, 0.5437, 0.4688, 0.8625],\n-                [0.7719, 0.4104, 0.4531, 0.7125],\n-                [0.5000, 0.4927, 0.9969, 0.9854],\n-                [0.1688, 0.2000, 0.2063, 0.0917],\n-                [0.5492, 0.2760, 0.0578, 0.2187],\n-                [0.4992, 0.4990, 0.9984, 0.9979],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.1591, 0.3262, 0.2841, 0.5175],\n-                [0.4678, 0.2463, 0.2746, 0.4275],\n-                [0.3030, 0.2956, 0.6042, 0.5913],\n-                [0.1023, 0.1200, 0.1250, 0.0550],\n-                [0.3329, 0.1656, 0.0350, 0.1312],\n-                [0.3026, 0.2994, 0.6051, 0.5987],\n-            ]\n-        )\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3, atol=1e-3)\n-        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3, atol=1e-3)\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1056]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1056]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n-        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1056\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.2625, 0.5437, 0.4688, 0.8625],\n+                    [0.7719, 0.4104, 0.4531, 0.7125],\n+                    [0.5000, 0.4927, 0.9969, 0.9854],\n+                    [0.1688, 0.2000, 0.2063, 0.0917],\n+                    [0.5492, 0.2760, 0.0578, 0.2187],\n+                    [0.4992, 0.4990, 0.9984, 0.9979],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.1591, 0.3262, 0.2841, 0.5175],\n+                    [0.4678, 0.2463, 0.2746, 0.4275],\n+                    [0.3030, 0.2956, 0.6042, 0.5913],\n+                    [0.1023, 0.1200, 0.1250, 0.0550],\n+                    [0.3329, 0.1656, 0.0350, 0.1312],\n+                    [0.3026, 0.2994, 0.6051, 0.5987],\n+                ]\n+            )\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3, atol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3, atol=1e-3)\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1056]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1056]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->Yolos\n     def test_max_width_max_height_resizing_and_pad_strategy(self):"
        }
    ],
    "stats": {
        "total": 1669,
        "additions": 1387,
        "deletions": 282
    }
}