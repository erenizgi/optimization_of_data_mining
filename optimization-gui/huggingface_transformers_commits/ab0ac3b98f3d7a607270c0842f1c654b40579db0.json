{
    "author": "gante",
    "message": "CI: fix `efficientnet` pipeline timeout and prevent future similar issues due to large image size (#33123)\n\n* fix param not being passed in tested; add exceptions\r\n\r\n* better source of model name\r\n\r\n* Update utils/create_dummy_models.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "ab0ac3b98f3d7a607270c0842f1c654b40579db0",
    "files": [
        {
            "sha": "4162e1891409328db2b039ea4590f73bf157101f",
            "filename": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab0ac3b98f3d7a607270c0842f1c654b40579db0/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab0ac3b98f3d7a607270c0842f1c654b40579db0/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py?ref=ab0ac3b98f3d7a607270c0842f1c654b40579db0",
            "patch": "@@ -83,6 +83,7 @@ def prepare_config_and_inputs(self):\n \n     def get_config(self):\n         return EfficientNetConfig(\n+            image_size=self.image_size,\n             num_channels=self.num_channels,\n             kernel_sizes=self.kernel_sizes,\n             in_channels=self.in_channels,"
        },
        {
            "sha": "911783bc5cfbacb1c9be3ff0d82dfd1c9b88aac0",
            "filename": "tests/utils/tiny_model_summary.json",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab0ac3b98f3d7a607270c0842f1c654b40579db0/tests%2Futils%2Ftiny_model_summary.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab0ac3b98f3d7a607270c0842f1c654b40579db0/tests%2Futils%2Ftiny_model_summary.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftiny_model_summary.json?ref=ab0ac3b98f3d7a607270c0842f1c654b40579db0",
            "patch": "@@ -1718,7 +1718,7 @@\n         \"model_classes\": [\n             \"EfficientNetForImageClassification\"\n         ],\n-        \"sha\": \"6ed195ee636d2c0b885139da8c7b45d57ebaeee0\"\n+        \"sha\": \"993d088cf937b8a90b61f68677cd8f261321c745\"\n     },\n     \"EfficientNetModel\": {\n         \"tokenizer_classes\": [],\n@@ -7243,4 +7243,4 @@\n         ],\n         \"sha\": \"e144d9f1fe39c21eda1177702640e126892605ce\"\n     }\n-}\n\\ No newline at end of file\n+}"
        },
        {
            "sha": "e151b37d52ba065e470b14b9697ee5a2d63a05c2",
            "filename": "utils/create_dummy_models.py",
            "status": "modified",
            "additions": 26,
            "deletions": 5,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab0ac3b98f3d7a607270c0842f1c654b40579db0/utils%2Fcreate_dummy_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab0ac3b98f3d7a607270c0842f1c654b40579db0/utils%2Fcreate_dummy_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dummy_models.py?ref=ab0ac3b98f3d7a607270c0842f1c654b40579db0",
            "patch": "@@ -504,6 +504,27 @@ def convert_feature_extractor(feature_extractor, tiny_config):\n     if to_convert:\n         feature_extractor = feature_extractor.__class__(**kwargs)\n \n+    # Sanity check: on tiny image feature extractors, a large image size results in slow CI -- up to the point where it\n+    # can result in timeout issues.\n+    if (\n+        isinstance(feature_extractor, BaseImageProcessor)\n+        and hasattr(feature_extractor, \"size\")\n+        and isinstance(feature_extractor.size, dict)\n+    ):\n+        largest_image_size = max(feature_extractor.size.values())\n+        if largest_image_size > 64:\n+            # hardcoded exceptions\n+            models_with_large_image_size = (\"deformable_detr\", \"flava\", \"grounding_dino\", \"mgp_str\", \"swiftformer\")\n+            if any(model_name in tiny_config.model_type for model_name in models_with_large_image_size):\n+                pass\n+            else:\n+                raise ValueError(\n+                    f\"Image size of {tiny_config.model_type} is too large ({feature_extractor.size}). \"\n+                    \"Please reduce it to 64 or less on each dimension. The following steps are usually the \"\n+                    \"easiest solution: 1) confirm that you're setting `image_size` in your ModelTester class; \"\n+                    \"2) ensure that it gets passed to the tester config init, `get_config()`.\"\n+                )\n+\n     return feature_extractor\n \n \n@@ -526,27 +547,27 @@ def _sanity_check(fast_tokenizer, slow_tokenizer, keep_fast_tokenizer=False):\n         # sanity check 1: fast and slow tokenizers should be compatible (vocab_size)\n         if fast_tokenizer is not None and slow_tokenizer is not None:\n             if fast_tokenizer.vocab_size != slow_tokenizer.vocab_size:\n-                warning_messagae = (\n+                warning_message = (\n                     \"The fast/slow tokenizers \"\n                     f\"({fast_tokenizer.__class__.__name__}/{slow_tokenizer.__class__.__name__}) have different \"\n                     \"vocabulary size: \"\n                     f\"fast_tokenizer.vocab_size = {fast_tokenizer.vocab_size} and \"\n                     f\"slow_tokenizer.vocab_size = {slow_tokenizer.vocab_size}.\"\n                 )\n-                result[\"warnings\"].append(warning_messagae)\n+                result[\"warnings\"].append(warning_message)\n                 if not keep_fast_tokenizer:\n                     fast_tokenizer = None\n                 slow_tokenizer = None\n \n         # sanity check 2: fast and slow tokenizers should be compatible (length)\n         if fast_tokenizer is not None and slow_tokenizer is not None:\n             if len(fast_tokenizer) != len(slow_tokenizer):\n-                warning_messagae = (\n+                warning_message = (\n                     f\"The fast/slow tokenizers () have different length: \"\n                     f\"len(fast_tokenizer) = {len(fast_tokenizer)} and \"\n                     f\"len(slow_tokenizer) = {len(slow_tokenizer)}.\"\n                 )\n-                result[\"warnings\"].append(warning_messagae)\n+                result[\"warnings\"].append(warning_message)\n                 if not keep_fast_tokenizer:\n                     fast_tokenizer = None\n                 slow_tokenizer = None\n@@ -1395,7 +1416,7 @@ def create_tiny_models(\n         raise ValueError(f\"This script should be run from the root of the clone of `transformers` {clone_path}\")\n \n     report_path = os.path.join(output_path, \"reports\")\n-    os.makedirs(report_path)\n+    os.makedirs(report_path, exist_ok=True)\n \n     _pytorch_arch_mappings = [\n         x"
        },
        {
            "sha": "3c376bdbdaaf45fe9d9623ac4cd292421fbcb0ef",
            "filename": "utils/get_test_info.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab0ac3b98f3d7a607270c0842f1c654b40579db0/utils%2Fget_test_info.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab0ac3b98f3d7a607270c0842f1c654b40579db0/utils%2Fget_test_info.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_test_info.py?ref=ab0ac3b98f3d7a607270c0842f1c654b40579db0",
            "patch": "@@ -53,7 +53,15 @@ def get_module_path(test_file):\n def get_test_module(test_file):\n     \"\"\"Get the module of a model test file.\"\"\"\n     test_module_path = get_module_path(test_file)\n-    test_module = importlib.import_module(test_module_path)\n+    try:\n+        test_module = importlib.import_module(test_module_path)\n+    except AttributeError as exc:\n+        # e.g. if you have a `tests` folder in `site-packages`, created by another package, when trying to import\n+        # `tests.models...`\n+        raise ValueError(\n+            f\"Could not import module {test_module_path}. Confirm that you don't have a package with the same root \"\n+            \"name installed or in your environment's `site-packages`.\"\n+        ) from exc\n \n     return test_module\n "
        }
    ],
    "stats": {
        "total": 46,
        "additions": 38,
        "deletions": 8
    }
}