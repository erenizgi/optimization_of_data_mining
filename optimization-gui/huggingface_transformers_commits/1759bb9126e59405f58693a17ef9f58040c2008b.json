{
    "author": "zucchini-nlp",
    "message": "Fix: StaticCache & `inputs_embeds` (#32932)\n\nsquash commit",
    "sha": "1759bb9126e59405f58693a17ef9f58040c2008b",
    "files": [
        {
            "sha": "f202e2fb2aab8110b7cba7256e5a79e0af377456",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1759bb9126e59405f58693a17ef9f58040c2008b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1759bb9126e59405f58693a17ef9f58040c2008b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1759bb9126e59405f58693a17ef9f58040c2008b",
            "patch": "@@ -1481,6 +1481,7 @@ def _prepare_cache_for_generation(\n         model_kwargs: Dict,\n         assistant_model: \"PreTrainedModel\",\n         batch_size: int,\n+        max_cache_length: int,\n         device: torch.device,\n     ) -> bool:\n         \"\"\"\n@@ -1547,8 +1548,8 @@ def _prepare_cache_for_generation(\n                     )\n                 model_kwargs[cache_name] = self._get_cache(\n                     cache_implementation=generation_config.cache_implementation,\n-                    batch_size=generation_config.num_beams * generation_config.num_return_sequences * batch_size,\n-                    max_cache_len=generation_config.max_length,\n+                    batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,\n+                    max_cache_len=max_cache_length,\n                     device=device,\n                     model_kwargs=model_kwargs,\n                 )\n@@ -1888,7 +1889,16 @@ def generate(\n         # TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\n         cache_name = \"past_key_values\" if \"mamba\" not in self.__class__.__name__.lower() else \"cache_params\"\n         user_defined_cache = model_kwargs.get(cache_name)\n-        self._prepare_cache_for_generation(generation_config, model_kwargs, assistant_model, batch_size, device)\n+        max_cache_length = generation_config.max_length\n+        if (\n+            inputs_tensor.shape[1] != input_ids_length\n+            and model_input_name == \"inputs_embeds\"\n+            and not self.config.is_encoder_decoder\n+        ):\n+            max_cache_length += inputs_tensor.shape[1]\n+        self._prepare_cache_for_generation(\n+            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n+        )\n \n         # 8. determine generation mode\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n@@ -1936,8 +1946,8 @@ def generate(\n                 raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n             if not model_kwargs[\"use_cache\"]:\n                 raise ValueError(\"assisted generate requires `use_cache=True`\")\n-            if generation_config.cache_implementation == \"static\":\n-                raise ValueError(\"assisted generate is not supported with `static_cache`\")\n+            if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"]:\n+                raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n             if self._is_stateful:\n                 # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n                 # which is not possible with stateful models (they can't reset to a previous subset of generated text)"
        },
        {
            "sha": "65507795c84dd807431ce9d440e6f36cfb9a35d2",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/1759bb9126e59405f58693a17ef9f58040c2008b/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1759bb9126e59405f58693a17ef9f58040c2008b/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=1759bb9126e59405f58693a17ef9f58040c2008b",
            "patch": "@@ -1453,6 +1453,9 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             model = model_class(config).to(torch_device).eval()\n             signature = inspect.signature(model.forward).parameters.keys()\n \n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n             # Without padding\n             model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n             next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n@@ -1593,6 +1596,59 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 outputs_from_embeds_wo_ids.tolist(),\n             )\n \n+    @pytest.mark.generate\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        \"\"\"\n+        Test that StaticCache can generate from inputs_embeds and calculates max_cache_length\n+        correctly in `generate()`. We force the model to not stop generation until max-length is reached\n+        to verify that the cache length is indeed set correctly and we don't run out of index when slicing the cache.\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_static_cache:\n+                self.skipTest(reason=\"This model does not support the static cache format\")\n+\n+            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            if config.is_encoder_decoder:\n+                self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+                self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n+\n+            model.config.use_cache = True\n+            model.config.is_decoder = True\n+            batch_size, seq_length = input_ids.shape\n+            max_cache_len = 30\n+\n+            # here we force to not stop at eos and go until max-length\n+            model.generation_config.eos_token_id = model.config.eos_token_id = -1\n+            generation_kwargs = {\n+                \"max_length\": max_cache_len,\n+                \"cache_implementation\": \"static\",\n+                \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+            }\n+\n+            head_dim = (\n+                model.config.head_dim\n+                if hasattr(model.config, \"head_dim\")\n+                else model.config.hidden_size // model.config.num_attention_heads\n+            )\n+            num_key_value_heads = (\n+                model.config.num_attention_heads\n+                if getattr(config, \"num_key_value_heads\", None) is None\n+                else model.config.num_key_value_heads\n+            )\n+            num_hidden_layers = config.num_hidden_layers\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+            outputs = model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generation_kwargs)\n+\n+            # we should get `max_length` in shape, not `max_length - embeds_length`\n+            cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n+            self.assertTrue(isinstance(outputs.past_key_values, StaticCache))\n+            self.assertTrue(len(outputs.past_key_values.key_cache) == num_hidden_layers)\n+            self.assertTrue(outputs.past_key_values.key_cache[0].shape == cache_shape)\n+\n     @pytest.mark.generate\n     def test_generate_continue_from_past_key_values(self):\n         # Tests that we can continue generating from past key values, returned from a previous `generate` call"
        },
        {
            "sha": "918ed847f83d9e1068a6c0bdbf4a83fcba0a0222",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 98,
            "deletions": 3,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/1759bb9126e59405f58693a17ef9f58040c2008b/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1759bb9126e59405f58693a17ef9f58040c2008b/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=1759bb9126e59405f58693a17ef9f58040c2008b",
            "patch": "@@ -16,9 +16,10 @@\n \n import unittest\n \n+from parameterized import parameterized\n from pytest import mark\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, is_torch_available, pipeline\n+from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, HybridCache, is_torch_available, pipeline\n from transformers.testing_utils import (\n     require_flash_attn,\n     require_read_token,\n@@ -59,7 +60,7 @@ class Gemma2ModelTest(GemmaModelTest, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = ()\n+    all_generative_model_classes = (Gemma2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Gemma2Model,\n@@ -89,6 +90,101 @@ def test_model_outputs_equivalence(self, **kwargs):\n     def test_eager_matches_sdpa_inference(self):\n         pass\n \n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache which is not compatible with dola decoding\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @parameterized.expand([(1, False), (1, True), (4, False)])\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support old tuple format at all\")\n+    def test_new_cache_format(self, num_beams, do_sample):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support continue from past kv\")\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support low_memory generation\")\n+    def test_beam_search_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    # overwrite because HybridCache has fixed length for key/values\n+    def _check_attentions_for_generate(\n+        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+    ):\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n+        )\n+        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+\n+        for idx, iter_attentions in enumerate(attentions):\n+            tgt_len = min_length + idx if not use_cache else 1\n+            src_len = min_length + idx if not use_cache else max_length\n+\n+            expected_shape = (\n+                batch_size * num_beam_groups,\n+                config.num_attention_heads,\n+                tgt_len,\n+                src_len,\n+            )\n+            # check attn size\n+            self.assertListEqual(\n+                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n+            )\n+\n+    # overwrite because HybridCache has fixed length for key/values\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config, num_beam_groups=1):\n+        self.assertIsInstance(past_key_values, HybridCache)\n+\n+        # check shape key, value (batch, head, max_seq_length, head_features)\n+        head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n+        num_key_value_heads = (\n+            config.num_attention_heads\n+            if getattr(config, \"num_key_value_heads\", None) is None\n+            else config.num_key_value_heads\n+        )\n+        num_hidden_layers = config.num_hidden_layers\n+\n+        # we should get `max_length` in shape, not `max_length - embeds_length`\n+        # `+1` because the test in Mixin subtracts 1 which is needed for tuple cache\n+        static_cache_shape = (batch_size, num_key_value_heads, seq_length + 1, head_dim)\n+        static_layers = [layer_idx for layer_idx, boolean in enumerate(past_key_values.is_sliding) if not boolean]\n+        self.assertTrue(len(past_key_values.key_cache) == num_hidden_layers)\n+        self.assertTrue(past_key_values.key_cache[static_layers[0]].shape == static_cache_shape)\n+\n     @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_sdpa_equivalence(self):\n         pass\n@@ -203,6 +299,5 @@ def test_model_9b_flash_attn(self):\n \n         output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n-        print(output_text)\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)"
        }
    ],
    "stats": {
        "total": 177,
        "additions": 169,
        "deletions": 8
    }
}