{
    "author": "SunMarc",
    "message": "Remove `local_rank` arg from `TrainingArguments` (#41382)",
    "sha": "0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
    "files": [
        {
            "sha": "0b33e47c03c8706da9048c46f3652bdd3bb36fe5",
            "filename": "examples/legacy/multiple_choice/run_multiple_choice.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fmultiple_choice%2Frun_multiple_choice.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -99,18 +99,18 @@ def main():\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n+        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n     )\n     logger.warning(\n         \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_rank,\n+        training_args.local_process_index,\n         training_args.device,\n         training_args.n_gpu,\n-        bool(training_args.local_rank != -1),\n+        bool(training_args.parallel_mode.value == \"distributed\"),\n         training_args.fp16,\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n         transformers.utils.logging.enable_default_handler()\n         transformers.utils.logging.enable_explicit_format()"
        },
        {
            "sha": "9ec8d4d852cdfad2f9e72b37fb292824ab2940c7",
            "filename": "examples/legacy/question-answering/run_squad_trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fquestion-answering%2Frun_squad_trainer.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -80,18 +80,18 @@ def main():\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n+        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n     )\n     logger.warning(\n         \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_rank,\n+        training_args.local_process_index,\n         training_args.device,\n         training_args.n_gpu,\n-        bool(training_args.local_rank != -1),\n+        bool(training_args.parallel_mode.value == \"distributed\"),\n         training_args.fp16,\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n         transformers.utils.logging.enable_default_handler()\n         transformers.utils.logging.enable_explicit_format()"
        },
        {
            "sha": "db1db37d983494ff93618dbbbdddd36159b19b48",
            "filename": "examples/legacy/run_language_modeling.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Frun_language_modeling.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Frun_language_modeling.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_language_modeling.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -212,18 +212,18 @@ def main():\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n+        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n     )\n     logger.warning(\n         \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_rank,\n+        training_args.local_process_index,\n         training_args.device,\n         training_args.n_gpu,\n-        bool(training_args.local_rank != -1),\n+        bool(training_args.parallel_mode.value == \"distributed\"),\n         training_args.fp16,\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n         transformers.utils.logging.enable_default_handler()\n         transformers.utils.logging.enable_explicit_format()"
        },
        {
            "sha": "a39380bbab1c5f2580f15a34d04d309fb8b04ce5",
            "filename": "examples/legacy/seq2seq/finetune_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -171,11 +171,11 @@ def main():\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n+        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n     )\n     logger.warning(\n         \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_rank,\n+        training_args.local_process_index,\n         training_args.device,\n         training_args.n_gpu,\n         bool(training_args.parallel_mode == ParallelMode.DISTRIBUTED),\n@@ -184,7 +184,7 @@ def main():\n     transformers.utils.logging.enable_default_handler()\n     transformers.utils.logging.enable_explicit_format()\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n     logger.info(\"Training/evaluation parameters %s\", training_args)\n "
        },
        {
            "sha": "c73b4f99d9e251ce4cae7fc5a25f46b619561d25",
            "filename": "examples/legacy/token-classification/run_ner.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Frun_ner.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -125,18 +125,18 @@ def main():\n     logging.basicConfig(\n         format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n+        level=logging.INFO if training_args.local_process_index in [-1, 0] else logging.WARN,\n     )\n     logger.warning(\n         \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n-        training_args.local_rank,\n+        training_args.local_process_index,\n         training_args.device,\n         training_args.n_gpu,\n-        bool(training_args.local_rank != -1),\n+        bool(training_args.parallel_mode.value == \"distributed\"),\n         training_args.fp16,\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n         transformers.utils.logging.enable_default_handler()\n         transformers.utils.logging.enable_explicit_format()"
        },
        {
            "sha": "635a947bbdebca5076c4d487922fb09edee2a7c0",
            "filename": "examples/pytorch/audio-classification/run_audio_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -236,7 +236,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "05fa924d62b373b52c04e4a97d681350d84d6eae",
            "filename": "examples/pytorch/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -265,7 +265,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "78e4ba22b588028dd1a47e5ea024c226de2cb121",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -219,7 +219,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "f3313b4b42f4868b81b77cf3695312efa21bde67",
            "filename": "examples/pytorch/image-pretraining/run_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -211,7 +211,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "f9c874b7578f4e9166585d67e5bc7107cced7945",
            "filename": "examples/pytorch/image-pretraining/run_mim.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -275,7 +275,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "e2bc4a289eba3d132d2be7b956202016053a0e32",
            "filename": "examples/pytorch/instance-segmentation/run_instance_segmentation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -357,7 +357,7 @@ def main():\n     # Setup logging and log on each process the small summary:\n     setup_logging(training_args)\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "53fa7f13460f60185aa402f69d40e60563d35be5",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -311,7 +311,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "828958e77563d55849deae504104ba7dadacb9df",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -338,7 +338,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "027905ac420080181713592b79c749ad38e68fd4",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -283,7 +283,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):"
        },
        {
            "sha": "c9d038d4e51a235a8e91ad395e626127c12b11ba",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -263,7 +263,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "30786a7af7ff2cced7e712303d301e61b2bbe364",
            "filename": "examples/pytorch/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -207,7 +207,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "7624c122f2d6a97ae5fcb51b92a80bc28e021def",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -367,7 +367,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "92d2cfd1949bb83af3fe0c991846816b086afbac",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -256,7 +256,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "68924ba9f06e267c3d95f91cc99312fe6a251c9d",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -254,7 +254,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "1a2793d58d470a638f021d77282828bec2488abc",
            "filename": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -302,7 +302,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "6c773a5115f36536ba1b526cecea47eb090a0ce4",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -215,7 +215,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "0edd243433fc51007a257183f9e0004bebb86186",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -435,15 +435,15 @@ def main():\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n         handlers=[logging.StreamHandler(sys.stdout)],\n     )\n-    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n+    logger.setLevel(logging.INFO if is_main_process(training_args.local_process_index) else logging.WARN)\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n     logger.info(\"Training/evaluation parameters %s\", training_args)\n \n@@ -726,7 +726,7 @@ def compute_metrics(pred):\n     # make sure all processes wait until data is saved\n     with training_args.main_process_first():\n         # only the main process saves them\n-        if is_main_process(training_args.local_rank):\n+        if is_main_process(training_args.local_process_index):\n             # save feature extractor, tokenizer and config\n             feature_extractor.save_pretrained(training_args.output_dir)\n             tokenizer.save_pretrained(training_args.output_dir)"
        },
        {
            "sha": "0e985dfe70b91516f3a766b1a4e804f057acf927",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -412,15 +412,15 @@ def main():\n         datefmt=\"%m/%d/%Y %H:%M:%S\",\n         handlers=[logging.StreamHandler(sys.stdout)],\n     )\n-    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n+    logger.setLevel(logging.INFO if is_main_process(training_args.local_process_index) else logging.WARN)\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n     logger.info(\"Training/evaluation parameters %s\", training_args)\n \n@@ -721,7 +721,7 @@ def compute_metrics(pred):\n     # make sure all processes wait until data is saved\n     with training_args.main_process_first():\n         # only the main process saves them\n-        if is_main_process(training_args.local_rank):\n+        if is_main_process(training_args.local_process_index):\n             # save feature extractor, tokenizer and config\n             feature_extractor.save_pretrained(training_args.output_dir)\n             tokenizer.save_pretrained(training_args.output_dir)"
        },
        {
            "sha": "70ca37461ba2061fdfd93387bbdd976231b8f3c0",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -326,17 +326,17 @@ def main():\n     transformers.utils.logging.enable_default_handler()\n     transformers.utils.logging.enable_explicit_format()\n \n-    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n+    logger.setLevel(logging.INFO if is_main_process(training_args.local_process_index) else logging.WARN)\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n \n     # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n+    if is_main_process(training_args.local_process_index):\n         transformers.utils.logging.set_verbosity_info()\n     logger.info(\"Training/evaluation parameters %s\", training_args)\n \n@@ -557,7 +557,7 @@ def compute_metrics(pred):\n     # make sure all processes wait until data is saved\n     with training_args.main_process_first():\n         # only the main process saves them\n-        if is_main_process(training_args.local_rank):\n+        if is_main_process(training_args.local_process_index):\n             # save feature extractor, tokenizer and config\n             feature_extractor.save_pretrained(training_args.output_dir)\n             tokenizer.save_pretrained(training_args.output_dir)"
        },
        {
            "sha": "ba6d34cd12cb4ca099b6b7b913396b92b79fa0d2",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -356,7 +356,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "cf1c03af605c0b33f9285ce41259966006d16f52",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -315,7 +315,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "804788b30f962c33e3d1454beca45a6816757307",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -260,7 +260,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "ac3be437ecd8165804f0d2e8a64d03e0e7b71c30",
            "filename": "examples/pytorch/text-classification/run_xnli.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -218,7 +218,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "60548096e3a50070d8afaea379d39f1a9fd6acd4",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -257,7 +257,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "e1d3c4ca387a6abc350fd54641dbfc05cf758406",
            "filename": "examples/pytorch/translation/run_translation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -304,7 +304,7 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")"
        },
        {
            "sha": "be45697762e24182a83a802543bb6a268af5ee44",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -4514,9 +4514,7 @@ def _nested_gather(self, tensors, name=None):\n             tensors = nested_xla_mesh_reduce(tensors, name)\n         elif is_sagemaker_mp_enabled():\n             tensors = smp_gather(tensors)\n-        elif (self.args.distributed_state is not None and self.args.distributed_state.distributed_type != \"NO\") or (\n-            self.args.distributed_state is None and self.args.local_rank != -1\n-        ):\n+        elif self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n             tensors = distributed_concat(tensors)\n         return tensors\n "
        },
        {
            "sha": "cd7489bcbdb933efc795c2a03ff4f44225c1d731",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -383,8 +383,6 @@ class TrainingArguments:\n             on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n             the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n             experimental API and it may change.\n-        local_rank (`int`, *optional*, defaults to -1):\n-            Rank of the process during distributed training.\n         ddp_backend (`str`, *optional*):\n             The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n         tpu_num_cores (`int`, *optional*):\n@@ -998,7 +996,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    local_rank: int = field(default=-1, metadata={\"help\": \"For distributed training: local_rank\"})\n     ddp_backend: Optional[str] = field(\n         default=None,\n         metadata={\n@@ -1900,8 +1897,7 @@ def _setup_devices(self) -> \"torch.device\":\n             self._n_gpu = 0\n         elif is_sagemaker_mp_enabled():\n             accelerator_state_kwargs[\"enabled\"] = False\n-            local_rank = smp.local_rank()\n-            device = torch.device(\"cuda\", local_rank)\n+            device = torch.device(\"cuda\", smp.local_rank())\n             torch.cuda.set_device(device)\n         elif is_sagemaker_dp_enabled():\n             accelerator_state_kwargs[\"_use_sagemaker_dp\"] = True\n@@ -1925,7 +1921,6 @@ def _setup_devices(self) -> \"torch.device\":\n                 del os.environ[\"ACCELERATE_USE_DEEPSPEED\"]\n         if not is_sagemaker_mp_enabled():\n             device = self.distributed_state.device\n-            self.local_rank = self.distributed_state.local_process_index\n         if dist.is_available() and dist.is_initialized() and self.parallel_mode != ParallelMode.DISTRIBUTED:\n             logger.warning(\n                 \"torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. \"\n@@ -2015,9 +2010,7 @@ def parallel_mode(self):\n             return ParallelMode.SAGEMAKER_MODEL_PARALLEL\n         elif is_sagemaker_dp_enabled():\n             return ParallelMode.SAGEMAKER_DATA_PARALLEL\n-        elif (\n-            self.distributed_state is not None and self.distributed_state.distributed_type != DistributedType.NO\n-        ) or (self.distributed_state is None and self.local_rank != -1):\n+        elif self.distributed_state is not None and self.distributed_state.distributed_type != DistributedType.NO:\n             return ParallelMode.DISTRIBUTED\n         elif self.n_gpu > 1:\n             return ParallelMode.NOT_DISTRIBUTED"
        },
        {
            "sha": "696d144c1dd78a5548e3c6c46169a6c632a8db1a",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -235,8 +235,8 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n-        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n+        + f\"distributed training: {bool(training_args.parallel_mode.value == 'distributed')}, 16-bits training: {training_args.fp16}\"\n     )\n     logger.info(f\"Training/evaluation parameters {training_args}\")\n "
        },
        {
            "sha": "cec877506b6b1a0888ca1a67ed650bb31b9352d3",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -207,8 +207,8 @@ def main():\n \n     # Log on each process the small summary:\n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n-        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n+        + f\"distributed training: {bool(training_args.parallel_mode.value == 'distributed')}, 16-bits training: {training_args.fp16}\"\n     )\n     # Set the verbosity to info of the Transformers logger (on main process only):\n     if training_args.should_log:"
        },
        {
            "sha": "37669013e679185d6875f6f0326ec4965ae2257c",
            "filename": "tests/trainer/test_trainer_distributed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -140,7 +140,7 @@ def test_trainer(self):\n     training_args = parser.parse_args_into_dataclasses()[0]\n \n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, n_gpu: {training_args.n_gpu}, \"\n         f\"distributed training: {training_args.parallel_mode != ParallelMode.NOT_DISTRIBUTED}\"\n     )\n \n@@ -152,7 +152,7 @@ def test_trainer(self):\n         def compute_metrics(p: EvalPrediction) -> dict:\n             sequential = list(range(len(dataset)))\n             success = p.predictions.tolist() == sequential and p.label_ids.tolist() == sequential\n-            if not success and training_args.local_rank == 0:\n+            if not success and training_args.local_process_index == 0:\n                 logger.warning(\n                     \"Predictions and/or labels do not match expected results:\\n  - predictions: \"\n                     f\"{p.predictions.tolist()}\\n  - labels: {p.label_ids.tolist()}\\n  - expected: {sequential}\""
        },
        {
            "sha": "75ee37015444e221f111c060a5fb105ead3e4e1d",
            "filename": "tests/trainer/test_trainer_tpu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/tests%2Ftrainer%2Ftest_trainer_tpu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91/tests%2Ftrainer%2Ftest_trainer_tpu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_tpu.py?ref=0419ff881d7bb503f4fc0f0a7a5aac3d012c9b91",
            "patch": "@@ -68,7 +68,7 @@ def main():\n     training_args = parser.parse_args_into_dataclasses()[0]\n \n     logger.warning(\n-        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, \"\n+        f\"Process rank: {training_args.local_process_index}, device: {training_args.device}, \"\n         f\"tpu_num_cores: {training_args.tpu_num_cores}\",\n     )\n "
        }
    ],
    "stats": {
        "total": 135,
        "additions": 63,
        "deletions": 72
    }
}