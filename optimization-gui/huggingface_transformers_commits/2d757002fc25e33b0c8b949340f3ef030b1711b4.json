{
    "author": "ArthurZucker",
    "message": "red-ci on main, fix copies (#33356)\n\n* fix copies\r\n\r\n* ???",
    "sha": "2d757002fc25e33b0c8b949340f3ef030b1711b4",
    "files": [
        {
            "sha": "0d12c800c156f710b3b18b8dbe25276c9c0047f1",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=2d757002fc25e33b0c8b949340f3ef030b1711b4",
            "patch": "@@ -916,7 +916,7 @@ def forward(\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n             the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n \n@@ -986,7 +986,7 @@ def forward(\n         )\n \n         # Expand the attention mask\n-        if use_sdpa_attention_masks:\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n             # Expand the attention mask for SDPA.\n             # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n             if self.config.is_decoder:\n@@ -1013,7 +1013,7 @@ def forward(\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n \n-            if use_sdpa_attention_masks:\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n                 # Expand the attention mask for SDPA.\n                 # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n                 encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa("
        },
        {
            "sha": "bbf16ec039b4cd450767d3709076623140f28e8b",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=2d757002fc25e33b0c8b949340f3ef030b1711b4",
            "patch": "@@ -857,7 +857,7 @@ def forward(\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n             the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n \n@@ -927,7 +927,7 @@ def forward(\n         )\n \n         # Expand the attention mask\n-        if use_sdpa_attention_masks:\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n             # Expand the attention mask for SDPA.\n             # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n             if self.config.is_decoder:\n@@ -954,7 +954,7 @@ def forward(\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n \n-            if use_sdpa_attention_masks:\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n                 # Expand the attention mask for SDPA.\n                 # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n                 encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa("
        },
        {
            "sha": "2adae33fbd50a849e65e237d32a12bcd0fee2de8",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=2d757002fc25e33b0c8b949340f3ef030b1711b4",
            "patch": "@@ -858,7 +858,7 @@ def forward(\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n             the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n \n@@ -928,7 +928,7 @@ def forward(\n         )\n \n         # Expand the attention mask\n-        if use_sdpa_attention_masks:\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n             # Expand the attention mask for SDPA.\n             # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n             if self.config.is_decoder:\n@@ -955,7 +955,7 @@ def forward(\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n \n-            if use_sdpa_attention_masks:\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n                 # Expand the attention mask for SDPA.\n                 # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n                 encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa("
        },
        {
            "sha": "f86abf823e902612e4b06582b290b871ef063b1c",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2d757002fc25e33b0c8b949340f3ef030b1711b4/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=2d757002fc25e33b0c8b949340f3ef030b1711b4",
            "patch": "@@ -839,7 +839,7 @@ def forward(\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n             the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n \n@@ -909,7 +909,7 @@ def forward(\n         )\n \n         # Expand the attention mask\n-        if use_sdpa_attention_masks:\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n             # Expand the attention mask for SDPA.\n             # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n             if self.config.is_decoder:\n@@ -936,7 +936,7 @@ def forward(\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n \n-            if use_sdpa_attention_masks:\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n                 # Expand the attention mask for SDPA.\n                 # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n                 encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa("
        }
    ],
    "stats": {
        "total": 24,
        "additions": 12,
        "deletions": 12
    }
}