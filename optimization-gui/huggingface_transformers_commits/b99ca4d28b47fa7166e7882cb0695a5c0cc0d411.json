{
    "author": "yonigozlan",
    "message": "Add support for OpenAI api \"image_url\" input in chat for image-text-to-text pipeline (#34562)\n\n* add support for openai api image_url input\r\n\r\n* change continue to elif\r\n\r\n* Explicitely add support for OpenAI/TGI chat format\r\n\r\n* rewrite content to transformers chat format and add tests\r\n\r\n* Add support for typing of image type in chat templates\r\n\r\n* add base64 to possible image types\r\n\r\n* refactor nesting",
    "sha": "b99ca4d28b47fa7166e7882cb0695a5c0cc0d411",
    "files": [
        {
            "sha": "5afba0d7c0410ed5ee7a0f4d53d0f791b43c6f8c",
            "filename": "src/transformers/pipelines/image_text_to_text.py",
            "status": "modified",
            "additions": 26,
            "deletions": 10,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/b99ca4d28b47fa7166e7882cb0695a5c0cc0d411/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b99ca4d28b47fa7166e7882cb0695a5c0cc0d411/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_text_to_text.py?ref=b99ca4d28b47fa7166e7882cb0695a5c0cc0d411",
            "patch": "@@ -75,16 +75,32 @@ def retrieve_images_in_messages(\n     retrieved_images = []\n     for message in messages:\n         for content in message[\"content\"]:\n-            if isinstance(content, dict) and content.get(\"type\") == \"image\":\n-                if \"image\" in content:\n-                    retrieved_images.append(content[\"image\"])\n-                elif idx_images < len(images):\n-                    retrieved_images.append(images[idx_images])\n-                    idx_images += 1\n-                else:\n-                    raise ValueError(\n-                        \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n-                    )\n+            if isinstance(content, dict):\n+                if content.get(\"type\") == \"image\":\n+                    for key in [\"image\", \"url\", \"path\", \"base64\"]:\n+                        if key in content:\n+                            retrieved_images.append(content[key])\n+                            break\n+                    else:\n+                        if idx_images < len(images):\n+                            retrieved_images.append(images[idx_images])\n+                            idx_images += 1\n+                        else:\n+                            raise ValueError(\n+                                \"The number of images in the chat messages should be the same as the number of images passed to the pipeline.\"\n+                            )\n+                # Add support for OpenAI/TGI chat format\n+                elif content.get(\"type\") == \"image_url\":\n+                    if isinstance(content.get(\"image_url\"), dict) and \"url\" in content[\"image_url\"]:\n+                        retrieved_images.append(content[\"image_url\"][\"url\"])\n+                        # Rewrite content to be in the Transformers chat format\n+                        content[\"type\"] = \"image\"\n+                        content[\"image\"] = content[\"image_url\"][\"url\"]\n+                        del content[\"image_url\"]\n+                    else:\n+                        raise ValueError(\n+                            \"Wrong format for 'image_url' content type. The content should have an 'image_url' dict with a 'url' key.\"\n+                        )\n \n     # The number of images passed should be consistent with the number of images in the chat without an image key\n     if idx_images != len(images):"
        },
        {
            "sha": "7b9e17edd36fe996236dc857c41260b3a9d34097",
            "filename": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b99ca4d28b47fa7166e7882cb0695a5c0cc0d411/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b99ca4d28b47fa7166e7882cb0695a5c0cc0d411/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py?ref=b99ca4d28b47fa7166e7882cb0695a5c0cc0d411",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import base64\n import unittest\n \n from transformers import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING, is_vision_available\n@@ -258,3 +259,46 @@ def test_model_pt_chat_template_new_text(self):\n                 }\n             ],\n         )\n+\n+    @slow\n+    @require_torch\n+    def test_model_pt_chat_template_image_url(self):\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image_url\",\n+                        \"image_url\": {\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+                        },\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image in one sentence.\"},\n+                ],\n+            }\n+        ]\n+        outputs = pipe(text=messages, return_full_text=False, max_new_tokens=10)[0][\"generated_text\"]\n+        self.assertEqual(outputs, \"The image captures the iconic Statue of Liberty, a\")\n+\n+    @slow\n+    @require_torch\n+    def test_model_pt_chat_template_image_url_base64(self):\n+        with open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\", \"rb\") as image_file:\n+            base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")\n+\n+        pipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image_url\",\n+                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image in one sentence.\"},\n+                ],\n+            }\n+        ]\n+        outputs = pipe(text=messages, return_full_text=False, max_new_tokens=10)[0][\"generated_text\"]\n+        self.assertEqual(outputs, \"Two cats are sleeping on a pink blanket, with\")"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 70,
        "deletions": 10
    }
}