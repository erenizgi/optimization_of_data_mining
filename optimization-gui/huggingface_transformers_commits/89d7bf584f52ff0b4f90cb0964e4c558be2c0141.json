{
    "author": "tibor-reiss",
    "message": "ðŸš¨ðŸš¨ðŸš¨ Uniformize kwargs for TrOCR Processor (#34587)\n\n* Make kwargs uniform for TrOCR\r\n\r\n* Add tests\r\n\r\n* Put back current_processor\r\n\r\n* Remove args\r\n\r\n* Add todo comment\r\n\r\n* Code review - breaking change",
    "sha": "89d7bf584f52ff0b4f90cb0964e4c558be2c0141",
    "files": [
        {
            "sha": "16b75b9812b482c93ad4a492fd601d225a649f19",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 26,
            "deletions": 11,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/89d7bf584f52ff0b4f90cb0964e4c558be2c0141/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89d7bf584f52ff0b4f90cb0964e4c558be2c0141/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=89d7bf584f52ff0b4f90cb0964e4c558be2c0141",
            "patch": "@@ -18,8 +18,16 @@\n \n import warnings\n from contextlib import contextmanager\n+from typing import List, Union\n \n-from ...processing_utils import ProcessorMixin\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class TrOCRProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n \n \n class TrOCRProcessor(ProcessorMixin):\n@@ -61,7 +69,14 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         self.current_processor = self.image_processor\n         self._in_target_context_manager = False\n \n-    def __call__(self, *args, **kwargs):\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[TrOCRProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n         When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n         [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n@@ -70,21 +85,21 @@ def __call__(self, *args, **kwargs):\n         \"\"\"\n         # For backward compatibility\n         if self._in_target_context_manager:\n-            return self.current_processor(*args, **kwargs)\n-\n-        images = kwargs.pop(\"images\", None)\n-        text = kwargs.pop(\"text\", None)\n-        if len(args) > 0:\n-            images = args[0]\n-            args = args[1:]\n+            return self.current_processor(images, **kwargs)\n \n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n \n+        output_kwargs = self._merge_kwargs(\n+            TrOCRProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n         if images is not None:\n-            inputs = self.image_processor(images, *args, **kwargs)\n+            inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         if text is not None:\n-            encodings = self.tokenizer(text, **kwargs)\n+            encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n         if text is None:\n             return inputs"
        },
        {
            "sha": "b76af40280f2fe708df79b6c41d1bfdb7a97ab33",
            "filename": "tests/models/trocr/test_processor_trocr.py",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/89d7bf584f52ff0b4f90cb0964e4c558be2c0141/tests%2Fmodels%2Ftrocr%2Ftest_processor_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89d7bf584f52ff0b4f90cb0964e4c558be2c0141/tests%2Fmodels%2Ftrocr%2Ftest_processor_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftrocr%2Ftest_processor_trocr.py?ref=89d7bf584f52ff0b4f90cb0964e4c558be2c0141",
            "patch": "@@ -0,0 +1,129 @@\n+import os\n+import shutil\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers.models.xlm_roberta.tokenization_xlm_roberta import VOCAB_FILES_NAMES\n+from transformers.testing_utils import (\n+    require_sentencepiece,\n+    require_tokenizers,\n+    require_vision,\n+)\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import TrOCRProcessor, ViTImageProcessor, XLMRobertaTokenizerFast\n+\n+\n+@require_sentencepiece\n+@require_tokenizers\n+@require_vision\n+class TrOCRProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    text_input_name = \"labels\"\n+    processor_class = TrOCRProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        vocab_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"want\", \"##want\", \"##ed\", \"wa\", \"un\", \"runn\", \"##ing\", \",\", \"low\", \"lowest\"]  # fmt: skip\n+        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n+\n+        image_processor = ViTImageProcessor.from_pretrained(\"hf-internal-testing/tiny-random-vit\")\n+        tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n+        processor = TrOCRProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return XLMRobertaTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    def get_image_processor(self, **kwargs):\n+        return ViTImageProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    def test_save_load_pretrained_default(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        processor = TrOCRProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = TrOCRProcessor.from_pretrained(self.tmpdirname)\n+\n+        self.assertIsInstance(processor.tokenizer, XLMRobertaTokenizerFast)\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertIsInstance(processor.image_processor, ViTImageProcessor)\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        processor = TrOCRProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())\n+        processor.save_pretrained(self.tmpdirname)\n+        tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS)\", eos_token=\"(EOS)\")\n+        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n+\n+        processor = TrOCRProcessor.from_pretrained(\n+            self.tmpdirname, bos_token=\"(BOS)\", eos_token=\"(EOS)\", do_normalize=False, padding_value=1.0\n+        )\n+\n+        self.assertIsInstance(processor.tokenizer, XLMRobertaTokenizerFast)\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n+\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n+        self.assertIsInstance(processor.image_processor, ViTImageProcessor)\n+\n+    def test_image_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        processor = TrOCRProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        image_input = self.prepare_image_inputs()\n+\n+        input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n+        input_processor = processor(images=image_input, return_tensors=\"np\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_tokenizer(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        processor = TrOCRProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        input_str = \"lower newer\"\n+\n+        encoded_processor = processor(text=input_str)\n+        encoded_tok = tokenizer(input_str)\n+\n+        for key in encoded_tok.keys():\n+            self.assertListEqual(encoded_tok[key], encoded_processor[key])\n+\n+    def test_processor_text(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        processor = TrOCRProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+\n+        self.assertListEqual(list(inputs.keys()), [\"pixel_values\", \"labels\"])\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n+    def test_tokenizer_decode(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        processor = TrOCRProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+        predicted_ids = [[1, 4, 5, 8, 1, 0, 8], [3, 4, 3, 1, 1, 8, 9]]\n+\n+        decoded_processor = processor.batch_decode(predicted_ids)\n+        decoded_tok = tokenizer.batch_decode(predicted_ids)\n+\n+        self.assertListEqual(decoded_tok, decoded_processor)"
        }
    ],
    "stats": {
        "total": 166,
        "additions": 155,
        "deletions": 11
    }
}