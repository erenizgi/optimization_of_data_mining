{
    "author": "bzantium",
    "message": "[WIP] add deepseek-v3 (#35926)\n\n* init commit\n\n* style\n\n* take comments into account\n\n* add deepseekv3 modeling\n\n* remove redundant code\n\n* apply make style\n\n* apply fix-copies\n\n* make format\n\n* add init files\n\n* rename deepseekv3 into deepseek_v3 based on its model_type\n\n* rename deepseekv3 into deepseek_v3 based on its model_type\n\n* deepseek-v3 not deepseek_v3\n\n* set model_type as deepseek_v3\n\n* use default docs\n\n* apply make\n\n* fill type and docstring\n\n* add rope_config_validation\n\n* use custom DeepseekV3MLP\n\n* hold code only for checkpoints congifuration; remove redundant\n\n* revise rope yarn for DeepSeek variation\n\n* rename DeepSeek-V3\n\n* some refactoring\n\n* revise load_hook to work properly; make moe func trainable; use llama instead of mixtral\n\n* fix attention forward\n\n* use -1 for not-changing dim when to use exapnd\n\n* refactor DeepseekV3TopkRouter\n\n* use reshape_for_rope instead of load_hook; revise attention forward for TP; rename q_head_dim with qk_head_dim\n\n* register pre_hook and hook both\n\n* make style\n\n* use n_shared_experts\n\n* Update src/transformers/models/deepseek_v3/configuration_deepseek_v3.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* add test file\n\n* update modeling_file according to modular file\n\n* make style\n\n* add mapping for DeepseekV3ForSequenceClassification\n\n* remove aux_loss_alpha\n\n* add deepseek_v3 for perf\n\n* add deepseek_v3\n\n* rename test as deepseekv3\n\n* use tiny-deepseek-v3\n\n* remove DeepseekV3ForSequenceClassification\n\n* cache before padding\n\n* remote output_router_logits\n\n* Revert \"remote output_router_logits\"\n\nThis reverts commit f264f800d04950390db8413b9efb24cef8186330.\n\n* remove output_router_logits\n\n* make e_score_correction_bias as buffer\n\n* skip tests not compatible\n\n* make style\n\n* make e_score_correction_bias as buffer\n\n* use rope_interleave instead of load_hook\n\n* skip tests not compatible with MLA\n\n* add doc for rope_interleave\n\n* fix typo\n\n* remove torch.no_grad for selecting topk\n\n* fix post merge issue\n\n* mrege with main and simplify\n\n* nits\n\n* final\n\n* small fixes\n\n* fix\n\n* support TP better\n\n* stash\n\n* changes currently requires\n\n* remove synch\n\n* more fixes for TP\n\n* temp fix for TP : some attention layers's FP8 scales are too small + shared is local colwise and anything is local if FP8 because weights are used\n\n* updates to have generation work!\n\n* push most of the changes\n\n* reorder functions + call for contributions!\n\n* update readme\n\n* nits\n\n* update\n\n* ruff was updated on main\n\n* merge with main and fix copies\n\n* revert unrelated changes\n\n* route all tokens to all experts when testing to avoid no gradient iddues\n\n* finish fixing all tests\n\n* fixup\n\n* nit\n\n* clean config\n\n* last readme changes\n\n* nit\n\n* do cnit\n\n* typo\n\n* last nit\n\n* one more one more\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: arthur@huggingface.co <arthur@ip-26-0-165-131.ec2.internal>",
    "sha": "eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
    "files": [
        {
            "sha": "4dd3cc6d3d73b1f969cd5133224e6cc98cf6bead",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -415,6 +415,8 @@\n         title: DeBERTa\n       - local: model_doc/deberta-v2\n         title: DeBERTa-v2\n+      - local: model_doc/deepseek_v3\n+        title: DeepSeek-V3\n       - local: model_doc/dialogpt\n         title: DialoGPT\n       - local: model_doc/diffllama"
        },
        {
            "sha": "c3322a102f6e4e8416e683956b3e90b99d3d8895",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "added",
            "additions": 184,
            "deletions": 0,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -0,0 +1,184 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# DeepSeek-V3\n+\n+## Overview\n+\n+The DeepSeek-V3 model was proposed in [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) by DeepSeek-AI Team.\n+\n+The abstract from the paper is the following:\n+We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n+\n+## Limitations and call for contribution!\n+\n+We are super happy to make this code community-powered, and would love to see how you can best optimize the following: \n+\n+- current implementation uses the \"naive\" attention compution (so not really MLA)\n+- current implementation loops through the experts. This should be replaced. Pointers to use `get_packed_weights` from `intetrations/tensor_parallel`. \n+- current implementation uses the eleuther formula for ROPE, using the orginal one would be more efficient! (should still follow our API)\n+- static cache is not supported (this should be just a generation config issue / config shape issues)\n+\n+### Usage tips\n+The model uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures for efficient inference and cost-effective training. It employs an auxiliary-loss-free strategy for load balancing and multi-token prediction training objective. The model can be used for various language tasks after being pre-trained on 14.8 trillion tokens and going through Supervised Fine-Tuning and Reinforcement Learning stages.\n+\n+You can run the model in `FP8` automatically, using 2 nodes of 8 H100 should be more than enough! \n+\n+```python\n+# `run_deepseek_v1.py`\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+import torch\n+torch.manual_seed(30)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"deepseek-r1\")\n+\n+chat = [\n+  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n+  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n+  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n+]\n+\n+\n+model = AutoModelForCausalLM.from_pretrained(\"deepseek-r1\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n+import time\n+start = time.time()\n+outputs = model.generate(inputs, max_new_tokens=50)\n+print(tokenizer.batch_decode(outputs))\n+print(time.time()-start)\n+```\n+This generated: \n+\n+``````\n+<ï½œAssistantï½œ><think>\n+Okay, the user wants to demonstrate how chat templating works. Let me break down what that means. Chat templating is about structuring the conversation data, especially for models that need specific input formats. Maybe they're referring to something like how messages are formatted with roles (user, assistant, system) in APIs like OpenAI.\n+\n+First, I should explain what chat templating is. It's the process of formatting conversation data into a structured format that the model can understand. This usually includes roles and content. For example, user messages, assistant responses, and system messages each have their own role tags.\n+\n+They might want an example. Let me think of a simple conversation. The user says \"Hello, how are you?\" and the assistant responds \"I'm doing great. How can I help you today?\" Then the user follows up with wanting to show off chat templating. So the example should include the history and the new message.\n+\n+In some frameworks, like Hugging Face's Transformers, chat templates are applied using Jinja2 templates. The template might look something like combining system messages, then looping through user and assistant messages with appropriate tags. For instance, using {% for message in messages %} and assigning roles like <|user|>, <|assistant|>, etc.\n+\n+I should structure the example with the messages array, showing each role and content. Then apply a hypothetical template to convert that into a formatted string the model uses. Also, mention that different models have different templating requirements, like using special tokens or varying role labels.\n+\n+Wait, the user mentioned \"chat templating\" in the context of showing off. Maybe they want a practical example they can present. So providing a code snippet or a structured data example would be helpful. Let me outline a typical messages array and then the templated output.\n+\n+Also, it's important to note that proper templating ensures the model knows the conversation flow, which is crucial for generating coherent responses. Maybe include a note about why it's important, like maintaining context and role-specific processing.\n+\n+Let me check if there are any common mistakes or things to avoid. For example, not closing tags properly, or mismatching roles. But maybe that's too detailed unless the user asks. Focus on the positive example first.\n+\n+Putting it all together, the response should have an example messages array, the applied template, and the final formatted string. Maybe use angle brackets or special tokens as placeholders. Also, mention that this helps in training or fine-tuning models with structured data.\n+\n+I think that's a solid approach. Let me structure it step by step to make it clear.\n+</think>\n+\n+Chat templating is a way to structure conversation data (e.g., user/assistant interactions) into a format that language models understand. This is especially important for models trained to handle multi-turn dialogues, where the input must explicitly separate roles (user, assistant, system, etc.) and messages. Letâ€™s break this down with an example!\n+\n+---\n+\n+### **Step 1: Raw Conversation History**\n+Suppose we have this conversation:\n+- **User**: \"Hello, how are you?\"\n+- **Assistant**: \"I'm doing great. How can I help you today?\"\n+- **User**: \"I'd like to show off how chat templating works!\"\n+\n+---\n+\n+### **Step 2: Structured Messages**\n+In frameworks like Hugging Face Transformers or OpenAI, conversations are often formatted as a list of dictionaries with `role` and `content`:\n+```python\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n+    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n+    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n+]\n+```\n+\n+---\n+\n+### **Step 3: Apply a Chat Template**\n+A **chat template** converts this structured data into a single string formatted for the model. For example, using a Jinja-style template (common in Hugging Face):\n+\n+```jinja\n+{% for message in messages %}\n+    {% if message['role'] == 'user' %}\n+        <|user|>{{ message['content'] }}<|end|>\n+    {% elif message['role'] == 'assistant' %}\n+        <|assistant|>{{ message['content'] }}<|end|>\n+    {% endif %}\n+{% endfor %}\n+<|assistant|>\n+```\n+\n+---\n+\n+### **Step 4: Final Templated Output**\n+Applying the template to our `messages` list would produce:\n+```text\n+<|user|>Hello, how are you?<|end|>\n+<|assistant|>I'm doing great. How can I help you today?<|end|>\n+<|user|>I'd like to show off how chat templating works!<|end|>\n+<|assistant|>\n+```\n+\n+This tells the model:  \n+1. The conversation history (user/assistant turns).  \n+2. The modelâ€™s turn to generate a response (`<|assistant|>` at the end).  \n+\n+---\n+\n+### **Key Notes**:\n+- **Role Separation**: Tags like `<|user|>` and `<|assistant|>` help the model distinguish speakers.\n+- **Special Tokens**: Models often use unique tokens (e.g., `<|end|>`) to mark message boundaries.\n+- **Flexibility**: Templates vary by model (e.g., OpenAI uses `{\"role\": \"user\", \"content\": \"...\"}` instead of tags).\n+\n+---\n+\n+### **Why This Matters**:\n+- **Consistency**: Ensures the model understands dialogue structure.\n+- **Context Preservation**: Maintains the flow of multi-turn conversations.\n+- **Alignment**: Matches the format the model was trained on for better performance.\n+\n+Want to dive deeper or see a specific frameworkâ€™s implementation (e.g., OpenAI, Llama, Mistral)? Let me know! ðŸ˜Š<ï½œendâ–ofâ–sentenceï½œ>\n+``````\n+\n+Use the following to run it\n+```bash\n+torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0|1 --rdzv-id an_id --rdzv-backend c10d --rdzv-endpoint master_addr:master_port run_deepseek_r1.py\n+```\n+\n+If you have: \n+```bash\n+[rank0]: ncclInternalError: Internal check failed.\n+[rank0]: Last error:\n+[rank0]: Bootstrap : no socket interface found\n+```\n+error, it means NCCL was probably not loaded. \n+\n+\n+## DeepseekV3Config\n+\n+[[autodoc]] DeepseekV3Config\n+\n+## DeepseekV3Model\n+\n+[[autodoc]] DeepseekV3Model\n+    - forward\n+\n+## DeepseekV3ForCausalLM\n+\n+[[autodoc]] DeepseekV3ForCausalLM\n+    - forward"
        },
        {
            "sha": "3892541b66d9917721929f7ed09c96b871ed5456",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -345,6 +345,7 @@\n     ],\n     \"models.deberta_v2\": [\"DebertaV2Config\"],\n     \"models.decision_transformer\": [\"DecisionTransformerConfig\"],\n+    \"models.deepseek_v3\": [\"DeepseekV3Config\"],\n     \"models.deformable_detr\": [\"DeformableDetrConfig\"],\n     \"models.deit\": [\"DeiTConfig\"],\n     \"models.deprecated\": [],\n@@ -2023,6 +2024,13 @@\n             \"DecisionTransformerPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.deepseek_v3\"].extend(\n+        [\n+            \"DeepseekV3ForCausalLM\",\n+            \"DeepseekV3Model\",\n+            \"DeepseekV3PreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.deformable_detr\"].extend(\n         [\n             \"DeformableDetrForObjectDetection\",\n@@ -5546,6 +5554,9 @@\n     from .models.decision_transformer import (\n         DecisionTransformerConfig,\n     )\n+    from .models.deepseek_v3 import (\n+        DeepseekV3Config,\n+    )\n     from .models.deformable_detr import (\n         DeformableDetrConfig,\n     )\n@@ -7175,6 +7186,11 @@\n             DecisionTransformerModel,\n             DecisionTransformerPreTrainedModel,\n         )\n+        from .models.deepseek_v3 import (\n+            DeepseekV3ForCausalLM,\n+            DeepseekV3Model,\n+            DeepseekV3PreTrainedModel,\n+        )\n         from .models.deformable_detr import (\n             DeformableDetrForObjectDetection,\n             DeformableDetrModel,"
        },
        {
            "sha": "d4e472a990dcfc1226d03a12bdeb7f3bd993dc7f",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -291,7 +291,7 @@ def w8a8_block_fp8_matmul_compile(\n     return output.to(output_dtype)\n \n \n-class FP8Linear(nn.Module):\n+class FP8Linear(nn.Linear):\n     dtype = torch.float8_e4m3fn\n \n     def __init__(\n@@ -304,17 +304,20 @@ def __init__(\n         device=None,\n         activation_scheme=\"dynamic\",\n     ):\n-        super().__init__()\n+        super().__init__(in_features, out_features)\n         self.in_features = in_features\n         self.out_features = out_features\n \n-        self.register_buffer(\"weight\", torch.empty(out_features, in_features, dtype=FP8Linear.dtype, device=device))\n+        self.weight = torch.nn.Parameter(torch.empty(out_features, in_features, dtype=FP8Linear.dtype, device=device))\n \n-        scale_out_features = (out_features + block_size[0] - 1) // block_size[0]\n-        scale_in_features = (in_features + block_size[1] - 1) // block_size[1]\n-        self.register_buffer(\n-            \"weight_scale_inv\", torch.empty(scale_out_features, scale_in_features, dtype=torch.float32, device=device)\n-        )\n+        if self.weight.element_size() == 1:\n+            scale_out_features = (out_features + block_size[0] - 1) // block_size[0]\n+            scale_in_features = (in_features + block_size[1] - 1) // block_size[1]\n+            self.weight_scale_inv = nn.Parameter(\n+                torch.empty(scale_out_features, scale_in_features, dtype=torch.float32, device=device)\n+            )\n+        else:\n+            self.register_parameter(\"weight_scale_inv\", None)\n \n         self.block_size = block_size\n \n@@ -330,11 +333,11 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n             return F.linear(input, self.weight, self.bias)\n         else:\n             # Context manager used to switch among the available cuda devices\n-            with torch.cuda.device(input.device):\n-                qinput, scale = act_quant(input, self.block_size[1])\n+            # with torch.cuda.device(input.device):\n+            qinput, scale = act_quant(input, self.block_size[1])\n             # Blocks the CPU until all CUDA operations on the specified device are complete. It is used to ensure that the results of the\n             # preceding operations are ready before proceeding\n-            torch.cuda.synchronize(device=input.device)\n+            # torch.cuda.synchronize(device=self.weight.device)\n             with torch.cuda.device(input.device):\n                 output = w8a8_block_fp8_matmul_triton(\n                     qinput,\n@@ -344,14 +347,15 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n                     self.block_size,\n                     output_dtype=input.dtype,\n                 )\n-            torch.cuda.synchronize(device=input.device)\n+            torch.cuda.synchronize()\n             if self.bias is not None:\n                 output = output + self.bias\n             return output.to(dtype=input.dtype)\n \n \n def _replace_with_fp8_linear(\n     model,\n+    tp_plan=None,\n     modules_to_not_convert=None,\n     current_key_name=None,\n     quantization_config=None,\n@@ -378,10 +382,12 @@ def _replace_with_fp8_linear(\n                         block_size=quantization_config.weight_block_size,\n                     )\n                     has_been_replaced = True\n+            # when changing a layer the TP PLAN for that layer should be updated. TODO\n \n         if len(list(module.children())) > 0:\n             _, has_been_replaced = _replace_with_fp8_linear(\n                 module,\n+                tp_plan,\n                 modules_to_not_convert,\n                 current_key_name,\n                 quantization_config,\n@@ -404,9 +410,9 @@ def replace_with_fp8_linear(\n     if quantization_config.modules_to_not_convert is not None:\n         modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n     modules_to_not_convert = list(set(modules_to_not_convert))\n-\n     model, has_been_replaced = _replace_with_fp8_linear(\n         model,\n+        tp_plan=model._tp_plan,\n         modules_to_not_convert=modules_to_not_convert,\n         quantization_config=quantization_config,\n     )"
        },
        {
            "sha": "f8a04c037b9353cd1450dc25ff7f8004a175319f",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -231,8 +231,8 @@ def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         distribute_module(\n             module,\n             device_mesh,\n-            partial(self._prepare_input_fn),\n-            partial(self._prepare_output_fn),\n+            partial(self._prepare_input_fn, None, None),\n+            partial(self._prepare_output_fn, None, None),\n         )\n \n \n@@ -484,7 +484,12 @@ def __init__(self):\n     # 1. We add hooks to the layer being loaded:\n     if current_module_plan is not None:\n         tp_layer = translate_to_torch_parallel_style(current_module_plan)\n-        tp_layer.prepare_module_tp(module, device_mesh)\n+        try:\n+            tp_layer.prepare_module_tp(module, device_mesh)\n+        except NotImplementedError as e:\n+            print(\n+                f\"Trying to prepare {layer_name}, but it's not supported. Corresponding module: {module} Fix it's TP plan: {e}\"\n+            )\n \n     # 2. We add hooks to the parrent module if needed\n     if \".\" in layer_name:\n@@ -531,6 +536,7 @@ def shard_and_distribute_module(\n             param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n         )\n     else:\n+        # TODO log no plan modules in set\n         param = param[...].to(param_casting_dtype)\n         if is_contiguous:\n             param = param.contiguous()"
        },
        {
            "sha": "7f7fbb3df46939ca345af91355abc35abc35b493",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 6,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -189,13 +189,31 @@ def _compute_yarn_parameters(\n     partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n     dim = int(head_dim * partial_rotary_factor)\n-    max_position_embeddings = config.max_position_embeddings\n     factor = config.rope_scaling[\"factor\"]\n+    attention_factor = config.rope_scaling.get(\"attention_factor\")\n+    mscale = config.rope_scaling.get(\"mscale\")\n+    mscale_all_dim = config.rope_scaling.get(\"mscale_all_dim\")\n+\n+    # NOTE: DeekSeek-V3 (and potentially other models) modify `max_position_embeddings` and have a\n+    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n+    # values to compute the default attention scaling factor, instead of using `factor`.\n+    if \"original_max_position_embeddings\" in config.rope_scaling:\n+        original_max_position_embeddings = config.rope_scaling[\"original_max_position_embeddings\"]\n+        factor = config.max_position_embeddings / original_max_position_embeddings\n+    else:\n+        original_max_position_embeddings = config.max_position_embeddings\n+\n+    def get_mscale(scale, mscale=1):\n+        if scale <= 1:\n+            return 1.0\n+        return 0.1 * mscale * math.log(scale) + 1.0\n \n     # Sets the attention factor as suggested in the paper\n-    attention_factor = config.rope_scaling.get(\"attention_factor\")\n     if attention_factor is None:\n-        attention_factor = 0.1 * math.log(factor) + 1.0\n+        if mscale and mscale_all_dim:\n+            attention_factor = float(get_mscale(factor, mscale) / get_mscale(factor, mscale_all_dim))\n+        else:\n+            attention_factor = get_mscale(factor)\n \n     # Optional config options\n     # beta_fast/beta_slow: as suggested in the paper, default to 32/1 (correspondingly)\n@@ -227,15 +245,14 @@ def linear_ramp_factor(min, max, dim):\n     inv_freq_extrapolation = 1.0 / pos_freqs\n     inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n \n-    low, high = find_correction_range(beta_fast, beta_slow, dim, base, max_position_embeddings)\n+    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings)\n \n     # Get n-dimensional rotational scaling corrected for extrapolation\n     inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).float().to(device)\n     inv_freq = (\n         inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)\n         + inv_freq_extrapolation * inv_freq_extrapolation_factor\n     )\n-\n     return inv_freq, attention_factor\n \n \n@@ -425,7 +442,14 @@ def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[se\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n-    optional_keys = {\"attention_factor\", \"beta_fast\", \"beta_slow\", \"original_max_position_embeddings\"}\n+    optional_keys = {\n+        \"attention_factor\",\n+        \"beta_fast\",\n+        \"beta_slow\",\n+        \"original_max_position_embeddings\",\n+        \"mscale\",\n+        \"mscale_all_dim\",\n+    }\n     received_keys = set(rope_scaling.keys())\n     _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n "
        },
        {
            "sha": "137bd01c01de26ada35623540ef2d9f350467230",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -779,8 +779,7 @@ def _load_state_dict_into_meta_model(\n         device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n \n     is_quantized = hf_quantizer is not None\n-    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_quantized\n-\n+    is_meta_state_dict = shard_file.endswith(\".safetensors\")\n     file_pointer = None\n     if is_meta_state_dict:\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n@@ -795,7 +794,7 @@ def _load_state_dict_into_meta_model(\n             serialized_param_name = reverse_renaming_mapping[param_name]\n             param = file_pointer.get_slice(serialized_param_name)\n         else:\n-            param = empty_param  # It is actually not empty!\n+            param = empty_param.to(tensor_device)  # It is actually not empty!\n \n         to_contiguous, casting_dtype = _infer_parameter_dtype(\n             model,\n@@ -813,7 +812,7 @@ def _load_state_dict_into_meta_model(\n                 param_name,\n                 casting_dtype,\n                 to_contiguous,\n-                tensor_device,  # the rank\n+                int(os.environ[\"RANK\"]),  # the rank\n                 device_mesh,\n             )\n         else:\n@@ -4102,11 +4101,12 @@ def from_pretrained(\n                 raise EnvironmentError(\"tensor parallel is only supported for `torch>=2.5`.\")\n             if not torch.distributed.is_initialized():\n                 try:\n-                    logger.warning(\"Tensor Parallel requires torch.distributed to be initialized first.\")\n                     rank = int(os.environ[\"RANK\"])\n                     world_size = int(os.environ[\"WORLD_SIZE\"])\n-                    torch.distributed.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n-                    torch.cuda.set_device(rank)\n+                    torch.distributed.init_process_group(\n+                        \"nccl\", rank=rank, world_size=world_size, init_method=\"env://\"\n+                    )\n+                    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n                 except Exception as e:\n                     raise EnvironmentError(\n                         \"We tried to initialize torch.distributed for you, but it failed, make\"\n@@ -4115,12 +4115,13 @@ def from_pretrained(\n \n             # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n             device_type = torch._C._get_accelerator().type\n-            device_module = torch.get_device_module(device_type)\n-            # Get device with index assuming equal number of devices per host\n-            tp_device = torch.device(device_type, torch.distributed.get_rank() % device_module.device_count())\n+            tp_device = torch.device(device_type, torch.cuda.current_device())\n+            if tp_device.index > 0:\n+                import sys\n+\n+                sys.stdout = open(os.devnull, \"w\")\n             # This is the easiest way to dispatch to the current process device\n             device_map = tp_device\n-\n             # Assuming sharding the model onto the world\n             world_size = torch.distributed.get_world_size()\n             device_mesh = torch.distributed.init_device_mesh(tp_device.type, (world_size,))\n@@ -4871,9 +4872,9 @@ def _load_pretrained_model(\n             expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n \n         # Warmup cuda to load the weights much faster on devices\n-        if device_map is not None and hf_quantizer is None:\n+        if device_map is not None:  # and hf_quantizer is None:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n-            caching_allocator_warmup(model_to_load, expanded_device_map)\n+            caching_allocator_warmup(model_to_load, expanded_device_map, factor=2 if hf_quantizer is None else 4)\n \n         error_msgs = []\n         mismatched_keys = []\n@@ -5834,7 +5835,7 @@ def expand_device_map(device_map, param_names):\n     return new_device_map\n \n \n-def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict):\n+def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, factor=2):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n     the model, which is actually the loading speed botteneck.\n@@ -5865,7 +5866,6 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict):\n         if _torch_distributed_available and torch.distributed.is_initialized()\n         else None\n     )\n-\n     total_byte_count = defaultdict(lambda: 0)\n     for param_name, device in accelerator_device_map.items():\n         param = model.get_parameter_or_buffer(param_name)\n@@ -5886,7 +5886,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict):\n             # Allow up to 95% of max device memory\n             byte_count = min(byte_count, int(0.95 * device_memory))\n         # Allocate memory\n-        _ = torch.empty(byte_count // 2, dtype=torch.float16, device=device, requires_grad=False)\n+        _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n \n \n def get_disk_only_shard_files(device_map, weight_map):"
        },
        {
            "sha": "64041f7f0fe2b427bc61247e3a2eb7bfbde31376",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -71,6 +71,7 @@\n     deberta,\n     deberta_v2,\n     decision_transformer,\n+    deepseek_v3,\n     deformable_detr,\n     deit,\n     deprecated,"
        },
        {
            "sha": "03ee0a5a2f0f4b23e0cec50d2dda35f3eabf77a2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -89,6 +89,7 @@\n         (\"deberta\", \"DebertaConfig\"),\n         (\"deberta-v2\", \"DebertaV2Config\"),\n         (\"decision_transformer\", \"DecisionTransformerConfig\"),\n+        (\"deepseek_v3\", \"DeepseekV3Config\"),\n         (\"deformable_detr\", \"DeformableDetrConfig\"),\n         (\"deit\", \"DeiTConfig\"),\n         (\"depth_anything\", \"DepthAnythingConfig\"),\n@@ -423,6 +424,7 @@\n         (\"deberta\", \"DeBERTa\"),\n         (\"deberta-v2\", \"DeBERTa-v2\"),\n         (\"decision_transformer\", \"Decision Transformer\"),\n+        (\"deepseek_v3\", \"DeepSeek-V3\"),\n         (\"deformable_detr\", \"Deformable DETR\"),\n         (\"deit\", \"DeiT\"),\n         (\"deplot\", \"DePlot\"),"
        },
        {
            "sha": "c98ac16988a2dd005216613dbc273f34392eae2e",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -88,6 +88,7 @@\n         (\"deberta\", \"DebertaModel\"),\n         (\"deberta-v2\", \"DebertaV2Model\"),\n         (\"decision_transformer\", \"DecisionTransformerModel\"),\n+        (\"deepseek_v3\", \"DeepseekV3Model\"),\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n         (\"depth_pro\", \"DepthProModel\"),\n@@ -514,6 +515,7 @@\n         (\"ctrl\", \"CTRLLMHeadModel\"),\n         (\"data2vec-text\", \"Data2VecTextForCausalLM\"),\n         (\"dbrx\", \"DbrxForCausalLM\"),\n+        (\"deepseek_v3\", \"DeepseekV3ForCausalLM\"),\n         (\"diffllama\", \"DiffLlamaForCausalLM\"),\n         (\"electra\", \"ElectraForCausalLM\"),\n         (\"emu3\", \"Emu3ForCausalLM\"),"
        },
        {
            "sha": "7756268821e90fd002bdb6722d3842c884c2ca3c",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -171,6 +171,13 @@\n                     \"DebertaV2TokenizerFast\" if is_tokenizers_available() else None,\n                 ),\n             ),\n+            (\n+                \"deepseek_v3\",\n+                (\n+                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+                ),\n+            ),\n             (\n                 \"diffllama\",\n                 ("
        },
        {
            "sha": "298f4c968375e6d90728a3d4e78c5046e4591c01",
            "filename": "src/transformers/models/deepseek_v3/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2F__init__.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_deepseek_v3 import *\n+    from .modeling_deepseek_v3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "8f04f9a8e9dd450fd0d0910482dcf37f1e316810",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "added",
            "additions": 247,
            "deletions": 0,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -0,0 +1,247 @@\n+# coding=utf-8\n+# Copyright 2025 bzantium and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on the DeepSeekV3 implementations from the DeepSeek AI team. (https://huggingface.co/deepseek-ai/DeepSeek-V3)\n+\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"DeepSeekV3 model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+DEEPSEEK_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n+\n+\n+class DeepseekV3Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DeepseekV3Model`]. It is used to instantiate an DeepSeek\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the DeepSeek-V3.\n+    e.g. [bzantium/tiny-deepseek-v3](https://huggingface.co/bzantium/tiny-deepseek-v3)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 129280):\n+            Vocabulary size of the Deep model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`DeepseekV3Model`]\n+        hidden_size (`int`, *optional*, defaults to 7168):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 18432):\n+            Dimension of the MLP representations.\n+        moe_intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the MoE representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 61):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 128):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 128):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        n_shared_experts (`int`, *optional*, defaults to 1):\n+            Number of shared experts.\n+        n_routed_experts (`int`, *optional*, defaults to 256):\n+            Number of routed experts.\n+        routed_scaling_factor (`float`, *optional*, defaults to 2.5):\n+            Scaling factor or routed experts.\n+        kv_lora_rank (`int`, *optional*, defaults to 512):\n+            Rank of the LoRA matrices for key and value projections.\n+        q_lora_rank (`int`, *optional*, defaults to 1536):\n+            Rank of the LoRA matrices for query projections.\n+        qk_rope_head_dim (`int`, *optional*, defaults to 64):\n+            Dimension of the query/key heads that use rotary position embeddings.\n+        v_head_dim (`int`, *optional*, defaults to 128):\n+            Dimension of the value heads.\n+        qk_nope_head_dim (`int`, *optional*, defaults to 128):\n+            Dimension of the query/key heads that don't use rotary position embeddings.\n+        n_group (`int`, *optional*, defaults to 8):\n+            Number of groups for routed experts.\n+        topk_group (`int`, *optional*, defaults to 4):\n+            Number of selected groups for each token(for each token, ensuring the selected experts is only within `topk_group` groups).\n+        num_experts_per_tok (`int`, *optional*, defaults to 8):\n+            Number of selected experts, None means dense model.\n+        first_k_dense_replace (`int`, *optional*, defaults to 3):\n+            Number of dense layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n+                                                            \\--k dense layers--/\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the weights of the routed experts.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 4096):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n+            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n+            issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n+            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n+            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n+            `max_position_embeddings` to the expected new maximum.\n+        rope_interleave (`bool`, *optional*, defaults to `True`):\n+            Whether to interleave the rotary position embeddings.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import DeepseekV3Model, DeepseekV3Config\n+\n+    >>> # Initializing a Deepseek-V3 style configuration\n+    >>> configuration = DeepseekV3Config()\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"deepseek_v3\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n+        \"layers.*.mlp.experts.*.gate_proj\": \"local_colwise\",\n+        \"layers.*.mlp.experts.*.up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.experts.*.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.*\": \"local\",  # each expert is wrapped in a module list\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"local_colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.shared_experts\": \"local\",\n+        \"layers.*.mlp.gate_proj\": \"local_colwise\",\n+        \"layers.*.mlp.up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp\": \"gather\",  # This is the only moment where results are gathered\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=129280,\n+        hidden_size=7168,\n+        intermediate_size=18432,\n+        moe_intermediate_size=2048,\n+        num_hidden_layers=61,\n+        num_attention_heads=128,\n+        num_key_value_heads=128,\n+        n_shared_experts=1,\n+        n_routed_experts=256,\n+        routed_scaling_factor=2.5,\n+        kv_lora_rank=512,\n+        q_lora_rank=1536,\n+        qk_rope_head_dim=64,\n+        v_head_dim=128,\n+        qk_nope_head_dim=128,\n+        n_group=8,\n+        topk_group=4,\n+        num_experts_per_tok=8,\n+        first_k_dense_replace=3,\n+        norm_topk_prob=True,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=4096,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=0,\n+        eos_token_id=1,\n+        pretraining_tp=1,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        rope_interleave=True,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.n_shared_experts = n_shared_experts\n+        self.n_routed_experts = n_routed_experts\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.kv_lora_rank = kv_lora_rank\n+        self.q_lora_rank = q_lora_rank\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n+        self.head_dim = qk_rope_head_dim\n+        self.n_group = n_group\n+        self.topk_group = topk_group\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.first_k_dense_replace = first_k_dense_replace\n+        self.norm_topk_prob = norm_topk_prob\n+        self.rope_interleave = rope_interleave\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.pretraining_tp = pretraining_tp\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"DeepseekV3Config\"]"
        },
        {
            "sha": "cab1e41cd7cdcdeab0a091b211a859203e14f875",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "added",
            "additions": 1061,
            "deletions": 0,
            "changes": 1061,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -0,0 +1,1061 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/deepseek_v3/modular_deepseek_v3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_v3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+from typing import Callable, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    LossKwargs,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.deprecation import deprecate_kwarg\n+from .configuration_deepseek_v3 import DeepseekV3Config\n+\n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+logger = logging.get_logger(__name__)\n+_CONFIG_FOR_DOC = \"DeepseekV3Config\"\n+\n+\n+class DeepseekV3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        DeepseekV3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class DeepseekV3RotaryEmbedding(nn.Module):\n+    def __init__(self, config: DeepseekV3Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class DeepseekV3MLP(nn.Module):\n+    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class DeepseekV3TopkRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.top_k = config.num_experts_per_tok\n+        self.n_routed_experts = config.n_routed_experts\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.n_group = config.n_group\n+        self.topk_group = config.topk_group\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros((self.n_routed_experts)))\n+\n+    @torch.no_grad()\n+    def get_topk_indices(self, scores):\n+        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+        group_scores = (\n+            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n+        )\n+        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+        group_mask = torch.zeros_like(group_scores)\n+        group_mask.scatter_(1, group_idx, 1)\n+        score_mask = (\n+            group_mask.unsqueeze(-1)\n+            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .reshape(-1, self.n_routed_experts)\n+        )\n+        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        return topk_indices\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n+        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n+        scores = router_logits.sigmoid()\n+        topk_indices = self.get_topk_indices(scores)\n+        topk_weights = scores.gather(1, topk_indices)\n+        if self.norm_topk_prob:\n+            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n+            topk_weights /= denominator\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n+\n+\n+class DeepseekV3MoE(nn.Module):\n+    \"\"\"\n+    A mixed expert module containing shared experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.experts = nn.ModuleList(\n+            [\n+                DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size)\n+                for _ in range(config.n_routed_experts)\n+            ]\n+        )\n+        self.gate = DeepseekV3TopkRouter(config)\n+        self.shared_experts = DeepseekV3MLP(\n+            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n+        )\n+\n+    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n+        r\"\"\"\n+        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n+        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n+        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n+        expert_mask = expert_mask.permute(2, 0, 1)\n+\n+        for expert_idx in range(len(self.experts)):\n+            expert = self.experts[expert_idx]\n+            mask = expert_mask[expert_idx]\n+            token_indices, weight_indices = torch.where(mask)\n+\n+            if token_indices.numel() > 0:\n+                expert_weights = topk_weights[token_indices, weight_indices]\n+                expert_input = hidden_states[token_indices]\n+                expert_output = expert(expert_input)\n+                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n+                final_hidden_states.index_add_(0, token_indices, weighted_output)\n+\n+        # in original deepseek, the output of the experts are gathered once we leave this module\n+        # thus the moe module is itelsf an IsolatedParallel module\n+        # and all expert are \"local\" meaning we shard but we don't gather\n+        return final_hidden_states.type(hidden_states.dtype)\n+\n+    def forward(self, hidden_states):\n+        residuals = hidden_states\n+        orig_shape = hidden_states.shape\n+        topk_indices, topk_weights = self.gate(hidden_states)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = hidden_states + self.shared_experts(residuals)\n+        return hidden_states\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb_interleave(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    r\"\"\"\n+    TODO let's just use the original freqcis computation to not have the view\n+    transpose + reshape! This is not optimized!\n+    Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`):\n+            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n+            used to pass offsetted position ids when working with a KV-cache.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    b, h, s, d = q.shape\n+    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n+\n+    b, h, s, d = k.shape\n+    k = k.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n+\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def yarn_get_mscale(scale=1, mscale=1):\n+    if scale <= 1:\n+        return 1.0\n+    return 0.1 * mscale * math.log(scale) + 1.0\n+\n+\n+class DeepseekV3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: DeepseekV3Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.num_heads = config.num_attention_heads\n+        self.rope_theta = config.rope_theta\n+        self.q_lora_rank = config.q_lora_rank\n+        self.qk_rope_head_dim = config.qk_rope_head_dim\n+        self.kv_lora_rank = config.kv_lora_rank\n+        self.v_head_dim = config.v_head_dim\n+        self.qk_nope_head_dim = config.qk_nope_head_dim\n+        self.qk_head_dim = config.qk_head_dim\n+\n+        self.is_causal = True\n+        self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n+        self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)\n+        self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n+\n+        self.kv_a_proj_with_mqa = nn.Linear(\n+            config.hidden_size,\n+            self.kv_lora_rank + self.qk_rope_head_dim,\n+            bias=config.attention_bias,\n+        )\n+        self.kv_a_layernorm = DeepseekV3RMSNorm(self.kv_lora_rank)\n+        self.kv_b_proj = nn.Linear(\n+            self.kv_lora_rank,\n+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n+            bias=False,\n+        )\n+\n+        self.o_proj = nn.Linear(\n+            self.num_heads * self.v_head_dim,\n+            config.hidden_size,\n+            bias=config.attention_bias,\n+        )\n+\n+        self.scaling = self.qk_head_dim ** (-0.5)\n+        if self.config.rope_scaling is not None:\n+            mscale_all_dim = self.config.rope_scaling.get(\"mscale_all_dim\", 0)\n+            scaling_factor = self.config.rope_scaling[\"factor\"]\n+            if mscale_all_dim:\n+                mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n+                self.scaling = self.scaling * mscale * mscale\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+        query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n+        key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n+\n+        q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states))).view(query_shape).transpose(1, 2)\n+        q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n+\n+        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n+        k_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+\n+        k_pass = self.kv_b_proj(self.kv_a_layernorm(k_pass)).view(key_shape).transpose(1, 2)\n+        k_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n+\n+        k_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)\n+\n+        cos, sin = position_embeddings\n+        if self.config.rope_interleave:  # support using interleaved weights for efficiency\n+            q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\n+        else:\n+            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)\n+        k_rot = k_rot.expand(*k_pass.shape[:-1], -1)\n+\n+        query_states = torch.cat((q_pass, q_rot), dim=-1)\n+        key_states = torch.cat((k_pass, k_rot), dim=-1)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            attn_output = attn_output[:, :, :, : self.v_head_dim]\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class DeepseekV3DecoderLayer(nn.Module):\n+    def __init__(self, config: DeepseekV3Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = DeepseekV3Attention(config=config, layer_idx=layer_idx)\n+\n+        if layer_idx >= config.first_k_dense_replace:\n+            self.mlp = DeepseekV3MoE(config)\n+        else:\n+            self.mlp = DeepseekV3MLP(config)\n+\n+        self.input_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+DEEPSEEK_V3_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`DeepseekV3Config`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare DeepseekV3 Model outputting raw hidden-states without any specific head on top.\",\n+    DEEPSEEK_V3_START_DOCSTRING,\n+)\n+class DeepseekV3PreTrainedModel(PreTrainedModel):\n+    config_class = DeepseekV3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"DeepseekV3DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DeepseekV3TopkRouter):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.Parameter):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+\n+\n+DEEPSEEK_V3_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare DeepseekV3 Model outputting raw hidden-states without any specific head on top.\",\n+    DEEPSEEK_V3_START_DOCSTRING,\n+)\n+class DeepseekV3Model(DeepseekV3PreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DeepseekV3DecoderLayer`]\n+\n+    Args:\n+        config: DeepseekV3Config\n+    \"\"\"\n+\n+    _keys_to_ignore_on_load_unexpected = [r\"model\\.layers\\.61.*\"]\n+\n+    def __init__(self, config: DeepseekV3Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [DeepseekV3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = DeepseekV3RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to place the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+class DeepseekV3ForCausalLM(DeepseekV3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = DeepseekV3Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n+    @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, DeepseekV3ForCausalLM\n+\n+        >>> model = DeepseekV3ForCausalLM.from_pretrained(\"meta-deepseek_v3/DeepseekV3-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-deepseek_v3/DeepseekV3-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"DeepseekV3PreTrainedModel\", \"DeepseekV3Model\", \"DeepseekV3ForCausalLM\"]"
        },
        {
            "sha": "7713eb3b270707e452561e90268acb8ed99e8dbb",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "added",
            "additions": 368,
            "deletions": 0,
            "changes": 368,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -0,0 +1,368 @@\n+import math\n+from typing import Callable, Optional, Tuple\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..llama.modeling_llama import (\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+    rotate_half,\n+)\n+from .configuration_deepseek_v3 import DeepseekV3Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DeepseekV3RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class DeepseekV3RotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+def apply_rotary_pos_emb_interleave(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    r\"\"\"\n+    TODO let's just use the original freqcis computation to not have the view\n+    transpose + reshape! This is not optimized!\n+    Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`):\n+            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n+            used to pass offsetted position ids when working with a KV-cache.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    b, h, s, d = q.shape\n+    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n+\n+    b, h, s, d = k.shape\n+    k = k.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n+\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def yarn_get_mscale(scale=1, mscale=1):\n+    if scale <= 1:\n+        return 1.0\n+    return 0.1 * mscale * math.log(scale) + 1.0\n+\n+\n+class DeepseekV3MLP(nn.Module):\n+    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class DeepseekV3TopkRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.top_k = config.num_experts_per_tok\n+        self.n_routed_experts = config.n_routed_experts\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.n_group = config.n_group\n+        self.topk_group = config.topk_group\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros((self.n_routed_experts)))\n+\n+    @torch.no_grad()\n+    def get_topk_indices(self, scores):\n+        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+        group_scores = (\n+            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n+        )\n+        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+        group_mask = torch.zeros_like(group_scores)\n+        group_mask.scatter_(1, group_idx, 1)\n+        score_mask = (\n+            group_mask.unsqueeze(-1)\n+            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .reshape(-1, self.n_routed_experts)\n+        )\n+        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        return topk_indices\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n+        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n+        scores = router_logits.sigmoid()\n+        topk_indices = self.get_topk_indices(scores)\n+        topk_weights = scores.gather(1, topk_indices)\n+        if self.norm_topk_prob:\n+            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n+            topk_weights /= denominator\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n+\n+\n+class DeepseekV3MoE(nn.Module):\n+    \"\"\"\n+    A mixed expert module containing shared experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.experts = nn.ModuleList(\n+            [\n+                DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size)\n+                for _ in range(config.n_routed_experts)\n+            ]\n+        )\n+        self.gate = DeepseekV3TopkRouter(config)\n+        self.shared_experts = DeepseekV3MLP(\n+            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n+        )\n+\n+    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n+        r\"\"\"\n+        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n+        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n+        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n+        expert_mask = expert_mask.permute(2, 0, 1)\n+\n+        for expert_idx in range(len(self.experts)):\n+            expert = self.experts[expert_idx]\n+            mask = expert_mask[expert_idx]\n+            token_indices, weight_indices = torch.where(mask)\n+\n+            if token_indices.numel() > 0:\n+                expert_weights = topk_weights[token_indices, weight_indices]\n+                expert_input = hidden_states[token_indices]\n+                expert_output = expert(expert_input)\n+                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n+                final_hidden_states.index_add_(0, token_indices, weighted_output)\n+\n+        # in original deepseek, the output of the experts are gathered once we leave this module\n+        # thus the moe module is itelsf an IsolatedParallel module\n+        # and all expert are \"local\" meaning we shard but we don't gather\n+        return final_hidden_states.type(hidden_states.dtype)\n+\n+    def forward(self, hidden_states):\n+        residuals = hidden_states\n+        orig_shape = hidden_states.shape\n+        topk_indices, topk_weights = self.gate(hidden_states)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = hidden_states + self.shared_experts(residuals)\n+        return hidden_states\n+\n+\n+class DeepseekV3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: DeepseekV3Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.num_heads = config.num_attention_heads\n+        self.rope_theta = config.rope_theta\n+        self.q_lora_rank = config.q_lora_rank\n+        self.qk_rope_head_dim = config.qk_rope_head_dim\n+        self.kv_lora_rank = config.kv_lora_rank\n+        self.v_head_dim = config.v_head_dim\n+        self.qk_nope_head_dim = config.qk_nope_head_dim\n+        self.qk_head_dim = config.qk_head_dim\n+\n+        self.is_causal = True\n+        self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n+        self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank)\n+        self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n+\n+        self.kv_a_proj_with_mqa = nn.Linear(\n+            config.hidden_size,\n+            self.kv_lora_rank + self.qk_rope_head_dim,\n+            bias=config.attention_bias,\n+        )\n+        self.kv_a_layernorm = DeepseekV3RMSNorm(self.kv_lora_rank)\n+        self.kv_b_proj = nn.Linear(\n+            self.kv_lora_rank,\n+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n+            bias=False,\n+        )\n+\n+        self.o_proj = nn.Linear(\n+            self.num_heads * self.v_head_dim,\n+            config.hidden_size,\n+            bias=config.attention_bias,\n+        )\n+\n+        self.scaling = self.qk_head_dim ** (-0.5)\n+        if self.config.rope_scaling is not None:\n+            mscale_all_dim = self.config.rope_scaling.get(\"mscale_all_dim\", 0)\n+            scaling_factor = self.config.rope_scaling[\"factor\"]\n+            if mscale_all_dim:\n+                mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n+                self.scaling = self.scaling * mscale * mscale\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+        query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n+        key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n+\n+        q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states))).view(query_shape).transpose(1, 2)\n+        q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n+\n+        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n+        k_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+\n+        k_pass = self.kv_b_proj(self.kv_a_layernorm(k_pass)).view(key_shape).transpose(1, 2)\n+        k_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n+\n+        k_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)\n+\n+        cos, sin = position_embeddings\n+        if self.config.rope_interleave:  # support using interleaved weights for efficiency\n+            q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\n+        else:\n+            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)\n+        k_rot = k_rot.expand(*k_pass.shape[:-1], -1)\n+\n+        query_states = torch.cat((q_pass, q_rot), dim=-1)\n+        key_states = torch.cat((k_pass, k_rot), dim=-1)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            attn_output = attn_output[:, :, :, : self.v_head_dim]\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class DeepseekV3DecoderLayer(LlamaDecoderLayer, nn.Module):\n+    def __init__(self, config: DeepseekV3Config, layer_idx: int):\n+        nn.Module().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = DeepseekV3Attention(config=config, layer_idx=layer_idx)\n+\n+        if layer_idx >= config.first_k_dense_replace:\n+            self.mlp = DeepseekV3MoE(config)\n+        else:\n+            self.mlp = DeepseekV3MLP(config)\n+\n+        self.input_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+\n+class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DeepseekV3TopkRouter):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif isinstance(module, nn.Parameter):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+\n+\n+class DeepseekV3Model(LlamaModel):\n+    _keys_to_ignore_on_load_unexpected = [r\"model\\.layers\\.61.*\"]\n+\n+\n+class DeepseekV3ForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+__all__ = [\n+    \"DeepseekV3PreTrainedModel\",\n+    \"DeepseekV3Model\",\n+    \"DeepseekV3ForCausalLM\",\n+]"
        },
        {
            "sha": "63be1917d0e5e98f6b5633d186c952cb9b48beb7",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -2812,6 +2812,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class DeepseekV3ForCausalLM(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class DeepseekV3Model(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class DeepseekV3PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class DeformableDetrForObjectDetection(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/deepseek_v3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/tests%2Fmodels%2Fdeepseek_v3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/tests%2Fmodels%2Fdeepseek_v3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2F__init__.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a"
        },
        {
            "sha": "f452b028a20be824521408c14a5800a4f383519e",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "added",
            "additions": 657,
            "deletions": 0,
            "changes": 657,
            "blob_url": "https://github.com/huggingface/transformers/blob/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eca74d13677e7bba69f7f7bcdc1d05e54f77a41a/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=eca74d13677e7bba69f7f7bcdc1d05e54f77a41a",
            "patch": "@@ -0,0 +1,657 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch DeepseekV3 model.\"\"\"\n+\n+import unittest\n+\n+from packaging import version\n+from parameterized import parameterized\n+\n+from transformers import AutoTokenizer, DeepseekV3Config, is_torch_available, set_seed\n+from transformers.testing_utils import (\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        DeepseekV3ForCausalLM,\n+        DeepseekV3Model,\n+    )\n+    from transformers.models.deepseek_v3.modeling_deepseek_v3 import (\n+        DeepseekV3RotaryEmbedding,\n+    )\n+\n+\n+class DeepseekV3ModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_token_type_ids=False,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=32,\n+        intermediate_size=37,\n+        moe_intermediate_size=12,\n+        num_hidden_layers=5,\n+        num_attention_heads=4,\n+        num_key_value_heads=4,\n+        n_shared_experts=1,\n+        n_routed_experts=8,\n+        routed_scaling_factor=2.5,\n+        kv_lora_rank=16,\n+        q_lora_rank=32,\n+        qk_rope_head_dim=16,\n+        v_head_dim=32,\n+        qk_nope_head_dim=32,\n+        n_group=2,\n+        topk_group=1,\n+        num_experts_per_tok=8,\n+        first_k_dense_replace=2,\n+        norm_topk_prob=True,\n+        aux_loss_alpha=0.001,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        attention_probs_dropout_prob=0.1,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        num_labels=3,\n+        num_choices=4,\n+        pad_token_id=0,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_token_type_ids = use_token_type_ids\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.n_shared_experts = n_shared_experts\n+        self.n_routed_experts = n_routed_experts\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.kv_lora_rank = kv_lora_rank\n+        self.q_lora_rank = q_lora_rank\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.n_group = n_group\n+        self.topk_group = topk_group\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.first_k_dense_replace = first_k_dense_replace\n+        self.norm_topk_prob = norm_topk_prob\n+        self.aux_loss_alpha = aux_loss_alpha\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.pad_token_id = pad_token_id\n+        self.scope = scope\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n+\n+        token_type_ids = None\n+        if self.use_token_type_ids:\n+            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n+\n+        sequence_labels = None\n+        token_labels = None\n+        choice_labels = None\n+        if self.use_labels:\n+            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n+            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n+\n+    def get_config(self):\n+        return DeepseekV3Config(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            moe_intermediate_size=self.moe_intermediate_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            n_shared_experts=self.n_shared_experts,\n+            n_routed_experts=self.n_routed_experts,\n+            routed_scaling_factor=self.routed_scaling_factor,\n+            kv_lora_rank=self.kv_lora_rank,\n+            q_lora_rank=self.q_lora_rank,\n+            qk_rope_head_dim=self.qk_rope_head_dim,\n+            v_head_dim=self.v_head_dim,\n+            qk_nope_head_dim=self.qk_nope_head_dim,\n+            n_group=self.n_group,\n+            topk_group=self.topk_group,\n+            num_experts_per_tok=self.num_experts_per_tok,\n+            first_k_dense_replace=self.first_k_dense_replace,\n+            norm_topk_prob=self.norm_topk_prob,\n+            aux_loss_alpha=self.aux_loss_alpha,\n+            hidden_act=self.hidden_act,\n+            max_position_embeddings=self.max_position_embeddings,\n+            initializer_range=self.initializer_range,\n+            use_cache=True,\n+            pad_token_id=self.pad_token_id,\n+            attention_dropout=self.attention_probs_dropout_prob,\n+        )\n+\n+    def create_and_check_model(\n+        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        model = DeepseekV3Model(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask)\n+        result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_model_as_decoder(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        choice_labels,\n+        encoder_hidden_states,\n+        encoder_attention_mask,\n+    ):\n+        config.add_cross_attention = True\n+        model = DeepseekV3Model(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+        )\n+        result = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+        )\n+        result = model(input_ids, attention_mask=input_mask)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_for_causal_lm(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        choice_labels,\n+        encoder_hidden_states,\n+        encoder_attention_mask,\n+    ):\n+        model = DeepseekV3ForCausalLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n+\n+    def create_and_check_decoder_model_past_large_inputs(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        choice_labels,\n+        encoder_hidden_states,\n+        encoder_attention_mask,\n+    ):\n+        config.is_decoder = True\n+        config.add_cross_attention = True\n+        model = DeepseekV3ForCausalLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        # first forward pass\n+        outputs = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            use_cache=True,\n+        )\n+        past_key_values = outputs.past_key_values\n+\n+        # create hypothetical multiple next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n+        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n+\n+        # append to next input_ids and\n+        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n+        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n+\n+        output_from_no_past = model(\n+            next_input_ids,\n+            attention_mask=next_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_hidden_states=True,\n+        )[\"hidden_states\"][0]\n+        output_from_past = model(\n+            next_tokens,\n+            attention_mask=next_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            output_hidden_states=True,\n+        )[\"hidden_states\"][0]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n+\n+        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            token_type_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class DeepseekV3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            DeepseekV3Model,\n+            DeepseekV3ForCausalLM,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    all_generative_model_classes = (DeepseekV3ForCausalLM,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": DeepseekV3Model,\n+            \"text-generation\": DeepseekV3ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+\n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = DeepseekV3ForCausalLM if is_torch_available() else None\n+\n+    def setUp(self):\n+        self.model_tester = DeepseekV3ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DeepseekV3Config, hidden_size=37)\n+\n+    @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n+    def test_model_outputs_equivalence(self, **kwargs):\n+        pass\n+\n+    @parameterized.expand([(\"random\",), (\"same\",)])\n+    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with assisted decoding\")\n+    def test_prompt_lookup_decoding_matches_greedy_search(self, assistant_type):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with assisted decoding\")\n+    def test_assisted_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache which is not compatible with dola decoding\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support continue from past kv\")\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support low_memory generation\")\n+    def test_beam_search_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3 has HybridCache and doesn't support contrastive generation\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n+    )\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n+    )\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\"DeepseekV3's eager attn/sdpa attn outputs are expected to be different\")\n+    def test_sdpa_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n+    def test_beam_search_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n+    def test_generate_compilation_all_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n+    @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n+    def test_greedy_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_model_various_embeddings(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        for type in [\"absolute\", \"relative_key\", \"relative_key_query\"]:\n+            config_and_inputs[0].position_embedding_type = type\n+            self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @parameterized.expand([(\"yarn\",)])\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        short_input = ids_tensor([1, 10], config.vocab_size)\n+        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        original_model = DeepseekV3Model(config)\n+        original_model.to(torch_device)\n+        original_model.eval()\n+        original_short_output = original_model(short_input).last_hidden_state\n+        original_long_output = original_model(long_input).last_hidden_state\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n+        scaled_model = DeepseekV3Model(config)\n+        scaled_model.to(torch_device)\n+        scaled_model.eval()\n+        scaled_short_output = scaled_model(short_input).last_hidden_state\n+        scaled_long_output = scaled_model(long_input).last_hidden_state\n+\n+        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n+        # maximum sequence length, so the outputs for the short input should match.\n+        if scaling_type == \"dynamic\":\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n+        else:\n+            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+\n+        # The output should be different for long inputs\n+        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n+\n+    def test_model_rope_scaling(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        scaling_factor = 10\n+        short_input_length = 10\n+        long_input_length = int(config.max_position_embeddings * 1.5)\n+\n+        # Inputs\n+        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n+\n+        # Sanity check original RoPE\n+        original_rope = DeepseekV3RotaryEmbedding(config=config).to(torch_device)\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n+\n+        # Sanity check linear RoPE scaling\n+        # New position \"x\" should match original position with index \"x/scaling_factor\"\n+        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n+        linear_scaling_rope = DeepseekV3RotaryEmbedding(config=config).to(torch_device)\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n+        for new_position in range(0, long_input_length, scaling_factor):\n+            original_position = int(new_position // scaling_factor)\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n+\n+        # Sanity check Dynamic NTK RoPE scaling\n+        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n+        # with scaling_factor (or that `inv_freq` decreases)\n+        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n+        ntk_scaling_rope = DeepseekV3RotaryEmbedding(config=config).to(torch_device)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n+        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n+        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n+\n+        # Sanity check Yarn RoPE scaling\n+        # Scaling should be over the entire input\n+        config.rope_scaling = {\"type\": \"yarn\", \"factor\": scaling_factor}\n+        yarn_scaling_rope = DeepseekV3RotaryEmbedding(config=config).to(torch_device)\n+        yarn_cos_short, yarn_sin_short = yarn_scaling_rope(x, position_ids_short)\n+        yarn_cos_long, yarn_sin_long = yarn_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(yarn_cos_short, yarn_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(yarn_sin_short, yarn_sin_long[:, :short_input_length, :])\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_cos_short, original_cos_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_sin_short, original_sin_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_cos_long, original_cos_long)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_sin_long, original_sin_long)\n+\n+    @unittest.skip(reason=\"Deepseek-V3 uses MLA on all models so the KV cache is a non standard format\")\n+    def test_past_key_values_format(self):\n+        pass\n+\n+    @require_torch_sdpa\n+    @slow\n+    def test_eager_matches_sdpa_generate(self):\n+        \"\"\"\n+        Overwritting the common test as the test is flaky on tiny models\n+        \"\"\"\n+        max_new_tokens = 30\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"bzantium/tiny-deepseek-v3\")\n+\n+        model_sdpa = DeepseekV3ForCausalLM.from_pretrained(\n+            \"bzantium/tiny-deepseek-v3\",\n+            torch_dtype=torch.float16,\n+            low_cpu_mem_usage=True,\n+        ).to(torch_device)\n+\n+        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+\n+        model_eager = DeepseekV3ForCausalLM.from_pretrained(\n+            \"bzantium/tiny-deepseek-v3\",\n+            torch_dtype=torch.float16,\n+            low_cpu_mem_usage=True,\n+            attn_implementation=\"eager\",\n+        ).to(torch_device)\n+\n+        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+        texts = [\n+            \"hi here's a longer context, getting longer and\",\n+            \"Hello this is a very long sentence my friend, very long for real\",\n+            \"Today I am in Paris and\",\n+        ]\n+\n+        for padding_side in [\"left\", \"right\"]:\n+            tokenizer.padding_side = padding_side\n+            tokenizer.pad_token = tokenizer.eos_token\n+\n+            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n+            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n+\n+            with self.subTest(f\"{padding_side}\"):\n+                torch.testing.assert_close(\n+                    res_eager,\n+                    res_sdpa,\n+                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n+                )\n+\n+\n+@require_torch_accelerator\n+class DeepseekV3IntegrationTest(unittest.TestCase):\n+    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # Depending on the hardware we get different logits / generations\n+    cuda_compute_capability_major_version = None\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        if is_torch_available() and torch.cuda.is_available():\n+            # 8 is for A100 / A10 and 7 for T4\n+            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_read_token\n+    def test_compile_static_cache(self):\n+        # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2\n+        # work as intended. See https://github.com/pytorch/pytorch/issues/121943\n+        if version.parse(torch.__version__) < version.parse(\"2.3.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        NUM_TOKENS_TO_GENERATE = 40\n+        # Note on `EXPECTED_TEXT_COMPLETION`'s diff: the current value matches the original test if the original test\n+        # was changed to have a cache of 53 tokens (as opposed to 4096), on Ampere GPUs.\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial \"\n+            \"reference frames, and 2) the laws of physics are the same for all inertial reference frames.\\nThe \"\n+            \"theory of relativ\",\n+            \"My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, \"\n+            \"my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my p\",\n+        ]\n+\n+        prompts = [\n+            \"Simply put, the theory of relativity states that \",\n+            \"My favorite all time favorite condiment is ketchup.\",\n+        ]\n+        tokenizer = AutoTokenizer.from_pretrained(\"bzantium/tiny-deepseek-v3\", pad_token=\"</s>\", padding_side=\"right\")\n+        model = DeepseekV3ForCausalLM.from_pretrained(\n+            \"bzantium/tiny-deepseek-v3\", device_map=torch_device, torch_dtype=torch.float16\n+        )\n+        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+\n+        # Dynamic Cache\n+        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n+        dynamic_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, dynamic_text)\n+\n+        # Static Cache\n+        generated_ids = model.generate(\n+            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n+        )\n+        static_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, static_text)\n+\n+        # Static Cache + compile\n+        model._cache = None  # clear cache object, initialized when we pass `cache_implementation=\"static\"`\n+        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n+        generated_ids = model.generate(\n+            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n+        )\n+        static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)"
        }
    ],
    "stats": {
        "total": 2707,
        "additions": 2669,
        "deletions": 38
    }
}