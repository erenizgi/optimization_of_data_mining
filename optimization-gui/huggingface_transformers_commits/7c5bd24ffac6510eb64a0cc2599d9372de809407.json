{
    "author": "faaany",
    "message": "[tests] make quanto tests device-agnostic (#36328)\n\n* make device-agnostic\n\n* name change",
    "sha": "7c5bd24ffac6510eb64a0cc2599d9372de809407",
    "files": [
        {
            "sha": "45ef7616ec62a27d905c23225edae45ccb53594b",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c5bd24ffac6510eb64a0cc2599d9372de809407/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c5bd24ffac6510eb64a0cc2599d9372de809407/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=7c5bd24ffac6510eb64a0cc2599d9372de809407",
            "patch": "@@ -22,7 +22,6 @@\n     require_optimum_quanto,\n     require_read_token,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -181,11 +180,11 @@ def test_generate_quality_cpu(self):\n         \"\"\"\n         self.check_inference_correctness(self.quantized_model, \"cpu\")\n \n-    def test_generate_quality_cuda(self):\n+    def test_generate_quality_accelerator(self):\n         \"\"\"\n-        Simple test to check the quality of the model on cuda by comparing the generated tokens with the expected tokens\n+        Simple test to check the quality of the model on accelerators by comparing the generated tokens with the expected tokens\n         \"\"\"\n-        self.check_inference_correctness(self.quantized_model, \"cuda\")\n+        self.check_inference_correctness(self.quantized_model, torch_device)\n \n     def test_quantized_model_layers(self):\n         from optimum.quanto import QBitsTensor, QModuleMixin, QTensor\n@@ -215,7 +214,7 @@ def test_quantized_model_layers(self):\n             )\n             self.quantized_model.to(0)\n         self.assertEqual(\n-            self.quantized_model.transformer.h[0].self_attention.query_key_value.weight._data.device.type, \"cuda\"\n+            self.quantized_model.transformer.h[0].self_attention.query_key_value.weight._data.device.type, torch_device\n         )\n \n     def test_serialization_bin(self):\n@@ -430,7 +429,7 @@ class QuantoQuantizationQBitsTensorSerializationTest(QuantoQuantizationSerializa\n     weights = \"int4\"\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class QuantoQuantizationActivationTest(unittest.TestCase):\n     def test_quantize_activation(self):\n         quantization_config = QuantoConfig(\n@@ -443,7 +442,7 @@ def test_quantize_activation(self):\n \n \n @require_optimum_quanto\n-@require_torch_gpu\n+@require_torch_accelerator\n class QuantoKVCacheQuantizationTest(unittest.TestCase):\n     @slow\n     @require_read_token"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 6,
        "deletions": 7
    }
}