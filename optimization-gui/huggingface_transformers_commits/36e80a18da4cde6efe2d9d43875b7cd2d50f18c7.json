{
    "author": "paulpak58",
    "message": "[modeling][lfm2] LFM2: Remove deprecated seen_tokens (#39342)\n\n* [modeling][lfm2] remove deprecated seen_tokens\n\n* [modular][lfm2] remove deprecated seen_tokens from modular file",
    "sha": "36e80a18da4cde6efe2d9d43875b7cd2d50f18c7",
    "files": [
        {
            "sha": "4931a3a46e04cd186bce95b22e19c8451828d1d1",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/36e80a18da4cde6efe2d9d43875b7cd2d50f18c7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36e80a18da4cde6efe2d9d43875b7cd2d50f18c7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=36e80a18da4cde6efe2d9d43875b7cd2d50f18c7",
            "patch": "@@ -179,10 +179,6 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        # Update the number of seen tokens\n-        if layer_idx == self.first_attention_layer:\n-            self._seen_tokens += key_states.shape[-2]\n-\n         # Update the cache\n         if key_states is not None:\n             if len(self.key_cache) <= layer_idx:"
        },
        {
            "sha": "338e6ec5242decb40998c544154c92e8c936df24",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/36e80a18da4cde6efe2d9d43875b7cd2d50f18c7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36e80a18da4cde6efe2d9d43875b7cd2d50f18c7/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=36e80a18da4cde6efe2d9d43875b7cd2d50f18c7",
            "patch": "@@ -140,10 +140,6 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        # Update the number of seen tokens\n-        if layer_idx == self.first_attention_layer:\n-            self._seen_tokens += key_states.shape[-2]\n-\n         # Update the cache\n         if key_states is not None:\n             if len(self.key_cache) <= layer_idx:"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 0,
        "deletions": 8
    }
}