{
    "author": "ydshieh",
    "message": "Update expected values (after switching to A10) (#39157)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* empty\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "4c1715b6109184b062198793c3922ae1cffa79f9",
    "files": [
        {
            "sha": "813d2bd79673e96480fa2e21606435d51ac7508e",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 22,
            "deletions": 9,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -570,9 +570,14 @@ def test_inference_no_head(self):\n         expected_shape = torch.Size((1, 300, 256))\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n         expected_slice = torch.tensor(\n-            [[0.4222, 0.7471, 0.8760], [0.6395, -0.2729, 0.7127], [-0.3090, 0.7642, 0.9529]]\n+            [\n+                [0.4223, 0.7474, 0.8760],\n+                [0.6397, -0.2727, 0.7126],\n+                [-0.3089, 0.7643, 0.9529],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=2e-4, atol=2e-4)\n \n     def test_inference_object_detection_head(self):\n         model = ConditionalDetrForObjectDetection.from_pretrained(\"microsoft/conditional-detr-resnet-50\").to(\n@@ -592,26 +597,34 @@ def test_inference_object_detection_head(self):\n         expected_shape_logits = torch.Size((1, model.config.num_queries, model.config.num_labels))\n         self.assertEqual(outputs.logits.shape, expected_shape_logits)\n         expected_slice_logits = torch.tensor(\n-            [[-10.4372, -5.7558, -8.6764], [-10.5410, -5.8704, -8.0590], [-10.6827, -6.3469, -8.3923]]\n+            [\n+                [-10.4371, -5.7565, -8.6765],\n+                [-10.5413, -5.8700, -8.0589],\n+                [-10.6824, -6.3477, -8.3927],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=2e-4, atol=2e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n-            [[0.7733, 0.6576, 0.4496], [0.5171, 0.1184, 0.9094], [0.8846, 0.5647, 0.2486]]\n+            [\n+                [0.7733, 0.6576, 0.4496],\n+                [0.5171, 0.1184, 0.9095],\n+                [0.8846, 0.5647, 0.2486],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=2e-4, atol=2e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n             outputs, threshold=0.3, target_sizes=[image.size[::-1]]\n         )[0]\n-        expected_scores = torch.tensor([0.8330, 0.8313, 0.8039, 0.6829, 0.5355]).to(torch_device)\n+        expected_scores = torch.tensor([0.8330, 0.8315, 0.8039, 0.6829, 0.5354]).to(torch_device)\n         expected_labels = [75, 17, 17, 75, 63]\n-        expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)\n+        expected_slice_boxes = torch.tensor([38.3109, 72.1002, 177.6301, 118.4511]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=2e-4, atol=2e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n         torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)"
        },
        {
            "sha": "65df028ce6efcf799f62635bac3b1a43e1830c5f",
            "filename": "tests/models/convnext/test_modeling_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -286,9 +286,9 @@ def test_inference_image_classification_head(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n-        expected_slice = torch.tensor([-0.0260, -0.4739, 0.1911]).to(torch_device)\n+        expected_slice = torch.tensor([-0.0261, -0.4739, 0.1910]).to(torch_device)\n \n-        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=2e-4, atol=2e-4)\n \n \n @require_torch"
        },
        {
            "sha": "f0b6b414335b4cfade3b7415bbec74a664649c5b",
            "filename": "tests/models/cvt/test_modeling_cvt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -185,6 +185,10 @@ def test_inputs_embeds(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n+    # Larger differences on A10 than T4\n+    def test_batching_equivalence(self, atol=2e-4, rtol=2e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n@@ -265,6 +269,6 @@ def test_inference_image_classification_head(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n-        expected_slice = torch.tensor([0.9285, 0.9015, -0.3150]).to(torch_device)\n+        expected_slice = torch.tensor([0.9287, 0.9016, -0.3152]).to(torch_device)\n \n-        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=2e-4, atol=2e-4)"
        },
        {
            "sha": "b26db579d0fffbc00e73565e6348df26c37b8836",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -758,6 +758,7 @@ def prepare_img():\n \n @require_torch\n @require_vision\n+@slow\n class DFineModelIntegrationTest(unittest.TestCase):\n     @cached_property\n     def default_image_processor(self):\n@@ -778,37 +779,38 @@ def test_inference_object_detection_head(self):\n \n         expected_logits = torch.tensor(\n             [\n-                [-3.8097816, -4.7724586, -5.994499],\n-                [-5.2974715, -9.499067, -6.1653666],\n-                [-5.3502765, -3.9530406, -6.3630295],\n+                [-3.8221, -4.7679, -6.0063],\n+                [-5.2994, -9.5009, -6.1697],\n+                [-5.3103, -3.8005, -6.2972],\n             ]\n         ).to(torch_device)\n         expected_boxes = torch.tensor(\n             [\n-                [0.7677696, 0.41479152, 0.46441072],\n-                [0.16912134, 0.19869131, 0.2123824],\n-                [0.2581653, 0.54818195, 0.47512347],\n+                [0.7678, 0.4148, 0.4644],\n+                [0.1691, 0.1987, 0.2124],\n+                [0.2582, 0.5482, 0.4751],\n             ]\n         ).to(torch_device)\n \n-        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, atol=2e-4, rtol=2e-4)\n \n         expected_shape_boxes = torch.Size((1, 300, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=2e-4, rtol=2e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n             outputs, threshold=0.0, target_sizes=[image.size[::-1]]\n         )[0]\n-        expected_scores = torch.tensor([0.9642, 0.9542, 0.9536, 0.8548], device=torch_device)\n+\n+        expected_scores = torch.tensor([0.9616, 0.9541, 0.9541, 0.8551], device=torch_device)\n         expected_labels = [15, 65, 15, 57]\n         expected_slice_boxes = torch.tensor(\n             [\n-                [1.3186283e01, 5.4130211e01, 3.1726535e02, 4.7212445e02],\n-                [4.0275269e01, 7.2975174e01, 1.7620003e02, 1.1776848e02],\n-                [3.4276117e02, 2.3427944e01, 6.3998401e02, 3.7477191e02],\n-                [5.8418274e-01, 1.1794567e00, 6.3933154e02, 4.7485995e02],\n+                [1.3358e01, 5.4123e01, 3.1726e02, 4.7222e02],\n+                [4.0274e01, 7.2972e01, 1.7620e02, 1.1777e02],\n+                [3.4270e02, 2.3427e01, 6.3998e02, 3.7476e02],\n+                [5.7796e-01, 1.1773e00, 6.3933e02, 4.7486e02],\n             ],\n             device=torch_device,\n         )"
        },
        {
            "sha": "126c9d7f6938ae3576ef2f7fe08dc71946f3c9e3",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 20,
            "deletions": 8,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -787,7 +787,11 @@ def test_inference_no_head(self):\n         expected_shape = torch.Size((1, 300, 256))\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n         expected_slice = torch.tensor(\n-            [[-0.4879, -0.2594, 0.4524], [-0.4997, -0.4258, 0.4329], [-0.8220, -0.4996, 0.0577]]\n+            [\n+                [-0.4878, -0.2593, 0.4521],\n+                [-0.4999, -0.4257, 0.4326],\n+                [-0.8220, -0.4997, 0.0578],\n+            ]\n         ).to(torch_device)\n         torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=2e-4, rtol=2e-4)\n \n@@ -806,26 +810,34 @@ def test_inference_object_detection_head(self):\n         expected_shape_logits = torch.Size((1, model.config.num_queries, model.config.num_labels))\n         self.assertEqual(outputs.logits.shape, expected_shape_logits)\n         expected_slice_logits = torch.tensor(\n-            [[-10.1765, -5.5243, -8.9324], [-9.8138, -5.6721, -7.5161], [-10.3054, -5.6081, -8.5931]]\n+            [\n+                [-10.1764, -5.5247, -8.9324],\n+                [-9.8137, -5.6730, -7.5163],\n+                [-10.3056, -5.6075, -8.5935],\n+            ]\n         ).to(torch_device)\n         torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, atol=3e-4, rtol=3e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n-            [[0.3708, 0.3000, 0.2753], [0.5211, 0.6125, 0.9495], [0.2897, 0.6730, 0.5459]]\n+            [\n+                [0.3708, 0.3000, 0.2754],\n+                [0.5211, 0.6126, 0.9494],\n+                [0.2897, 0.6731, 0.5460],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=3e-4, rtol=3e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n             outputs, threshold=0.3, target_sizes=[image.size[::-1]]\n         )[0]\n-        expected_scores = torch.tensor([0.8732, 0.8563, 0.8554, 0.6079, 0.5896]).to(torch_device)\n+        expected_scores = torch.tensor([0.8732, 0.8563, 0.8554, 0.6080, 0.5895]).to(torch_device)\n         expected_labels = [17, 75, 17, 75, 63]\n-        expected_boxes = torch.tensor([14.6970, 49.3892, 320.5165, 469.2765]).to(torch_device)\n+        expected_boxes = torch.tensor([14.6931, 49.3886, 320.5176, 469.2762]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        torch.testing.assert_close(results[\"scores\"], expected_scores, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, atol=3e-4, rtol=3e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        torch.testing.assert_close(results[\"boxes\"][0, :], expected_boxes, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_boxes, atol=3e-4, rtol=3e-4)"
        },
        {
            "sha": "fc30b10e1427c1ea91d6b4ff017dd11ba02f8d4b",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 36,
            "deletions": 15,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -677,30 +677,38 @@ def test_inference_object_detection_head(self):\n         self.assertEqual(outputs.logits.shape, expected_shape_logits)\n \n         expected_logits = torch.tensor(\n-            [[-9.6645, -4.3449, -5.8705], [-9.7035, -3.8504, -5.0724], [-10.5634, -5.3379, -7.5116]]\n+            [\n+                [-9.6644, -4.3434, -5.8707],\n+                [-9.7035, -3.8503, -5.0721],\n+                [-10.5633, -5.3387, -7.5119],\n+            ]\n         ).to(torch_device)\n         expected_boxes = torch.tensor(\n-            [[0.8693, 0.2289, 0.2492], [0.3150, 0.5489, 0.5845], [0.5563, 0.7580, 0.8518]]\n+            [\n+                [0.8693, 0.2290, 0.2492],\n+                [0.3150, 0.5489, 0.5845],\n+                [0.5563, 0.7580, 0.8518],\n+            ]\n         ).to(torch_device)\n \n-        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=2e-4, atol=2e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=2e-4, atol=2e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n             outputs, threshold=0.3, target_sizes=[image.size[::-1]]\n         )[0]\n-        expected_scores = torch.tensor([0.7999, 0.7894, 0.6331, 0.4720, 0.4382]).to(torch_device)\n+        expected_scores = torch.tensor([0.7999, 0.7895, 0.6332, 0.4719, 0.4382]).to(torch_device)\n         expected_labels = [17, 17, 75, 75, 63]\n-        expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)\n+        expected_slice_boxes = torch.tensor([16.4960, 52.8387, 318.2565, 470.7831]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=2e-4, atol=2e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=2e-4, atol=2e-4)\n \n     def test_inference_object_detection_head_with_box_refine_two_stage(self):\n         model = DeformableDetrForObjectDetection.from_pretrained(\n@@ -720,17 +728,25 @@ def test_inference_object_detection_head_with_box_refine_two_stage(self):\n         self.assertEqual(outputs.logits.shape, expected_shape_logits)\n \n         expected_logits = torch.tensor(\n-            [[-6.7108, -4.3213, -6.3777], [-8.9014, -6.1799, -6.7240], [-6.9315, -4.4735, -6.2298]]\n+            [\n+                [-6.7112, -4.3216, -6.3781],\n+                [-8.9035, -6.1738, -6.7249],\n+                [-6.9314, -4.4736, -6.2303],\n+            ]\n         ).to(torch_device)\n         expected_boxes = torch.tensor(\n-            [[0.2583, 0.5499, 0.4683], [0.7652, 0.9068, 0.4882], [0.5490, 0.2763, 0.0564]]\n+            [\n+                [0.2582, 0.5499, 0.4683],\n+                [0.7652, 0.9084, 0.4884],\n+                [0.5490, 0.2763, 0.0564],\n+            ]\n         ).to(torch_device)\n \n-        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=2e-4, atol=2e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=2e-4, atol=2e-4)\n \n     @require_torch_accelerator\n     def test_inference_object_detection_head_equivalence_cpu_accelerator(self):\n@@ -753,10 +769,15 @@ def test_inference_object_detection_head_equivalence_cpu_accelerator(self):\n             gpu_outputs = model(pixel_values.to(torch_device), pixel_mask.to(torch_device))\n \n         # 3. assert equivalence\n+        # (on A10, the differences get larger than on T4)\n         for key in cpu_outputs.keys():\n-            assert torch.allclose(cpu_outputs[key], gpu_outputs[key].cpu(), atol=1e-4)\n+            torch.testing.assert_close(cpu_outputs[key], gpu_outputs[key].cpu(), atol=2e-2, rtol=2e-2)\n \n         expected_logits = torch.tensor(\n-            [[-9.9051, -4.2541, -6.4852], [-9.6947, -4.0854, -6.8033], [-10.0665, -5.8470, -7.7003]]\n+            [\n+                [-9.9051, -4.2541, -6.4852],\n+                [-9.6947, -4.0854, -6.8033],\n+                [-10.0665, -5.8470, -7.7003],\n+            ]\n         )\n-        assert torch.allclose(cpu_outputs.logits[0, :3, :3], expected_logits, atol=1e-4)\n+        assert torch.allclose(cpu_outputs.logits[0, :3, :3], expected_logits, atol=2e-4)"
        },
        {
            "sha": "2af2ca92115f98f221f4a15d6c81ccc3b8ac89c7",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 37,
            "deletions": 17,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1715b6109184b062198793c3922ae1cffa79f9/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=4c1715b6109184b062198793c3922ae1cffa79f9",
            "patch": "@@ -586,9 +586,13 @@ def test_inference_no_head(self):\n         expected_shape = torch.Size((1, 100, 256))\n         assert outputs.last_hidden_state.shape == expected_shape\n         expected_slice = torch.tensor(\n-            [[0.0616, -0.5146, -0.4032], [-0.7629, -0.4934, -1.7153], [-0.4768, -0.6403, -0.7826]]\n+            [\n+                [0.0622, -0.5142, -0.4034],\n+                [-0.7628, -0.4935, -1.7153],\n+                [-0.4751, -0.6386, -0.7818],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=2e-4, atol=2e-4)\n \n     def test_inference_object_detection_head(self):\n         model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(torch_device)\n@@ -606,29 +610,37 @@ def test_inference_object_detection_head(self):\n         expected_shape_logits = torch.Size((1, model.config.num_queries, model.config.num_labels + 1))\n         self.assertEqual(outputs.logits.shape, expected_shape_logits)\n         expected_slice_logits = torch.tensor(\n-            [[-19.1194, -0.0893, -11.0154], [-17.3640, -1.8035, -14.0219], [-20.0461, -0.5837, -11.1060]]\n+            [\n+                [-19.1211, -0.0881, -11.0188],\n+                [-17.3641, -1.8045, -14.0229],\n+                [-20.0415, -0.5833, -11.1005],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=2e-4, atol=2e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n-            [[0.4433, 0.5302, 0.8853], [0.5494, 0.2517, 0.0529], [0.4998, 0.5360, 0.9956]]\n+            [\n+                [0.4433, 0.5302, 0.8852],\n+                [0.5494, 0.2517, 0.0529],\n+                [0.4998, 0.5360, 0.9955],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=2e-4, atol=2e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n             outputs, threshold=0.3, target_sizes=[image.size[::-1]]\n         )[0]\n         expected_scores = torch.tensor([0.9982, 0.9960, 0.9955, 0.9988, 0.9987]).to(torch_device)\n         expected_labels = [75, 75, 63, 17, 17]\n-        expected_slice_boxes = torch.tensor([40.1633, 70.8115, 175.5471, 117.9841]).to(torch_device)\n+        expected_slice_boxes = torch.tensor([40.1615, 70.8090, 175.5476, 117.9810]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=2e-4, atol=2e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=2e-4, atol=2e-4)\n \n     def test_inference_panoptic_segmentation_head(self):\n         model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\").to(torch_device)\n@@ -646,23 +658,27 @@ def test_inference_panoptic_segmentation_head(self):\n         expected_shape_logits = torch.Size((1, model.config.num_queries, model.config.num_labels + 1))\n         self.assertEqual(outputs.logits.shape, expected_shape_logits)\n         expected_slice_logits = torch.tensor(\n-            [[-18.1565, -1.7568, -13.5029], [-16.8888, -1.4138, -14.1028], [-17.5709, -2.5080, -11.8654]]\n+            [\n+                [-18.1523, -1.7592, -13.5019],\n+                [-16.8866, -1.4139, -14.1025],\n+                [-17.5735, -2.5090, -11.8666],\n+            ]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=2e-4, atol=2e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n-            [[0.5344, 0.1789, 0.9285], [0.4420, 0.0572, 0.0875], [0.6630, 0.6887, 0.1017]]\n+            [[0.5344, 0.1790, 0.9284], [0.4421, 0.0571, 0.0875], [0.6632, 0.6886, 0.1015]]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=2e-4, atol=2e-4)\n \n         expected_shape_masks = torch.Size((1, model.config.num_queries, 200, 267))\n         self.assertEqual(outputs.pred_masks.shape, expected_shape_masks)\n         expected_slice_masks = torch.tensor(\n-            [[-7.7558, -10.8788, -11.9797], [-11.8881, -16.4329, -17.7451], [-14.7316, -19.7383, -20.3004]]\n+            [[-7.8408, -11.0104, -12.1279], [-12.0299, -16.6498, -17.9806], [-14.8995, -19.9940, -20.5646]]\n         ).to(torch_device)\n-        torch.testing.assert_close(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, rtol=2e-3, atol=2e-3)\n \n         # verify postprocessing\n         results = image_processor.post_process_panoptic_segmentation(\n@@ -674,7 +690,7 @@ def test_inference_panoptic_segmentation_head(self):\n             torch_device\n         )\n         expected_number_of_segments = 5\n-        expected_first_segment = {\"id\": 1, \"label_id\": 17, \"was_fused\": False, \"score\": 0.994097}\n+        expected_first_segment = {\"id\": 1, \"label_id\": 17, \"was_fused\": False, \"score\": 0.9941}\n \n         number_of_unique_segments = len(torch.unique(results[\"segmentation\"]))\n         self.assertTrue(\n@@ -716,6 +732,10 @@ def test_inference_no_head(self):\n         expected_shape = torch.Size((1, 100, 256))\n         assert outputs.last_hidden_state.shape == expected_shape\n         expected_slice = torch.tensor(\n-            [[0.0616, -0.5146, -0.4032], [-0.7629, -0.4934, -1.7153], [-0.4768, -0.6403, -0.7826]]\n+            [\n+                [0.0622, -0.5142, -0.4034],\n+                [-0.7628, -0.4935, -1.7153],\n+                [-0.4751, -0.6386, -0.7818],\n+            ]\n         ).to(torch_device)\n         torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        }
    ],
    "stats": {
        "total": 204,
        "additions": 138,
        "deletions": 66
    }
}