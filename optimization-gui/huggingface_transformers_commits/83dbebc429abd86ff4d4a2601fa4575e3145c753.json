{
    "author": "gante",
    "message": "[trainer] ensure special tokens in model configs are aligned with tokenizer at train time (#38441)\n\n* tmp commit\n\n* add test\n\n* make fixup\n\n* reset warns/info in test",
    "sha": "83dbebc429abd86ff4d4a2601fa4575e3145c753",
    "files": [
        {
            "sha": "7f61bd1ebc882cc0082804416d0ff6979e7bcc8a",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/83dbebc429abd86ff4d4a2601fa4575e3145c753/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/83dbebc429abd86ff4d4a2601fa4575e3145c753/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=83dbebc429abd86ff4d4a2601fa4575e3145c753",
            "patch": "@@ -792,8 +792,8 @@ def validate(self, strict=False):\n                 )\n                 if logging.get_verbosity() >= logging.WARNING:\n                     warning_message += \" Set `TRANSFORMERS_VERBOSITY=info` for more details.\"\n-                logger.warning(warning_message)\n-                logger.info(info_message)\n+                logger.warning_once(warning_message)\n+                logger.info_once(info_message)\n \n     def save_pretrained(\n         self,"
        },
        {
            "sha": "394a7109d8aca6629ab1611425c15f25c4f1d018",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/83dbebc429abd86ff4d4a2601fa4575e3145c753/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/83dbebc429abd86ff4d4a2601fa4575e3145c753/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=83dbebc429abd86ff4d4a2601fa4575e3145c753",
            "patch": "@@ -905,6 +905,76 @@ def _move_model_to_device(self, model, device):\n         if self.args.parallel_mode == ParallelMode.TPU and hasattr(model, \"tie_weights\"):\n             model.tie_weights()\n \n+    def _align_special_tokens(self):\n+        \"\"\"\n+        Aligns the special tokens of the tokenizer with the model configs.\n+\n+        A new tokens may be defined in the tokenizer for fine-tuning purposes, e.g. an \"end of turn\" token may be\n+        added on chat models. In that case, we want the model configs to be aligned with the tokenizer, so that all\n+        downstream uses work as expected. This alignment should happen before training, to ensure the prediction step\n+        uses the new tokens as well.\n+        \"\"\"\n+        if isinstance(self.processing_class, ProcessorMixin):\n+            tokenizer = self.processing_class.tokenizer\n+        else:\n+            tokenizer = self.tokenizer\n+        model_has_generation_config = (\n+            hasattr(self.model, \"generation_config\") and self.model.generation_config is not None\n+        )\n+        updated_tokens = {}\n+\n+        # 1 - Align EOS token. EOS is more complex than the others, as `generation_config` may hold more than one EOS\n+        # token.\n+        tokenizer_has_new_eos = tokenizer.eos_token_id != self.model.config.eos_token_id\n+        if model_has_generation_config:\n+            # `generation_config.eos_token_id` is None: direct comparision\n+            if self.model.generation_config.eos_token_id is None:\n+                tokenizer_has_new_eos |= tokenizer.eos_token_id != self.model.generation_config.eos_token_id\n+            else:\n+                # `generation_config.eos_token_id` is an `int`: convert it to list (and continue below)\n+                if isinstance(self.model.generation_config.eos_token_id, int):\n+                    self.model.generation_config.eos_token_id = [self.model.generation_config.eos_token_id]\n+                # `generation_config.eos_token_id` is a `list`: check if the tokenizer's EOS token is in the list\n+                tokenizer_has_new_eos |= tokenizer.eos_token_id not in self.model.generation_config.eos_token_id\n+\n+        if tokenizer_has_new_eos:\n+            updated_tokens[\"eos_token_id\"] = tokenizer.eos_token_id\n+            self.model.config.eos_token_id = tokenizer.eos_token_id\n+            # The generation config may hold more than one EOS token. We preserve the original EOS tokens: any of the\n+            # EOS tokens defined here will halt generation.\n+            if model_has_generation_config:\n+                all_eos_tokens = [tokenizer.eos_token_id] + list(self.model.generation_config.eos_token_id)\n+                self.model.generation_config.eos_token_id = [token for token in all_eos_tokens if token is not None]\n+\n+        # 2 - Align BOS\n+        tokenizer_has_new_bos = tokenizer.bos_token_id != self.model.config.bos_token_id\n+        if model_has_generation_config:\n+            tokenizer_has_new_bos |= tokenizer.bos_token_id != self.model.generation_config.bos_token_id\n+\n+        if tokenizer_has_new_bos:\n+            updated_tokens[\"bos_token_id\"] = tokenizer.bos_token_id\n+            self.model.config.bos_token_id = tokenizer.bos_token_id\n+            if model_has_generation_config:\n+                self.model.generation_config.bos_token_id = tokenizer.bos_token_id\n+\n+        # 3 - Align PAD\n+        tokenizer_has_new_pad = tokenizer.pad_token_id != self.model.config.pad_token_id\n+        if model_has_generation_config:\n+            tokenizer_has_new_pad |= tokenizer.pad_token_id != self.model.generation_config.pad_token_id\n+\n+        if tokenizer_has_new_pad:\n+            updated_tokens[\"pad_token_id\"] = tokenizer.pad_token_id\n+            self.model.config.pad_token_id = tokenizer.pad_token_id\n+            if model_has_generation_config:\n+                self.model.generation_config.pad_token_id = tokenizer.pad_token_id\n+\n+        # 4 - Warn users about the changes\n+        if len(updated_tokens) > 0:\n+            logger.warning(\n+                \"The tokenizer has new special tokens that are also defined in the model configs. The model \"\n+                f\"configs were aligned accordingly. Updated tokens: {updated_tokens}\"\n+            )\n+\n     def _set_signature_columns_if_needed(self):\n         if self._signature_columns is None:\n             # Inspect model forward signature to keep only the arguments it accepts.\n@@ -2162,6 +2232,10 @@ def train(\n \n         self.is_in_train = True\n \n+        # If the model uses a tokenizer, it may have a new tokens for fine-tuning purposes.\n+        if isinstance(self.processing_class, (PreTrainedTokenizerBase, ProcessorMixin)):\n+            self._align_special_tokens()\n+\n         # Attach NEFTune hooks if necessary\n         if self.neftune_noise_alpha is not None:\n             self.model = self._activate_neftune(self.model)"
        },
        {
            "sha": "765019f2e5f25b74adf09eb97b006c69eb0349f5",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/83dbebc429abd86ff4d4a2601fa4575e3145c753/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/83dbebc429abd86ff4d4a2601fa4575e3145c753/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=83dbebc429abd86ff4d4a2601fa4575e3145c753",
            "patch": "@@ -153,32 +153,38 @@ def test_validate(self):\n         logger = transformers_logging.get_logger(\"transformers.generation.configuration_utils\")\n \n         # A correct configuration will not throw any warning\n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             GenerationConfig()\n         self.assertEqual(len(captured_logs.out), 0)\n \n         # Inconsequent but technically wrong configuration will throw a warning (e.g. setting sampling\n         # parameters with `do_sample=False`). May be escalated to an error in the future.\n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             GenerationConfig(return_dict_in_generate=False, output_scores=True)\n         self.assertNotEqual(len(captured_logs.out), 0)\n \n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)  # store for later\n         self.assertNotEqual(len(captured_logs.out), 0)\n \n         # Expanding on the case above, we can update a bad configuration to get rid of the warning. Ideally,\n         # that is done by unsetting the parameter (i.e. setting it to None)\n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             # BAD - 0.9 means it is still set, we should warn\n             generation_config_bad_temperature.update(temperature=0.9)\n         self.assertNotEqual(len(captured_logs.out), 0)\n \n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             # CORNER CASE - 1.0 is the default, we can't detect whether it is set by the user or not, we shouldn't warn\n             generation_config_bad_temperature.update(temperature=1.0)\n         self.assertEqual(len(captured_logs.out), 0)\n \n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             # OK - None means it is unset, nothing to warn about\n             generation_config_bad_temperature.update(temperature=None)\n@@ -198,12 +204,14 @@ def test_validate(self):\n             GenerationConfig(logits_processor=\"foo\")\n \n         # Model-specific parameters will NOT raise an exception or a warning\n+        logger.warning_once.cache_clear()\n         with CaptureLogger(logger) as captured_logs:\n             GenerationConfig(foo=\"bar\")\n         self.assertEqual(len(captured_logs.out), 0)\n \n         # By default we throw a short warning. However, we log with INFO level the details.\n         # Default: we don't log the incorrect input values, only a short summary. We explain how to get more details.\n+        logger.warning_once.cache_clear()\n         with LoggingLevel(logging.WARNING):\n             with CaptureLogger(logger) as captured_logs:\n                 GenerationConfig(do_sample=False, temperature=0.5)\n@@ -212,6 +220,8 @@ def test_validate(self):\n         self.assertIn(\"Set `TRANSFORMERS_VERBOSITY=info` for more details\", captured_logs.out)\n \n         # INFO level: we share the full deets\n+        logger.warning_once.cache_clear()\n+        logger.info_once.cache_clear()\n         with LoggingLevel(logging.INFO):\n             with CaptureLogger(logger) as captured_logs:\n                 GenerationConfig(do_sample=False, temperature=0.5)"
        },
        {
            "sha": "6ee9d23e35ec288e1acd45766ce483b072f81320",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 48,
            "deletions": 7,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/83dbebc429abd86ff4d4a2601fa4575e3145c753/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/83dbebc429abd86ff4d4a2601fa4575e3145c753/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=83dbebc429abd86ff4d4a2601fa4575e3145c753",
            "patch": "@@ -48,6 +48,7 @@\n     default_data_collator,\n     enable_full_determinism,\n     get_polynomial_decay_schedule_with_warmup,\n+    is_datasets_available,\n     is_torch_available,\n     logging,\n     set_seed,\n@@ -161,6 +162,8 @@\n     if is_safetensors_available():\n         import safetensors.torch\n \n+if is_datasets_available():\n+    import datasets\n \n # for version specific tests in TrainerIntegrationTest\n require_accelerate_version_min_0_28 = partial(require_accelerate, min_version=\"0.28\")\n@@ -519,7 +522,6 @@ def forward(self, input_ids, **kwargs):\n             return logits\n \n     def create_dummy_dataset_for_text_generation(vocab_size, seq_length, num_samples):\n-        import datasets\n         import numpy as np\n \n         # Create random input sequences\n@@ -595,8 +597,6 @@ def get_regression_trainer(\n         )\n \n     def get_language_model_trainer(**kwargs):\n-        import datasets\n-\n         dataset = datasets.load_dataset(\"fka/awesome-chatgpt-prompts\")\n         model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n         tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n@@ -773,8 +773,6 @@ def test_reproducible_training(self):\n             self.check_trained_model(trainer.model, alternate_seed=True)\n \n     def test_trainer_with_datasets(self):\n-        import datasets\n-\n         np.random.seed(42)\n         x = np.random.normal(size=(64,)).astype(np.float32)\n         y = 2.0 * x + 3.0 + np.random.normal(scale=0.1, size=(64,)).astype(np.float32)\n@@ -823,7 +821,6 @@ def test_model_init(self):\n     @slow\n     def test_gradient_accumulation_loss_alignment_with_model_loss(self):\n         set_seed(42)\n-        import datasets\n \n         model_name = \"nickypro/tinyllama-15M\"\n         dataset_name = \"wikitext\"\n@@ -923,7 +920,6 @@ def tokenize_function(examples):\n \n     def test_gradient_accumulation_loss_alignment_with_loss_func(self):\n         set_seed(42)\n-        import datasets\n \n         model_name = \"roneneldan/TinyStories-33M\"\n         dataset_name = \"wikitext\"\n@@ -4960,6 +4956,51 @@ def test_best_model_checkpoint_behavior(self):\n \n             assert len(os.listdir(tmpdir)) == trainer.state.global_step // 2\n \n+    def test_special_token_aligment(self):\n+        \"\"\"\n+        Tests that special token changes in the tokenizer result in model configs updates when using the trainer, to\n+        ensure special tokens are aligned across configs\n+        \"\"\"\n+\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n+\n+        # add new special tokens to tokenizer, so we can test that trainer aligns the model configs with the tokenizer\n+        tokenizer.eos_token = \"<|im_end|>\"\n+        tokenizer.pad_token = \"<|im_end|>\"\n+        tokenizer.bos_token = \"<|im_start|>\"\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|im_end|>\", \"<|im_start|>\"]})\n+\n+        # the model needs to have its embedding layer resized accordingly\n+        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=64)\n+\n+        # create a random dataset from the **new** vocab size\n+        x = torch.randint(0, len(tokenizer), (64,))\n+        dataset = RepeatDataset(x, length=2)\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            training_args = TrainingArguments(\n+                output_dir=tmpdir, report_to=\"none\", max_steps=1, per_device_train_batch_size=1\n+            )\n+            trainer = Trainer(\n+                model=model,\n+                args=training_args,\n+                processing_class=tokenizer,\n+                train_dataset=dataset,\n+            )\n+\n+            # We haven't started training -> not yet aligned\n+            self.assertNotEqual(trainer.model.config.eos_token_id, tokenizer.eos_token_id)\n+            self.assertNotEqual(trainer.model.config.pad_token_id, tokenizer.pad_token_id)\n+            self.assertNotEqual(trainer.model.config.bos_token_id, tokenizer.bos_token_id)\n+\n+            trainer.train()\n+\n+            # Must be aligned as soon as we start training\n+            self.assertEqual(trainer.model.config.eos_token_id, tokenizer.eos_token_id)\n+            self.assertEqual(trainer.model.config.pad_token_id, tokenizer.pad_token_id)\n+            self.assertEqual(trainer.model.config.bos_token_id, tokenizer.bos_token_id)\n+\n \n @require_torch\n @is_staging_test"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 134,
        "deletions": 9
    }
}