{
    "author": "remi-or",
    "message": "Move the Mi355 to regular docker (#41989)\n\n* Move the Mi355 to regular docker\n\n* Disable gfx950 compilation for FA on AMD",
    "sha": "80305364e2faa402ee39068a7fc0b27ac0ef9d10",
    "files": [
        {
            "sha": "07c64bb0b1f16bbc1384d0115cd87260070b03bc",
            "filename": ".github/workflows/self-scheduled-amd-mi355-caller.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/80305364e2faa402ee39068a7fc0b27ac0ef9d10/.github%2Fworkflows%2Fself-scheduled-amd-mi355-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/80305364e2faa402ee39068a7fc0b27ac0ef9d10/.github%2Fworkflows%2Fself-scheduled-amd-mi355-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd-mi355-caller.yml?ref=80305364e2faa402ee39068a7fc0b27ac0ef9d10",
            "patch": "@@ -21,7 +21,7 @@ jobs:\n       job: run_models_gpu\n       slack_report_channel: \"#amd-hf-ci\"\n       runner_group: hfc-amd-mi355\n-      docker: huggingface/testing-rocm7.0-preview\n+      docker: huggingface/transformers-pytorch-amd-gpu\n       ci_event: Scheduled CI (AMD) - mi355\n       report_repo_id: hf-transformers-bot/transformers-ci-dummy\n     secrets: inherit\n@@ -33,7 +33,7 @@ jobs:\n       job: run_pipelines_torch_gpu\n       slack_report_channel: \"#amd-hf-ci\"\n       runner_group: hfc-amd-mi355\n-      docker: huggingface/testing-rocm7.0-preview\n+      docker: huggingface/transformers-pytorch-amd-gpu\n       ci_event: Scheduled CI (AMD) - mi355\n       report_repo_id: hf-transformers-bot/transformers-ci-dummy\n     secrets: inherit\n@@ -45,7 +45,7 @@ jobs:\n       job: run_examples_gpu\n       slack_report_channel: \"#amd-hf-ci\"\n       runner_group: hfc-amd-mi355\n-      docker: huggingface/testing-rocm7.0-preview\n+      docker: huggingface/transformers-pytorch-amd-gpu\n       ci_event: Scheduled CI (AMD) - mi355\n       report_repo_id: hf-transformers-bot/transformers-ci-dummy\n     secrets: inherit"
        },
        {
            "sha": "05cb4bebc6a4076f743ddf162f98ddd2b548a577",
            "filename": "docker/transformers-pytorch-amd-gpu/Dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/80305364e2faa402ee39068a7fc0b27ac0ef9d10/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/80305364e2faa402ee39068a7fc0b27ac0ef9d10/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile?ref=80305364e2faa402ee39068a7fc0b27ac0ef9d10",
            "patch": "@@ -39,7 +39,7 @@ RUN python3 -m pip install --no-cache-dir \"torchcodec==0.5\"\n # Install flash attention from source. Tested with commit 6387433156558135a998d5568a9d74c1778666d8\n RUN git clone https://github.com/ROCm/flash-attention/ -b tridao && \\\n     cd flash-attention && \\\n-    GPU_ARCHS=\"gfx942;gfx950\" python setup.py install  \n-# GPU_ARCHS builds for MI300, MI325 and MI355\n+    GPU_ARCHS=\"gfx942\" python setup.py install  \n+# GPU_ARCHS builds for MI300, MI325 but not MI355: we would need to add `;gfx950` but it takes too long to build.\n \n RUN python3 -m pip install --no-cache-dir einops"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}