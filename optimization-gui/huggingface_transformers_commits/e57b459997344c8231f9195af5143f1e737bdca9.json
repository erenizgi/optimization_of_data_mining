{
    "author": "Isotr0py",
    "message": "Split and clean up GGUF quantization tests (#35502)\n\n* clean up ggml test\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* port remaining tests\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* further cleanup\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* format\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* fix broken tests\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* update comment\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* fix\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* reorganize tests\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* k-quants use qwen2.5-0.5B\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* move ggml tokenization test\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* remove dead code\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* add assert for serilization test\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n* use str for parameterize\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>\r\n\r\n---------\r\n\r\nSigned-off-by: Isotr0py <2037008807@qq.com>",
    "sha": "e57b459997344c8231f9195af5143f1e737bdca9",
    "files": [
        {
            "sha": "e00186618ae662318f502536c8964a51a0a13320",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 191,
            "deletions": 313,
            "changes": 504,
            "blob_url": "https://github.com/huggingface/transformers/blob/e57b459997344c8231f9195af5143f1e737bdca9/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e57b459997344c8231f9195af5143f1e737bdca9/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=e57b459997344c8231f9195af5143f1e737bdca9",
            "patch": "@@ -15,6 +15,8 @@\n import tempfile\n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import AddedToken, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n from transformers.testing_utils import (\n     require_gguf,\n@@ -23,20 +25,205 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_available\n+from transformers.utils import is_gguf_available, is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n+if is_gguf_available():\n+    from gguf import GGMLQuantizationType as QuantType\n+\n+\n+@require_gguf\n+@require_torch_gpu\n+@slow\n+class GgufQuantizationTests(unittest.TestCase):\n+    \"\"\"\n+    Test cases for weights dequantization with GGUF models.\n+    Note: The quantization names should keep aligned with `GGMLQuantizationType` in gguf-py:\n+    https://github.com/ggerganov/llama.cpp/blob/4b0c638b9a68f577cb2066b638c9f622d91ee661/gguf-py/gguf/constants.py#L1545-L1576\n+    So quantization like Q4_K_M or Q4_K_S dshouldn't be added to this tests.\n+    \"\"\"\n+\n+    example_text = \"Hello\"\n+\n+    def run_gguf_model(self, gguf_model_id: str, gguf_filename: str, expected_text: str):\n+        tokenizer = AutoTokenizer.from_pretrained(gguf_model_id, gguf_file=gguf_filename)\n+        model = AutoModelForCausalLM.from_pretrained(gguf_model_id, gguf_file=gguf_filename).to(torch_device)\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), expected_text)\n+\n+    @parameterized.expand(\n+        [\n+            # standard quants\n+            (\"Q4_0\", \"Hello, World!\\n\\nStep 3: Add\"),\n+            (\"Q5_0\", \"Hello, World!\\n\\n5. Use a library\"),\n+            (\"Q8_0\", \"Hello, World!\\n\\n5. Use a library\"),\n+        ],\n+    )\n+    def test_standard_quants(self, quant_type: str, expected_text: str):\n+        gguf_model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n+        filename_format = \"tinyllama-1.1b-chat-v1.0.{quant_type}.gguf\"\n+        gguf_filename = filename_format.format(quant_type=quant_type)\n+        self.run_gguf_model(gguf_model_id, gguf_filename, expected_text)\n+\n+    # k-quants\n+    @parameterized.expand(\n+        [\n+            (\"Q2_K\", \"Hello, I'm a 22 year old female\"),\n+            (\"Q3_K\", \"Hello\\n\\nI am trying to create a simple program that\"),\n+            (\"Q4_K\", \"Hello\\n\\nI am trying to create a simple program that\"),\n+            (\"Q5_K\", \"Helloveda is a 1999 Indian\"),\n+            (\"Q6_K\", \"Hello\\n\\nI am trying to create a simple program that\"),\n+        ],\n+    )\n+    def test_k_quants(self, quant_type: str, expected_text: str):\n+        gguf_model_id = \"legraphista/Qwen2.5-0.5B-Instruct-IMat-GGUF\"\n+        filename_format = \"Qwen2.5-0.5B-Instruct.{quant_type}.gguf\"\n+        gguf_filename = filename_format.format(quant_type=quant_type)\n+        self.run_gguf_model(gguf_model_id, gguf_filename, expected_text)\n+\n+    @parameterized.expand(\n+        [\n+            # i-matrix\n+            (\"IQ1_S\", \"Hello, I'm a friend of mine, I\"),\n+            (\"IQ1_M\", \"Hello, I am interested in purching a copy of\"),\n+            (\"IQ2_XXS\", \"Hello, I'm a software engineer. I'\"),\n+            (\"IQ2_XS\", \"Hello World!\\n\\n```\\n<|user|\"),\n+            (\"IQ2_S\", \"Hello World!\\n\\n```\\n<|user|\"),\n+            (\"IQ3_XXS\", \"Hello, I am interested in your product. Can you\"),\n+            (\"IQ4_XS\", \"Hello, world!\\n\\n5. Using a loop\"),\n+            (\"IQ3_S\", \"Hello, World!\\n\\n5. Python:\\n\"),\n+            (\"IQ4_NL\", \"Hello, world!\\n\\n5. Using a loop\"),\n+        ],\n+    )\n+    def test_imatrix_quants(self, quant_type: str, expected_text: str):\n+        gguf_model_id = \"duyntnet/TinyLlama-1.1B-Chat-v1.0-imatrix-GGUF\"\n+        filename_format = \"TinyLlama-1.1B-Chat-v1.0-{quant_type}.gguf\"\n+        gguf_filename = filename_format.format(quant_type=quant_type)\n+        self.run_gguf_model(gguf_model_id, gguf_filename, expected_text)\n+\n \n @require_gguf\n @require_torch_gpu\n @slow\n class GgufIntegrationTests(unittest.TestCase):\n+    \"\"\"\n+    Test cases for basic interoperability with GGUF models:\n+    - Tokenization\n+    - Model dtype casting and serialization\n+    \"\"\"\n+\n+    example_text = \"Hello\"\n     original_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n-    model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n-    imatrix_model_id = \"duyntnet/TinyLlama-1.1B-Chat-v1.0-imatrix-GGUF\"\n+    gguf_model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n+    gguf_filename = \"tinyllama-1.1b-chat-v1.0.{quant_type}.gguf\"\n+\n+    def test_tokenization_xnli(self):\n+        import tqdm\n+        from datasets import load_dataset\n+\n+        q8_0_gguf_model_id = self.gguf_filename.format(quant_type=QuantType.Q8_0.name)\n+        gguf_tokenizer = AutoTokenizer.from_pretrained(self.gguf_model_id, gguf_file=q8_0_gguf_model_id)\n+        original_tokenizer = AutoTokenizer.from_pretrained(self.original_model_id)\n+\n+        dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"go\")\n+        for item in tqdm.tqdm(dataset[\"validation\"]):\n+            string = item[\"code\"]\n+            encoded1 = gguf_tokenizer.encode(string)\n+            encoded2 = original_tokenizer.encode(string)\n+\n+            self.assertEqual(encoded1, encoded2)\n+\n+            decoded1 = gguf_tokenizer.decode(encoded1, skip_special_tokens=True)\n+            decoded2 = original_tokenizer.decode(encoded2, skip_special_tokens=True)\n+\n+            self.assertEqual(decoded1, decoded2)\n+\n+        dataset = load_dataset(\"facebook/xnli\", \"all_languages\")\n+\n+        for i, item in enumerate(tqdm.tqdm(dataset[\"train\"].select(range(100)))):\n+            for string in item[\"premise\"].values():\n+                encoded1 = gguf_tokenizer.encode(string)\n+                encoded2 = original_tokenizer.encode(string)\n+\n+                self.assertEqual(encoded1, encoded2)\n+\n+                decoded1 = gguf_tokenizer.decode(encoded1, skip_special_tokens=True)\n+                decoded2 = original_tokenizer.decode(encoded2, skip_special_tokens=True)\n+\n+                self.assertEqual(decoded1, decoded2)\n+\n+        # With special tokens\n+        gguf_tokenizer = AutoTokenizer.from_pretrained(self.gguf_model_id, gguf_file=q8_0_gguf_model_id)\n+        original_tokenizer = AutoTokenizer.from_pretrained(self.original_model_id)\n+\n+        gguf_tokenizer.add_special_tokens(\n+            {\"additional_special_tokens\": [AddedToken(\"<token>\", rstrip=False, lstrip=False)]}\n+        )\n+        original_tokenizer.add_special_tokens(\n+            {\"additional_special_tokens\": [AddedToken(\"<token>\", rstrip=False, lstrip=False)]}\n+        )\n+\n+        text = \"Hello <token>. <token> Hello\"\n+\n+        encoded1 = gguf_tokenizer.encode(text)\n+        encoded2 = original_tokenizer.encode(text)\n+\n+        self.assertEqual(encoded1, encoded2)\n+\n+        decoded1 = gguf_tokenizer.decode(encoded1, skip_special_tokens=True)\n+        decoded2 = original_tokenizer.decode(encoded2, skip_special_tokens=True)\n+\n+        self.assertEqual(decoded1, decoded2)\n+\n+    def test_q2_k_serialization(self):\n+        q2_k_gguf_model_id = self.gguf_filename.format(quant_type=QuantType.Q2_K.name)\n+        EXPECTED_TEXT = \"Hello, World!\\n\\n[10:0\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.gguf_model_id, gguf_file=q2_k_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(self.gguf_model_id, gguf_file=q2_k_gguf_model_id).to(torch_device)\n+\n+        orig_text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        orig_out = model.generate(**orig_text, max_new_tokens=10)\n+        self.assertEqual(tokenizer.decode(orig_out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            tokenizer.save_pretrained(tmpdirname)\n+\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname).to(torch_device)\n+            tokenizer = AutoTokenizer.from_pretrained(tmpdirname)\n+\n+            text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+            out = model.generate(**text, max_new_tokens=10)\n+\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_q6_k_fp16(self):\n+        q6_k_gguf_model_id = self.gguf_filename.format(quant_type=QuantType.Q6_K.name)\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.gguf_model_id, gguf_file=q6_k_gguf_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gguf_model_id, gguf_file=q6_k_gguf_model_id, torch_dtype=torch.float16\n+        ).to(torch_device)\n+\n+        self.assertTrue(model.lm_head.weight.dtype == torch.float16)\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, World!\\n\\nStep 3: Add\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+\n+@require_gguf\n+@require_torch_gpu\n+@slow\n+class GgufModelTests(unittest.TestCase):\n     mistral_model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n     qwen2_model_id = \"Qwen/Qwen1.5-0.5B-Chat-GGUF\"\n     qwen2moe_model_id = \"gdax/Qwen1.5-MoE-A2.7B_gguf\"\n@@ -68,34 +255,13 @@ class GgufIntegrationTests(unittest.TestCase):\n     original_gemma2_model_id = \"google/gemma-2-2b-it\"\n     gemma2_model_id = \"bartowski/gemma-2-2b-it-GGUF\"\n \n-    # standard quants\n-    q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n-    q5_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q5_0.gguf\"\n-    q8_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q8_0.gguf\"\n-    # k-quants\n-    q2_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q2_K.gguf\"\n-    q3_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q3_K_L.gguf\"\n-    q4_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n-    q5_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf\"\n-    q6_k_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n-    q4_k_m_stablelm_model_id = \"stablelm-3b-4e1t.q4_k_m.gguf\"\n-    # imatrix\n-    iq1_m_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ1_M.gguf\"\n-    iq1_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ1_S.gguf\"\n-    iq2_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ2_S.gguf\"\n-    iq2_xs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ2_XS.gguf\"\n-    iq2_xxs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ2_XXS.gguf\"\n-    iq3_s_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ3_S.gguf\"\n-    iq3_xxs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ3_XXS.gguf\"\n-    iq4_xs_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ4_XS.gguf\"\n-    iq4_nl_gguf_model_id = \"TinyLlama-1.1B-Chat-v1.0-IQ4_NL.gguf\"\n-\n     q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n     q4_0_qwen2_model_id = \"qwen1_5-0_5b-chat-q4_0.gguf\"\n     q8_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B_Q8_0.gguf\"\n     q4_llama3_model_id = \"Meta-Llama-3-8B-Q4_K_M.gguf\"\n     fp16_bloom_model_id = \"bloom-560m.fp16.gguf\"\n+    q4_k_m_stablelm_model_id = \"stablelm-3b-4e1t.q4_k_m.gguf\"\n     fp16_stablelm2_model_id = \"stablelm-2-1_6b.fp16.gguf\"\n     q8_bloom_model_id = \"bloom-560m.q8_0.gguf\"\n     f16_tinyllama_model_id = \"TinyLlama-1.1B-Chat-v1.0.FP16.gguf\"\n@@ -120,237 +286,6 @@ class GgufIntegrationTests(unittest.TestCase):\n \n     example_text = \"Hello\"\n \n-    def test_q2_k(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q2_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q2_k_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n[10:0\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q2_k_serialization(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q2_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q2_k_gguf_model_id).to(torch_device)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            model.save_pretrained(tmpdirname)\n-            tokenizer.save_pretrained(tmpdirname)\n-\n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname).to(torch_device)\n-            tokenizer = AutoTokenizer.from_pretrained(tmpdirname)\n-\n-            text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-            out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n[10:0\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q3_k(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q3_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q3_k_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n```\\n<|user\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q5_0(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q5_0_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q5_0_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Use a library\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q5_k(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q5_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q5_k_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\nStep 3: Add\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q4_0(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q4_0_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q4_0_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\nStep 3: Add\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q4_k_m(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q4_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q4_k_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Python:\\n\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q6_k(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q6_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q6_k_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\nStep 3: Add\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q6_k_fp16(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q6_k_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.model_id, gguf_file=self.q6_k_gguf_model_id, torch_dtype=torch.float16\n-        ).to(torch_device)\n-\n-        self.assertTrue(model.lm_head.weight.dtype == torch.float16)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\nStep 3: Add\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_q8_0(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q8_0_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, gguf_file=self.q8_0_gguf_model_id).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Use a library\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq1_s(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_s_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_s_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, I'm a friend of mine, I\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq1_m(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_m_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq1_m_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, I am interested in purching a copy of\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq2_s(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_s_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_s_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello World!\\n\\n```\\n<|user|\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq2_xs(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xs_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xs_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello World!\\n\\n```\\n<|user|\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq2_xxs(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xxs_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq2_xxs_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, I'm a software engineer. I'\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq3_s(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_s_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_s_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Python:\\n\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq3_xxs(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_xxs_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq3_xxs_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, I am interested in your product. Can you\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq4_xs(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_xs_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_xs_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, world!\\n\\n5. Using a loop\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_iq4_nl(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_nl_gguf_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(self.imatrix_model_id, gguf_file=self.iq4_nl_gguf_model_id).to(\n-            torch_device\n-        )\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, world!\\n\\n5. Using a loop\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n-    def test_f16(self):\n-        tokenizer = AutoTokenizer.from_pretrained(self.tinyllama_model_id, gguf_file=self.f16_tinyllama_model_id)\n-        model = AutoModelForCausalLM.from_pretrained(\n-            self.tinyllama_model_id, gguf_file=self.f16_tinyllama_model_id\n-        ).to(torch_device)\n-\n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n-\n-        EXPECTED_TEXT = \"Hello, World!\\n\\n5. Node.js\"\n-        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n-\n     def test_mistral_q4_0(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.mistral_model_id, gguf_file=self.q4_0_mistral_model_id)\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -904,60 +839,3 @@ def test_gemma2_weights_conversion_fp32(self):\n                 torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n             else:\n                 raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n-\n-    def test_tokenization_xnli(self):\n-        import tqdm\n-        from datasets import load_dataset\n-\n-        gguf_tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q8_0_gguf_model_id)\n-        original_tokenizer = AutoTokenizer.from_pretrained(self.original_model_id)\n-\n-        dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"go\")\n-        for item in tqdm.tqdm(dataset[\"validation\"]):\n-            string = item[\"code\"]\n-            encoded1 = gguf_tokenizer.encode(string)\n-            encoded2 = original_tokenizer.encode(string)\n-\n-            self.assertEqual(encoded1, encoded2)\n-\n-            decoded1 = gguf_tokenizer.decode(encoded1, skip_special_tokens=True)\n-            decoded2 = original_tokenizer.decode(encoded2, skip_special_tokens=True)\n-\n-            self.assertEqual(decoded1, decoded2)\n-\n-        dataset = load_dataset(\"facebook/xnli\", \"all_languages\")\n-\n-        for i, item in enumerate(tqdm.tqdm(dataset[\"train\"].select(range(100)))):\n-            for string in item[\"premise\"].values():\n-                encoded1 = gguf_tokenizer.encode(string)\n-                encoded2 = original_tokenizer.encode(string)\n-\n-                self.assertEqual(encoded1, encoded2)\n-\n-                decoded1 = gguf_tokenizer.decode(encoded1, skip_special_tokens=True)\n-                decoded2 = original_tokenizer.decode(encoded2, skip_special_tokens=True)\n-\n-                self.assertEqual(decoded1, decoded2)\n-\n-        # With special tokens\n-        gguf_tokenizer = AutoTokenizer.from_pretrained(self.model_id, gguf_file=self.q8_0_gguf_model_id)\n-        original_tokenizer = AutoTokenizer.from_pretrained(self.original_model_id)\n-\n-        gguf_tokenizer.add_special_tokens(\n-            {\"additional_special_tokens\": [AddedToken(\"<token>\", rstrip=False, lstrip=False)]}\n-        )\n-        original_tokenizer.add_special_tokens(\n-            {\"additional_special_tokens\": [AddedToken(\"<token>\", rstrip=False, lstrip=False)]}\n-        )\n-\n-        text = \"Hello <token>. <token> Hello\"\n-\n-        encoded1 = gguf_tokenizer.encode(text)\n-        encoded2 = original_tokenizer.encode(text)\n-\n-        self.assertEqual(encoded1, encoded2)\n-\n-        decoded1 = gguf_tokenizer.decode(encoded1, skip_special_tokens=True)\n-        decoded2 = original_tokenizer.decode(encoded2, skip_special_tokens=True)\n-\n-        self.assertEqual(decoded1, decoded2)"
        }
    ],
    "stats": {
        "total": 504,
        "additions": 191,
        "deletions": 313
    }
}