{
    "author": "gante",
    "message": "[gemma3] `Gemma3ForConditionalGeneration` compatible with assisted generation (#40791)\n\n* gemma3vision compatible with assisted generation\n\n* docstring\n\n* BC\n\n* docstring\n\n* failing checks\n\n* make fixup\n\n* apply changes to modular\n\n* misc fixes\n\n* is_initialized\n\n* fix poor rebase",
    "sha": "3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
    "files": [
        {
            "sha": "e6f2645a766e5846020b31fc5b0190698f773d18",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 12,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -31,6 +31,7 @@ class CacheLayerMixin(ABC):\n     def __init__(self):\n         self.keys: Optional[torch.Tensor] = None\n         self.values: Optional[torch.Tensor] = None\n+        self.is_initialized = False\n \n     def __repr__(self):\n         return f\"{self.__class__.__name__}\"\n@@ -54,19 +55,19 @@ def get_max_cache_shape(self) -> int: ...\n \n     def offload(self):\n         \"\"\"Offload this layer's data to CPU device.\"\"\"\n-        if self.keys is not None:\n+        if self.is_initialized:\n             self.keys = self.keys.to(\"cpu\", non_blocking=True)\n             self.values = self.values.to(\"cpu\", non_blocking=True)\n \n     def prefetch(self):\n         \"\"\"In case of layer offloading, this allows to move the data back to the layer's device ahead of time.\"\"\"\n-        if self.keys is not None and self.keys.device != self.device:\n+        if self.is_initialized and self.keys.device != self.device:\n             self.keys = self.keys.to(self.device, non_blocking=True)\n             self.values = self.values.to(self.device, non_blocking=True)\n \n     def reset(self) -> None:\n         \"\"\"Resets the cache values while preserving the objects\"\"\"\n-        if self.keys is not None:\n+        if self.is_initialized:\n             self.keys.zero_()\n             self.values.zero_()\n         # This attribute is set on several Layers\n@@ -92,6 +93,7 @@ def lazy_initialization(self, key_states: torch.Tensor):\n         self.dtype, self.device = key_states.dtype, key_states.device\n         self.keys = torch.tensor([], dtype=self.dtype, device=self.device)\n         self.values = torch.tensor([], dtype=self.dtype, device=self.device)\n+        self.is_initialized = True\n \n     def update(\n         self,\n@@ -111,7 +113,7 @@ def update(\n             tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n-        if self.keys is None:\n+        if not self.is_initialized:\n             self.lazy_initialization(key_states)\n \n         self.keys = torch.cat([self.keys, key_states], dim=-2)\n@@ -127,7 +129,7 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n \n     def get_seq_length(self) -> int:\n         \"\"\"Returns the sequence length of the cached states.\"\"\"\n-        if self.keys is None or self.keys.numel() == 0:\n+        if not self.is_initialized or self.keys.numel() == 0:\n             return 0\n         return self.keys.shape[-2]\n \n@@ -193,7 +195,7 @@ def update(\n             tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n-        if self.keys is None:\n+        if not self.is_initialized:\n             self.lazy_initialization(key_states)\n \n         self.cumulative_length += key_states.shape[-2]\n@@ -295,6 +297,8 @@ def lazy_initialization(self, key_states: torch.Tensor):\n             torch._dynamo.mark_static_address(self.keys)\n             torch._dynamo.mark_static_address(self.values)\n \n+        self.is_initialized = True\n+\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -313,7 +317,7 @@ def update(\n             tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n-        if self.keys is None:\n+        if not self.is_initialized:\n             self.lazy_initialization(key_states)\n \n         # Some old models give None for `cache_position` or even omit passing `cache_kwargs` when used as cross-attention,\n@@ -343,7 +347,7 @@ def get_seq_length(self) -> int:\n         \"\"\"Returns the sequence length of the cached states.\"\"\"\n         # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n         # limit the check to the first batch member and head dimension.\n-        return (self.keys[0, 0].any(dim=-1)).sum() if self.keys is not None else 0\n+        return (self.keys[0, 0].any(dim=-1)).sum() if self.is_initialized else 0\n \n     def get_max_cache_shape(self) -> int:\n         \"\"\"Return the maximum cache shape of the cache\"\"\"\n@@ -388,7 +392,7 @@ def update(\n             tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n         \"\"\"\n         # Lazy initialization\n-        if self.keys is None:\n+        if not self.is_initialized:\n             self.lazy_initialization(key_states)\n \n         cache_position = cache_kwargs.get(\"cache_position\")\n@@ -518,7 +522,7 @@ def update(\n         self.cumulative_length += key_states.shape[-2]\n \n         # Lazy initialization\n-        if self.keys is None:\n+        if not self.is_initialized:\n             self.lazy_initialization(key_states)\n             self._quantized_keys = self._quantize(key_states.contiguous(), axis=self.axis_key)\n             self._quantized_values = self._quantize(value_states.contiguous(), axis=self.axis_value)\n@@ -859,6 +863,11 @@ def is_compileable(self) -> bool:\n             return False\n         return all(layer.is_compileable for layer in self.layers)\n \n+    @property\n+    def is_initialized(self) -> bool:\n+        \"\"\"Return whether the cache data is initialized\"\"\"\n+        return len(self.layers) > 0 and all(layer.is_initialized for layer in self.layers)\n+\n     @property\n     def is_sliding(self) -> list[bool]:\n         \"\"\"Return whether the layers of the cache are sliding window\"\"\"\n@@ -871,8 +880,6 @@ def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         if layer_idx < len(self.layers):\n             return self.layers[layer_idx].keys, self.layers[layer_idx].values\n-        # elif len(self.layers) == 0:\n-        #     return None, None\n         else:\n             raise KeyError(\n                 f\"Cache only has {len(self.layers)} layers, attempted to access layer with index {layer_idx}\""
        },
        {
            "sha": "126b683e672dfd2cafceaf4cd7054a324f313d52",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -1369,7 +1369,12 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n )\n \n \n-def layer_type_validation(layer_types: list[str]):\n-    \"\"\"Check that each entry in `layer_types` are allowed.\"\"\"\n+def layer_type_validation(layer_types: list[str], num_hidden_layers: Optional[int] = None):\n+    \"\"\"Check that `layer_types` is correctly defined.\"\"\"\n     if not all(layer_type in ALLOWED_LAYER_TYPES for layer_type in layer_types):\n         raise ValueError(f\"The `layer_types` entries must be in {ALLOWED_LAYER_TYPES}\")\n+    if num_hidden_layers is not None and num_hidden_layers != len(layer_types):\n+        raise ValueError(\n+            f\"`num_hidden_layers` ({num_hidden_layers}) must be equal to the number of layer types \"\n+            f\"({len(layer_types)})\"\n+        )"
        },
        {
            "sha": "a455e69d03ffaf42fcba2ae45ff3f69e072dbf8b",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -185,7 +185,7 @@ def __init__(\n                 )\n \n         # We need to roll back the cache in assisted generation, only DynamicCache is supported\n-        self.generation_config.cache_implementation = None\n+        self.generation_config.cache_implementation = \"dynamic_full\"\n \n         if (\n             is_sklearn_available()\n@@ -298,6 +298,10 @@ def _update_past_and_masks(\n             )\n             self.assistant_kwargs = _prepare_token_type_ids(self.assistant_kwargs, input_ids.shape[-1])\n \n+            # This unsets `dynamic_full`, needed to initialize a new cache for the assistant. After the first forward\n+            # pass on each generation, we reuse the cache instead.\n+            self.generation_config.cache_implementation = None\n+\n         return has_past_key_values\n \n     def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int) -> dict:"
        },
        {
            "sha": "d43ba87ab52cd2d37be405ada2e70f7d7ebdc333",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 14,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -1934,6 +1934,10 @@ def _prepare_cache_for_generation(\n                     \"Cache object) is unsupported. Please use only one of the two.\"\n                 )\n             if isinstance(user_defined_cache, tuple) and self._supports_default_dynamic_cache():\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                    \"You should pass an instance of `Cache` instead.\"\n+                )\n                 model_kwargs[cache_name] = (\n                     DynamicCache.from_legacy_cache(user_defined_cache)\n                     if not requires_cross_attention_cache\n@@ -1946,14 +1950,13 @@ def _prepare_cache_for_generation(\n         if generation_config.use_cache is False:\n             return\n \n-        # Quick escape route 3: model that only supports legacy caches or models that supply it in `prepare_inputs_for_generation` (mamba, zamba, ...)\n+        # Quick escape route 3: model that only supports legacy caches or models that supply it in\n+        # `prepare_inputs_for_generation` (mamba, zamba, ...)\n         if not self._supports_default_dynamic_cache():\n             if generation_config.cache_implementation is not None:\n-                warnings.warn(\n-                    \"This model does not support `Cache` instances, it only supports the legacy cache format (tuple \"\n-                    f\"of tuples). `cache_implementation` (set to {generation_config.cache_implementation}) will be \"\n-                    \"ignored.\",\n-                    UserWarning,\n+                logger.warning_once(\n+                    \"This model does not support `Cache` instances. `cache_implementation` (set to \"\n+                    f\"{generation_config.cache_implementation}) will be ignored.\",\n                 )\n             return\n \n@@ -1985,8 +1988,9 @@ def _prepare_cache_for_generation(\n             if generation_config.cache_implementation in ALL_STATIC_CACHE_IMPLEMENTATIONS:\n                 if generation_config.cache_implementation in DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS:\n                     logger.warning_once(\n-                        f\"Using `cache_implementation='{generation_config.cache_implementation}' is deprecated. Please only \"\n-                        f\"use one of {STATIC_CACHE_IMPLEMENTATIONS}, and the layer structure will be inferred automatically.\"\n+                        f\"Using `cache_implementation='{generation_config.cache_implementation}' is deprecated. \"\n+                        f\"Please only use one of {STATIC_CACHE_IMPLEMENTATIONS}, and the layer structure will be \"\n+                        \"inferred automatically.\"\n                     )\n                 model_kwargs[cache_name] = self._get_cache(\n                     cache_implementation=generation_config.cache_implementation,\n@@ -2010,8 +2014,8 @@ def _prepare_cache_for_generation(\n \n                 if backend == \"quanto\" and not is_optimum_quanto_available():\n                     raise ImportError(\n-                        \"You need to install optimum-quanto in order to use KV cache quantization with optimum-quanto backend. \"\n-                        \"Please install it via  with `pip install optimum-quanto`\"\n+                        \"You need to install optimum-quanto in order to use KV cache quantization with optimum-quanto \"\n+                        \"backend. Please install it via  with `pip install optimum-quanto`\"\n                     )\n                 elif backend == \"HQQ\" and not is_hqq_available():\n                     raise ImportError(\n@@ -2026,11 +2030,18 @@ def _prepare_cache_for_generation(\n \n         # Use DynamicCache instance by default. This will avoid back and forth from legacy format that\n         # keeps copying the cache thus using much more memory\n+        # TODO (joao): remove this `else` when we remove the last traces of the legacy cache format (v4.58.0, search\n+        # for `instance(past_key_values, Cache)` as well). In general, if `cache_implementation` is unset, cache\n+        # initialization should happen inside the model at prefill time.\n         else:\n-            model_kwargs[cache_name] = (\n-                DynamicCache(**dynamic_cache_kwargs)\n-                if not requires_cross_attention_cache\n-                else EncoderDecoderCache(DynamicCache(**dynamic_cache_kwargs), DynamicCache(**dynamic_cache_kwargs))\n+            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n+\n+        # TODO (joao): this logic is incomplete, e.g. `offloaded` should apply to both caches. Refactor this function\n+        # to correctly pass parameterization to both caches.\n+        if requires_cross_attention_cache and not isinstance(model_kwargs[cache_name], EncoderDecoderCache):\n+            model_kwargs[cache_name] = EncoderDecoderCache(\n+                model_kwargs[cache_name],  # self-attention cache\n+                DynamicCache(**dynamic_cache_kwargs),  # cross-attention cache\n             )\n \n     def _supports_logits_to_keep(self) -> bool:"
        },
        {
            "sha": "c92f63cad312651fd750fb12156166f95ce3d8d4",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -226,7 +226,7 @@ def __init__(\n                 \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n __all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "91ed748e0361841b9e9df272a3feb6884fcd6fd9",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -247,7 +247,7 @@ def __init__(\n                 \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Cohere2RotaryEmbedding(CohereRotaryEmbedding):"
        },
        {
            "sha": "c8596ddc3828b70d28df3d115bb46558d49ab210",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -200,7 +200,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "0ced6651d41c86f43504e04a4d0de7a1947d7712",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -213,7 +213,7 @@ def __init__(\n             ]\n         if \"sliding_window\" in self.layer_types:\n             self.cache_implementation = \"hybrid\"\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "d366354bda2f6b5054070b0a5d837d3f26d20c65",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -248,7 +248,7 @@ def __init__(\n             ]\n         if \"sliding_window\" in self.layer_types:\n             self.cache_implementation = \"hybrid\"\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs"
        },
        {
            "sha": "d43ec4c47371411c0b15b34afe4d82456d4f0cf8",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -176,7 +176,7 @@ def __init__(\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n __all__ = [\"Gemma2Config\"]"
        },
        {
            "sha": "47d612de5a4bc9154eb3307cb0af9dad4a94c17e",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -200,7 +200,7 @@ def __init__(\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Gemma2RMSNorm(GemmaRMSNorm):"
        },
        {
            "sha": "15d055654b11be82d9a1fead45c87dafc915487c",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -240,7 +240,7 @@ def __init__(\n                 \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Gemma3Config(PretrainedConfig):"
        },
        {
            "sha": "7a91db1905f7c36e62708430ee0ab46c1a121152",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -920,11 +920,21 @@ def forward(\n                 \"past_key_values\": past_key_values,\n                 \"position_ids\": position_ids,\n             }\n-            if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n+            # NOTE: this `is_prefill` logic is not flawless, it fails when we're using a cache eagerly initialized\n+            # (e.g. compiled prefill) AND `pixel_values` are not provided. Determining prefill in that case requires\n+            # checking data values, which is not compile-compatible.\n+            is_prefill = (\n+                not use_cache\n+                or past_key_values is None\n+                or not past_key_values.is_initialized\n+                or pixel_values is not None\n+            )\n+            if token_type_ids is not None and is_prefill:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n \n                 # First find where a new image block starts: 1 if image and previous not image\n-                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+                # The images cannot attend to future images, but can attend to all prev images and to itself\n+                # bidirectionally\n                 is_image = (token_type_ids == 1).to(cache_position.device)\n                 new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n                 image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1"
        },
        {
            "sha": "f0658f9825f8b3e762222448ad11ecc87660d784",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -251,7 +251,7 @@ def __init__(\n                 \"sliding_attention\" if bool((i + 1) % self._sliding_window_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Gemma3Config(PretrainedConfig):\n@@ -838,11 +838,21 @@ def forward(\n                 \"past_key_values\": past_key_values,\n                 \"position_ids\": position_ids,\n             }\n-            if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n+            # NOTE: this `is_prefill` logic is not flawless, it fails when we're using a cache eagerly initialized\n+            # (e.g. compiled prefill) AND `pixel_values` are not provided. Determining prefill in that case requires\n+            # checking data values, which is not compile-compatible.\n+            is_prefill = (\n+                not use_cache\n+                or past_key_values is None\n+                or not past_key_values.is_initialized\n+                or pixel_values is not None\n+            )\n+            if token_type_ids is not None and is_prefill:\n                 # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n \n                 # First find where a new image block starts: 1 if image and previous not image\n-                # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+                # The images cannot attend to future images, but can attend to all prev images and to itself\n+                # bidirectionally\n                 is_image = (token_type_ids == 1).to(cache_position.device)\n                 new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n                 image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1"
        },
        {
            "sha": "3502d2a423c9ba53ff84a96d83d51fbe520ce201",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -277,7 +277,7 @@ def __init__(\n         else:\n             self.layer_types = layer_types\n \n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_kv_shared_layers = num_kv_shared_layers"
        },
        {
            "sha": "0264d77c02d5dacacfdfd65f68ca082e41297220",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -290,7 +290,7 @@ def __init__(\n         else:\n             self.layer_types = layer_types\n \n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         self.hidden_size_per_layer_input = hidden_size_per_layer_input\n         self.num_kv_shared_layers = num_kv_shared_layers"
        },
        {
            "sha": "6459e9a7fd4a936033f06b95fd7fc4c1381fe31b",
            "filename": "src/transformers/models/gpt_oss/configuration_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -103,7 +103,7 @@ def __init__(\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         self.attention_bias = True\n         self.max_position_embeddings = max_position_embeddings"
        },
        {
            "sha": "7ced47cb94366737adfd6b9f3589293f7b029f59",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -378,7 +378,7 @@ def __init__(\n             self.layer_types = [\n                 \"chunked_attention\" if no_rope else \"full_attention\" for no_rope in self.no_rope_layers\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Llama4Config(PretrainedConfig):"
        },
        {
            "sha": "2ab46efb2cf822548242ffce2fdb122dbce8b6b8",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -224,7 +224,7 @@ def __init__(\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n __all__ = [\"MiniMaxConfig\"]"
        },
        {
            "sha": "d3af8beab87f75328932188a04eb00aab1ecf701",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -178,7 +178,7 @@ def __init__(\n             self.layer_types = [\n                 \"full_attention\" if bool((i + 1) % 2) else \"linear_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class MiniMaxRMSNorm(MixtralRMSNorm):"
        },
        {
            "sha": "4d75e25092f4c04ef119682f61b9b32241ffb398",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -207,7 +207,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "7bd36b7a3c0dda2da0ce85be66fa4d3705ba5b81",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -407,7 +407,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Qwen2_5OmniThinkerConfig(PretrainedConfig):\n@@ -787,7 +787,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        },
        {
            "sha": "07cd851d4f88cb4bb28d015aa3a423369a7d94d2",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -442,7 +442,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class Qwen2_5OmniThinkerConfig(PretrainedConfig):\n@@ -822,7 +822,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        },
        {
            "sha": "a7a489c3e867258db7ecfd8988bbcdd90b0c4ade",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -252,7 +252,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'."
        },
        {
            "sha": "1f9e0a3a5bc4cec2084eddf8ed44734e9e757053",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -241,7 +241,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'."
        },
        {
            "sha": "0b642913dce51c2b398bfbe0c25c4183821eb2f6",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -215,7 +215,7 @@ def __init__(\n                 else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "148166cbd16fc3f83663d32a66a73e375971c05d",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -248,7 +248,7 @@ def __init__(\n                 \"linear_attention\" if bool((i + 1) % interval_pattern) else \"full_attention\"\n                 for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         # linear attention part\n         self.linear_conv_kernel_dim = linear_conv_kernel_dim"
        },
        {
            "sha": "325703f782c0163ee738e0bd78295318285d3b33",
            "filename": "src/transformers/models/smollm3/configuration_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fconfiguration_smollm3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -235,7 +235,7 @@ def __init__(\n                     layer_types.append(\"full_attention\")\n \n         self.layer_types = layer_types\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'."
        },
        {
            "sha": "cd82fccbd7cce0c8d505b628b2dfa1069f84f9f3",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -254,7 +254,7 @@ def __init__(\n                     layer_types.append(\"full_attention\")\n \n         self.layer_types = layer_types\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'."
        },
        {
            "sha": "217a24df0417a157072ff72a6fd8535dd5540907",
            "filename": "src/transformers/models/t5gemma/configuration_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fconfiguration_t5gemma.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -178,7 +178,7 @@ def __init__(\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n class T5GemmaConfig(PretrainedConfig):"
        },
        {
            "sha": "1b93ae6ccb04f9299fee6101a8d0d554e148d5fe",
            "filename": "src/transformers/models/vaultgemma/configuration_vaultgemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fconfiguration_vaultgemma.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -176,7 +176,7 @@ def __init__(\n             self.layer_types = [\n                 \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n             ]\n-        layer_type_validation(self.layer_types)\n+        layer_type_validation(self.layer_types, self.num_hidden_layers)\n \n \n __all__ = [\"VaultGemmaConfig\"]"
        },
        {
            "sha": "51fa682c7be4a1f7a59d81ae6fc767b838963d8e",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -1639,6 +1639,10 @@ def assert_screenout(out, what):\n \n \n def set_model_tester_for_less_flaky_test(test_case):\n+    # NOTE: this function edits the config object, which may lead to hard-to-debug side-effects. Use with caution.\n+    # Do not use in tests/models where objects behave very differently based on the config's hidden layer settings\n+    # (e.g. KV caches, sliding window attention, ...)\n+\n     # TODO (if possible): Avoid exceptional cases\n     exceptional_classes = [\n         \"ZambaModelTester\","
        },
        {
            "sha": "680002d4600b1eb96b286b88a40050bda9bfb284",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -52,7 +52,6 @@\n     require_torch_multi_accelerator,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n-    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n@@ -675,10 +674,6 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n         # - assisted_decoding does not support `use_cache = False`\n         # - assisted_decoding does not support `batch_size > 1`\n \n-        # No idea why this cause problem!\n-        if type(self).__name__ not in [\"Gemma3nTextModelTest\"]:\n-            set_model_tester_for_less_flaky_test(self)\n-\n         for model_class in self.all_generative_model_classes:\n             if model_class._is_stateful:\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n@@ -720,6 +715,8 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             #    the assistant model is correct\n             # c) there are at least two forward passes in the main model, to ensure the input preparation of\n             #    the main model is correct\n+            # d) use a cache type compatible with rollbacks (only dynamic cache atm). Otherwise, there may be\n+            #     differences vs model-specific default cache\n             generation_kwargs = {\n                 \"eos_token_id\": -1,  # see a)\n                 \"max_new_tokens\": 4,  # see c)\n@@ -731,6 +728,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n+                \"cache_implementation\": \"dynamic_full\",  # see d)\n             }\n             logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n \n@@ -804,6 +802,8 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             #    prompt lookup is correct\n             # c) there are at least two forward passes in the main model, to ensure the input preparation of\n             #    the main model is correct\n+            # d) use a cache type compatible with rollbacks (only dynamic cache atm). Otherwise, there may be\n+            #     differences vs model-specific default cache\n             generation_kwargs = {\n                 \"eos_token_id\": -1,  # see a)\n                 \"max_new_tokens\": 4,  # see c)\n@@ -815,6 +815,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n+                \"cache_implementation\": \"dynamic_full\",  # see d)\n             }\n             logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n \n@@ -872,6 +873,8 @@ def test_assisted_decoding_sample(self):\n             #    the assistant model is correct\n             # c) there are at least two forward passes in the main model, to ensure the input preparation of\n             #    the main model is correct\n+            # d) use a cache type compatible with rollbacks (only dynamic cache atm). Otherwise, there may be\n+            #     differences vs model-specific default cache\n             assistant_model = model\n             assistant_model.generation_config.num_assistant_tokens = 2  # see b)\n             assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"  # see b)\n@@ -887,6 +890,7 @@ def test_assisted_decoding_sample(self):\n                 \"output_attentions\": self.has_attentions,\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n+                \"cache_implementation\": \"dynamic_full\",  # see d)\n             }\n             logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n             output_assisted = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n@@ -1183,7 +1187,6 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n         \"\"\"Tests that we can generate from `inputs_embeds` instead of `input_ids` in LLMs, VLMs, etc\"\"\"\n         # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n         # if fails, you should probably update the `prepare_inputs_for_generation` function\n-        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n@@ -1851,7 +1854,6 @@ def _test_attention_implementation(self, attn_implementation):\n             \"flash_attention_3\": \"_supports_flash_attn\",\n         }\n \n-        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                 self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n@@ -2222,8 +2224,6 @@ def test_custom_4d_attention_mask(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        set_model_tester_for_less_flaky_test(self)\n-\n         for model_class in self.all_generative_model_classes:\n             if not model_class._can_compile_fullgraph:\n                 self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")"
        },
        {
            "sha": "ddef6e0d6bc10d0ce44907f08c0721e1238d5ad3",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -345,17 +345,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @parameterized.expand([(\"random\",), (\"same\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma3 does not seem to be compatible with assisted decoding\")\n-    def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n-        pass\n-\n-    @pytest.mark.generate\n-    @unittest.skip(\"Gemma3 does not seem to be compatible with assisted decoding\")\n-    def test_assisted_decoding_sample(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Siglip (vision backbone) uses the same initialization scheme as the Flax original implementation\"\n     )"
        },
        {
            "sha": "5e4b774a8bd03e07a049b001c030bd3669bf82d5",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 30,
            "deletions": 6,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -385,23 +385,47 @@ def test_eager_matches_sdpa_inference(\n         output_attentions,\n         enable_kernels,\n     ):\n-        \"We need to relax a bit the `atols` for fp32 here due to the altup projections\"\n+        \"We need to relax a bit the `atols` and `rtols` for fp32 here due to the altup projections\"\n         atols = {\n-            (\"cpu\", False, torch.float32): 1e-3,  # this was relaxed\n+            (\"cpu\", False, torch.float32): 5e-2,  # this was relaxed\n             (\"cpu\", False, torch.float16): 5e-3,\n             (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-3,  # this was relaxed\n+            (\"cpu\", True, torch.float32): 5e-2,  # this was relaxed\n             (\"cpu\", True, torch.float16): 5e-3,\n             (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-3,  # this was relaxed\n+            (\"cuda\", False, torch.float32): 5e-2,  # this was relaxed\n             (\"cuda\", False, torch.bfloat16): 1e-2,\n             (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-3,  # this was relaxed\n+            (\"cuda\", True, torch.float32): 5e-2,  # this was relaxed\n             (\"cuda\", True, torch.bfloat16): 1e-2,\n             (\"cuda\", True, torch.float16): 5e-3,\n         }\n+\n+        rtols = {\n+            (\"cpu\", False, torch.float32): 1e-2,  # this was relaxed\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-2,  # this was relaxed\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-2,  # this was relaxed\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-2,  # this was relaxed\n+            (\"cuda\", True, torch.bfloat16): 3e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+\n         _test_eager_matches_sdpa_inference(\n-            self, name, dtype, padding_side, use_attention_mask, output_attentions, enable_kernels, atols=atols\n+            self,\n+            name,\n+            dtype,\n+            padding_side,\n+            use_attention_mask,\n+            output_attentions,\n+            enable_kernels,\n+            atols=atols,\n+            rtols=rtols,\n         )\n \n     @pytest.mark.generate"
        },
        {
            "sha": "4e95b1f255a5cfc3d58dbb359fb0d119401d914e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3b3f6cd0c19bc1202502eb66f7e760ef7c39ea04",
            "patch": "@@ -100,7 +100,6 @@\n     run_test_using_subprocess,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n-    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n@@ -225,8 +224,6 @@ def _test_eager_matches_sdpa_inference(\n             (\"cuda\", True, torch.float16): 5e-3,\n         }\n \n-    set_model_tester_for_less_flaky_test(self)\n-\n     def _can_output_attn(model):\n         parameters = inspect.signature(model.forward).parameters\n         if \"output_attentions\" in parameters:\n@@ -1095,8 +1092,6 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                     msg += str(e)\n                     raise AssertionError(msg)\n \n-        set_model_tester_for_less_flaky_test(self)\n-\n         config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n         set_config_for_less_flaky_test(config)\n "
        }
    ],
    "stats": {
        "total": 243,
        "additions": 151,
        "deletions": 92
    }
}