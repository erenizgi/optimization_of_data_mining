{
    "author": "qubvel",
    "message": "Handle Trainer `tokenizer` kwarg deprecation with decorator (#33887)\n\n* Handle deprecation with decorator\r\n\r\n* Fix for seq2seq Trainer",
    "sha": "2f25ab95dbe220350aedf1b66fe6f2e12346c605",
    "files": [
        {
            "sha": "a1805adf8328f280bb651d13c7120bae782c9a64",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f25ab95dbe220350aedf1b66fe6f2e12346c605/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f25ab95dbe220350aedf1b66fe6f2e12346c605/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=2f25ab95dbe220350aedf1b66fe6f2e12346c605",
            "patch": "@@ -177,6 +177,7 @@\n     logging,\n     strtobool,\n )\n+from .utils.deprecation import deprecate_kwarg\n from .utils.quantization_config import QuantizationMethod\n \n \n@@ -326,11 +327,6 @@ class Trainer:\n              The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n              `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n              dataset prepending the dictionary key to the metric name.\n-        tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n-            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n-            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n-            interrupted training or reuse the fine-tuned model.\n-            This is now deprecated.\n         processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):\n             Processing class used to process the data. If provided, will be used to automatically process the inputs\n             for the model, and it will be saved along the model to make it easier to rerun an interrupted training or\n@@ -385,14 +381,14 @@ class Trainer:\n     # Those are used as methods of the Trainer in examples.\n     from .trainer_pt_utils import _get_learning_rate, log_metrics, metrics_format, save_metrics, save_state\n \n+    @deprecate_kwarg(\"tokenizer\", new_name=\"processing_class\", version=\"5.0.0\", raise_if_both_names=True)\n     def __init__(\n         self,\n         model: Union[PreTrainedModel, nn.Module] = None,\n         args: TrainingArguments = None,\n         data_collator: Optional[DataCollator] = None,\n         train_dataset: Optional[Union[Dataset, IterableDataset, \"datasets.Dataset\"]] = None,\n         eval_dataset: Optional[Union[Dataset, Dict[str, Dataset], \"datasets.Dataset\"]] = None,\n-        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n         processing_class: Optional[\n             Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]\n         ] = None,\n@@ -437,17 +433,6 @@ def __init__(\n         # force device and distributed setup init explicitly\n         args._setup_devices\n \n-        if tokenizer is not None:\n-            if processing_class is not None:\n-                raise ValueError(\n-                    \"You cannot specify both `tokenizer` and `processing_class` at the same time. Please use `processing_class`.\"\n-                )\n-            warnings.warn(\n-                \"`tokenizer` is now deprecated and will be removed in v5, please use `processing_class` instead.\",\n-                FutureWarning,\n-            )\n-            processing_class = tokenizer\n-\n         if model is None:\n             if model_init is not None:\n                 self.model_init = model_init"
        },
        {
            "sha": "adbf89bb21aea5299acdcdfbed253803696533c4",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f25ab95dbe220350aedf1b66fe6f2e12346c605/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f25ab95dbe220350aedf1b66fe6f2e12346c605/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=2f25ab95dbe220350aedf1b66fe6f2e12346c605",
            "patch": "@@ -25,6 +25,7 @@\n from .integrations.deepspeed import is_deepspeed_zero3_enabled\n from .trainer import Trainer\n from .utils import logging\n+from .utils.deprecation import deprecate_kwarg\n \n \n if TYPE_CHECKING:\n@@ -43,14 +44,14 @@\n \n \n class Seq2SeqTrainer(Trainer):\n+    @deprecate_kwarg(\"tokenizer\", new_name=\"processing_class\", version=\"5.0.0\", raise_if_both_names=True)\n     def __init__(\n         self,\n         model: Union[\"PreTrainedModel\", nn.Module] = None,\n         args: \"TrainingArguments\" = None,\n         data_collator: Optional[\"DataCollator\"] = None,\n         train_dataset: Optional[Dataset] = None,\n         eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n-        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         processing_class: Optional[\n             Union[\"PreTrainedTokenizerBase\", \"BaseImageProcessor\", \"FeatureExtractionMixin\", \"ProcessorMixin\"]\n         ] = None,\n@@ -66,7 +67,6 @@ def __init__(\n             data_collator=data_collator,\n             train_dataset=train_dataset,\n             eval_dataset=eval_dataset,\n-            tokenizer=tokenizer,\n             processing_class=processing_class,\n             model_init=model_init,\n             compute_metrics=compute_metrics,"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 4,
        "deletions": 19
    }
}