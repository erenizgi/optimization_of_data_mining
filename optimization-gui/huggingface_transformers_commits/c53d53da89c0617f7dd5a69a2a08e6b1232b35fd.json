{
    "author": "geetu040",
    "message": "ðŸš¨ðŸš¨ðŸš¨ Fix sdpa in SAM and refactor relative position embeddings (#36422)\n\n* fall back to eager if output_attentions\n\n* improve relative position embeddings\n\n* run modular on got_ocr2\n\n* run-slow: sam\n\n* fix run-length encoding\n\n* fix tf processor errors\n\n* update tf_sam\n\n* fix compile error\n\n* re-run tests",
    "sha": "c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
    "files": [
        {
            "sha": "180e9f177736af96b40fdb3883750d6b3a39d6f4",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
            "patch": "@@ -114,9 +114,8 @@ def get_rel_pos(self, q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.\n \n         return rel_pos_resized[relative_coords.long()]\n \n-    def add_decomposed_rel_pos(\n+    def get_decomposed_rel_pos(\n         self,\n-        attn: torch.Tensor,\n         query: torch.Tensor,\n         rel_pos_h: torch.Tensor,\n         rel_pos_w: torch.Tensor,\n@@ -128,8 +127,6 @@ def add_decomposed_rel_pos(\n         https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n \n         Args:\n-            attn (`torch.Tensor`):\n-                attention map.\n             query (`torch.Tensor`):\n                 query q in the attention layer with shape (batch_size, query_height * query_width, channel).\n             rel_pos_h (`torch.Tensor`):\n@@ -142,8 +139,8 @@ def add_decomposed_rel_pos(\n                 spatial sequence size of key k with (key_height, key_width).\n \n         Returns:\n-            attn (`torch.Tensor`):\n-                attention map with added relative positional embeddings.\n+            decomposed_rel_pos (`torch.Tensor`):\n+                decomposed relative position embeddings.\n         \"\"\"\n         query_height, query_width = q_size\n         key_height, key_width = k_size\n@@ -154,10 +151,10 @@ def add_decomposed_rel_pos(\n         reshaped_query = query.reshape(batch_size, query_height, query_width, dim)\n         rel_h = torch.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n         rel_w = torch.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n-        attn = attn.reshape(batch_size, query_height, query_width, key_height, key_width)\n-        attn = attn + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n-        attn = attn.reshape(batch_size, query_height * query_width, key_height * key_width)\n-        return attn\n+\n+        decomposed_rel_pos = rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n+\n+        return decomposed_rel_pos\n \n     def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n         batch_size, height, width, _ = hidden_states.shape\n@@ -173,9 +170,11 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n         attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n \n         if self.use_rel_pos:\n-            attn_weights = self.add_decomposed_rel_pos(\n-                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            decomposed_rel_pos = self.get_decomposed_rel_pos(\n+                query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n             )\n+            decomposed_rel_pos = decomposed_rel_pos.reshape_as(attn_weights)\n+            attn_weights = attn_weights + decomposed_rel_pos\n \n         attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n "
        },
        {
            "sha": "903a0cbf134ec14f47cbdc211f715f57afcb62b0",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
            "patch": "@@ -1381,7 +1381,7 @@ def _mask_to_rle_pytorch(input_mask: \"torch.Tensor\"):\n             continue\n         btw_idxs = cur_idxs[1:] - cur_idxs[:-1]\n         counts = [] if input_mask[i, 0] == 0 else [0]\n-        counts += [cur_idxs[0].item()] + btw_idxs.tolist() + [height * width - cur_idxs[-1]]\n+        counts += [cur_idxs[0].item()] + btw_idxs.tolist() + [height * width - cur_idxs[-1].item()]\n         out.append({\"size\": [height, width], \"counts\": counts})\n     return out\n \n@@ -1401,7 +1401,7 @@ def _mask_to_rle_tf(input_mask: \"tf.Tensor\"):\n     # Encode run length\n     out = []\n     for i in range(batch_size):\n-        cur_idxs = change_indices[change_indices[:, 0] == i, 1] + 1\n+        cur_idxs = change_indices[change_indices[:, 0] == i][:, 1] + 1\n         if len(cur_idxs) == 0:\n             # No changes => either all 0 or all 1\n             # If the entire mask is 0, RLE is [height*width] or if the entire mask is 1, RLE is [0, height*width].\n@@ -1412,7 +1412,9 @@ def _mask_to_rle_tf(input_mask: \"tf.Tensor\"):\n             continue\n         btw_idxs = cur_idxs[1:] - cur_idxs[:-1]\n         counts = [] if input_mask[i, 0] == 0 else [0]\n-        counts += [cur_idxs[0].item()] + btw_idxs.tolist() + [height * width - cur_idxs[-1]]\n+        counts += (\n+            [cur_idxs[0].numpy().item()] + btw_idxs.numpy().tolist() + [height * width - cur_idxs[-1].numpy().item()]\n+        )\n         out.append({\"size\": [height, width], \"counts\": counts})\n     return out\n "
        },
        {
            "sha": "7fdb7a81dd46866565040d71eb6f765fe1684a8b",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 31,
            "deletions": 74,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
            "patch": "@@ -820,9 +820,8 @@ def get_rel_pos(self, q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.\n \n         return rel_pos_resized[relative_coords.long()]\n \n-    def add_decomposed_rel_pos(\n+    def get_decomposed_rel_pos(\n         self,\n-        attn: torch.Tensor,\n         query: torch.Tensor,\n         rel_pos_h: torch.Tensor,\n         rel_pos_w: torch.Tensor,\n@@ -834,8 +833,6 @@ def add_decomposed_rel_pos(\n         https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n \n         Args:\n-            attn (`torch.Tensor`):\n-                attention map.\n             query (`torch.Tensor`):\n                 query q in the attention layer with shape (batch_size, query_height * query_width, channel).\n             rel_pos_h (`torch.Tensor`):\n@@ -848,8 +845,8 @@ def add_decomposed_rel_pos(\n                 spatial sequence size of key k with (key_height, key_width).\n \n         Returns:\n-            attn (`torch.Tensor`):\n-                attention map with added relative positional embeddings.\n+            decomposed_rel_pos (`torch.Tensor`):\n+                decomposed relative position embeddings.\n         \"\"\"\n         query_height, query_width = q_size\n         key_height, key_width = k_size\n@@ -860,10 +857,10 @@ def add_decomposed_rel_pos(\n         reshaped_query = query.reshape(batch_size, query_height, query_width, dim)\n         rel_h = torch.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n         rel_w = torch.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n-        attn = attn.reshape(batch_size, query_height, query_width, key_height, key_width)\n-        attn = attn + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n-        attn = attn.reshape(batch_size, query_height * query_width, key_height * key_width)\n-        return attn\n+\n+        decomposed_rel_pos = rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n+\n+        return decomposed_rel_pos\n \n     def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n         batch_size, height, width, _ = hidden_states.shape\n@@ -879,9 +876,11 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n         attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n \n         if self.use_rel_pos:\n-            attn_weights = self.add_decomposed_rel_pos(\n-                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            decomposed_rel_pos = self.get_decomposed_rel_pos(\n+                query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n             )\n+            decomposed_rel_pos = decomposed_rel_pos.reshape_as(attn_weights)\n+            attn_weights = attn_weights + decomposed_rel_pos\n \n         attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n \n@@ -909,47 +908,19 @@ class SamVisionSdpaAttention(SamVisionAttention):\n     def __init__(self, config, window_size):\n         super().__init__(config, window_size)\n \n-    def add_decomposed_rel_pos(\n-        self,\n-        query: torch.Tensor,\n-        rel_pos_h: torch.Tensor,\n-        rel_pos_w: torch.Tensor,\n-        q_size: Tuple[int, int],\n-        k_size: Tuple[int, int],\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n-        https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n-        This method is reimplemented to follow the implementation in:\n-        https://github.com/pytorch-labs/segment-anything-fast/blob/main/segment_anything_fast/modeling/image_encoder.py   # noqa B950\n-        This implementation is more memory efficient when using SDPA in the forward method.\n-        Args:\n-            q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n-            rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n-            rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n-            q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n-            k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n-\n-        Returns:\n-            attn (Tensor): attention map with added relative positional embeddings.\n-        \"\"\"\n-        query_height, query_width = q_size\n-        key_height, key_width = k_size\n-        relative_position_height = self.get_rel_pos(query_height, key_height, rel_pos_h)\n-        relative_position_width = self.get_rel_pos(query_width, key_width, rel_pos_w)\n-\n-        batch_size, _, dim = query.shape\n-        reshaped_query = query.reshape(batch_size, query_height, query_width, dim)\n-        rel_h = torch.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n-        rel_w = torch.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n-        rel_h = rel_h.unsqueeze(-1)\n-        rel_w = rel_w.unsqueeze(-2)\n-        rel_h = rel_h.reshape(batch_size, query_height * query_width, key_height, 1)\n-        rel_w = rel_w.reshape(batch_size, query_height * query_width, 1, key_width)\n-\n-        return rel_h, rel_w\n-\n     def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n+        if output_attentions:\n+            logger.warning_once(\n+                \"`SamVisionSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                output_attentions=output_attentions,\n+            )\n+\n         batch_size, height, width, _ = hidden_states.shape\n         # qkv with shape (3, B, nHead, H * W, C)\n         qkv = (\n@@ -960,25 +931,21 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n         # q, k, v with shape (B * nHead, H * W, C)\n         query, key, value = qkv.reshape(3, batch_size * self.num_attention_heads, height * width, -1).unbind(0)\n \n-        rel_h, rel_w = None, None\n+        attn_bias = None\n         if self.use_rel_pos:\n-            rel_h, rel_w = self.add_decomposed_rel_pos(\n+            decomposed_rel_pos = self.get_decomposed_rel_pos(\n                 query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n             )\n+            decomposed_rel_pos = decomposed_rel_pos.reshape(\n+                batch_size, self.num_attention_heads, height * width, height * width\n+            )\n+            attn_bias = decomposed_rel_pos\n \n         query = query.view(batch_size, self.num_attention_heads, height * width, -1)\n         key = key.view(batch_size, self.num_attention_heads, height * width, -1)\n         value = value.view(batch_size, self.num_attention_heads, height * width, -1)\n \n-        if self.use_rel_pos:\n-            rel_h = rel_h.view(batch_size, self.num_attention_heads, rel_h.size(1), rel_h.size(2), rel_h.size(3))\n-            rel_w = rel_w.view(batch_size, self.num_attention_heads, rel_w.size(1), rel_w.size(2), rel_w.size(3))\n-            attn_bias = (rel_h + rel_w).view(\n-                batch_size, self.num_attention_heads, rel_h.size(2), rel_h.size(3) * rel_w.size(4)\n-            )\n-            attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=attn_bias)\n-        else:\n-            attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value)\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=attn_bias)\n \n         attn_output = (\n             attn_output.view(batch_size, self.num_attention_heads, height, width, -1)\n@@ -988,17 +955,7 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n \n         attn_output = self.proj(attn_output)\n \n-        if output_attentions:\n-            # For output_attentions, calculate the attention weights\n-            attn_weights = (query @ key.transpose(-2, -1)) * self.scale\n-            if attn_bias is not None:\n-                attn_weights = attn_weights + attn_bias\n-            attn_weights = F.softmax(attn_weights, dim=-1)\n-            outputs = (attn_output, attn_weights)\n-        else:\n-            outputs = (attn_output, None)\n-\n-        return outputs\n+        return attn_output, None\n \n \n SAM_VISION_ATTENTION_CLASSES = {"
        },
        {
            "sha": "29a2335cb12ec88bc94d3675fb68863e9540aa43",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
            "patch": "@@ -982,9 +982,8 @@ def get_rel_pos(self, q_size: int, k_size: int, rel_pos: tf.Tensor) -> tf.Tensor\n \n         return tf.gather(rel_pos_resized, tf.cast(relative_coords, tf.int32))\n \n-    def add_decomposed_rel_pos(\n+    def get_decomposed_rel_pos(\n         self,\n-        attn: tf.Tensor,\n         query: tf.Tensor,\n         rel_pos_h: tf.Tensor,\n         rel_pos_w: tf.Tensor,\n@@ -996,8 +995,6 @@ def add_decomposed_rel_pos(\n         https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n \n         Args:\n-            attn (`tf.Tensor`):\n-                attention map.\n             query (`tf.Tensor`):\n                 query q in the attention layer with shape (batch_size, query_height * query_width, channel).\n             rel_pos_h (`tf.Tensor`):\n@@ -1010,8 +1007,8 @@ def add_decomposed_rel_pos(\n                 spatial sequence size of key k with (key_height, key_width).\n \n         Returns:\n-            attn (`tf.Tensor`):\n-                attention map with added relative positional embeddings.\n+            decomposed_rel_pos (`torch.Tensor`):\n+                decomposed relative position embeddings.\n         \"\"\"\n         query_height, query_width = q_size\n         key_height, key_width = k_size\n@@ -1022,10 +1019,12 @@ def add_decomposed_rel_pos(\n         reshaped_query = tf.reshape(query, (batch_size, query_height, query_width, dim))\n         rel_h = tf.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n         rel_w = tf.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n-        attn = tf.reshape(attn, (batch_size, query_height, query_width, key_height, key_width))\n-        attn = attn + tf.expand_dims(rel_h, axis=-1) + tf.expand_dims(rel_w, axis=-2)\n-        attn = tf.reshape(attn, (batch_size, query_height * query_width, key_height * key_width))\n-        return attn\n+\n+        rel_h = tf.expand_dims(rel_h, axis=-1)\n+        rel_w = tf.expand_dims(rel_w, axis=-2)\n+        decomposed_rel_pos = rel_h + rel_w\n+\n+        return decomposed_rel_pos\n \n     def call(self, hidden_states: tf.Tensor, output_attentions=False, training=False) -> tf.Tensor:\n         batch_size, height, width, _ = shape_list(hidden_states)\n@@ -1039,9 +1038,11 @@ def call(self, hidden_states: tf.Tensor, output_attentions=False, training=False\n         attn_weights = tf.matmul(query * self.scale, key, transpose_b=True)\n \n         if self.use_rel_pos:\n-            attn_weights = self.add_decomposed_rel_pos(\n-                attn_weights, query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            decomposed_rel_pos = self.get_decomposed_rel_pos(\n+                query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n             )\n+            decomposed_rel_pos = tf.reshape(decomposed_rel_pos, shape_list(attn_weights))\n+            attn_weights = attn_weights + decomposed_rel_pos\n \n         attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n "
        },
        {
            "sha": "c85abdc739e85a905a5f86889cc15e26b4b0124d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
            "patch": "@@ -979,7 +979,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                     kwarg_value = kwargs.get(modality_key, \"__empty__\")\n                 else:\n                     kwarg_value = \"__empty__\"\n-                if kwarg_value != \"__empty__\":\n+                if not isinstance(kwarg_value, str) or kwarg_value != \"__empty__\":\n                     output_kwargs[modality][modality_key] = kwarg_value\n                     used_keys.add(modality_key)\n "
        },
        {
            "sha": "d621f5428733c4fdbbfbdc0a375a46b68f885ebf",
            "filename": "tests/models/sam/test_processor_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c53d53da89c0617f7dd5a69a2a08e6b1232b35fd/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py?ref=c53d53da89c0617f7dd5a69a2a08e6b1232b35fd",
            "patch": "@@ -312,7 +312,7 @@ def test_rle_encoding(self):\n         # This is shape (1, 2, 2).\n         # Flattened in Fortran order -> [0, 1, 1, 1].\n         # The RLE for [0,1,1,1] is [1, 3].\n-        input_mask = tf.tensor([[[0, 1], [1, 1]]], dtype=tf.int64)\n+        input_mask = tf.constant([[[0, 1], [1, 1]]], dtype=tf.int64)\n         rle = _mask_to_rle_tf(input_mask)\n \n         self.assertEqual(len(rle), 1)"
        }
    ],
    "stats": {
        "total": 165,
        "additions": 62,
        "deletions": 103
    }
}