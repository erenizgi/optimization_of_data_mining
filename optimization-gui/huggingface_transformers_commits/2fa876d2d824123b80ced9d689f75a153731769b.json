{
    "author": "faaany",
    "message": "[tests] make cuda-only tests device-agnostic (#35607)\n\n* intial commit\n\n* remove unrelated files\n\n* further remove\n\n* Update test_trainer.py\n\n* fix style",
    "sha": "2fa876d2d824123b80ced9d689f75a153731769b",
    "files": [
        {
            "sha": "f5af373f49bc0a4a090e4bf058a5b08a1d98277a",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -32,7 +32,6 @@\n     require_accelerate,\n     require_fsdp,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     require_torch_multi_accelerator,\n     slow,\n     torch_device,\n@@ -288,7 +287,7 @@ def test_training_and_can_resume_normally(self, state_dict_type):\n \n     @require_torch_multi_accelerator\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_fsdp\n     def test_fsdp_cpu_offloading(self):\n         try:"
        },
        {
            "sha": "7499a5599b7c4b89879d29b8e5d4bacf4f3a11dd",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -33,6 +33,7 @@\n     require_flash_attn,\n     require_optimum_quanto,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n@@ -2043,7 +2044,7 @@ def test_generate_with_quant_cache(self):\n                 model.generate(**generation_kwargs, **inputs_dict)\n \n     @pytest.mark.generate\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_generate_compile_model_forward(self):\n         \"\"\"\n@@ -3791,10 +3792,12 @@ def test_assisted_decoding_in_different_gpu(self):\n         self.assertTrue(input_length <= out.shape[-1] <= input_length + 20)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_assisted_decoding_model_in_gpu_assistant_in_cpu(self):\n         # PT-only test: TF doesn't support assisted decoding yet.\n-        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\"cuda\")\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n+            torch_device\n+        )\n         assistant = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\").to(\n             \"cpu\"\n         )"
        },
        {
            "sha": "5e18b006a5d815bdc58cb47a848dfccb13dc6745",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -27,6 +27,7 @@\n from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_fp16,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n@@ -1565,7 +1566,7 @@ def test_forward_signature(self):\n             self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_model_from_pretrained(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n         model = Blip2TextModelWithProjection.from_pretrained(model_name)\n@@ -2191,7 +2192,7 @@ def test_expansion_in_processing(self):\n \n         self.assertTrue(generated_text_expanded == generated_text)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_inference_itm(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n         processor = Blip2Processor.from_pretrained(model_name)\n@@ -2210,7 +2211,7 @@ def test_inference_itm(self):\n         self.assertTrue(torch.allclose(torch.nn.Softmax()(out_itm[0].cpu()), expected_scores, rtol=1e-3, atol=1e-3))\n         self.assertTrue(torch.allclose(out[0].cpu(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torch_fp16\n     def test_inference_itm_fp16(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n@@ -2232,7 +2233,7 @@ def test_inference_itm_fp16(self):\n         )\n         self.assertTrue(torch.allclose(out[0].cpu().float(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torch_fp16\n     def test_inference_vision_with_projection_fp16(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n@@ -2256,7 +2257,7 @@ def test_inference_vision_with_projection_fp16(self):\n         ]\n         self.assertTrue(np.allclose(out.image_embeds[0][0][:6].tolist(), expected_image_embeds, atol=1e-3))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torch_fp16\n     def test_inference_text_with_projection_fp16(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\""
        },
        {
            "sha": "64dfb5b6495511fa937999cee1f0a2a94866e23e",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -676,7 +676,7 @@ def test_eager_matches_sdpa_generate(self):\n                 )\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class DiffLlamaIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -689,7 +689,7 @@ def setUpClass(cls):\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2"
        },
        {
            "sha": "eb1205db9cc122167965c98570da07fa612394e2",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -23,7 +23,7 @@\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_torch_multi_gpu,\n     slow,\n     torch_device,\n@@ -426,7 +426,7 @@ def recursive_check(tuple_object, dict_object):\n \n \n @require_torch\n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class FalconMambaIntegrationTests(unittest.TestCase):\n     def setUp(self):"
        },
        {
            "sha": "0444ad14f26976bd8fe527c41b1af8a4734bbde7",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -22,7 +22,7 @@\n from parameterized import parameterized\n \n from transformers import FuyuConfig, is_torch_available, is_vision_available\n-from transformers.testing_utils import require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device\n from transformers.utils import cached_property\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -327,7 +327,7 @@ def test_model_parallelism(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class FuyuModelIntegrationTest(unittest.TestCase):\n     @cached_property\n     def default_processor(self):"
        },
        {
            "sha": "664616306d88fcaffce509d97ed8ef464ff3c456",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -26,7 +26,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -541,7 +540,7 @@ def _reinitialize_config(base_config, new_kwargs):\n             config = _reinitialize_config(base_config, {\"rope_scaling\": {\"rope_type\": \"linear\"}})  # missing \"factor\"\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class LlamaIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -695,7 +694,7 @@ def test_model_7b_dola_generation(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2"
        },
        {
            "sha": "70de4d9cf1edf06c42203436e1e15b14f2f7fa96",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -424,7 +424,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"Mistral flash attention does not support right padding\")\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class MistralIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations"
        },
        {
            "sha": "cf192b8bd79ec1fe7f3cd56b960bf9dc0d1046bd",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -22,6 +22,7 @@\n from transformers.testing_utils import (\n     require_flash_attn,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -471,7 +472,7 @@ def setUpClass(cls):\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_logits(self):\n         model_id = \"hf-internal-testing/Mixtral-tiny\"\n         dummy_input = torch.LongTensor([[0, 1, 0], [0, 1, 0]]).to(torch_device)\n@@ -507,7 +508,7 @@ def test_small_model_logits(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_logits_batched(self):\n         model_id = \"hf-internal-testing/Mixtral-tiny\"\n         dummy_input = torch.LongTensor([[0, 0, 0, 0, 0, 0, 1, 2, 3], [1, 1, 2, 3, 4, 5, 6, 7, 8]]).to(torch_device)"
        },
        {
            "sha": "249706c1c4705f83b044dad688cf090fc3d0d482",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -26,6 +26,7 @@\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_sdpa,\n     slow,\n@@ -103,7 +104,7 @@ def test_model_outputs_equivalence(self, **kwargs):\n         pass\n \n     @require_torch_sdpa\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_sdpa_equivalence(self):\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "75c0e6f1c78d58fb3acec8c1ac555ef01601fe2c",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -26,7 +26,7 @@\n from transformers.testing_utils import (\n     require_timm,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -865,7 +865,7 @@ def test_inference_object_detection_head_batched(self):\n         ]\n         self.assertListEqual([result[\"classes\"] for result in results], expected_classes)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         processor = self.default_processor\n         image = prepare_img()\n@@ -878,8 +878,8 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n             cpu_outputs = model(**encoding)\n \n         # 2. run model on GPU\n-        model.to(\"cuda\")\n-        encoding = encoding.to(\"cuda\")\n+        model.to(torch_device)\n+        encoding = encoding.to(torch_device)\n         with torch.no_grad():\n             gpu_outputs = model(**encoding)\n "
        },
        {
            "sha": "368e2dd140f38cfcf7841cbdcaf7740ccbccd8a9",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -28,7 +28,13 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import cached_property\n \n from ...test_configuration_common import ConfigTester\n@@ -631,7 +637,7 @@ def test_initialization(self):\n         self.assertTrue(not failed_cases, message)\n \n     @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_inference_with_different_dtypes(self, torch_dtype_str):\n         torch_dtype = {\n@@ -653,7 +659,7 @@ def test_inference_with_different_dtypes(self, torch_dtype_str):\n                 _ = model(**self._prepare_for_class(inputs_dict, model_class))\n \n     @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_inference_equivalence_for_static_and_dynamic_anchors(self, torch_dtype_str):\n         torch_dtype = {"
        },
        {
            "sha": "d6993469e043aa6c653e892578c41c22d4a5d404",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -23,6 +23,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -412,7 +413,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class Starcoder2IntegrationTest(unittest.TestCase):\n     def test_starcoder2_batched_generation_sdpa(self):\n         EXPECTED_TEXT = ["
        },
        {
            "sha": "52fec78d1e893849a822f9e95386669f91b1dce4",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -27,7 +27,7 @@\n     require_sentencepiece,\n     require_tokenizers,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -1646,7 +1646,7 @@ def test_contrastive_search_t5(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_compile_static_cache(self):\n         NUM_TOKENS_TO_GENERATE = 40\n         EXPECTED_TEXT_COMPLETION = [\n@@ -1686,7 +1686,7 @@ def test_compile_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_compile_static_cache_encoder(self):\n         prompts = [\n             \"summarize: Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial \""
        },
        {
            "sha": "7504ae009d05178ab9680497c2c87c1d37439716",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -28,7 +28,6 @@\n     require_tf,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     require_torch_or_tf,\n     torch_device,\n )\n@@ -553,7 +552,7 @@ def run_pipeline_test(self, text_generator, _):\n \n     @require_torch\n     @require_accelerate\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_small_model_pt_bloom_accelerate(self):\n         import torch\n "
        },
        {
            "sha": "2022c33665763646471d8b6484b2e2605585c425",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -21,6 +21,7 @@\n     require_accelerate,\n     require_optimum_quanto,\n     require_read_token,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -123,7 +124,7 @@ def test_conversion_with_modules_to_not_convert(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_optimum_quanto\n @require_accelerate\n class QuantoQuantizationTest(unittest.TestCase):\n@@ -268,7 +269,7 @@ def test_compare_with_quanto(self):\n         quantize(model.transformer, weights=w_mapping[self.weights])\n         freeze(model.transformer)\n         self.check_same_model(model, self.quantized_model)\n-        self.check_inference_correctness(model, device=\"cuda\")\n+        self.check_inference_correctness(model, device=torch_device)\n \n     @unittest.skip\n     def test_load_from_quanto_saved(self):"
        },
        {
            "sha": "0d12bf77d861fee1ef8484badcbb1dfa06a50e16",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -1862,7 +1862,6 @@ def test_resize_position_vector_embeddings(self):\n     def test_resize_tokens_embeddings(self):\n         if not self.test_resize_embeddings:\n             self.skipTest(reason=\"test_resize_embeddings is set to `False`\")\n-\n         (\n             original_config,\n             inputs_dict,\n@@ -2017,7 +2016,7 @@ def test_resize_tokens_embeddings(self):\n             torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n \n     @require_deepspeed\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_resize_tokens_embeddings_with_deepspeed(self):\n         ds_config = {\n             \"zero_optimization\": {\n@@ -2123,7 +2122,7 @@ def test_resize_embeddings_untied(self):\n                 model(**self._prepare_for_class(inputs_dict, model_class))\n \n     @require_deepspeed\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_resize_embeddings_untied_with_deepspeed(self):\n         ds_config = {\n             \"zero_optimization\": {\n@@ -3202,7 +3201,7 @@ def check_device_map_is_respected(self, model, device_map):\n \n     @require_accelerate\n     @mark.accelerate_tests\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_disk_offload_bin(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -3243,7 +3242,7 @@ def test_disk_offload_bin(self):\n \n     @require_accelerate\n     @mark.accelerate_tests\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_disk_offload_safetensors(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -3278,7 +3277,7 @@ def test_disk_offload_safetensors(self):\n \n     @require_accelerate\n     @mark.accelerate_tests\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_cpu_offload(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -4746,7 +4745,7 @@ def test_custom_4d_attention_mask(self):\n             torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-4)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_torch_compile_for_training(self):\n         if version.parse(torch.__version__) < version.parse(\"2.3\"):\n             self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")"
        },
        {
            "sha": "6e90b3d7e4059d05a11b5662c60c97502149c210",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fa876d2d824123b80ced9d689f75a153731769b/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=2fa876d2d824123b80ced9d689f75a153731769b",
            "patch": "@@ -1831,7 +1831,7 @@ def test_adalomo(self):\n         _ = trainer.train()\n \n     @require_grokadamw\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_grokadamw(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -1852,7 +1852,7 @@ def test_grokadamw(self):\n         _ = trainer.train()\n \n     @require_schedulefree\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_schedulefree_adam(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)"
        }
    ],
    "stats": {
        "total": 104,
        "additions": 57,
        "deletions": 47
    }
}