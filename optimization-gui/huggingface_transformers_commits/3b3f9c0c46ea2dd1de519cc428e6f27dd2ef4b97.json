{
    "author": "rev2607",
    "message": "fix(voxtral): correct typo in apply_transcription_request (#39572)\n\n* fix(voxtral): correct typo in apply_transcription_request\n\n* temporary wrapper: apply_transcrition_request\n\n* Update processing_voxtral.py\n\n* style: sort imports in processing_voxtral.py\n\n* docs(voxtral): fix typo in voxtral.md\n\n* make style\n\n* doc update\n\n---------\n\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>",
    "sha": "3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97",
    "files": [
        {
            "sha": "ad15631a96049feefa3733e97df536772bf9d0dd",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 60,
            "deletions": 9,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97",
            "patch": "@@ -37,7 +37,11 @@ Voxtral builds on Ministral-3B by adding audio processing capabilities:\n \n ## Usage\n \n-Let's first load the model!\n+### Audio Instruct Mode\n+\n+The model supports audio-text instructions, including multi-turn and multi-audio interactions, all processed in batches.\n+\n+➡️ audio + text instruction\n ```python\n from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n@@ -47,14 +51,7 @@ repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n-```\n-\n-### Audio Instruct Mode\n \n-The model supports audio-text instructions, including multi-turn and multi-audio interactions, all processed in batches.\n-\n-➡️ audio + text instruction\n-```python\n conversation = [\n     {\n         \"role\": \"user\",\n@@ -82,6 +79,15 @@ print(\"=\" * 80)\n \n ➡️ multi-audio + text instruction \n ```python\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n conversation = [\n     {\n         \"role\": \"user\",\n@@ -113,6 +119,15 @@ print(\"=\" * 80)\n \n ➡️ multi-turn:\n ```python\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n conversation = [\n     {\n         \"role\": \"user\",\n@@ -158,6 +173,15 @@ print(\"=\" * 80)\n \n ➡️ text only:\n ```python\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n conversation = [\n     {\n         \"role\": \"user\",\n@@ -184,6 +208,15 @@ print(\"=\" * 80)\n \n ➡️ audio only:\n ```python\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n conversation = [\n     {\n         \"role\": \"user\",\n@@ -210,6 +243,15 @@ print(\"=\" * 80)\n \n ➡️ batched inference!\n ```python\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n conversations = [\n     [\n         {\n@@ -262,7 +304,16 @@ for decoded_output in decoded_outputs:\n Use the model to transcribe audio (supports English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian)!\n \n ```python\n-inputs = processor.apply_transcrition_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\")\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n+inputs = processor.apply_transcription_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\", model_id=repo_id)\n inputs = inputs.to(device, dtype=torch.bfloat16)\n \n outputs = model.generate(**inputs, max_new_tokens=500)"
        },
        {
            "sha": "598529bf5c2fcf1b394910fdef682e141b10fc50",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \n import io\n+import warnings\n from typing import Optional, Union\n \n from ...utils import is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n@@ -242,7 +243,7 @@ def __call__(\n         the text. Please refer to the docstring of the above methods for more information.\n         This methods does not support audio. To prepare the audio, please use:\n         1. `apply_chat_template` [`~VoxtralProcessor.apply_chat_template`] method.\n-        2. `apply_transcrition_request` [`~VoxtralProcessor.apply_transcrition_request`] method.\n+        2. `apply_transcription_request` [`~VoxtralProcessor.apply_transcription_request`] method.\n \n         Args:\n             text (`str`, `list[str]`, `list[list[str]]`):\n@@ -284,7 +285,7 @@ def __call__(\n         return BatchFeature(data=out, tensor_type=common_kwargs.pop(\"return_tensors\", None))\n \n     # TODO: @eustlb, this should be moved to mistral_common + testing\n-    def apply_transcrition_request(\n+    def apply_transcription_request(\n         self,\n         language: Union[str, list[str]],\n         audio: Union[str, list[str], AudioInput],\n@@ -306,7 +307,7 @@ def apply_transcrition_request(\n         language = \"en\"\n         audio = \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\"\n \n-        inputs = processor.apply_transcrition_request(language=language, audio=audio, model_id=model_id)\n+        inputs = processor.apply_transcription_request(language=language, audio=audio, model_id=model_id)\n         ```\n \n         Args:\n@@ -431,6 +432,17 @@ def apply_transcrition_request(\n \n         return texts\n \n+    # Deprecated typo'd method for backward compatibility\n+    def apply_transcrition_request(self, *args, **kwargs):\n+        \"\"\"\n+        Deprecated typo'd method. Use `apply_transcription_request` instead.\n+        \"\"\"\n+        warnings.warn(\n+            \"`apply_transcrition_request` is deprecated due to a typo and will be removed in a future release. Please use `apply_transcription_request` instead.\",\n+            FutureWarning,\n+        )\n+        return self.apply_transcription_request(*args, **kwargs)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to MistralCommonTokenizer's [`~MistralCommonTokenizer.batch_decode`]. Please"
        },
        {
            "sha": "5b7ffcd802f209321089a74afcc9e06880b59c78",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97",
            "patch": "@@ -493,7 +493,7 @@ def test_transcribe_mode_audio_input(self):\n         model = VoxtralForConditionalGeneration.from_pretrained(\n             self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n         )\n-        inputs = self.processor.apply_transcrition_request(\n+        inputs = self.processor.apply_transcription_request(\n             language=\"en\",\n             audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n             model_id=self.checkpoint_name,"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 76,
        "deletions": 13
    }
}