{
    "author": "yijun-lee",
    "message": "ğŸŒ [i18n-KO] Translated `biogpt.md` to Korean (#33773)\n\n* docs: ko: biogpt.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: manual edits\r\n\r\n* fix: resolve suggestion\r\n\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>\r\n\r\n---------\r\n\r\nCo-authored-by: Chulhwa (Evan) Han <cjfghk5697@ajou.ac.kr>",
    "sha": "5809b43a62093d8cc799448af2adff27896d8c3b",
    "files": [
        {
            "sha": "06482e37c61c5e33cee01e1c42b11c2b98d0d51a",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5809b43a62093d8cc799448af2adff27896d8c3b/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5809b43a62093d8cc799448af2adff27896d8c3b/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=5809b43a62093d8cc799448af2adff27896d8c3b",
            "patch": "@@ -334,8 +334,8 @@\n         title: (ë²ˆì—­ì¤‘) BigBird\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) BigBirdPegasus\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) BioGpt\n+      - local: model_doc/biogpt\n+        title: BioGpt\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Blenderbot\n       - local: in_translation"
        },
        {
            "sha": "4b24af8493b9d5dc025416476cb75876c91e5249",
            "filename": "docs/source/ko/model_doc/biogpt.md",
            "status": "added",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/5809b43a62093d8cc799448af2adff27896d8c3b/docs%2Fsource%2Fko%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5809b43a62093d8cc799448af2adff27896d8c3b/docs%2Fsource%2Fko%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbiogpt.md?ref=5809b43a62093d8cc799448af2adff27896d8c3b",
            "patch": "@@ -0,0 +1,109 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# BioGPT [[biogpt]]\n+\n+## ê°œìš” [[overview]]\n+\n+BioGPTëŠ” Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liuì— ì˜í•´ [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) ì—ì„œ ì œì•ˆëœ ëª¨ë¸ì…ë‹ˆë‹¤. BioGPTëŠ” ìƒë¬¼ì˜í•™ í…ìŠ¤íŠ¸ ìƒì„±ê³¼ ë§ˆì´ë‹ì„ ìœ„í•´ ë„ë©”ì¸ì— íŠ¹í™”ëœ ìƒì„±í˜• ì‚¬ì „ í•™ìŠµ íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. BioGPTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë”°ë¥´ë©°, 1,500ë§Œ ê°œì˜ PubMed ì´ˆë¡ì„ ì´ìš©í•´ ì²˜ìŒë¶€í„° í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ìƒë¬¼ì˜í•™ ë¶„ì•¼ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì€ ì¼ë°˜ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„ ì ì  ë” ë§ì€ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ ì–¸ì–´ ë¶„ì•¼ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì˜ ë‘ ê°€ì§€ ì£¼ìš” ê³„í†µì¸ BERT(ë° ê·¸ ë³€í˜•)ì™€ GPT(ë° ê·¸ ë³€í˜•) ì¤‘ ì²« ë²ˆì§¸ëŠ” ìƒë¬¼ì˜í•™ ë¶„ì•¼ì—ì„œ BioBERTì™€ PubMedBERTì™€ ê°™ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì—°êµ¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ë‹¤ì–‘í•œ ë¶„ë¥˜ ê¸°ë°˜ì˜ ìƒë¬¼ì˜í•™ ì‘ì—…ì—ì„œ í° ì„±ê³µì„ ê±°ë‘ì—ˆì§€ë§Œ, ìƒì„± ëŠ¥ë ¥ì˜ ë¶€ì¡±ì€ ê·¸ë“¤ì˜ ì ìš© ë²”ìœ„ë¥¼ ì œí•œí–ˆìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€ê·œëª¨ ìƒë¬¼ì˜í•™ ë¬¸í—Œì„ ì‚¬ì „ í•™ìŠµí•œ ë„ë©”ì¸ íŠ¹í™” ìƒì„±í˜• íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸ì¸ BioGPTë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” 6ê°œì˜ ìƒë¬¼ì˜í•™ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ BioGPTë¥¼ í‰ê°€í•œ ê²°ê³¼, ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì—ì„œ ì´ì „ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ, BC5CDR, KD-DTI, DDI ì—”ë“œ-íˆ¬-ì—”ë“œ ê´€ê³„ ì¶”ì¶œ ì‘ì—…ì—ì„œ ê°ê° 44.98%, 38.42%, 40.76%ì˜ F1 ì ìˆ˜ë¥¼ ê¸°ë¡í•˜ì˜€ìœ¼ë©°, PubMedQAì—ì„œ 78.2%ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í•´ ìƒˆë¡œìš´ ê¸°ë¡ì„ ì„¸ì› ìŠµë‹ˆë‹¤. ë˜í•œ í…ìŠ¤íŠ¸ ìƒì„±ì— ëŒ€í•œ ì‚¬ë¡€ ì—°êµ¬ëŠ” ìƒë¬¼ì˜í•™ ìš©ì–´ì— ëŒ€í•œ ìœ ì°½í•œ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ìˆì–´ BioGPTì˜ ì¥ì ì„ ë”ìš± ì…ì¦í–ˆìŠµë‹ˆë‹¤.*\n+\n+ì´ ëª¨ë¸ì€ [kamalkraj](https://huggingface.co/kamalkraj)ì— ì˜í•´ ê¸°ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/microsoft/BioGPT)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ [[usage-tips]]\n+\n+- BioGPTëŠ” ì ˆëŒ€ì  ìœ„ì¹˜ ì„ë² ë”©(absolute position embedding)ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, ì…ë ¥ì„ ì™¼ìª½ì´ ì•„ë‹Œ ì˜¤ë¥¸ìª½ì—ì„œ íŒ¨ë”©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤.\n+- BioGPTëŠ” ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§(Casual Langague Modeling, CLM) ëª©í‘œë¡œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì—, ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ BioGPTëŠ” êµ¬ë¬¸ì ìœ¼ë¡œ ì¼ê´€ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ `run_generation.py`ì—ì„œ ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+- ì´ ëª¨ë¸ì€ `past_key_values`(PyTorch ìš©)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” ì´ì „ì— ê³„ì‚°ëœ í‚¤/ê°’ ì–´í…ì…˜ ìŒì…ë‹ˆë‹¤. ì´ ê°’ì„ ì‚¬ìš©í•˜ë©´ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì´ë¯¸ ê³„ì‚°ëœ ê°’ì„ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•Šë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. PyTorchì—ì„œ `past_key_values` ì¸ìˆ˜ëŠ” BioGptForCausalLM.forward() ë©”ì†Œë“œì—ì„œ ìì„¸íˆ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+\n+### Scaled Dot Product Attention(SDPA) ì‚¬ìš© [[using-scaled-dot-product-attention-sdpa]]\n+\n+PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ìŠ¤ì¼€ì¼ëœ ì ê³± ì–´í…ì…˜(SDPA) ì—°ì‚°ìë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ê³¼ ì‚¬ìš© ì¤‘ì¸ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì—¬ëŸ¬ êµ¬í˜„ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) ë˜ëŠ” [GPU ì¶”ë¡ ](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention) í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+`torch>=2.1.1`ì—ì„œ êµ¬í˜„ì´ ê°€ëŠ¥í•œ ê²½ìš° SDPAëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, `attn_implementation=\"sdpa\"`ë¥¼ `from_pretrained()`ì—ì„œ ì„¤ì •í•˜ì—¬ SDPA ì‚¬ìš©ì„ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```\n+from transformers import BioGptForCausalLM\n+model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+```\n+\n+NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, Ubuntu 20.04 í™˜ê²½ì—ì„œ `float16` ë° CausalLM í—¤ë“œê°€ ìˆëŠ” `microsoft/biogpt` ëª¨ë¸ë¡œ ë¡œì»¬ ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼, í›ˆë ¨ ì¤‘ ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n+\n+ìµœì ì˜ ì†ë„ í–¥ìƒì„ ìœ„í•´ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16` ë˜ëŠ” `torch.bfloat16`)ë¡œ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+\n+| num_training_steps | batch_size | seq_len | is cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | sdpa peak mem (MB) | Mem saving (%) |\n+|--------------------|------------|---------|---------|----------------------------|---------------------------|-------------|---------------------|--------------------|----------------|\n+| 100                | 1          | 128     | False   | 0.038                      | 0.031                     | 21.301      | 1601.862            | 1601.497           | 0.023          |\n+| 100                | 1          | 256     | False   | 0.039                      | 0.034                     | 15.084      | 1624.944            | 1625.296           | -0.022         |\n+| 100                | 2          | 128     | False   | 0.039                      | 0.033                     | 16.820      | 1624.567            | 1625.296           | -0.045         |\n+| 100                | 2          | 256     | False   | 0.065                      | 0.059                     | 10.255      | 1672.164            | 1672.164           | 0.000          |\n+| 100                | 4          | 128     | False   | 0.062                      | 0.058                     | 6.998       | 1671.435            | 1672.164           | -0.044         |\n+| 100                | 4          | 256     | False   | 0.113                      | 0.100                     | 13.316      | 2350.179            | 1848.435           | 27.144         |\n+| 100                | 8          | 128     | False   | 0.107                      | 0.098                     | 9.883       | 2098.521            | 1848.435           | 13.530         |\n+| 100                | 8          | 256     | False   | 0.222                      | 0.196                     | 13.413      | 3989.980            | 2986.492           | 33.601         |\n+\n+NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, Ubuntu 20.04 í™˜ê²½ì—ì„œ `float16` ë° AutoModel í—¤ë“œê°€ ìˆëŠ” `microsoft/biogpt` ëª¨ë¸ë¡œ ì¶”ë¡  ì¤‘ ë‹¤ìŒê³¼ ê°™ì€ ì†ë„ í–¥ìƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n+\n+| num_batches | batch_size | seq_len | is cuda | is half | use mask | Per token latency eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem eager (MB) | Mem BT (MB) | Mem saved (%) |\n+|-------------|------------|---------|---------|---------|----------|------------------------------|-----------------------------|-------------|----------------|--------------|---------------|\n+| 50          | 1          | 64      | True    | True    | True     | 0.115                        | 0.098                       | 17.392      | 716.998        | 716.998      | 0.000         |\n+| 50          | 1          | 128     | True    | True    | True     | 0.115                        | 0.093                       | 24.640      | 730.916        | 730.916      | 0.000         |\n+| 50          | 2          | 64      | True    | True    | True     | 0.114                        | 0.096                       | 19.204      | 730.900        | 730.900      | 0.000         |\n+| 50          | 2          | 128     | True    | True    | True     | 0.117                        | 0.095                       | 23.529      | 759.262        | 759.262      | 0.000         |\n+| 50          | 4          | 64      | True    | True    | True     | 0.113                        | 0.096                       | 18.325      | 759.229        | 759.229      | 0.000         |\n+| 50          | 4          | 128     | True    | True    | True     | 0.186                        | 0.178                       | 4.289       | 816.478        | 816.478      | 0.000         |\n+\n+\n+## ë¦¬ì†ŒìŠ¤ [[resources]]\n+\n+- [ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/language_modeling)\n+\n+## BioGptConfig [[transformers.BioGptConfig]]\n+\n+[[autodoc]] BioGptConfig\n+\n+\n+## BioGptTokenizer [[transformers.BioGptTokenizer]]\n+\n+[[autodoc]] BioGptTokenizer\n+    - save_vocabulary\n+\n+\n+## BioGptModel [[transformers.BioGptModel]]\n+\n+[[autodoc]] BioGptModel\n+    - forward\n+\n+\n+## BioGptForCausalLM [[transformers.BioGptForCausalLM]]\n+\n+[[autodoc]] BioGptForCausalLM\n+    - forward\n+\n+\n+## BioGptForTokenClassification [[transformers.BioGptForTokenClassification]]\n+\n+[[autodoc]] BioGptForTokenClassification\n+    - forward\n+\n+\n+## BioGptForSequenceClassification [[transformers.BioGptForSequenceClassification]]\n+\n+[[autodoc]] BioGptForSequenceClassification\n+    - forward\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 113,
        "additions": 111,
        "deletions": 2
    }
}