{
    "author": "MekkCyber",
    "message": "Refactor torchao docs  (#37490)\n\n* refactor docs\n\n* add serialization\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* reorder\n\n* add link\n\n* change automatic to autoquant\n\nCo-authored-by: DerekLiu35 <91234588+DerekLiu35@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* nits\n\n* refactor\n\n* add colab\n\n* update\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: DerekLiu35 <91234588+DerekLiu35@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "8669c016d26fdabd56271930a840f322dafece9a",
    "files": [
        {
            "sha": "8af09ce548b4c0d1923b99b4af0e81d9c462541b",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 357,
            "deletions": 96,
            "changes": 453,
            "blob_url": "https://github.com/huggingface/transformers/blob/8669c016d26fdabd56271930a840f322dafece9a/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8669c016d26fdabd56271930a840f322dafece9a/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=8669c016d26fdabd56271930a840f322dafece9a",
            "patch": "@@ -11,171 +11,324 @@ rendered properly in your Markdown viewer.\n \n # torchao\n \n+[![Open In Colab: Torchao Demo](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quantization/torchao.ipynb)\n+\n [torchao](https://github.com/pytorch/ao) is a PyTorch architecture optimization library with support for custom high performance data types, quantization, and sparsity. It is composable with native PyTorch features such as [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for even faster inference and training.\n \n-Install torchao with the following command.\n+See the table below for additional torchao features.\n+\n+| Feature | Description |\n+|--------|-------------|\n+| **Quantization Aware Training (QAT)** | Train quantized models with minimal accuracy loss (see [QAT README](https://github.com/pytorch/ao/blob/main/torchao/quantization/qat/README.md)) |\n+| **Float8 Training** | High-throughput training with float8 formats (see [torchtitan](https://github.com/pytorch/torchtitan/blob/main/docs/float8.md) and [Accelerate](https://huggingface.co/docs/accelerate/usage_guides/low_precision_training#configuring-torchao) docs) |\n+| **Sparsity Support** | Semi-structured (2:4) sparsity for faster inference (see [Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity](https://pytorch.org/blog/accelerating-neural-network-training/) blog post) |\n+| **Optimizer Quantization** | Reduce optimizer state memory with 4 and 8-bit variants of Adam |\n+| **KV Cache Quantization** | Enables long context inference with lower memory (see [KV Cache Quantization](https://github.com/pytorch/ao/blob/main/torchao/_models/llama/README.md)) |\n+| **Custom Kernels Support** | use your own `torch.compile` compatible ops |\n+| **FSDP2** | Composable with FSDP2 for training|\n+\n+> [!TIP]\n+> Refer to the torchao [README.md](https://github.com/pytorch/ao#torchao-pytorch-architecture-optimization) for more details about the library.\n+\n+\n+torchao supports the [quantization techniques](https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md) below.\n+\n+- A16W8 Int8 WeightOnly Quantization\n+- A16W4 WeightOnly Quantization\n+- A8W8 Int8 Dynamic Quantization\n+- A16W8 Float8 WeightOnly Quantization\n+- Autoquantization\n+\n+\n+Check the table below to see if your hardware is compatible.\n+\n+| Component | Compatibility |\n+|----------|----------------|\n+| CUDA Versions | âœ… cu118, cu124, cu126, cu128 |\n+| CPU | âœ… change `device_map=\"cpu\"` (see examples below) |\n+\n+\n+\n+Install torchao from PyPi or the PyTorch index with the following commands.\n+\n+<hfoptions id=\"install torchao\">\n+<hfoption id=\"PyPi\">\n \n ```bash\n # Updating ðŸ¤— Transformers to the latest version, as the example script below uses the new auto compilation\n-pip install --upgrade torch torchao transformers\n+# Stable release from Pypi which will default to CUDA 12.4\n+pip install --upgrade torchao transformers\n ```\n+</hfoption> \n+<hfoption id=\"PyTorch Index\">\n+Stable Release from the PyTorch index\n+```bash\n+pip install torchao --extra-index-url https://download.pytorch.org/whl/cu124 # options are cpu/cu118/cu124/cu126\n+```\n+</hfoption>\n+</hfoptions>\n \n-torchao supports many quantization types for different data types (int4, float8, weight only, etc.).\n-Starting with version 0.10.0, torchao provides enhanced flexibility through the `AOBaseConfig` API, allowing for more customized quantization configurations.\n-And full access to the techniques offered in the torchao library.\n+If your torcha version is below 0.10.0, you need to upgrade it, please refer to the [deprecation notice](#deprecation-notice) for more details.\n+\n+## Quantization examples\n+\n+TorchAO provides a variety of quantization configurations. Each configuration can be further customized with parameters such as `group_size`, `scheme`, and `layout` to optimize for specific hardware and model architectures.\n+\n+For a complete list of available configurations, see the [quantization API documentation](https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py).\n \n You can manually choose the quantization types and settings or automatically select the quantization types.\n \n-<hfoptions id=\"torchao\">\n-<hfoption id=\"manual\">\n+Create a [`TorchAoConfig`] and specify the quantization type and `group_size` of the weights to quantize (for int8 weight only and int4 weight only). Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method.\n \n+<hfoptions id=\"examples\">\n+<hfoption id=\"int8-weight-only cuda\">\n \n-Create a [`TorchAoConfig`] and specify the quantization type and `group_size` of the weights to quantize. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method.\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int8WeightOnlyConfig\n \n-> [!TIP]\n-> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`. This is only available in torchao 0.8.0+.\n+quant_config = Int8WeightOnlyConfig(group_size=128)\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-In torchao 0.10.0+, you can use the more flexible `AOBaseConfig` approach instead of string identifiers:\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+\n+<hfoption id=\"int8-weight-only cpu\">\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int8WeightOnlyConfig\n+\n+quant_config = Int8WeightOnlyConfig(group_size=128)\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"cpu\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+<hfoption id=\"int4-weight-only cuda\">\n \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n from torchao.quantization import Int4WeightOnlyConfig\n \n-# Using AOBaseConfig instance (torchao >= 0.10.0)\n quant_config = Int4WeightOnlyConfig(group_size=128)\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Meta-Llama-3-8B\",\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n     torch_dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n \n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+</hfoption>\n \n-## Available Quantization Schemes\n-\n-TorchAO provides a variety of quantization configurations:\n+<hfoption id=\"int4-weight-only cpu\">\n \n-- `Int4WeightOnlyConfig`\n-- `Int8WeightOnlyConfig`\n-- `Int8DynamicActivationInt8WeightConfig`\n-- `Float8WeightOnlyConfig`\n+> [!TIP]\n+> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`.\n \n-Each configuration can be further customized with parameters such as `group_size`, `scheme`, and `layout` to optimize for specific hardware and model architectures.\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.dtypes import Int4CPULayout\n \n-For a complete list of available configurations, see our [quantization API documentation](https://github.com/pytorch/ao/blob/main/torchao/quantization/quant_api.py).\n+quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout())\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n \n-> **âš ï¸ DEPRECATION WARNING**\n->\n-> Starting with version 0.10.0, the string-based API for quantization configuration (e.g., `TorchAoConfig(\"int4_weight_only\", group_size=128)`) is **deprecated** and will be removed in a future release.\n->\n-> Please use the new `AOBaseConfig`-based approach instead:\n->\n-> ```python\n-> # Old way (deprecated)\n-> quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n->\n-> # New way (recommended)\n-> from torchao.quantization import Int4WeightOnlyConfig\n-> quant_config = Int4WeightOnlyConfig(group_size=128)\n-> quantization_config = TorchAoConfig(quant_type=quant_config)\n-> ```\n->\n-> The new API offers greater flexibility, better type safety, and access to the full range of features available in torchao.\n->\n-> ## Migration Guide\n->\n-> Here's how to migrate from common string identifiers to their `AOBaseConfig` equivalents:\n->\n-> | Old String API | New `AOBaseConfig` API |\n-> |----------------|------------------------|\n-> | `\"int4_weight_only\"` | `Int4WeightOnlyConfig()` |\n-> | `\"int8_weight_only\"` | `Int8WeightOnlyConfig()` |\n-> | `\"int8_dynamic_activation_int8_weight\"` | `Int8DynamicActivationInt8WeightConfig()` |\n->\n-> All configuration objects accept parameters for customization (e.g., `group_size`, `scheme`, `layout`).\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"cpu\",\n+    quantization_config=quantization_config\n+)\n \n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\")\n \n-Below is the API for for torchao < `0.9.0`\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+<hfoption id=\"int8-dynamic-quantization cuda\">\n \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n \n-quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+quant_config = Int8DynamicActivationInt8WeightConfig()\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Meta-Llama-3-8B\",\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n     torch_dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n \n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n+</hfoption>\n+<hfoption id=\"int8-dynamic-quantization cpu\">\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n+\n+quant_config = Int8DynamicActivationInt8WeightConfig()\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"cpu\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\")\n \n-Run the code below to benchmark the quantized models performance.\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+</hfoption>\n+<hfoption id=\"float8-weight-only cuda\">\n \n ```py\n-from torch._inductor.utils import do_bench_using_profiling\n-from typing import Callable\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Float8WeightOnlyConfig\n \n-def benchmark_fn(func: Callable, *args, **kwargs) -> float:\n-    \"\"\"Thin wrapper around do_bench_using_profiling\"\"\"\n-    no_args = lambda: func(*args, **kwargs)\n-    time = do_bench_using_profiling(no_args)\n-    return time * 1e3\n+quant_config = Float8WeightOnlyConfig()\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n \n-MAX_NEW_TOKENS = 1000\n-print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n-bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n-output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n-print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n ```\n+</hfoption>\n+<hfoption id=\"float8-weight-only cpu\">\n \n-> [!TIP]\n-> For best performance, you can use recommended settings by calling `torchao.quantization.utils.recommended_inductor_config_setter()`\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Float8WeightOnlyConfig\n+\n+quant_config = Float8WeightOnlyConfig()\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"cpu\",\n+    quantization_config=quantization_config\n+)\n \n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n </hfoption>\n-<hfoption id=\"automatic\">\n \n-The [autoquant](https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) API automatically chooses a quantization type for quantizable layers (`nn.Linear`) by micro-benchmarking on input type and shape and compiling a single linear layer.\n+</hfoptions>\n+\n+### Autoquant\n+\n+If you want to automatically choose a quantization type for quantizable layers (`nn.Linear`) you can use the [autoquant](https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) API.\n+\n+The `autoquant` API automatically chooses a quantization type by micro-benchmarking on input type and shape and compiling a single linear layer.\n \n Create a [`TorchAoConfig`] and set to `\"autoquant\"`. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method. Finally, call `finalize_autoquant` on the quantized model to finalize the quantization and log the input shapes.\n \n-> [!TIP]\n-> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`. This is only available in torchao 0.8.0+.\n \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n \n quantization_config = TorchAoConfig(\"autoquant\", min_sqnr=None)\n quantized_model = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Meta-Llama-3-8B\",\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n     torch_dtype=\"auto\",\n     device_map=\"auto\",\n     quantization_config=quantization_config\n )\n \n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n@@ -186,7 +339,127 @@ quantized_model.finalize_autoquant()\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n-Run the code below to benchmark the quantized models performance.\n+\n+## Serialization\n+\n+torchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor) for maximum flexibility in supporting new quantized torch.Tensor formats. [Safetensors](https://huggingface.co/docs/safetensors/en/index) serialization and deserialization does not work with torchao.\n+\n+To avoid arbitrary user code execution, torchao sets `weights_only=True` in [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to ensure only tensors are loaded. Any known user functions can be whitelisted with [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals).\n+\n+```py\n+# don't serialize model with Safetensors\n+output_dir = \"llama3-8b-int4wo-128\"\n+quantized_model.save_pretrained(\"llama3-8b-int4wo-128\", safe_serialization=False)\n+```\n+\n+## Loading quantized models\n+\n+Loading a quantized model depends on the quantization scheme. For quantization schemes, like int8 and float8, you can quantize the model on any device and also load it on any device. The example below demonstrates quantizing a model on the CPU and then loading it on CUDA.\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int8WeightOnlyConfig\n+\n+quant_config = Int8WeightOnlyConfig(group_size=128)\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"cpu\",\n+    quantization_config=quantization_config\n+)\n+# save the quantized model\n+output_dir = \"llama-3.1-8b-torchao-int8-cuda\"\n+quantized_model.save_pretrained(output_dir, safe_serialization=False)\n+\n+# reload the quantized model\n+reloaded_model = AutoModelForCausalLM.from_pretrained(\n+    output_dir, \n+    device_map=\"auto\", \n+    torch_dtype=torch.bfloat16\n+)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+output = reloaded_model.generate(**input_ids, max_new_tokens=10)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+```\n+For int4, the model can only be loaded on the same device it was quantized on because the layout is specific to the device. The example below demonstrates quantizing and loading a model on the CPU.\n+\n+```py\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n+from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.dtypes import Int4CPULayout\n+\n+quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout())\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+\n+# Load and quantize the model\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.1-8B-Instruct\",\n+    torch_dtype=\"auto\",\n+    device_map=\"cpu\",\n+    quantization_config=quantization_config\n+)\n+# save the quantized model\n+output_dir = \"llama-3.1-8b-torchao-int4-cpu\"\n+quantized_model.save_pretrained(output_dir, safe_serialization=False)\n+\n+# reload the quantized model\n+reloaded_model = AutoModelForCausalLM.from_pretrained(\n+    output_dir, \n+    device_map=\"cpu\", \n+    torch_dtype=torch.bfloat16\n+)\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n+input_text = \"What are we having for dinner?\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+\n+output = reloaded_model.generate(**input_ids, max_new_tokens=10)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+```\n+\n+## âš ï¸ Deprecation Notice\n+\n+> Starting with version 0.10.0, the string-based API for quantization configuration (e.g., `TorchAoConfig(\"int4_weight_only\", group_size=128)`) is **deprecated** and will be removed in a future release.\n+>\n+> Please use the new `AOBaseConfig`-based approach instead:\n+>\n+> ```python\n+> # Old way (deprecated)\n+> quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+>\n+> # New way (recommended)\n+> from torchao.quantization import Int4WeightOnlyConfig\n+> quant_config = Int4WeightOnlyConfig(group_size=128)\n+> quantization_config = TorchAoConfig(quant_type=quant_config)\n+> ```\n+>\n+> The new API offers greater flexibility, better type safety, and access to the full range of features available in torchao.\n+>\n+> [Migration Guide](#migration-guide)\n+>\n+> Here's how to migrate from common string identifiers to their `AOBaseConfig` equivalents:\n+>\n+> | Old String API | New `AOBaseConfig` API |\n+> |----------------|------------------------|\n+> | `\"int4_weight_only\"` | `Int4WeightOnlyConfig()` |\n+> | `\"int8_weight_only\"` | `Int8WeightOnlyConfig()` |\n+> | `\"int8_dynamic_activation_int8_weight\"` | `Int8DynamicActivationInt8WeightConfig()` |\n+>\n+> All configuration objects accept parameters for customization (e.g., `group_size`, `scheme`, `layout`).\n+\n+\n+\n+## Resources\n+\n+For a better sense of expected performance, view the [benchmarks](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks) for various models with CUDA and XPU backends. You can also run the code below to benchmark a model yourself.\n \n ```py\n from torch._inductor.utils import do_bench_using_profiling\n@@ -199,30 +472,18 @@ def benchmark_fn(func: Callable, *args, **kwargs) -> float:\n     return time * 1e3\n \n MAX_NEW_TOKENS = 1000\n-print(\"autoquantized model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n+print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n \n bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n ```\n \n-</hfoption>\n-</hfoptions>\n-\n-## Serialization\n-\n-torchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor) for maximum flexibility in supporting new quantized torch.Tensor formats. [Safetensors](https://huggingface.co/docs/safetensors/en/index) serialization and deserialization does not work with torchao.\n-\n-To avoid arbitrary user code execution, torchao sets `weights_only=True` in [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to ensure only tensors are loaded. Any known user functions can be whitelisted with [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals).\n-\n-```py\n-# don't serialize model with Safetensors\n-output_dir = \"llama3-8b-int4wo-128\"\n-quantized_model.save_pretrained(\"llama3-8b-int4wo-128\", safe_serialization=False)\n-```\n+> [!TIP]\n+> For best performance, you can use recommended settings by calling `torchao.quantization.utils.recommended_inductor_config_setter()`\n \n-## Resources\n+Refer to [Other Available Quantization Techniques](https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques) for more examples and documentation.\n \n-For a better sense of expected performance, view the [benchmarks](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks) for various models with CUDA and XPU backends.\n+## Issues\n \n-Refer to [Other Available Quantization Techniques](https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques) for more examples and documentation.\n+If you encounter any issues with the Transformers integration, please open an issue on the [Transformers](https://github.com/huggingface/transformers/issues) repository. For issues directly related to torchao, please open an issue on the [torchao](https://github.com/pytorch/ao/issues) repository.\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 453,
        "additions": 357,
        "deletions": 96
    }
}